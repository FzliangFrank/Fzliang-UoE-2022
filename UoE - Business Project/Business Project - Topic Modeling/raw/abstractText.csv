,id,title,abstractText,techAbstractText,potentialImpact
0,EF01F1FE-28B2-4D9D-90E0-522062E1E39D,Machine-Understandable Auctions,"Context:In the future many services currently performed by humans will be performed by software programs on the Internet. These programs are what we call agents. A good example is a travel agent. This could be a computer program that asks you where you want to go and how much money you can afford to spend. The agent could then automatically search Internet sites and book your flights, hotel and entertainment. Another example is a trading agent; it could buy and sell shares automatically for you. The problem is: would you trust it to spend your money? Many other agents on the Internet might try to cheat it and sell it overpriced shares, for example. One solution to this problem is to have rules for the agents. An auction is a good example; in an auction there are rules that say who is allowed to bid and who should win at the end. A well designed auction can make sure that every agent gets a fair deal. But how can the agent know if an auction is well designed? The agent would have to be able to look at the rules of the auction and check that they are fair. At the moment there is no way that an agent could do this automatically.Aims and Objectives:This project will try to give agents an automatic way to find out if the rules of an auction guarantee a fair deal. The first step is to write down the rules of an auction in a computer language. Next, the rules need to be checked to see what the agents are allowed to do in the auction. There are automatic checking programs that can be adapted for this purpose. The problem with these checking programs is that they would take a very long time to check a complicated auction. The project will try to find methods which can be used to simpilfy this problem, so that it can be done in a reasonable amount of time. We also want to find out how simple an auction needs to be in order for the checking to work. Finally we will write a piece of software which can automate the whole auction checking process.Potential Applications and Benefits:If agents have an automatic way to check that an auction is fair, then they can roam the internet and join in any auctions they like. An agent can come to a new trading site, look at the rules that site is using, and check that they are fair. If you are the owner of this type of agent, then you might have more trust in the agent and allow it to spend your money. You would have a guarantee that it would only join in an auction if the rules of the auction ensured that it is fair. If many more people start trusting agents to spend their money then a lot more commerce could be done electronically. This would make commerce much more efficient and ultimately save a lot of money.",,
1,A829AEF9-7EE4-4E75-A82E-96D6986AC296,"Automating Simulation Output Analysis (AutoSimOA):Selection of Warm-up, Replications and Run-Length","Simulation models are used in many organisations for planning and better managing organisational systems e.g. manufacturing plant or service operations. A key part of the process of developing and using a simulation model is to experiment with the model. In order to obtain accurate measures of a model's performance care must be taken to obtain sufficient good data from the model. Particular issues are removing initialisation bias, running the model for long enough and performing sufficient replications (runs with different streams of random numbers). Decisions regarding these issues require statistical skills which many simulation modellers do not possess. As a result, many simulation models may be used poorly and incorrect conclusions reached. This research aims to develop an 'analyser' that will automatically analyse the output from a simulation model and advise the simulation modeller on an appropriate warm-up period, run-length and number of replications. In the first stage of the research existing methods for analysing simulation output will be tested to identify candidate methods for inclusion in the analyser. Candidate methods will then be adapted where necessary to make them suitable for automation. In the final stage of the research a prototype analyser will be developed and tested. The methods and analyser will be tested on example data, using real simulation models and with simulation users.",,
2,ECCA920F-7D7D-48AA-A23C-591D230CD6AD,Feasibility Study: Integrating Games-Based Learning and Computational Modelling to Control MRSA.,"MRSA is a 'super-bug' that is difficult to control using antibiotics, and is most common in hospitals. Around 1 in 10 people admitted to hospital will contract an infection during their stay. You can become infected through physical contact with either another infected person or a surface like a door handle already touched by an infected person. If you are really ill, as many people are in hospital, the infection can kill you. There are ways to limit the spread of MRSA in hospitals, including healthcare workers following strict hygiene measures such as washing their hands between patients, isolating infected patients and even closing down hospital wards where infections occur. The problem is that each option can cost a lot of money which could otherwise be used to treat patients and nobody knows the best way to use these different options together to manage an outbreak of MRSA.Researchers have turned to computer modelling to help them understand how to manage the complicated range of factors involved in spreading the infection. The models show that the pattern of movements of healthcare workers among patients is really important and if the activities of healthcare workers are managed properly then spread can be limited. Also crucial is the degree to which healthcare workers follow the hygiene measures. Effective training of healthcare workers on the importance of following the hygiene rules can also limit spread. Most important of all, the models show that each individual person involved can make a big difference to the occurrence and spread of the disease. To decide how to manage the spread of infection in a particular ward, you need to know about the ward layout and the people that work and are being treated in that ward. Unfortunately, none of the models cater for differences in the behaviour of individual healthcare workers and the health of individual patients. Also, these models do not represent the layout of the ward, and how healthcare workers move around in the ward itself. We have developed a computer model that takes into account both the layout and the individuals in a hospital ward. We will add data about healthcare worker behaviour, healthcare activities among patients and individual patient health from a special study ward in a hospital into the model. We will also include data from different studies on the different ways to manage the spread of MRSA. We will combine our MRSA spread model with an existing training tool to teach healthcare workers about the importance of following the hygiene rules. We will clearly demonstrate to them how many patients one careless person can infect, and how careful they need to be to help reduce infections. This training tool uses a computer game approach to provide an interesting way of teaching.Since the computer model predicts realistic outcomes, it can also be used by managers to choose the best method of containing an ongoing outbreak. We will use artificially intelligent search techniques to identify the best way of combining different approaches to contain the spread. The same approach can also be used by managers to identify ways of reducing infections in the first place, and to plan ahead by getting the computer to simulate different possible scenarios and to identify ways of dealing with them. The system that we are aiming to build will help hospitals manage the spread of MRSA in a cost-effective way. It will show hospital managers possible ways of limiting spread in their hospital, and teach healthcare workers about the difference they can make in reducing the chance of an outbreak.",,
3,A7DCF5FE-8648-4182-9D46-C2D6350FF288,Novel Approaches to Radiotherapy Planning and Scheduling in the NHS,"The problem of efficient radiotherapy planning and resource management In oncology departments, In terms of both manpower and the availability of equipment, has been recognised as a key to their smooth running. The various activities, starting from patient referral through to the delivery of the appropriate treatment, form a complex system, for which generating a high quality planning and scheduling solution is a challenging real-world problem that significantly impacts on healthcare staff and patients.This ambitious research proposal concerns both the generation of possible radiotherapy treatments, for patients and the scheduling of resources. This is a joint research proposal between two research groups from the University of Nottingham and Coventry University with expertise from differing but complementary disciplines. Two large hospitals, Nottingham City Hospital and the UHCW NHS Trust in Coventry, which are both providing radiotherapy treatment to a large population throughout their respective regions, are acting as collaborators on the project. They will provide real-world data and expertise in the domain of radiotherapy treatment. The proposed research requires a multidisciplinary effort, aiming at combining different operational research and artificial intelligence disciplines within a complex real-world medical environment.A successful outcome to the proposed research would significantly improve the efficiency and the quality of radiotherapy treatment in the UK. It could lead to a reduction of waiting time and waiting lists for treatments, a reduction of stress levels in patients and improved consistency in terms of dose delivery. Most Important of all, it has a definite potential to increase the survival rate of cancer patients",,
4,FB0D07AC-1AD1-4DFA-8CC4-F257CC0B45D6,The White Rose Grid e-Science Centre,"This proposal is in support of the continuation of funding to the White Rose Grid (WRG) e-Science Centre of Excellence, and is submitted by the WRG Executive who manages and drives its activities and developments. The White Rose Grid is a unique metropolitan grid which offers HPC and grid services based on the integrated computational and data resources of the three Universities - Leeds, Sheffield and York. The WRG was established with, and is continually being enhanced by a large investment (5M) from the three institutions and the Regional Development Agency - Yorkshire Forward. Its facilities underpin a number of WR initiatives, such as the White Rose Research Triangle, and there are sound business reasons to operate and expand the WRG. The WRG e-Science Centre will continue to bring together e-Science activities of the three Universities, and to link the WRG with the initiatives of the UK e-Science Core Programme, and with international grid research activities. In addition to our e-Research specialised topics (i.e. the three core areas) our unique strengths are in: * Proven ability to collaborate effectively at every level: scientifically (e.g. the three Universities' researchers contribute to the DAME project), at the WRG Executive level (all three Universities' members participate), at technical level (e.g. the three Computing Services liaise closely), and at grid-users' level (common conferences and workshops for the three Universities). By working together we are able to employ complementary skills bases, knowledge and the international expertise of the three Universities to undertake a wide range of projects, tackling a large variety of issues. * Successfully supporting production grids: we constructed and now support the WRG as well as host and operate a core node of the National Grid Service.* Industrial outreach: our recent outreach project funded by Yorkshire Forward assessed regional and national interest in grid technologies (e.g. through two large conferences for industry covering grid topics), and developed a business model for a grid-based service. * Professional user support and grid training: it includes courses and talks on Java COG kit, Condor, Introduction to the National Grid Service, etc.* Working closely within a large community of application scientists: it includes engineers, biological scientists, physical scientists, and social scientists.* Productive engagement with international grid activities: close collaboration with the Chinese grid community through academic visits between Leeds and Beihang University in China; deployment of Chinese grid middleware - CROWN over the WRG and the Chinese grid; and being an active member of the WUN through visits among the partners and the development of the WUN Grid.",,
5,F8261491-63D6-4C08-8BBC-12F7EFDDA867,Semantic Media - Pervasive Annotation for e-Research,"The aim of the project is to investigate and innovate at the intersection of the Semantic Grid and the physical world, by focusing on the capture, distribution and use of semantic annotation in the context of pervasive devices. It addresses important computer science challenges that have arisen in e-Science projects by focusing on the future forms of scientific record that may emerge from the use of a pervasive e-Science infrastructure. The formation of this new form of scientific record raised research challenges in three distinct areas:Challenge 1: Recording the record requires a disparate set of information to be captured and related to each other. We need to significantly reduce the cost of capturing this additional information and develop appropriate representations for codifying this information. This will require:1. The development of devices and techniques that map from real world actions and activities to higher level semantic representations to provide real time information streams representing the ubiquitous computing environment. 2. The investigation of semantic annotation techniques to information streams drawn from real world sensors for use in e-Research applications using multiple ontologiesChallenge 2: Sharing the record requires a careful re-examination of the relationship between the underlying architecture and the semantic annotations used to form the record of activities. Sharing this record across a distributed community requires:3. The development of techniques to allow the distribution and synchronisation of multiple information sources across the infrastructure in a scalable and reliable manner.4. To create a distributed annotation architecture that move beyond existing centralised services such as RDF triplestores to address the engineering challenges inherent in meeting the requirements of continuous or real-time media across a distributed infrastructure.Challenge 3: Replaying the Record requires us to consider how best to represent the record to scientists and how this might best be understood by scientists. This will require. 5. The development of interfaces and devices that present the record to scientists through a range of different interface devices including mobile and ubiquitous computing devices.6. The development of techniques to support multiple perspectives on the underlying annotations to allow the record to be presented in an appropriate manner to those who would seek to replay it.",,
6,51E1392B-51AA-4B4E-AF6D-A02EABF3521D,An Infrastructure for Adaptive System Development,"Adaptive systems can modify their own behaviour at runtime. They have the potential to:1. Improve performance or cost, by tailoring the configuration of a system at runtime so as to match the varying requirements of the system to the changing pool of resources that may be available to support those requirements.2. Reduce the costs of installing, configuring and maintaining systems, by enabling systems to adjust their own configuration characteristics, based on the quality of service required, the actual rather than predicted load on the system, and the actual rather than predicted behaviour of the application in a given environment.Developing adaptive systems is generally a non-trivial task, with a need to design mechanisms for:(i) monitoring the application and the environment(ii) assessing the appropriateness of the current configuration to identify when adaptation may be required(iii) identifying which form of adaptation is likely to be most suitable in a given context(iv) carrying out the adaptation while the system is runningTherefore, the development of effective adaptive systems can be seen as being both important and difficult; the current state of the art tends to involve bespoke designs for the adaptivity of specific systems in specific contexts.As a result, this project will investigate how the development of adaptive systems can be made more systematic and cost-effective, in particular through the design and development of declarative representations of adaptive behaviour, and the automatic generation of implementations from those representations.In order to achieve this, the project will:1) Design generic but extensible interfaces that support the monitoring of distributed components, the assessment of the evolving runtime behaviour of these components, and the selection of responses that are deemed likely to be beneficial.2) Design or adopt languages that enable declarative description of the functionalities of the monitoring, assessment and response components.3) Design higher-level descriptions of adaptive behaviour that can be mapped to the components from 1 and the languages from 2 for implementation.4) Implement the functionalities of 1,2 and 3 in a service-based architecture.5) Evaluate the monitoring-assessment-response framework and the higher-level descriptions through their deployment to support adaptive query processing and adaptive workflow enactment.The project thus explores two principal design activities - the construction of the monitoring-assessment-response framework, and the the development of higher-level descriptions of adaptive behaviour that can be mapped into that framework.The resulting framework will be evaluated in terms of efficiency of construction and effectiveness in use by applying it to two forms of distributed computation that are key to e-science : Distributed Query Processing and Workflow.",,
7,DD9D1294-BDDC-4EAD-B46F-0F607D3E817B,"NETWORK: Interdisciplinary Cutting, Packing and Space Allocation","This proposal seeks the funds to establish a UK network in the scientifically and industrially challenging area of cutting, packing and space allocation. It aims to provide a new national foundation and infrastructure to support major interdisciplinary and inter-institutional approaches for the modelling, optimisation, analysis, implementation and understanding of cutting, packing and space allocation methodologies throughout the industrial, commercial and service sectors so as to underpin decision support system development in these sectors.One of the main aims of the network is establish a dialogue between academia and the industrial sector in order to inform the service sectors of the work that is being carried out within academia and bring to their attention the opportunities that exist for them to work with academic researchers (for example, Knowledge Transfer Partnerships, CASE awards etc.) Of course, we will also encourage inter-institutional, interdisciplinary blue skies, adventerous research and this network will enable researchers to collaborate and develop their research ideas.",,
8,8E9C6D92-18A8-48FC-AD5A-3109D6F328F4,e-Science Centre Call : Belfast e-Science Centre,"The extension of the World Wide Web to include not only textual information (as it is at present) but also a wide range of computational resources such as supercomputers, databases etc. is the main objective of the work of this proposal. This requires providing new software and facilities to enable access to these computational facilities in a transparent, secure and easy to use fashion.",,
9,F2273F10-4742-485D-8DC8-86683D63F1A7,Study of regularisation methods in machine learning,"Over the past decade the availability of powerful computers has opened the doors to the use of machine learning techniques in complex application domains such as those arising in computer vision, speech recognition, computational linguistics, marketing science, and bioinformatics, to mention but a few.A central approach in machine learning which has proved valuable in the above domains consists in computing a function from available data by minimising a regularisation error functional which balances different error/penalty criteria. For example, the regularisation error functional may involves the combination of a data term, measuring the empirical error on the data and a penalty term measuring the function complexity. The goal of this visit, during the period of January--June 2006, is to continue to explore both the theoretical and practical implications of the regularisation approach in machine learning as well as produce a first draft of a book on this topic. Prof. Micchelli shares a strong interest with Dr. Pontil in machine learning and the proposed visit will be the first opportunity for them to work together for an extensive period of time.Prof. Micchelli ranks high among the world leaders in computational mathematics. He has made fundamental contributions to that field, especially to problems concerning approximation, representation and estimation of functions. His work has been influential not only in mainstream mathematics but also in nearby fields, particularly in statistics and computer science. He is in the recent ISI list of 200 mathematicians world-wide who are most highly cited.",,
10,3DB23CA4-A004-4BE6-A965-978B275C8C2C,Exploratory Workshop on Cognitive Robotics and Control,"Cognitive robots - robots that may be said to think for themselves - appear in science fiction in the not too distant future. They replace humans in mundane chores (e.g. laundry), hazardous occupations (e.g. mine clearing) and caring (e.g. nursing the elderly). Considering that we do not fully understand the workings of our own mind and brain, how can we build such machines? The last few years of research into Robotics, Artificial Intelligence, Philosophy and Neuroscience have given insights into our ability to replicate human behaviour in artificial systems. Androids are appearing more lifelike, computer programs mine databases for new pharmaceutical drugs, we distinguish between cognition versus consciousness and imaging techniques are mapping the pathways in the brain. No longer are robots constructed preprogrammed or purely reactive to their environment. Robots can perform routine operations and react to disturbances in familiar situations. However, in complex tasks and varied environments they fail whilst humans succeed. A human's robust skills exhibit cognitive control where self-knowledge and self-adaptation in the underlying processes result in flexibility and adaptability to our environments. A 2 1/2 day workshop is proposed to examine cognitive robots and cognitive control. Now is the time for leading and future UK researchers to meet with researchers from the US and other EC countries to work on the principles underlying the next generation of robots that seek to possess cognition and cognitive control. This work may also help us understand how our own mind functions by providing artificial test subjects. This workshop will help place the UK in the centre of cognitive robotics research. It will facilitate international collaboration and set the agenda for research in this exciting field. Robots will enter our daily lives with increasing frequency so this research is vital if their potential is to be fully utilised in an acceptable manner.",,
11,F633B02B-5DE0-4ABF-8D6D-7AAF7143E4CE,Oxford Interdisciplinary e-Research Centre - building on e-Science,"The Oxford e-Science Centre helps researchers in science to do better, faster and different research using joined-up large-scale clusters of computers (the 'Grid') together with digital libraries, databases, and scientific software accessible from their own desktops. In the Oxford the e-Science Centre has been concerned to establish what researchers actually need from e-Science facilities and to help develop the applications to meet those requirements. Fundamentally, e-Science is about scientists working together even when located in different countries or within different types of organisation (universities or industry, for example). The Oxford e-Science Centre itself is a distributed, 'virtual' centre based within a number of departments including the University's computing services and its computer science department. As a result of the Centre's work the University now has an impressive range of e-Science projects with a particular emphasis on biomedical research. For example, the Integrative Biology Project is producing complex computational models, requiring very intensive use of the Grid, which to help study heart disease and cancer tumours. This funding proposal seeks to build on this success and ensure the University's other world-leading research activities are included, especially within the social sciences and humanities.The University is very supportive of e-Science (now often known as e-Research) and has invested 7.5 million in a purpose-built e-Science Laboratory which will include within it a new Interdisciplinary e-Research Centre (IeRC). It is through the IeRC that this funding proposal aims to help researchers in all subjects benefit from the exciting use of computer technologies which the e-Science programme at Oxford has developed.We will do this primarily by talking to researchers, learning about their requirements and keeping them informed about new technologies which others have found useful. We will also assist researchers to talk to each other through virtual collaboration tools (e.g. the AccessGrid ), to make better use of a national grid of computing clusters, and to encourage both researchers and other members of the University to dedicate some of their own desktop computing power to a University computer grid (known as a CampusGrid ). In effect, we are helping to build a virtual research environment at Oxford and to make sure uses, and is compatible with, virtual environments being developed nationally and across the globe.",,
12,489A9B3A-DE12-4495-8F74-71785527DA01,Supoprt for AISB'06: Adaptation in Artificial and Biological Systems,"This application seeks support for AISB'06, the annual convention of the Society for the Study of Artificial Intelligence and the Simulation of Behaviour (SSAISB), the UK's largest national society in the field of Artificial Intelligence. We seek a total of 13,656 consisting of 2,120 to cover expenses of invited plenary speakers, 10,000 to cover bursaries for postgraduate research students to attend the conference, and 1,536 for administrative costs incurred in allocating the bursaries, processing receipts and providing other administrative support for the convention (120 hours in total). This funding will ensure the impact of AISB'06 and thus help to maintain and enhance the leading position of the UK in Artificial Intelligence and related fields. EPSRC support of last year's convention was extremely important in making the event accessible to students, and will be again this year. A successful convention will be of particular benefit to PhD students who will have an opportunity to present their work to senior members of the community and receive feedback from them. Due to the breadth of the convention and the unprecedented number of symposia young researchers will be exposed to a wide range of areas.",,
13,9FB03289-84F3-4C0C-9496-E5DCF47900D8,Automated Reasoning in Large Structured Theories,"The main application domains of computer-based theorem proving are mathematical assistant systems, mathematical teaching assistants, and hardware and software verification in computer science. The formalisations of mathematical knowledge and software systems are typically organised in large structured theories and the conjectures of their properties need to be proved relative to some specific theory. In these domains human guidance of the automatic proof procedures is indispensable, even for theorems that are simple by human standards.Aside from providing guidance information about how to explore the search space, the human user has to select a relevant subset of the available axioms and lemmas which are provided to the theorem proving system and the performance of automatic proof procedures depends on the concise filtering of the available information. In case proof attempts fail, the user needs to either readjust the selected knowledge from the theories or he has to come up with a missing lemma which is not yet part of the knowledge contained in the theories. In these domains, a proof attempt consists not only of trying to prove a conjecture from a set of axioms, i.e. the classical theorem proving in-the-small , but takes place in a large, structured context of different theories. There has been intensive research and tool development for automated theorem proving, for representing and maintaining structured theories and for theory exploration to synthesise interesting properties of a new domain, but there has been little research about how to efficiently combine them to solve a given problem.In this project, we aim at the development of in-the-large reasoning procedures automating the combination of these three components in order to prove a given conjecture. To this end we will develop a set of criteria characterising the kind of information the components can provide: How can we filter relevant knowledge from the structure of the theories and the kind of knowledge contained therein; how can we analyse failed proof attempts in order to determine our needs for additional knowledge; how can we decide when additional theory exploration is necessary and how to combine theory exploration with structured theories. The developed criteria will then serve as a basis for the design and implement of in-the-large reasoning techniques.",,
14,77DB7358-BEE1-4598-AAF9-EFE349FA8A30,Classifiers Ensembles for Changing Environments,"Pattern Recognition deals with assigning class labels to objects. An object could be anything, e.g. a patient (to be diagnosed), a bank loan application (to be approved or rejected), a pronounced vowel (to be matched to one of a set of vowels), an e-mail message (to be marked as legitimate or spam), etc. In many domains, class descriptions change with time. Take for example the classification of e-mail messages into spam and legitimate. Spam e-mail was more easily recognisable in the past because of specific common words being used within. Now that e-mail filters can scan for such words and spot the spam easily, the spammers invented different ways to trick the filters, therefore the description of class spam has changed. User preferences also change with time. For example, suppose that the user subscribes to a newsletter announcing new software every month. Then software ads, considered junk mail in the past, will be now welcome, again changing the description of class spam. These changing environments are called in Machine Learning concept drift . A classifier is a function, algorithm or a device that can assign labels to objects. Classifier models have been developed for coping with concept drift. Recently, classifier ensembles have been employed for this task as well. Classifier ensembles (called also committees of classifiers) operate as a panel of experts. When the object comes to be labelled, each member of the ensemble assigns a class label to the object. The overall decision of the panel is derived on the basis of the individual opinions. One possible way for this is to take the majority vote. Classifier ensembles have been found to be more accurate than single classifiers for traditional (=static) classification problems. The hope is that they will be also better than single classifiers in the presence of concept drift. Current literature on classifier ensembles on changing environments can be streamlined into three strategies: (1) dynamic combiners (where only the way the individual opinions are combined changes with time), (2) updating of the ensemble members (assuming that each member of the ensemble is capable of dealing with concept drift individually, the way of combining the opinions may stay fixed; the changes will be taken care of by each member); (3) structural changes in the ensemble (here we may want to replace regularly the outdated members of the ensembles or the ones whose expertise is no longer adequate for the task). There are classifier ensembles in each of these categories proposed in the literature. We are planning to study these methods and propose new ones which will benefit from merging the three strategies. In order to compare the new methods with the most successful existing methods, we need a collection of data sets with different concept drifts. As there is no such benchmark collection (and an excellent collection is available for static environments, maintained at the University of California, Irvine, UCI), one of the objectives of this project is to build a suite of Matlab programmes which will generate data with various simulated types of concept drift. We will look into modifying benchmark data sets from UCI to simulate concept drift. We will also look for real data sets with true problem of concept drift, e.g., from robotics or tracking. Having put together a data set collection, we will code in Matlab a selected set of existing classification methods, both individual classifier and classifier ensembles for changing environment. The code will be provided for free download on the Internet and will be suitable for use as a Matlab toolbox. To design out toolbox we will draw upon the Matlab toolbox for pattern recognition, PRTools, developed at Delft University of Technology, The Netherlands. Apart from being included in the toolbox, the new methods will be reported in a publication submission.",,
15,66410DD3-207C-4B8B-BFAF-D13E9AC94078,UK kDD Symposium,"This proposal requests support, in the form of student bursaries, for the organisation and running of UKKDD'06 (the UK Knowledge Discovery in Data Symposium to be held in April 2006). The principal aim of the symposium is to progress the momentum established at the inaugural symposium held in April 2005. The establishment of a UK series of KDD symposium is seen as a new initiative whose objective is to bring together the various practitioners and research groups in the UK working both commercially or academical within discipline of KDD and its related enterprises. The symposium seeks to act as a show case for the current state of the art of UK KDD work, and to identify opportunities and establish partnerships to maintain the UK's standing within the international KDD community. Many of the leading UK experts on KDD will be presenting a collation of views and reports on current KDD topics.",,
16,D3ABFE24-BE61-47DE-B2C3-3DC46994DFF3,"Hyper-heuristics for Scheduling, Rostering and Routing: An International Collaboration","This proposal is applying for the resources to support a 3-month visiting fellowship (8 visits over a 5 year period) to give the Automated Scheduling, Optimisation and Planning group at the University of Nottingham the opportunity to undertake a sustained programme of research with Prof Michel Gendreau of Universite de Montreal in Canada. We are also requesting the resources for the Nottingham team to visit Prof Gendreau on a regular basis over the 5 year period. This exciting project will enable the ASAP group to draw upon Prof Gendreau's considerable expertise across a range of research directions in automated scheduling, rostering and routing. This project will provide particular value to EPSRC by enabling Prof Gendreau to contribute to several significant EPSRC projects that have already been funded. Particular research themes that Prof. Gendreau's visiting fellowship will contribute to are.1. Hyper-heuristics2. Fuzzy Reasoning and GRID scheduling3. Transport Scheduling4. Healthcare Personnel Rostering5. Path Planning6. Network RoutingHowever, this collaboration will certainly not be limited to these areas. Indeed, we see part of Prof Gendreau's role in this visiting fellowship as being that of an advisor to the group across all of our scheduling and optimisation research. The group currently has a portfolio of 25 externally funded research grants worth almost 4M. This includes two grants from BBSRC, three from the EU, two from East Midlands Development Agency/ HEFCE and the rest from EPSRC and industrial sources. Our research funding portfolio includes significant industrial support. We see Prof Gendreau's remit as a visiting fellow ranging over our entire portfolio. Of course, we do not know exactly what challenging research issues we will be facing in the years to come and we would like the flexibility to request Prof Gendreau's expertise as and when it is required.",,
17,FA6C0B2C-8CFD-4B42-B8D1-A5F2B37B9F29,Evolving and Generalising Very High Quality Control Knowledge for AI Planning,"This project is concerned with making AI planning practical by exploiting evolutionary learning techniques to acquire control knowledge automatically. This control knowledge can be used to prune useless branches of the search space and to propose promising branches. Whilst human-coded control rules have been shown to be useful in planning, their specification is an effort-intensive process which makes them difficult or impossible to generalise. Their use has tended to be confined to relatively simple domain models about which the human has a good understanding of the dynamics. We propose to learn powerful rules automatically from both static and dynamic sources of information about the planning domain and the process of planning within that domain. Furthermore, we will develop a method for learning generic rules that apply to classes of domains and that can be automatically customised for a particular domain. This reduces or even removes the burden on the human and results in scalable planning technology.",,
18,ECC5D4A4-0789-4F91-9D33-F8167D8BEC16,SEBASE: Software Engineering By Automated SEarch,"Current software engineering practice is a human-led search for solutions which meet needs and constraints under limited resources. Often there will be conflict, both between and within functional and non-functional criteria. Naturally, like other engineers, we search for a near optimal solution. As systems get bigger, more distributed, more dynamic and more critical, this labour-intensive search will hit fundamental limits. We will not be able to continue to develop, operate and maintain systems in the traditional way, without automating or partly automating the search for near optimal solutions. Automated search based solutions have a track record of success in other engineering disciplines, characterised by a large number of potential solutions, where there are many complex, competing and conflicting constraints and where construction of a perfect solution is either impossible or impractical. The SEMINAL network demonstrated that these techniques provide robust, cost-effective and high quality solutions for several problems in software engineering. Successes to date can be seen as strong pointers to search having great potential to serve as an overarching solution paradigm. The SEBASE project aims to provide a new approach to the way in which software engineering is understood and practised. It will move software engineering problems from human-based search to machine-based search. As a result, human effort will move up the abstraction chain, to focus on guiding the automated search, rather than performing it. This project will address key issues in software engineering, including scalability, robustness, reliability and stability. It will also study theoretical foundations of search algorithms and apply the insights gained to develop more effective and efficient search algorithms for large and complex software engineering problems. Such insights will have a major impact on the search algorithm community as well as the software engineering community.",,
19,F3FA2A8E-781B-433E-AE05-BE21E3469644,SEBASE: Software Engineering By Automated SEarch,"Current software engineering practice is a human-led search for solutions which meet needs and constraints under limited resources. Often there will be conflict, both between and within functional and non-functional criteria. Naturally, like other engineers, we search for a near optimal solution. As systems get bigger, more distributed, more dynamic and more critical, this labour-intensive search will hit fundamental limits. We will not be able to continue to develop, operate and maintain systems in the traditional way, without automating or partly automating the search for near optimal solutions. Automated search based solutions have a track record of success in other engineering disciplines, characterised by a large number of potential solutions, where there are many complex, competing and conflicting constraints and where construction of a perfect solution is either impossible or impractical. The SEMINAL network demonstrated that these techniques provide robust, cost-effective and high quality solutions for several problems in software engineering. Successes to date can be seen as strong pointers to search having great potential to serve as an overarching solution paradigm. The SEBASE project aims to provide a new approach to the way in which software engineering is understood and practised. It will move software engineering problems from human-based search to machine-based search. As a result, human effort will move up the abstraction chain, to focus on guiding the automated search, rather than performing it. This project will address key issues in software engineering, including scalability, robustness, reliability and stability. It will also study theoretical foundations of search algorithms and apply the insights gained to develop more effective and efficient search algorithms for large and complex software engineering problems. Such insights will have a major impact on the search algorithm community as well as the software engineering community.",,
20,C5260B86-A0E0-4689-B7B5-A46DEF39632E,SEBASE: Software Engineering By Automated SEarch,"Current software engineering practice is a human-led search for solutions which meet needs and constraints under limited resources. Often there will be conflict, both between and within functional and non-functional criteria. Naturally, like other engineers, we search for a near optimal solution. As systems get bigger, more distributed, more dynamic and more critical, this labour-intensive search will hit fundamental limits. We will not be able to continue to develop, operate and maintain systems in the traditional way, without automating or partly automating the search for near optimal solutions. Automated search based solutions have a track record of success in other engineering disciplines, characterised by a large number of potential solutions, where there are many complex, competing and conflicting constraints and where construction of a perfect solution is either impossible or impractical. The SEMINAL network demonstrated that these techniques provide robust, cost-effective and high quality solutions for several problems in software engineering. Successes to date can be seen as strong pointers to search having great potential to serve as an overarching solution paradigm. The SEBASE project aims to provide a new approach to the way in which software engineering is understood and practised. It will move software engineering problems from human-based search to machine-based search. As a result, human effort will move up the abstraction chain, to focus on guiding the automated search, rather than performing it. This project will address key issues in software engineering, including scalability, robustness, reliability and stability. It will also study theoretical foundations of search algorithms and apply the insights gained to develop more effective and efficient search algorithms for large and complex software engineering problems. Such insights will have a major impact on the search algorithm community as well as the software engineering community.",,
21,525693CF-D945-44CD-96DF-5E8C5F7AA707,"2nd International Workshop on Evolving Fuzzy Systems EFS'06, Ambelside, Lake District, 7-9 September 2006","The main challenge today is to design the next generation of intelligent systems that are able to have a higher level of flexibility and autonomy by using computational methods that borrow from Nature. A new area that is addressing these challenges is now emerging on the cross-roads of evolution-based and reasoning-based systems that are more flexible than existing adaptive systems.This emerging area of evolving fuzzy systems targets complex processes by developing novel learning methods and computationally efficient algorithms for real-time applications. In recognition of the growing importance of this area the IEEE Technical Committee on Fuzzy Systems within the Computational Intelligence Society decided in 2004 to set up a Task Force to which the Principal Investigator belongs. This Task Force had its First Workshop in Granada, Spain (17-19 March, 2005). A special issue on the same topic is now prepared by the IEEE Transactions on Fuzzy Systems. It was decided during the first Workshop to organize the second Workshop on Evolving Fuzzy Systems in Ambelside, UK. In this respect, the Workshop in the centre of this proposal is the next in the series organized by the IEEE Task Force and is also an attempt to broaden its appeal to a wider audience.This new emerging field is currently being shaped and the proposed Workshop will be an important event at which researchers from academia and industry active in this area will meet and discuss current achievements, problems and future directions. The event will take place at a moment in time when research on self-developing systems, that are capable of operating efficiently in unpredictable (possibly harsh) environment without human intervention, is intensively pursued world-wide and in relation to several important application areas including advanced manufacturing, aerospace, underwater exploration, defence, security and surveillance etc. Significant progress is mainly observed in the USA and the Far East (Japan, Taiwan, New Zealand). The proposed Workshop will help to ensure that the UK is recognized as one of the leading centres for research in this emerging area. In addition, the Workshop will be an excellent platform to launch the new Intelligent Systems Research Laboratory at Lancaster that includes a number of mobile robots. The Workshop is technically endorsed by such prestigious organizations as IEEE (Computational Intelligence Society and Systems, Man, and Cybernetics Society), IFSA (International Fuzzy Systems Association), EUSFLAT (EUropean Society on Fuzzy Systems And Technology) and will be supported by Lancaster University and the Department of Communication Systems.",,
22,8BBDDE76-A48E-4E74-8B1C-80B99A4A09FB,EPSRC Professorship - IRC Scoping Study,This proposal outlines the funding for Prof Ian Marshall to undertake a 9 month Director position to facilitate the preparation of an Autonomous Systems IRC.,,
23,5E68E657-03E6-4547-A8D5-CE0E40437AC8,iKAPP: an intelligent kite aerial photography platform,"This project aims to develop an intelligent platform for taking aerial digital photographs using a kite to lift the platform. Thisintelligent kite aerial photography platform (iKAPP) will use an onboard computer and two cameras with different resolutions (levels of detail). The platform will hang from the kite line below the kite and will have a stabilization system to control the direction that thehigher resolution camera points in. This stabilization will be controlled both by lightweight solid-state gyroscopes and by using the views of the two cameras to decide how the platform is moving, and how to compensate for the movement. This is difficult because the platform must be light enough to be lifted by the kite as well as have a fast enough computer to be able to process the images faster than the platform moves.The completes iKAPP and its software will be test flown beneath a kite and when it works properly it will be used to survey an archaeological site (probably an iron age hillfort). The images that the iKAPP captures will then be compared with images obtained using normal aerial photography techniques (using an aeroplane).If the iKAPP works properly it will be useful for lots of different surveying tasks including environmental surveys, archaeological surveys, crop monitoring and other recconaissance activities.",,
24,4BFB7FB1-2007-4C26-8E47-18CEC528A5F8,Two timescale immunological learning for idiotypic behaviour mediation,"Short-term learning can be defined as taking place over the lifetime of an individual, whereas long-term learning can be the net evolutionary result of knowledge accumulation. The integration of these approaches provides an advantage over the use of either one alone. Our immune system combines both effortlessly through the use of gene libraries and other evolutionary and immunological mechanisms. We aim to imitate this to derive useful rules to autonomously solve navigation / control problems. Without long-term learning, the controller initially lacks knowledge of the effectiveness of rules and therefore has no sense of which ones to use. The disadvantage of long-term learning in isolation is that it must be conducted offline in simulation, due to the time scales involved. The use of simulators can mean that results translate poorly to the physical world. Hence we provide feedback from real short-term experiments to validate the data. We propose here to investigate the relationship between short- and long -term learning by designing a robot controller that considers both learning cycles and allows their outputs to feed into each other. The chosen architecture is the creation of initial rules through an artificial gene library and their accelerated (simulated) evolution to establish a rule database, indexed by actuator function. A subset of the derived rules is transferred to a physical robot that continually adapts and re-selects them using the dynamics of an Artificial Immune System, with rules interconnected through an idiotypic network. The methodology thus provides a set of starting rules, a measure of their usefulness, a means for updating and thus improving the repertoire and a mechanism for adaptive rule selection.",,
0,A9A1BDD2-91E4-4D19-8CA6-2EFFDF2382FF,Learning the Structure of Music,"This project is aimed at the development of models and tools for the application of novel probabilistic machine learning techniques to the analysis of music. The underlying theme of the project is the learning of patterns linking different data arising simultaneously from the same piece of music. The sources of data will be as follows: a) musical scores (MIDI format), b) audio (recordings of the pieces), c) worm data (charting performance information), d) EEG data (of subjects listening to the music) and d) fMRI data (of subjects listening to the music).The linking patterns that we will be seeking involve pairs of data streams as follows: a) musical scores with worm data, b) musical scores with fMRI data and c) audio with EEG data. The first pair will be used to identify typical performance patterns of particular performers. The second and the third pairs will be used to identify the effects in the brain of particular musical patterns (such as melodic sequences, musical phrasings, harmonic progressions, etc.).The project will advance our understanding of the relationship between musical structure and performance and experience. The potential of such developments is quite wide ranging, with potential application in music therapy and entertainment. For example, it will contribute to the development of systems for artificial performance of music imitating the style of a performer on pieces that he or she may have never played before and systems for musical composition tailored to achieve specific effects (or moods) on the listener.",,
1,9893AF24-206F-4E17-B976-9070662711AD,The Cut Polytope and Related Convex Bodies,"Optimisation is concerned with methods for finding the 'best' option when one is faced with a huge range of possible options. More formally, it deals with maximising (or minimising) a function of some decision variables, subject to various constraints. The proposed project is concerned with an optimisation problem called the max-cut problem. It is concerned with partitioning a graph into two pieces so as to maximise the number of edges crossing from one piece to the other.As well as being an interesting problem in its own right, the max-cut problem also has many applications, for example in computer science, telecommunications, statistics, theoretical physics, computational biology, and branches of mathematics such as combinatorics, graph theory and the theory of metric spaces.The max-cut problem is difficult to solve both in theory and practice. Although there are good heuristic methods available which can obtain good quality solutions to large instances, the state-of-the-art exact methods are capable of solving instances with only up to around 100 vertices to proven optimality.The main goals of the proposed project are to deepen our understanding of the max-cut problem, to devise methods which are capable of solving larger instances to optimality, and to implement these methods as computer software. To do this, a theoretical study will have to be made of a certain geometric object known as the cut polytope, and certain related geometric objects.",,
2,C4BCC30D-8CE2-4A6D-B40D-8EC981BCF580,The National e-Science Centre 2006-2008,"The term e-Science has pervaded the workings of many research disciplines in the last few years. E-Science can be defined as the invention and application of computing methods to extend our capabilities in any research discipline. E-Science includes pioneering new methods for managing and exploiting the rapidly growing collections of data, scientific databases, the use of advanced computational models, the support of scientific collaboration, and the use of powerful computational infrastructures such as the Grid. The Universities of Edinburgh and Glasgow have been prominent in e-Science and were chosen to jointly host the National e-Science Centre , charged with developing, promoting and supporting e-Science nationwide. The NeSc provides national leadership and coordination, a focus for representing the UK e-Science programme internationally, regional outreach, and participation in developing the national infrastructure.The NeSC also augments (and leverages) many already strong assets within the Universities, including an extensive portfolio of e-Science projects, high-performance computing facilities, an ethos of engaging with a wide range of disciplines in Computer Science and a diverse range of curated research data and computationally intensive research applications. This proposal seeks the continuation of the NeSC for a further two year period following the expiry of the current grant in July 2006.",,
3,F05EA428-A32F-44C0-81AC-FE2B687039C8,Planning in Mixed Discrete-Continuous Domains,"The ability to plan intelligently is a fundamental component of autonomous behaviour. Many modern and future applications of AI, in the power industry, in planetary applications, in search and rescue applications and in domestic robot settings, require autonomous decision-making where a robot decides how best to behave to serve its purpose. This poses many challenges to the current state of the art. For example: existing artificial planning systems are unable to reason about complex continuous change, even though this occurs in most everyday situations; most planners are ill-equipped to make choices under uncertainty, except when this uncertainty can be precisely quantified (which is rare); the problems of plan generation and execution-time robustness of the resulting plan are often separated, but they must be considered in tandem in non-deterministic domains. We aim to widen the application of planning technology to problems that contain both planning and real time control elements. Many realistic problems of interest are of this type and this will open up avenues for exploitation of planners. We will push mixed discrete-continuous planning to the top of the international agenda by proposing to organise the 2008 planning competition around this theme.",,
4,C9FCDECD-8B15-4868-B310-EF802E7946CF,Workshop on Soft Methods in Probability and Statistics,"The SMPS 2006 workshop will provide a forum for research and discussion into the fusion of soft methods with probability and statistics, with the ultimate goal of integrated uncertainty modelling in complex systems involving human factors. In addition to probabilistic factors such as measurement error and other random effects, the modelling process often requires us to make qualitative and subject judgements that cannot easily be translated into precise probability values. Such judgements give rise to a number of different types of uncertainty including; fuzziness if they are based on linguistic information; epistemic uncertainty when their reliability is in question; ignorance when they are insufficient to identify or restrict key modelling parameters; imprecision when parameters and probability distributions can only be estimated within certain bounds. Statistical theory has not traditionally been concerned with modelling uncertainty arising in this manner but soft methods, a range of powerful techniques developed within AI, attempt to address those problems where the encoding of subjective information is unavoidable. These are mathematically sound uncertainty modelling methodologies which are complementary to conventional statistics and probability theory. Therefore, a more realistic modelling process providing decision makers with an accurate reflection of the true current state of our knowledge (and ignorance) requires an integrated framework incorporating both probability theory, statistics and soft methods. This fusion motivates innovative research at the interface between computer science (AI), mathematics and systems engineering. NOTE: In view of the fact that soft methods form part of the AI research area we feel that it would be most appropriate if this proposal were initially considered by the ICT panel",,
5,E1224A9D-76B3-43C7-82AE-704F3DA6C40B,Learning the Structure of Music,"This project is aimed at the development of models and tools for the application of novel probabilistic machine learning techniques to the analysis of music. The underlying theme of the project is the learning of patterns linking different data arising simultaneously from the same piece of music. The sources of data will be as follows: a) musical scores (MIDI format), b) audio (recordings of the pieces), c) worm data (charting performance information), d) EEG data (of subjects listening to the music) and d) fMRI data (of subjects listening to the music).The linking patterns that we will be seeking involve pairs of data streams as follows: a) musical scores with worm data, b) musical scores with fMRI data and c) audio with EEG data. The first pair will be used to identify typical performance patterns of particular performers. The second and the third pairs will be used to identify the effects in the brain of particular musical patterns (such as melodic sequences, musical phrasings, harmonic progressions, etc.).The project will advance our understanding of the relationship between musical structure and performance and experience. The potential of such developments is quite wide ranging, with potential application in music therapy and entertainment. For example, it will contribute to the development of systems for artificial performance of music imitating the style of a performer on pieces that he or she may have never played before and systems for musical composition tailored to achieve specific effects (or moods) on the listener.",,
6,6BFDABB8-109E-4BAE-9166-5776062534E4,Model Checking Agent Programming Languages,"Our overall goal in this project is to develop model checkingtechniques that can be applied to various agent-oriented programminglanguages. Building on our previous work on model checking forAgentSpeak, we will investigate the key aspects underlying not only AgentSpeak but other agent-oriented programming languages based on Java, such as 3APL, Concurrent MetateM, and Jadex. Based on that, wewill develop a Java infrastructure layer that is (a) relevant to theimplementation of these agent languages (b) has clear semantics, and(c) is able to be verified through an extended version of JPF, an open source Java model checker.As a result of this investigation, we will be able to develop bothformal semantics for these major constructs of agent programming aswell as Java libraries that implement them (based on the semantics)- the resulting intermediate agent language will be called the AgentInfrastructure Layer (AIL). As in our existing body of work, JPF is tobe used as the model checker with which to carry out formalverification of implemented agent-based systems. The idea is then toprovide automatic (and provably correct) translations from variousexisting agent-oriented programming languages.",,
7,E0A37EFD-7AB5-4DDB-8779-35077F83758C,Tenth European Conference on Logics in Artificial Intelligence,"This case for support requests funding to organise the Tenth EuropeanConference on Logics in Artificial Intelligence (JELIA-06) inLiverpool, UK. The purpose of the conference is to offer a bi-annualplatform for researchers active in the area of Logic and ArtificialIntelligence to meet, and to present and publish their work. The aim of JELIA-06 is to bring together active researchers interestedin all aspects concerning the use of Logics in Artificial Intelligenceto discuss current research, results, problems and applications of botha theoretical and practical nature. JELIA strives to foster links andfacilitate cross-fertilisation of ideas among researchers from variousdisciplines, among researchers from academia and industry, and betweentheoreticians and practitioners. Authors are invited to submit paperspresenting original and unpublished research in all areas related to theuse of Logics in AI.JELIA-06 will be held in Liverpool, from 13th to 15th September2006. This will emphasize the UK's key role in the international Logic&amp; AI community, and will provide researchers in the UK with easyaccess to the event. Most of the funds requested will be used forinternational invited speakers, grants for UK-based students andoverhead costs of the conference",,
8,5DB2BA9E-C37E-48C1-AE1B-9AE7EA16A6C4,Model Checking Agent Programming Languages,"Our overall goal in this project is to develop model checkingtechniques that can be applied to various agent-oriented programminglanguages. Building on our previous work on model checking forAgentSpeak, we will investigate the key aspects underlying not onlyAgentSpeak but other agent-oriented programming languages based onJava, such as 3APL, Concurrent MetateM, and Jadex. Based on that, wewill develop a Java infrastructure layer that is (a) relevant to theimplementation of these agent languages (b) has clear semantics, and(c) is able to be verified through an extended version of JPF, anopen source Java model checker.As a result of this investigation, we will be able to develop bothformal semantics for these major constructs of agent programming aswell as Java libraries that implement them (based on the semantics) -the resulting intermediate agent language will be called the AgentInfrastructure Layer (AIL). As in our existing body of work, JPF is tobe used as the model checker with which to carry out formalverification of implemented agent-based systems. The idea is then toprovide automatic (and provably correct) translations from variousexisting agent-oriented programming languages.",,
9,E6264B5F-DBC4-4F9C-85F6-15620D42C0A7,A New Generation of Trainable Machines for Multi-Task Learning,"The field of Machine Learning plays an increasingly important role in Computer Science and related disciplines. Over the past decade, the availability of powerful desktop computers has opened the door to synergistic interactions between empirical and theoretical studies of Machine Learning, showing thevalue of the ``learning from example'' paradigm in a wide variety of applications. Much effort has been devoted by Machine Learning researchers to the standard single task learning problem and exciting results have been derived. However, Machine Learning capabilities are still extremely limited when compared to those of humans. The human ability to generalise knowledge learned in one task in order to solve a new task is not available in current Machine Learning systems. Multi-task learning research has not yet received sufficient attention in the field. The standard single task learning approach builds on assumptions that are too restrictive to be easily extended to the novel learning scenarios which are envisaged in this proposal. Although interesting insights on multi-task learning have been provided, at present there is no comprehensive framework for multi-task learning and no cornerstone has yet been placed in the field. Thus, the main purpose of this proposal is to develop this area of Machine Learning research. The proposal focuses on Statistical Machine Learning methods for learning multiple related (classification or regression) tasks and integrating information across them. We shall design formal models of relationships between the tasks and develop (learning algorithms) for learning these relationships from data. We shall also develop the mathematical foundations (generalisation bounds, approximation results, convergence results) for multi-task learning, extending some key theoretical results for single tasklearning. Furthermore, the learning algorithms will be applied to two key applications, namely user preference modelling and multiple microarray gene expression data analysis. A central role in our approach is played by certain graph structures which allow us to model task relationships. This approach is very general and can be adapted to increasingly complex learning scenarios. The computational methods are based on the minimisation of certain penalty functionals via a large number of hyper-parameters associated with the tasks. The proposed research will lead to a new generation of trainable machines for multi-task learning, which will be more powerful and flexible models of learning, closer to human learning than previously developed Machine Learning frameworks.",,
10,F41CE1E4-3922-4C5B-B8D7-6D2AEC2A087D,Exploring Mechanisms of Cognitive and Behavioural Development in Humans and Machines,"During the developmental process in humans, qualitatively new behaviours are learnt by using simple abilities as a basis for learning more complex ones. This progressive increase in competence, provided by development, may be essential to make tractable the process of acquiring the higher-level cognitive and behavioural abilities observed in adults. Similarly, development could make tractable the creation of more intelligent machines, and specifically robots. Current methodologies for designing robot controllers depend on the prespecification of the rules governing the robot's behaviour. However, such predefinition relies on a significant design effort or on prolonged search and testing time. This makes prespecification intractable for a complex robot or one performing a complex task or operating in a complex environment. Furthermore, the resulting control system tends to be applicable only to the particular problem it was developed to solve and is brittle in the event of any unforeseen circumstances. These problems could be resolved by providing the robot with the ability to adapt the rules it follows and to autonomously create new rules for controlling behaviour. Developmental approaches could thus provide robots with a strategy for discovering how to behave and may be a practical necessity for producing robots capable of operating successfully in the real world. This project thus aims to explore the neural mechanisms underlying the developmental process in biology in order to extract appropriate computational mechanisms that could be used to implement a similar developmental process in a robot. Achieving this objective will not only provide mechanisms appropriate for creating more adaptive and autonomous robots but will also provide valuable insights into human development.",,
11,C1E3347F-6A61-46F6-B2C4-027B62647DCA,Next Generation Decision Support: Automating the Heuristic Design Process,"The current state of the art in decision support and search methodologies tends to focus on bespoke problem specific systems. Indeed, there are many examples of powerful and innovative search methodologies which have been tailored for specific applications and which underpin highly effective decision support systems. Over the last 20 years or so, there has been significant scientific progress in building search methodologies and in tailoring those methodologies (usually through hybridisation with problem specific methods and information) for a very wide range of application areas. Such methodologies can be extremely effective in the hands of an expert but they require specialist human knowledge to be applied effectively in complex real world problem solving environments. The goal of developing automated systems to replace the human expert in this role is only just beginning to be seriously addressed by the scientific community. The challenge of developing systems to intelligently select, evolve and develop search methods is an extremely ambitious and demanding research goal. The level of adventure should not be underestimated. The goal of exploring the boundaries between what is possible and what is not (with respect to the automation of the heuristic design process) represents one of the most important current research challenges to face the search and decision support community. The main aim of this major and far reaching programme of research is to address this challenge by investigating a wide range of promising and adventurous research directions in an integrated and co-ordinated manner. The successful development of automated systems to generate heuristic methods would underpin the next generation of search/optimisation systems that would be able to operate at a fundamentally more general level than current understanding can support. The aim is to develop systems which can operate upon a wider range of problems and problem instances than is possible with today's technology by automatically tailoring heuristics to particular problems and problem instances. Today, this process can only be effectively carried out by human experts.Of course, we know (from the No Free Lunch theorem and related work) that it will not be possible to build a completely general search method. However, we also know (from work carried out by ourselves and others) that it is possible to generate methods that are more general than the current state of the art. The question of how general we can make search systems is very much an open question and it is one that we will explore in this research programme. The emphasis will not follow conventional current thinking by concentrating on the development of systems to solve particular search/optimisation problems. Instead this major scientific undertaking will investigate the possibility of developing adaptive systems which can react to the conditions and the environment of the particular decision support problem in hand. The potential benefits of success in such a radical undertaking are enormous and permeate not only the disciplines of Artificial Intelligence and Operational Research but also the various disciplines that draw on and contribute to them. These include Computer Science, Mathematics, Business, Engineering, Computational Chemistry, Medicine, Architecture (space planning), Bioinformatics, Manufacturing and all areas of Management. The research will also impact upon automated heuristic selection and design across many diverse applications such as scheduling, timetabling, cutting/packing, protein folding, catalyst optimisation, medical decision making and others. This research initiative will enable us to explore risky and unconventional ideas across a range of disciplines and research council remits in a way that is not possible with standard grants and it will allow us to flexibly redirect our efforts across application areas as well as disciplines to explore ideas as they emerge.",,
12,3729B7EB-EC78-4D48-8BD1-4F2862FCBCAA,Learning the morphology of complex synthetic languages,"This project aims to apply advanced machine learning techniques in order to learn the morphology -- the way words are formed from constituents -- of synthetic (i.e., morphologically complex) languages. This will allow improved text-to-speech systems for complex languages such as isiZulu. Morphological analysis is the decomposition of words into their constituents (morphemes) with the assignment of grammatical features to each of constituents. To take a simple example in English, the word unhappier is decomposed into the following components: un(adjectival negative prefix)+happy(adjectival stem)+er(comparative suffix) taking into account both the allowed sequence of word constituents and the changes of the orthographic shape of these constituents when they are concatenated. Most morphological phenomena in the majority of European languages can be expressed by finite-state techniques such as regular expressions. This project, however, is concerned with the structurally more complex synthetic languages (mostly non-European). These languages exhibit complex recursive morphological structures that require more powerful mechanisms than finite-state automata. The main research goal of the project is to automatically decompose the word into its constituents by learning the rules for representing permissible sequences of word constituents and the rules that change the orthographic shape of the constituents. This involves tackling a set of open problems in morphological learning that prevents learning the whole set of morphological rules. We have chosen Inductive Logic Programming (ILP) for training as its logical foundations allow representing complex formalisms that can be expanded by stochastic features. ILP methods can also induce rules directly from unbounded data items such as strings, which makes annotation and training more naturally related to the underlying linguistics. The proposed research will have a tremendous benefit for producing Text-to-Speech Systems in developing African and Asian countries (and the practical delivery will be enabled by the partnerships and contacts forged by the Local Language Speech Technology Initiative, see www.llsti.org). The automated morphological analysis tools developed in this project will facilitate the creation of intelligible Text-to-Speech systems that require morphological analysis for (1) Automatic tone assignment, which is essential for most African languages.(2) Proper prosody, which includes stress assignment required for Russian, and phrase prediction required for most world languages including European ones.(3) Proper letter-to-sound rules required for the Indian languages Hindi and Telugu, the Turkish language and many others. The research will provide the technology for the implementation of indigenous and minority language voice services offered by mobile network providers (such as information on healthcare, jobs, agriculture, the environment etc.) Another important application for this technology will be screen readers for blind people in many Asian and African countries.",,
13,E779708D-A6F2-4D7C-A844-8EBA711B9EAF,Machine Learning for Resource Management in Next-Generation Optical Networks,"Recent developments in optical networking technology offer the prospect of greater flexibility and configurability over increasingly short timescales in order to address the demands of large capacity, highly bursty, intermittent data transfers, typically in accordance with performance constraints. The aim of this project is to investigate the application of confidence machines to the prediction of highly dynamic traffic behaviour in next generation optical networks and, consequentially, enable these networks to be operated more efficiently. This project is a new collaboration bringing together three strands of recent research: pre-booking resource management, multi-fractal traffic modelling, and confidence machines. As the traffic pattern varies across time granularities, the proposed pre-booking resource management mechanism is hierarchical, whereby the traffic prediction is decoupled into multiple levels. A key point of novelty in the proposal lies in its approach to prediction; namely, the use of confidence information when evaluating plausible alternative resource allocations over a continuum of timescales. Unlike conventional machine learning techniques, the predictions these confidence machines make are hedged: they incorporate an indicator of their own accuracy and reliability. These accuracy reliability measures allow service provider and network carrier to choose appropriate allocation strategies by eliminating unlikely resource demands. Therefore, resource management process can effectively perform a cost-benefit evaluation of alternative actions.The project will employ a technology-agnostic approach allowing a number of possible evolution scenarios for next generation optical networking to be considered. The outcome of this research will have important industrial repercussions for optical network efficiency and revenue generation capability, as well as theoretical advances to the evaluation of performance risk in the context of dynamic network behaviour. This latter aspect is likely to have further application, for example, with regard to the performance and resilience of utility computing, and not just the underlying transport.",,
14,D8197D81-5ECD-4CF2-82E5-3EB197140DB0,Machine Learning for Resource Management in Next-Generation Optical Networks,"Recent developments in optical networking technology offer the prospect of greater flexibility and configurability over increasingly short timescales in order to address the demands of large capacity, highly bursty, intermittent data transfers, typically in accordance with performance constraints. The aim of this project is to investigate the application of confidence machines to the prediction of highly dynamic traffic behaviour in next generation optical networks and, consequentially, enable these networks to be operated more efficiently. This project is a new collaboration bringing together three strands of recent research: pre-booking resource management, multi-fractal traffic modelling, and confidence machines. As the traffic pattern varies across time granularities, the proposed pre-booking resource management mechanism is hierarchical, whereby the traffic prediction is decoupled into multiple levels. A key point of novelty in the proposal lies in its approach to prediction; namely, the use of confidence information when evaluating plausible alternative resource allocations over a continuum of timescales. Unlike conventional machine learning techniques, the predictions these confidence machines make are hedged: they incorporate an indicator of their own accuracy and reliability. These accuracy reliability measures allow service provider and network carrier to choose appropriate allocation strategies by eliminating unlikely resource demands. Therefore, resource management process can effectively perform a cost-benefit evaluation of alternative actions.The project will employ a technology-agnostic approach allowing a number of possible evolution scenarios for next generation optical networking to be considered. The outcome of this research will have important industrial repercussions for optical network efficiency and revenue generation capability, as well as theoretical advances to the evaluation of performance risk in the context of dynamic network behaviour. This latter aspect is likely to have further application, for example, with regard to the performance and resilience of utility computing, and not just the underlying transport.",,
15,BEDD23E3-C742-4454-99EE-23B1B00ED03A,Stochastic Local Search Algorithms for Structural Proteomics,"Proteins are key elements of virtually all cellular functions. They are complex biological macromolecules that are composed of a sequence of amino acids, which is encoded by a gene in a genome. Amino acids are joined end-to-end during protein synthesis by the formation of peptide bonds. The sequence of peptide bonds forms a main chain or backbone for the protein. The functional properties of proteins depend upon their three-dimensional structures. Knowing the structure of a protein provides a basis for identifying the protein's function, and protein structures are necessary for many computational drug docking techniques. Unlike the structure of other biological macromolecules (e.g., DNA), proteins have complex, irregular structures that are difficult to predict. To understand the entire folding pathway, i.e., the complete dynamics and chemical changes involved in going from an unfolded linear state into a compact folded state is one of the most challenging problems in current biochemistry.proposed project focuses on the design and analysis of brand new classes of stochastic local search-based algorithms for structural proteomics, i.e. for protein folding as well as for related problems like protein structure alignments and protein sequence design (inverse to folding). All these problems can be formulated as optimisation problems. For instance, one of the basic paradigms in structural proteomics is given by Anfinsen's thermodynamic hypothesis: Proteins fold to a minimum energy state, and the information determining the three-dimensional structure (tertiary structure) of a protein resides in the chemistry of its primary structure. Therefore, the investigation of stochastic local search algorithms (SLSA) is a natural choice to tackle these very complex (NP-hard) problems. SLSAs such as Simulated Annealing and Genetic Algorithms have been successfully applied to a wide variety of combinatorial optimisation problems. Since it is practically impossible for protein folding to evaluate every feasible conformation, local search algorithms try to find an efficient walk through an (energy) landscape that is induced by all possible solutions, where neighbouring solutions are evaluated with respect to the value of the objective function. In our approach we investigate the applicability of inhomogeneous Markov chains in order to control these walks through landscapes, where we focus on different models of protein folding together with a variety of potential objective functions. The choice of inhomogeneous Markov chains will enable us to mathematically prove properties such as convergence to optimum solutions and time estimates for the confidence to be in an optimum solution, e.g., after T(1\d) computation steps, the probability of having found an optimum solution is greater than (1-d). key aspect of our proposed research is a theoretical and experimental analysis of key structural parameters of the energy landscape that affect the convergence to optimum solutions. Based on knowledge about parameters such as distribution and depth of local minima, we can analyse and characterise the behaviour of other popular local search methods such as genetic algorithms, tabu search, and memetic algorithms. Moreover, there is a conjecture that local minima represent misfolded conformations , i.e., stable conformations that have a locally optimum energy state. These misfolds play an important role in the current discussion about causative factors in diseases. So, our results will give new insights into a more detailed complexity characterisation of folding problems and the performance of stochastic local search. Our approach will also contribute to the classification of misfolds in terms of their structural characterisation as local minima with respect to different energy functions, which will be in the long term crucial for personalised drug design and life science in general.",,
16,F80E3C99-2678-429A-8CB4-A301C53F6E28,Lifelong Adaptation and Failure Recovery by Evolutionary Computation for Multiple Heterogeneous Robots,"The need for autonomy in the area of mobile robotics is increasing. With more applications relating to areas where robots may be out of contact for extended periods of time (such as during planetary or geographically remote earth-based exploration tasks), it is becoming increasingly important that they continue to work effectively and autonomously adapt to different environments without performing inappropriate actions which could jeopardise the robot or its surroundings. Multi-robot teams consisting of heterogeneous robots offer several advantages over single robots or homogeneous groups. These include the ability to use their individual capabilities to deal with a more diverse set of tasks, to complete a task when an individual robot fails through its own degradation, or to take over a task if environmental changes mean that the original robot assigned to the task can no longer perform it.This proposal envisions a multiple, heterogeneous robot system that is able to operate without external assistance for an extended period of time. The system should be able to identify when a robot has degraded and is unable to complete a task. The task can then be reallocated to another robot with a similar competence. Where the environment has changed and the robot is no longer appropriate for the task, this should be recognised and the task reassigned to another robot and the original robot reassigned elsewhere.This proposal intends using genetic and evolutionary computation (GEC) methods to achieve the above adaptation and failure recognition. GEC algorithms are a biologically inspired method of optimisation and search based on the concept of natural selection. Although GECs have been applied to the design of robots and their controllers in the past, they have rarely been applied to the problem of lifelong optimisation and adaptation of robots in dynamic environments. This proposal aims to use a GEC training phase to develop robot behaviour for diverse environments, and to determine whether different robots have the ability to perform specific tasks, and at what stage the robots fail within a particular environment. Using simulation for the training phase allows a faster, safer, evolution than on a real robot, then running these optimised chromosomes on a physical robot with lifelong evolutionary adaptation allows grounding in the real world. As multiple robots provide redundancy, and heterogeneous robots provide an ability to perform a wider range of tasks, this proposal intends to use a group of robots with different sensory and manipulation capabilities, and different control strategies to function together to achieve tasks in circumstances where an individual robot may fail, or where homogeneous robots may not have the capability to deal with a changed environment or task. Lifelong adaptation using GEC will be used to deal with limited changes in the environment. Determining at the training phase how well individual robots perform within specific environments can provide information as to which robots may be able to take over from a robot which has failed at a task due to its own degradation.",,
17,EE3E762D-CA85-418F-8E5E-987E3963D818,Optimising Hardware Acceleration for Financial Computation,"This proposal describes a three-year research project exploring novel methods and tools for hardware acceleration of financial computation in general, and for Monte Carlo simulation of financial models in particular. Our aim is to exploit the latest software and hardware technologies, particularly those based on advanced reconfigurable hardware such as FPGAs (Field-Programmable Gate Arrays), and to demonstrate the effectiveness of these technologies by applying them to overcome bottlenecks in current and future large-scale financial computation. The technical innovations of this project includes: (1) parameterisation, characterisation and efficient implementation of novel hardware architectures for financial computations; (2) exploitation of the latest software and hardware technologies, such as source-level transformation and advanced reconfigurable gate arrays; (3) techniques for reducing heat dissipation by extensive pipelining, (4) elements for an evolutionary approach to support hardware acceleration for financial analysis, such as adoption of commercial FPGA platforms, facilities to make the technology accessible to finance experts, comparison of standard fixed-point and floating point arithmetic, incremental compilation, and interface to grid technology; (5) elements for a disruptive approach to support hardware acceleration, such as run-time optimisation, coarse-grained devices, non-standard arithmetic, new application opportunities such as real-time risk analysis, and new platform and chip architectures; (6) static and dynamic customisations for adapting architectures to changes in environmental conditions to maintain effective operation, while meeting various constraints such as performance and power consumption; (7) prototype development frameworks for designing and deploying novel architectures supporting financial computations, by combining and specialising our libraries and tools; (8) large-scale applications, based on our experience in financial simulation, to drive the development of architectures and tools for novel computations.",,
18,E5322DFD-25C8-4413-9DEB-ABE0D7924DA0,Statistical learning with general proximity measures,"The current challenge facing research and industry is how to exploit meaningful patterns from data in order to explain the underlying phenomena and to develop predictive models. Proximity plays an essential role in such learning as it underpins the process of recognition. It is a natural link between observations of objects, and an overall judgement of their shared commonalities. Sets of objects can be studied starting from the perspective that some are similar, while others are different. So, a suitable proximity measure should be designed first to identify patterns and model clusters. Statistical learning offers a plethora of powerful techniques, which mostly rely on vectorial representations of objects in some input vector space. The proximity between objects is modelled as the Euclidean distance between their vector representations. Many currently available data are, however, non-vectorial by origin, e.g. text, binary files, shapes, ECG signals or protein sequences. Although some ways exist to represent them in a vectorial form, these may be of poor quality for the final goal or difficult to obtain. If objects contain an identifiable structure, such as shapes or texts, structural descriptions are advisable for whichedit distances can be used. Proximity is then the natural concept to represent heterogeneous information.Kernel methods are a statistical alternative to vectorial representations, but their application is limited due to the constraint of positive semidefinite (psd)kernels. If we wish to model the human ability of recognition, the traditional use of statistical learning does not address this question well. Many studies in cognitive science suggest that non-Euclidean or non-metric measures are used by humans to judge proximity. In statistical learning, proximity is imposed beforehand by the nature of Euclidean distance or a kernel.Many proximity measures are non-metric or non-Euclidean. These are e.g. derived when shapes or sequences are aligned in a template matching process. Non-metric examples are measures in the presence of partially occluded objects, pairwise alignments of proteins, variants of Hausdorff distance or normalised edit distances. Although there exists a zoo of measures used for comparison of objects, a proper framework for generalisation is still lacking. Researches usually rely on the nearest neighbor distances, or choose to correct the measure to be metric or Euclidean.The important question is when such type of approaches are both justified and beneficial for the final goal. If the measure describes the problem well, as judged by experts, and its deviation from metric or Euclidean behaviour is large, one expects a significant loss of information, when corrections are applied. If such deviations are small, they may be considered as noise. Our research is focused on understanding the properties of general proximity measures and developing useful learning strategies. Our goal is to investigate the use of non-Euclidean spaces and setting up learning paradigms there. We will develop indices to characterise general measures and used them to suggest a learning approach. The two important problems are:(1) design and learning of the proximity measure from a set of examplesand (2) given pairwise proximity data, propose the best learning paradigm. Since many proposals are already available in (1), (2) needs to be developed first by proposing learning methods in pseudo-Euclidean and Banach spaces. Additionally, a framework for asymmetric measures will be suggested.Having developed the tools, we will investigate ways of learning proximity measures from a set of examples. We plan to do it in a probabilistic contextand incorporate Bayesian reasoning. As a result, our methods will be applied to practical problems in machine learning, imaging and bioinformatics.",,
19,D4BE9C1D-6304-4C05-AFAC-E39866C5D0CE,"Connections between belief revision, belief merging and voting","There is a strong connection between logical theories of belief revision and merging and decision/voting theory. They share the common mechanism of trying to resolve conflicting alternatives. In belief revision, an agent is faced with the problem of choosing between several alternatives when trying to restore consistency to theory. Ideally, the choice process is conducted in a way that verifies a number of fairness principles. On the other hand, belief merging concerns with the problem of determining a group's beliefs from individual members' beliefs that are not always compatible with each other. Similarly, in voting systems, a social welfare function takes individual preferences into account in order to produce a collective preference. Here again certain fairness principles are desirable. In spite of the similarities, the basic approach and the methodologies used in each of these areas differ from one another. In a recent publication ( Belief revision, belief merging and voting ), we provided a framework that makes it possible to study them in an uniform way. Our initial findings suggest that there is a wealth of possibilities of research on the connection. The motivation for the workshop is to promote discussion and cross-fertilisation between the three areas.",,
20,F4F50380-F2F2-46B1-8D0B-DC4E91B6A462,Smart Sensing for Structural Health Monitoring (S3HM),"The cost of unplanned maintenance for structures and systems is a significant issue in a number of fields of Engineering. For example, aircraft are critical structures which fall within the domain of Mechanical Engineering, bridges fall within the remit of Civil Engineering. A potential approach to reducing maintenance costs is to convert all unplanned maintenance to planned maintenance. In order to do this, one needs a means of assessing the health of a structure on a continuous basis. This is the domain of Structural Health Monitoring (SHM). The current project aims to develop SHM technologies based on the consideration of measured vibration data from structures. The idea is that the measured data will inform the Engineer of the current state of health. The approach taken will be based on advanced concepts of pattern recognition and machine learning. Algorithms will be developed which can learn the patterns in data which signal damage. A major problem in SHM is concerned with false indications of damage caused by the environmental conditions (e.g. the temperature) of the structure changing. A major part of this project will be concerned with the development of algorithms which can deal with the issues of environmental uncertainty.Another advantage of the availability of effective SHM is that one can adopt a damage tolerant approach to the design of the structure, this will give potential reductions in the design safety factors which will be passed on as reductions in manufacture cost.",,
21,93644A00-CA3F-48BE-BB6B-C55C2A6986B4,Evolutionary ontologies for open agent environments,"This project aims to support ontology evolution and adaptation in dynamic, distributed social environments, where actors and their ontologies are not considered in isolation, but within the social fabric determined by their interactions. In this context, we need to provide knowledge representation mechanisms that support this process of evolution, adaptation, and consensus agreement, and consider them embedded in social fabric of agent interactions. The project aims at investigating an evolutionary model of the ontology lifecycle determined by the communication needs of open, dynamic agent environments.The main goals are:(1) To devise an evolutionary model of the ontology lifecycle that accounts for the social dynamics involved in the emergence and adaptation of knowledge in open agent environments.(2) To adapt and reuse ontology and semantic web approaches and tools in order to support the ontology lifecycle in social environments for agents, including the value and the impact of a change in the ontology, in the social network linking an agent with similar knowledge, and the technical and social costs of agreeing on a change.(3) Developments of negotiation mechanisms that support the evolution and adaptation of ontologies in agent environments.",,
22,1AB2B2C9-52C3-4785-B6CE-ADD51EB7B85C,Practical Reasoning Approaches for Web Ontologies and Multi-Agent Systems,"Logical and automated reasoning methods are crucial for web technologies and agent technologies for the intelligent processing of large ontologies, decision making based on knowledge bases of structured data, and formal specification and verification of multi-agent systems.Concerning ontology reasoning the current tableau description logic reasoners used for this purpose have a number of significant shortcomings. These include insufficient expressiveness, suboptimal complexity, over specialisation and incomplete formal treatment. All these are serious issues which need to be tackled and overcome if such systems are to form the backbone ontology reasoning for the semantic web. Concerning reasoning within and about multi-agent systems, in the form of intelligent decision making by agent and the formal specification and verification of agent systems, there exists a plethora of agent logics which have been proposed for this purpose, but almost no implemented reasoning systems for solving satisfiability and validity problems in these logics. This lack of system support is a serious problem.We will use techniques from first-order logic and resolution to develop a resolution framework for reasoning about expressive ontological languages and expressive agent logics. A series of tools will be developed to provide automated support for reasoning tasks in these areas. Principles of benchmarking will be studied, designed and used for empirical investigations of developed technologies and tools.",,
23,7702F304-0B65-453B-A18D-B7BA8F079BAB,Cognitive Systems Foresight: Human Attention and Machine Learning,"Human observers move their eyes in order to direct their attention to important aspects of a visual scene. There are models called salience maps; they predict where the eyes will move to when looking at a scene. At present, these models do not deal with video input, nor do they predict how an observer's task will affect where they look. In other words, there are no models for real-life viewing situations, where an observer has a specific task.We are proposing a new approach to this problem. We have access to video information from cameras used in urban surveillance, and to the operators whose job it is to spot abnormal behaviour in such video inputs. We shall obtain (previously unseen) video recordings of events in UK urban streets, and display them in a simulated control room to operators familiar with the town in question. We shall monitor where they look on the bank of video screens, and also when they decide that an event is abnormal and/or requires some form of intervention, e.g. calling the police. We shall use the record of eye fixations to teach a computer system to distinguish between normal and abnormal events. In this way, we shall be able to learn what is important for humans to do such surveillance by observing their eye fixation behaviour, for a realistic (and difficult) task and set of real-life video sequences. The project is important for four reasons. First, this will be the first attempt to develop a model of human attention/eye movements which will be firmly based on realistic video input and a real task. Second, this will be the first time that a computer system is able to learn from human behaviour in this way. Third, we will learn much about the ability of trained observers to cope with a demanding task as the number of TV monitors increases. Finally, we will develop an automated system which will be able to analyse the input from any urban CCTV camera in order to alert operators to look at that video stream - at present, most CCTV video streams are not observed by anyone since there are too many cameras for the number of human observers. Therefore, an automated alerting system is greatly neeeded and this project constitutes the best attempt to date to produce one.",,
24,C812A3EB-A4E9-4137-AF78-AEF65D0C03E0,Radio frequency identification and tracking of individual ants engaged in colony scale division of labour,"Radio Frequency IDentification (RFID) is an automatic identification method, allowing remote retrieval of identification codes from devices called RFID tags or transponders. An RFID tag is a small object, requiring no internal power source that can be attached to or incorporated into a product, animal, or person. These tags contain antennae that respond to prompts from an RFID transceiver by emitting a radio signal that codes for a unique ID. Recent progress in RFID technology has led to the miniaturization of such tags to sub-millimetre dimensions. These microtransponders, once activated by a well localised beam of light, emit a radio frequency identification code. The miniaturization of these tags, as well as their ultra light weight and low price, now permit the use of RFID for insect identification. To make the most of these new possibilities, we will use the RFID system to investigate social insects.The behaviour of ant colonies is extremely complex and includes collective phenomena like nest building, decision making and partitioning of the work force (division of labour). This colony scale activity is not the consequence of some central control but rather emerges from the actions of individual ants as each of them reacts to her immediate environment. The mechanisms in which diverse colony scale behaviours emerge from single ant actions have been extensively studied. We will develop a novel experimental setup incorporating RFID technology that will enable us continuously to track a large number of identified ants as they collaborate. This novel experimental tool will provide simultaneous experimental access at both the colony level and the level of individual ants. This will facilitate new insights and a deeper understanding of the connection between individual and collective behaviour in social insects.We intend to focus on the problem of 'division of labour', namely how an ant colony divides its work force among the different required tasks in response to changing external and internal conditions. The system we propose would allow the tracking, over time, of the ants engaged in different tasks as well as the ants that switch between tasks to maintain colony plasticity. In a second stage of the experiments, we will employ automatic computer controlled doors that would enable us, for example, to prevent specific ants from switching tasks. We will use these novel tools to investigate the role of different individuals and their importance to colony performance. We will focus our study on the importance of phenotypic diversity and ant individuality for division of labour. Would a hypothetical colony of perfectly identical ants perform as well as a colony composed of unique individuals? The role of individuality in division of labour is likely to be of key importance in other social insect phenomena as well as having implications for emerging technologies such as collective robotics and sensor networks.",,
0,818FB156-CBCE-42F5-9E9D-7EFF7FD8FF94,End-to-end integrated Statistical processing for Context-aware dialogue systems,"This project targets a new processing paradigm for the development andoptimization of spoken dialogue systems (SDS) that are context-aware,efficient, and most importantly robust to the uncertainty thatpervades natural language. We will develop tractable and effectivetechniques for the integrated end-to-end treatment of uncertainty incontext-aware SDS, using learning algorithms combinedwith Partially Observable Markov Decision Processes (POMDPs). Thisrequires us to develop effective methods for training and testing suchsystems. We will also determine, through system tests withreal users, whether the end-to-end statistical treatment ofuncertainty improves SDS for users, in comparison to rulebased and standard MDP-based techniques.No current SDS treats dialogue processing as an end-to-endintegrated statistical system, constrained by context, whereuncertainty in one process feeds into other processes, whereuncertainty in one dialogue state feeds into the nextdialogue state, and where this whole system is constrained viacontextual feedback. It is still standard practice to ignore theuncertainty in the output of a lower-level process by passing only asingle best analysis to higher-level processes, with the sideeffect that lower-level processes do not take into account importanthigh-level constraints. For example, contextual features ofdialogues such as user goals or previous speech acts are notsystematically exploited in speech recognition or utteranceinterpretation. This is a serious shortcoming for current SDS, given that uncertainty pervades and proliferates throughevery level of dialogue processing, from speech recognition errorsthrough interpretation ambiguities, to uncertain dialogue states andcompeting strategies. These problems lead to the currentsituation where SDS are not robust or efficient enoughfor any but very simple tasks.We will build and evaluate SDS which usestatistical processing end-to-end, and which use contextrepresentations to constrain the uncertainty inherent in dialogue. Wewill build on exisiting knowledge and techniques developed in the TALKproject, and well as recent corpora (COMMUNICATOR, TALK, AMI). TheSDS development tools, components, and environments usedand developed at Edinburgh's HCRC (e.g. DIPPER, HTK, Festival) alsoprovide a number of exisiting dialogue systems (FLIGHTS, TALK, WITAS), forming a platform to be extended usingthe new methods developed in the project. These systems can then beused for testing, evaluation, and further data collection.The proposal thus aims to improve dialogue system robustness andefficiency, and allow SDS to be developed and optimized usingdata-driven approaches. There is much user frustration with currently deployed SDS, so there is much to be gained from improved robustness andefficiency. Data-driven optimization will also lead to decreaseddeployment and development costs for industry. Thus the beneficiaries ofthis research will potentially be all futureusers of IT (including the illiterate andIT-illiterate, also in the developing world). In the short tomedium term, commercial applications include: interactive SDS, dialogue and meeting summarisation, interactiveentertainment, intelligent tutoring systems, intelligent personalassistants, and dialogue supported question-answering and search.With recent advances in speech recognition, parsing, context-sensitivestatistical dialogue management, the theory of learning with PartiallyObservable states, the availability of new,large, and richly annotated dialogue corpora, we are now in a position to treat dialogueprocessing as an end-to-end context-aware statistical system. Webelieve this model will lead to a breakthough in robust, efficient, and natural human-computer SDS, andhas the potential to radically improve the state-of-the-art indialogue management.",,
1,D83F7E89-7EBD-422B-A604-2057090239C7,Investigating the Computational Capacity of Cultured Neuronal Networks Using Maching Learning,"The brain is arguably one of the most complex computational platforms in existence. It can rapidly process information and is tolerant to faults and noise. Studying how the brain processes and encodes information in living animals is ethically questionable and is technically challenging as access is restricted by the skin, skull and sheer number of neurons. Recent research has concentrated on artificially culturing biological neurons in petri dishes. Over a matter of weeks these neurons begin to grow and branch out and begin to re-establish connections with neighbouring neurons and start to communicate with each other both chemically and electrically. The neurons are grown on a surface composed of multiple electrodes that provide an electrical connection to the neurons. This electical connection enables the electrical activity of the neurons in the culture to be recorded whilst also allowing neurons near each electrode to be artificially stimulated. The neural culture therefore forms an artificial brain to which external inputs can be applied by means of stimulation and from which outputs can be obtained by analysing the patterns of electrical activity the brain produces in response to stimulation.Such brains can be considered to be performing a type of computation on the signals that are applied to them. We know that neurons in such cultures have an inherent capacity to network and begin to communicate with each other however, we do not have a good understanding of how much computation such brains are capable of performing.In this project the neural cultures will be cultured locally in the University of Readings' new Electrophysiological research laboratory allowing real-time access to the recording and stimulation hardware via an intranet link-up.In order to test the abilities of such cultured neural networks we propose using them to control some of our existing mobile robots. This is to be achieved by applying a number of Machine Learning and Artificial Intelligence techniques in order to correctly translate robot sensor inputs into suitable patterns of stimulation and interpret the resulting patterns of neural activity as motor actions. In order to measure the amount of computation the cultured brain is performing we will use a surrogate (an artificial neural network that redistributes the input signal to the output) in place of the the cultured brain . Both the cultured brain and the surrogate will be applied to various behavioural tasks (such as obstacle avoidance and wall following) the difference in performance between the cultured brain and the surrogate will give us some measure of the processing capabilities of cultured neural networks when used in this way.In order to test our understanding of the processes occurring in the neural culture we will also build a model of the cultured brain and compare the results from our model to the results from the actual neural culture. We are particularly interested in how the input-output responses of cultured neural networks change over time and what bearing this has on the resultant robot behaviour.",,
2,FBF9CEE0-9B14-4FAA-B262-09996D45F92C,Reasoning with Uncertainty and Inconsistency in Structured Scientific Knowledge,"There is a huge and rapidly expanding amount of information available for scientists in various online resources. However, this wealth of information has created challenges for scientists who wish to locate and analyse knowledge from heterogeneous sources. Key problems that exist are that there is much uncertainty in individual sources of scientific knowledge, and many conflicts arising between different sources of scientific knowledge. Scientists therefore need tools that are tolerant of uncertainty and inconsistency in order to query and merge scientific knowledge.This project aims to facilitate the analysis of scientific knowledge by the development of technology for structured scientific knowledge (SSK). SSK is represented by a set of SSK reports each of which is a structured report that describes one or more scientific datasources (such as one or more journal articles, empirical datasets, etc). The format is an XML document with textentries restricted to individual words, values or simple phrases in scientific terminology. SSK is intended to help scientists understand the contents of a datasource. Each one contains summaritive information about the datasource (e.g. information from an abstract, summary of techniques used, etc) plus evaluative information about the datasource (eg. delineation of uncertainties and errors in the information source, qualifications of the key findings, etc). The summaritive information describes the information provided by the authors of the datasource, and the evaluative information describes the information provided by the users or authors of the datasource. SSK can be constructed by hand, by information extraction technology, and as a result of analysing datasources. In this project, we want to extend our existing work for merging and analysing heterogeneous structured information by harnessing formal theories for representing and reasoning with uncertain and inconsistent information. The result of the project will be a general theoretical framework for handling uncertainty and inconsistency in SSK, and a demonstration of the framework in a prototype implementation for querying and merging potentially conflicting SSK from heterogeneous sources.",,
3,A05595F9-73EE-4CA0-8F7C-E022B86051F6,Reasoning with Uncertainty and Inconsistency in Structured Scientific Knowledge,"There is a huge and rapidly expanding amount of information available for scientists in various online resources. However, this wealth of information has created challenges for scientists who wish to locate and analyse knowledge from heterogeneous sources. Key problems that exist are that there is much uncertainty in individual sources of scientific knowledge, and many conflicts arising between different sources of scientific knowledge. Scientists therefore need tools that are tolerant of uncertainty and inconsistency in order to query and merge scientific knowledge.This project aims to facilitate the analysis of scientific knowledge by the development of technology for structured scientific knowledge (SSK). SSK is represented by a set of SSK reports each of which is a structured report that describes one or more scientific datasources (such as one or more journal articles, empirical datasets, etc). The format is an XML document with textentries restricted to individual words, values or simple phrases in scientific terminology. SSK is intended to help scientists understand the contents of a datasource. Each one contains summaritive information about the datasource (e.g. information from an abstract, summary of techniques used, etc) plus evaluative information about the datasource (eg. delineation of uncertainties and errors in the information source, qualifications of the key findings, etc). The summaritive information describes the information provided by the authors of the datasource, and the evaluative information describes the information provided by the users or authors of the datasource. SSK can be constructed by hand, by information extraction technology, and as a result of analysing datasources. In this project, we want to extend our existing work for merging and analysing heterogeneous structured information by harnessing formal theories for representing and reasoning with uncertain and inconsistent information. The result of the project will be a general theoretical framework for handling uncertainty and inconsistency in SSK, and a demonstration of the framework in a prototype implementation for querying and merging potentially conflicting SSK from heterogeneous sources.",,
4,A8001288-9AE6-4328-9796-EB1EFA11DCA8,Argumentation Factory: Algorithms and Software for Industrial Strength Inconsistency Tolerance,"Humans constantly deal with conflicting information in their everyday lives, but until recently the problem has been largely avoided in computing. Being based on mathematical thinking, the normal approach to inconsistency in computing is to not tolerate it. This is done either by arbitrary removal of conflicting information or by recourse to human intervention. But as computers are being pushed into more intelligent roles with the need for greater robustness, inconsistency tolerance is an increasingly important topic in many areas of computer science including artificial intelligence, robotics, natural language processing, databases, information systems, and software engineering. Inconsistency is omnipresent in the world. So we need to design systems that can address the problems and the opportunities raised by the widespread existence of inconsistency. Recent developments in the theory of argumentation are suggesting that the development and application of argumentation systems could offer a significant technological advance in the development of robust inconsistency tolerance in a wide range of applications.Argumentation is a vital aspect of intelligent behaviour by humans. Consider diverse professionals such as politicians, journalists, clinicians, scientists, and administrators, who all need to collate and analyse information looking for pros and cons for consequences of importance when attempting to understand problems and make decisions. Hence, the development of argumentation systems for decision-support systems for professionals is a promising area. More generally, argumentation systems are increasingly being considered for applications in developing software engineering tools, for constituting an important component of multi-agent systems for negotiation and problem solving, and for data + knowledge fusion. In these kinds of application there is a need to analyse inconsistent information, find competing viewpoints, and resolve conflicts. By argumentation, we can determine that a certain proposition follows from certain assumptions but that one of these assumptions could be disproved (or 'undercut') by other assumptions in our premises. In this way an argumentation system could help us analyse which assumptions were really giving rise the inconsistency and which assumptions were harmless. Argumentation systems can be used to draw arguments from inconsistent information, and to compare them with counterarguments. The theory of logic-based argumentation is therefore helpful in analysing inconsistency and there have been impressive research advances recently. However, argumentation is computationally expensive, and little consideration has been given to how it can be done efficiently.We therefore have a pressing need to develop algorithms and software for generating constellations of arguments and counterarguments. For this, we need automated reasoning technology. However, existing automated reasoning is not designed for finding arguments: It can be used to find a proof of an inference from a set of premises. But it is not intended for finding minimal consistent sets of formulae for proving some inference. Furthermore, with the generating arguments and counterarguments, there is much inefficient recomputation of consistency checks, and of minimality checks, for the supports of the arguments.To address these shortcomings, we want to explore four inter-connected lines of research: (1) Develop algorithms and prototype implementation of system for harnessing existing automated reasoning technology for providing the entailment relation as part of the process of constructing arguments; (2) Develop algorithms and prototype implementation for contouring (a form of lemma generation) of knowledgebases; (3) Develop algorithms and prototype implementation for compilation of knowledgebases; and (4) Develop algorithms and prototype implementation for approximate argumentation.",,
5,8A7FD29B-0A57-4D1B-B9B9-0B07A0812257,Practical Reasoning Approaches for Web Ontologies and Multi-Agent Systems,"Logical and automated reasoning methods are crucial for web technologies and agent technologies for the intelligent processing of large ontologies, decision making based on knowledge bases of structured data, and formal specification and verification of multi-agent systems.Concerning ontology reasoning the current tableau description logic reasoners used for this purpose have a number of significant shortcomings. These include insufficient expressiveness, suboptimal complexity, over specialisation and incomplete formal treatment. All these are serious issues which need to be tackled and overcome if such systems are to form the backbone ontology reasoning for the semantic web. Concerning reasoning within and about multi-agent systems, in the form of intelligent decision making by agent and the formal specification and verification of agent systems, there exists a plethora of agent logics which have been proposed for this purpose, but almost no implemented reasoning systems for solving satisfiability and validity problems in these logics. This lack of system support is a serious problem.We will use techniques from first-order logic and resolution to develop a resolution framework for reasoning about expressive ontological languages and expressive agent logics. A series of tools will be developed to provide automated support for reasoning tasks in these areas. Principles of benchmarking will be studied, designed and used for empirical investigations of developed technologies and tools.",,
6,70680984-7FD2-412B-8561-6AE8FC39BD5D,Cognitive Systems Foresight: Human Attention and Machine Learning,"Human observers move their eyes in order to direct their attention to important aspects of a visual scene. There are models called salience maps; they predict where the eyes will move to when looking at a scene. At present, these models do not deal with video input, nor do they predict how an observer's task will affect where they look. In other words, there are no models for real-life viewing situations, where an observer has a specific task.We are proposing a new approach to this problem. We have access to video information from cameras used in urban surveillance, and to the operators whose job it is to spot abnormal behaviour in such video inputs. We shall obtain (previously unseen) video recordings of events in UK urban streets, and display them in a simulated control room to operators familiar with the town in question. We shall monitor where they look on the bank of video screens, and also when they decide that an event is abnormal and/or requires some form of intervention, e.g. calling the police. We shall use the record of eye fixations to teach a computer system to distinguish between normal and abnormal events. In this way, we shall be able to learn what is important for humans to do such surveillance by observing their eye fixation behaviour, for a realistic (and difficult) task and set of real-life video sequences. The project is important for four reasons. First, this will be the first attempt to develop a model of human attention/eye movements which will be firmly based on realistic video input and a real task. Second, this will be the first time that a computer system is able to learn from human behaviour in this way. Third, we will learn much about the ability of trained observers to cope with a demanding task as the number of TV monitors increases. Finally, we will develop an automated system which will be able to analyse the input from any urban CCTV camera in order to alert operators to look at that video stream - at present, most CCTV video streams are not observed by anyone since there are too many cameras for the number of human observers. Therefore, an automated alerting system is greatly neeeded and this project constitutes the best attempt to date to produce one.",,
7,F31957FC-EE08-4CB3-8D54-D0CB80F341E3,Workshop on the Future of (UK) Fuzzy Systems Research,"The Workshop on the Future of (UK) Fuzzy Systems Research will providea forum for discussions and debate into the strategic directions offuture research on fuzzy systems and also, for tutoring and trainingof UK-based research students who are working on the relevantsubjects. It is to be held in conjunction with the 2007 IEEEInternational Conference on Fuzzy Systems (FUZZ-IEEE 2007) and the2007 UK Workshop on Computational Intelligence (UKCI 2007). Thisapplication seeks support for three key items in organising andrunning this workshop: 1) expenses for two of the five invitedspeakers, 2) bursaries for fifteen postgraduate research students, and3) costs for a part-time administrative assistant to liaise betweenthe three events and to assist administration of the distribution ofbursaries.",,
8,8C679F14-C5AE-49C4-98A4-6EEDA7CEF012,Operational Research for Context Aware Intrusion Detection,"Computer security and intrusion detection systems are key areas for the future growth and prosperity of the UK. The largest and most difficult problem in the computer security industry today is how to deal with the volume of information as too many false attacks are being reported.The optimisation of intrusion detection, seen through the eyes of an Operational Researcher, can be achieved through appropriate mathematical models, similar to resource allocation problems such as Set Covering. Set Covering Problems are a staple of combinatorial optimisation and scheduling research. They are both mature areas, where current research has advanced to such levels that real-world problems can be solved successfully by using the latest mathematical modelling and heuristic optimisation techniques.It is the aim of this Fellowship to fuse Operational Research and intrusion detection and then transfer the successes of the former into the latter. Through careful mathematical modelling, I intend to transform the intrusion detection problem into a quasi Set Covering problem. I will then use my theoretical results and experience from this area to optimise the central processes. In essence, this will create a context aware intrusion detection system.An important aspect of the Fellowship is the opportunity to broaden my expertise to encompass other areas, notably mathematical modelling of intrusion detection. Such expertise is currently largely absent in the computer security community. I believe that the Fellowship will leave me in an extremely well placed position to start a unique research area at the intersection between Operational Research, optimisation and computer security.The Fellowship is supported by industrial collaborators (100,000+) and the University of Nottingham (70,000+ and PhD studentship).",,
9,A60CECE6-51DF-4433-9A63-37F3E9E62402,Warmstarting Techniques for Stochastic Programming Problems solved by Interior Point Methods,"Uncertainty in the data is a commonly observed phenomenon inoptimization problems with an application background. Commonlyoccurrences of uncertainty is the use of forecasted prices ordemands. It can be argued that nearly all practical optimizationproblems display uncertainty in the data, even if this is not madeexplicit in the chosen solution method. Applications such as networkoptimization in telecommunications, production planning and portfoliooptimization are of special interest to us.The stochastic programming approach to optimization under uncertaintyaims to take all possible future outcomes into account, weighing themwith their respective probabilities. This is achieved by use of anevent tree to approximate the underlying stochastic process. Itsstrength lies in the possibility to model risk-exposure, leading tomore robust model solutions. One of its weaknesses is the fact thatstochastic programming leads to problems with very large dimensions,making their solution challenging.Interior point methods (IPM) have emerged as a powerful solutionapproach for stochastic programming problems, being applicable togeneral formulations and making nonlinear problems tractable. Anappealing idea to speed up the solution of stochastic programmingproblems is to exploit the structure of the problem to construct andsolve an approximation (which is faster to solve) and use this toguide the solution process of the full problem. Unfortunately IPMsare well known for their difficulty in exploiting such advancedstarting point information. Despite progress in the theory andpractice of warmstarting IPMs further work is needed; it is our beliefthat major improvements can only be achieved by exploiting the problemstructure in applications like stochastic programming.The aim of this project is to speed up the solution of stochasticprograms by IPMs through a crash-starting scheme that uses asimplification of the original problem to construct a near-optimalsolution of the full problem and uses this to warmstart the interiorpoint method.The developed methodology will also be applied to dynamically adaptthe event tree during the solution process. The solution of a first coarseapproximation can be used to identify regions in which the event treeneeds to be refined and this refined model can be solved quickly fromthe coarse solution using the techniques developed earlier.Our emphasis will be on the development of a practical algorithm forthe fast solution of stochastic programms as well as theoreticalanalysis leading to a bound on the complexity of the resultingwarmstarted algorithm, improving on the known, more general results.The exploitation of parallelism in solution algorithms will becomeincreasingly more important as microprocessor manufacturers are forcedto move to multi-core architectures rather than increasing processorspeed. Therefore the efficient parallelisation of the developedalgorithms will be a focus point of our research.It is anticipated that this research will lead to faster solutionmethods for large stochastic programming problems while keeping theflexibility in modelling offered by using an interior point approachto solve the problem.A further consequence of this research will be a better understandingof the warmstarting properties of interior point methods. This area isof direct interest in many related research areas such as integerprogramming, nonlinear programming, and multicriteria optimization,where IPM have not yet had as large an impact due to theirwarmstarting difficulties.Ultimately the availability of better solution methods for largenonlinear stochastic programming problems will lead to wider adoptionof this methodology in applications, in turn leading to more robustsolutions being implemented.",,
10,E11466E3-65F3-4DFF-9F06-DD4EE9ED3AD9,Computational Logic of Euclidean Spaces,"Much of the spatial information we encounter in everyday situations isqualitative, rather than quantitative, in character. Thus, forinstance, we may know which of two objects is the closer withoutmeasuring their distances; we may perceive an object to be convexwithout being able to describe its precise shape; or we may identifytwo areas on a map as sharing a boundary without knowing the equationthat describes it. This observation has prompted the development,within Artificial Intelligence, of various formalisms for reasoningwith qualitative spatial information.Although substantial progress has been made in analysing themathematical foundations and computational characteristics of suchformalisms, most of that progress has centred on systems for reasoningabout highly abstract problems concerning (typically) arbitraryregions in very general classes of topological spaces. But of course,the geometrical entities of interest for practical problems are notarbitrary subsets of general topological spaces, but rathermathematically very well-behaved regions of 2 and 3-dimensionalEuclidean space; moreover, the geometrical properties and relationsthese problems are concerned with are typically not merely topological, butrather affine or even metric in character. Together, these factorsseverly limit the practical usefulness of current qualitative spatialreasoning formalisms. Overcoming this limitation represents anexciting mathematical and computational challenge.We propose to meet this challenge by drawing on developments inmathematical logic, geometrical topology, and algebraic geometry thatthe spatial reasoning literature in AI has so far failed fully toexploit. Specifically, we shall investigate the computationalproperties of spatial and spatio-temporal logics for reasoning aboutmathematically well-behaved regions of 2- and 3-dimensional Euclideanspace. We shall develop and implement algorithms for reasoning with these logics. This investigation will illuminate the important relationships betweenhitherto separate research traditions, provide new techniques foraddressing challenging problems in the mathematical geometry, andyield logics of direct relevance to practical spatial reasoningproblems.",,
11,B2E21596-C3E2-4B83-AEBB-9569C4A9A3D0,Automatic target cost and database design for unit-selection speech synthesis,"We propose to replace three components of a typical concatenativespeech synthesiser: the text selection algorithm (what to record forthe database), the target cost function (which units to select fromthe database) and the backoff strategy (what to do when the databasedoes not contain the desired unit).These components are currently designed independently using humanintuition. This is very hard, can only be done by experts, and meansthat each component is unlikely to be optimised with respect to theothers. We propose to base these three components on a singleunderlying model. The model will learn, from data, which speech unitsare perceptually interchangeable. This information will then be usedby the target cost function / backoff strategy, and when selecting thetext to be recorded. The proposed techniques will be implemented inthe Festival 2 speech synthesis system and evaluated using formallistening tests.We break down the research programme into three phases. In Phase 1, wewill gain a deeper understanding of current techniques. In Phase 2, wewill examine techniques for learning just the target cost/backoffstrategy, given an existing voice, then for learning thetext-selection algorithm for a given target cost/backoffstrategy. Finally, in Phase 3, we will devise a method for jointlylearning both together.",,
12,6CD018C1-E2B9-4AE9-9385-319A245BBDA8,New Algorithmic Techniques for Cancer Informatics,In this project we will use advanced data analysis techniques to uncover new biological insights within cancer research. Some of these new algorithmic methods are of our own invention and some new developments are proposed. These new methods include techniques to search for fusion genes: these are genetic anomalies if which two genetic coding regions are fused to produce a novel chimeric protein which causes cell proliferation. These and other targets may be found using new algorithmic methods which search through the large amounts od adat currently appearing in the biomedical sciences.,,
13,C1A73197-6F58-4D42-A78E-FC1816148962,Algorithmic Issues in Power Management by Speed Scaling (APM),"New technologies for mobile devices like 3G and Wi-Fi has brought new life changing user experiences. However, this development can be hindered by battery life, for instance, using 3G communication can shorten the talk time of mobile phones by up to 75%. Battery capacity is not able to catch up with the continuous growth of power requirements for devices. Furthermore, a large amount of heat is generated by device operation. In general, the more powerful the device is, the more heat is generated. Overheating can damage the life of electronic devices. Therefore, power management has imposed important design constraints on modern computing devices. To reduce energy consumption without sacrificing performance significantly, energy awareness'' becomes a crucial concept: a system should only deliver the required service so as to avoid superfluous energy consumption.Dynamic voltage / speed scaling (DVS) becomes a common technique to manage power consumption, e.g., current processors from AMD, Intel and Transmeta allow the processors to operate at various processor speeds. The motivation of DVS is due to the well known cube-root-rule which states that the power consumed is roughly the cube of the operating speed. Simply having processors that support DVS does not solve the problem because the most important question is How to dynamically adjust the speed of processor to maintain performance with the minimal energy and temperature?'' Although some algorithmic solutions have been proposed, most of them assume that the processor can operate at any (unbounded) speed, which is obviously not the case in practice. Furthermore, the models considered so far optimize either energy or temperature but not both. The current proposal aims at providing algorithmic solutions so as to facilitate more powerful, yet energy effective, devices. Specific objectives include1. developing an accurate abstract model of energy and temperature efficient scheduling,2. designing algorithms with mathematically provable performance guarantees, and3. providing a comprehensive evaluation of the proposed algorithms.The success of this project will impact on extending the battery life of mobile devices thus improving the services (e.g., multimedia) that they can support. The experience of Dr.\ Wong on job scheduling, in particular, her preliminary study on energy efficient deadline scheduling serves as a good foundation and is expected to be crucial for the success of the project.",,
14,83655B5D-C346-49E6-A325-F8228BD7A4B6,Novel line search procedure for very large scale optimisation,"In this work we are interested in the solution of general optimisation models comprised of a nonlinear objective function and subject to general nonlinear equality and inequality constraints. Of particular interest is the solution of very large scale problems arising in industrial and scientific research models, such as optimal control problems, molecular design, protein folding and a host of other areas requiring the availability of advanced solvers. Engineering models, particularly ones arising from process modelling, contain a very large proportion of equality constraints as well as operational inequality constraints and bounds, encapsulating limits in control parameter values and quality and operability satisfaction. The field of nonlinear optimisation has enjoyed many inspiring ideas in its many years of existence, and engineering practice as a result has benefited greatly by the availability of such methods within standard commercial simulation packages.This proposal intends to develop further on a new concept for very large scale optimisation developed in our research group. This methodology involves a novel line search procedure proposed originally by the principal investigator and co-workers, which for the first time presents a true non-monotone aspect over both objective and constraint values. Early results indicate that such attributes allow the optimisation solver to take fewer steps towards an optimum point and even to escape from local minima in some cases.",,
15,A63CA61F-9CD7-4BAC-BB0A-46A27F7332F5,Verifying requirements for resource-bounded agents,"The project will provide theoretical foundations and practical tools foranalysing resource requirements for systems of reasoning agents, suchas, for example, agents which reason using ontological rules to providea web service. The resources we shall consider include the time,memory, and communication bandwidth required by the agents to select thenext action to perform.The trend towards ever smaller agent platforms means that resourceutilisation is becoming an increasingly important factor in agent design and deployment.However the complex, often distributed, derivations implied by modernagent designs make it hard for agent developers to predict system resourcerequirements a priori. The development of formal frameworks andpractical verification tools to exploit them is therefore key to thesuccessful development of provably correct agent designs for emergingresource-limited agent paradigms such as mobile agents and platformssuch as PDAs and smart phones.The project builds on the previous joint work between the investigators andvisiting researchers using model-checking techniques to verify requirements.",,
16,E9BA604A-7BBB-4E9D-9BB6-CF4C0A45D62F,D-SCENT: Raising challenges to deception attempts using data scent trails,"Since 9-11 and 7-7, terrorism has been a major public concern. To ensure public safety and to protect the UK economy, research is needed that offers new methods to foil attacks before they are executed, to identify people and networks who might be preparing for or undertaking an attack, and to provide clear evidence that can be used to justify questioning, arrests and prosecutions. In this study, we will investigate whether deception can be identified and proved from 'scent trails', that is, coherent accounts of suspects' activities over time compiled from tracking their movements, communications and behaviours. We will develop software to derive inferences about what activities are consistent with suspects' scent trails and what are ruled out. These inferences will allow investigators to challenge suspects, both in real time (e.g., to encourage suspects to abandon an ongoing attack) and during interviews (e.g., to point out inconsistencies between a suspect's account and scent trail evidence that might change the course of an interview). The project will investigate scent trails in the context of people undertaking deceptive activities to gain advantage in adversarial 'treasure hunt'-type games. The games will be developed in consultation with stakeholders to provide a non-sensitive analogy to counter-terrorism contexts. Players, typically undergraduate students paid for participation, will be monitored during games via positional and communication data obtained from mobile devices enabled with geospatial positioning devices. Novel software for integrating these data will be developed to build up scent trails of players' activities during game play. Methods of artificial intelligence will be combined to derive inferences from the scent trails about what kinds of activity are possible and impossible given a player's location, trajectory, activities and links with others. We envision games with 3 teams: Team A represent the adversary, Team B the police or general public, and Team C the intelligence services. Team A scores points by visiting target locations within a time limit under a set of game rules that they must violate if they are to win. They must try to hide rule violations from Team B, who score points by preventing or identifying Team A's deceptions successfully. Team C can challenge Team A by sending them indications of the scent trails that are held or can feed Team B intelligence information. Moreover, the inferences from scent trails will support Team C in deciding how best to prove or falsify a suspicion during an interview with Team A players at key points during the games. By conducting observation of players during games, we can investigate how people change their behaviours when they are confronted with evidence that reveals their deceptions. We will also interview players at key points during games as a simulation of interviews with suspects, eliciting from players accounts of their activities before presenting them with challenges based on their own scent trails that are either consistent or inconsistent with legal game playing. This will allow interview and analysis techniques to be improved and will provide clues as to how people subsequently change their behaviour after they have been confronted with their deception. The results will also allow us to test between hypotheses deriving from forensic psychology as to how best to detect deception. The research also allows us to explore public awareness of, and response to, monitoring and surveillance in counter-terrorism. With an advisory panel of stakeholders and subject specialists representing key public and academic bodies, we will identify ethical and legal issues associated with collecting and using data on peoples' movements through public spaces. We will also conduct questionnaire studies with game players and others not involved in the games, to measure attitudes to monitoring and surveillance in game-playing and other contexts.",,
17,3F170652-3CDF-409D-A96F-D6163D3145EC,D-SCENT: Raising challenges to deception attempts using data scent trails,"Since 9-11 and 7-7, terrorism has been a major public concern. To ensure public safety and to protect the UK economy, research is needed that offers new methods to foil attacks before they are executed, to identify people and networks who might be preparing for or undertaking an attack, and to provide clear evidence that can be used to justify questioning, arrests and prosecutions. In this study, we will investigate whether deception can be identified and proved from 'scent trails', that is, coherent accounts of suspects' activities over time compiled from tracking their movements, communications and behaviours. We will develop software to derive inferences about what activities are consistent with suspects' scent trails and what are ruled out. These inferences will allow investigators to challenge suspects, both in real time (e.g., to encourage suspects to abandon an ongoing attack) and during interviews (e.g., to point out inconsistencies between a suspect's account and scent trail evidence that might change the course of an interview). The project will investigate scent trails in the context of people undertaking deceptive activities to gain advantage in adversarial 'treasure hunt'-type games. The games will be developed in consultation with stakeholders to provide a non-sensitive analogy to counter-terrorism contexts. Players, typically undergraduate students paid for participation, will be monitored during games via positional and communication data obtained from mobile devices enabled with geospatial positioning devices. Novel software for integrating these data will be developed to build up scent trails of players' activities during game play. Methods of artificial intelligence will be combined to derive inferences from the scent trails about what kinds of activity are possible and impossible given a player's location, trajectory, activities and links with others. We envision games with 3 teams: Team A represent the adversary, Team B the police or general public, and Team C the intelligence services. Team A scores points by visiting target locations within a time limit under a set of game rules that they must violate if they are to win. They must try to hide rule violations from Team B, who score points by preventing or identifying Team A's deceptions successfully. Team C can challenge Team A by sending them indications of the scent trails that are held or can feed Team B intelligence information. Moreover, the inferences from scent trails will support Team C in deciding how best to prove or falsify a suspicion during an interview with Team A players at key points during the games. By conducting observation of players during games, we can investigate how people change their behaviours when they are confronted with evidence that reveals their deceptions. We will also interview players at key points during games as a simulation of interviews with suspects, eliciting from players accounts of their activities before presenting them with challenges based on their own scent trails that are either consistent or inconsistent with legal game playing. This will allow interview and analysis techniques to be improved and will provide clues as to how people subsequently change their behaviour after they have been confronted with their deception. The results will also allow us to test between hypotheses deriving from forensic psychology as to how best to detect deception. The research also allows us to explore public awareness of, and response to, monitoring and surveillance in counter-terrorism. With an advisory panel of stakeholders and subject specialists representing key public and academic bodies, we will identify ethical and legal issues associated with collecting and using data on peoples' movements through public spaces. We will also conduct questionnaire studies with game players and others not involved in the games, to measure attitudes to monitoring and surveillance in game-playing and other contexts.",,
18,DBD62254-FB98-4791-85EC-16B5C6E5A131,UK Knowledge Discovery in Data Symoposium,"This proposal requests support, in the form of student bursaries, for the organisation and running of UKKDD'07 (the UK Knowledge Discovery in Data Symposium to be held in April 2007). The principal aim of the symposia is to progress the momentum established at the inaugural and second symposia held in April 2005 and 2006 respectively. The establishment of a UK series of KDD symposium is seen as a new initiative whose objective is to bring together the various practitioners and research groups in the UK working both commercially or academically within the discipline of KDD and its related enterprises. The symposium seeks to act as a show case for the current state of the art of UK KDD work, and to identify opportunities and establish partnerships to maintain the UK's standing within the international KDD community. Many of the leading UK experts on KDD will be presenting a collation of views and reports on current KDD topics.One of the final report assessors for the EPSRC support received for the 2005 symposium (which received an overall rating of outstanding ) commented that: The UK has successfully begun to develop a core research competence in [KDD] and the UK KDD Smposium has helped to strengthen this community. KDD is esswential not only for improving business practioce but also in undertsrtanding the large quantities of data being collected in medicine, biology, astronomy and most other sciences .",,
19,62969657-72D7-4E65-8C12-EBAB83E84864,Towards More Effective Multi-objective Meta-Heuristics to Solve Complex Combinatorial Problems,"This research project proposes the investigation of a number of challenging ideas in the field of multi-objective combinatorial search. Most of the research in this area has been based on recycling knowledge acquired from research on the single-objective case and this has inspired the extension of many single-objective techniques to their multi-objective variants. Many of these are extensions from evolutionary techniques such as genetic algorithms, evolutionary strategies, particle swarm optimisation and others. There are some extensions of other meta-heuristics such as tabu search and simulated annealing to multi-objective variants. Evolutionary approaches have received most of the attention in multi-objective heuristic search but I believe that a wider range of meta-heuristic techniques should be investigated. The research themes proposed here represent a considerable shift in emphasis. Specifically, the aim is to conceive more effective multi-objective meta-heuristics to tackle complex combinatorial problems in a more effective and efficient manner than the current state of the art is capable of. This proposal aims to re-think the design of multi-objective meta-heuristics by developing them within the multi-objective paradigm instead of modifying known single-objective approaches.",,
20,97B229FA-4563-485E-8807-B9A81CCB55BB,Composing and decomposing ontologies: a logic-based approach,"In computer science, ontologies are used to provide a common vocabulary for a domain ofinterest together with descriptions of the meaning of terms built from the vocabulary and relationships between them, like in an encycopedia. Ontologies in this sense are increasingly used in knowledge management systems, medical and bio-informatics and are set to play a key role in the semantic web and grid. In order to be computer-accessible, modern ontologies are formulated in an ontology language based on description logics (DLs) such as OWL. Current applications are leading to the development of large and complex ontologies (sometimes more than 300,000 different terms). Engineering and maintaining such ontologies is a complex task, and it has to be carried out with care for the ontology to be of use. It may involve a group of ontology engineers and domain experts co-operating in order to design the ontology, update it to reflect changes/developments in the domain, and integrate it with other ontologies so as to cover larger domains. Using a DL based ontology language such as OWL has two advantages: (1) they have an unambiguous semantics , which means that the meaning of terms is specified in unambiguous way, thereby enabling shared understanding of an ontology; and (2) we can make use of reasoning services of DL reasoners for ontology engineering and usage. The availability of these services has already changed how ontology engineers work. However, these services are not sufficient for engineering and maintaining large ontologies, especially in the collaborative case. Local changes to an ontology, and interactions between such changes, can have highly non-local effects that are currently unpredictable. The only time to examine these effects is after the changes have been made, in the light of all the proposed changes. And even then there are changes whose impact is not detectable using the current suite of reasoning services. To sum up, the state of ontology engineering is very similar to the state of software engineering before the advent of structured programming techniques: ontologies cannot be decomposed intosemantically distinct components, we cannot predict the scope of a (local) change, and how to re-use parts of ontologies or safely compose them are open problems. In software engineering,human documentation and rigorous process restrictions were put into place, as well as preliminary mechanisms for structuring programs. As these mechanisms have grown more sophisticated, they have led to new automated techniques for transforming programs for performance, understanding, and re-use. It has been convincingly argued that methodologies and algorithmic support for composing anddecomposing ontologies in a controlled way will be the key to supporting collaborative ontology engineering and re-use. More precisely, we plan to develop methodologies and algorithmic support for (T1) developing ontologies with interfaces (and acceptable restrictions on their usage) which guarantee that, if such an ontology is composed with other ontologies, it neither corrupts nor is corrupted by the ontologies they are composed with; (T2) evaluating the consequences of the composition of a set of given ontologies which may have been built in a completely unrestricted way; (T3) decomposing a large ontology into modules that can be edited in a controlled way.Above, terms such as ``in a controlled way'', ``corrupted ontology'', and ``consequence'' are deliberately left rather vague. Indeed, one goal of this project is to provide rigorous but practical and useful formal specifications of (T1)--(T3) above. It is only very recently that research in this direction has been carried out and the proposers have already made some pioneering contributions towards their actual development. The proposed approach is a continuation of this line of work, i.e., it is logic-based and founded on the notion of conservative extensions .",,
21,E8C07614-E8FA-4325-A7EE-6882871518EB,Watched Literals and Learning for Constraint Programming,"The following text is rated 67 on the Flesch reading-ease scale, with 60-70 the target for a general audience.Constraints allow us to express many facts in every day life and in puzzles. Think about the recent craze for the Sudoku puzzle. We have to put a number from 1 to 9 in each square of a 9x9 grid. The constraints are that the numbers in each row, column, and 3x3 square must all be different. Researchers in Constraint Programming study how to solve problems like this using computer programs, so that people do not have to solve them themselves. For Sudoku, that might take the fun out of it, but it is less fun scheduling aircraft to arrive at the right gate in a safe and economic way, and the consequences are more serious if you get it wrong. There are many other important problems which Constraint Programming can help with. In this research we will address a number of the most important questions underlying constraint programming. We hope to come up with new answers to old questions, as well as asking new questions which perhaps should have been asked a long time ago. The first question we will look at is how to deduce new facts from old. In constraint programming this is called propagation . In Sudoku, what should you do if you work out that a certain square cannot have a 9 in it? If you can work out any new facts from this, you want to do this as quickly and easily as possible. On the other hand, if there are no new facts to work out, what you'd like to do is nothing! We have recently developed a new way of doing propagation, to give us more chance of doing nothing when there is no chance of working out new facts. The technique is called watched literals . It is obviously better to do nothing instead of something, and so watched literals can make constraint programs run a lot faster. To show the real value of watched literals, we need to do a lot of work on showing how more and more different kinds of constraints can be made to work with them. We also need to understand the general properties of watched literals, to make it easier for us and others to develop new ways to propagate using them. The result should be better constraint programs.The second question we will look at is how to do some of the most basic operations in constraint programming. Some of the tasks we are looking at may take only fractions of a microsecond to do on a modern computer, but we still want to make those fractions as small as possible. To do this, we have to understand in excruciating detail what goes on every time a constraint program does something like checking to see if a number is still allowed to be put in a certain square on the Sudoku grid. Then we have to work out a lot of different ways of doing a simple task like this, build different constraint programs using each way, and perform experiments to see how each one performs. Then we are in a position to tell researchers in constraint programming how the most basic tasks should be done. This kind of work sounds basic, but it is basic in the sense of fundamental. This kind of fundamental research will let us, and everyone else in constraint programming, do their future work better. The third and final question we will look at is how to make constraint programs learn from their own mistakes. Any constraint program makes a lot of mistakes, maybe even billions, before finding the right answer. By learning from the early mistakes, we can get the constraint program to avoid making a lot of the later ones. This idea is very well understood, but has not yet made its way into the fastest constraint programs. The idea of watched literals that we mentioned earlier should marry very well with learning, so we will research how to do this. We think this is the ideal task for a PhD student to work on, building on the research of others while doing their own first piece of world-class research.",,
22,CE233B58-3D9D-45FC-A07B-E9EC15D1A4E3,Intelligent access to foreign data models,"The Virtual Observatory (VO) project within astronomy has two core problems: how to find data from scattered, and often under-resourced, archives, and once it is found how to make use of it, given that different archives will generally have significantly different models of how their data is structured. The High-Energy Physics (HEP) community doesn't really have the first problem -- there are rather fewer important accelerators and detectors, and so fewer data sources -- but does have the second, since different facilities have different, and necessarily inflexible, ideas about how to structure the data they produce.One approach is to convert data into and out of a consensus model, but it is proving unexpectedly difficult in practice for the VO community (incorporated in the International VO Alliance, or IVOA) to agree on such a model which is simultaneously rich enough to be useful, and simple enough that consensus is possible. The HEP community has only recently started to work on this problem, which in that context is referred to as the problem of finding common metadata (data about data) covering the datasets which are available. On top of the problems of reaching consensus, the VO has discovered that it actually needs more than one data model, to deal with the separate cases of searching for data (`find me all the datasets which have such-and-such a type of data within them'), and driving the processing of that data once it is found (`here is some data; do the right thing with it').This project aims to use a radically different and adventurous aproach, making it possible for software to extract from a given dataset the information which is relevant to that software. This approach avoids relying on a difficult consensus, and recognises that partial and indirect understanding of a dataset's structure can, in important cases, be enough for the software to do its work.This different approach avoids aiming for a possibly illusory consensus. Instead, we hope to make it easy for both data centres and the developers of data processing applications to precisely articulate the data model which they themselves have, and then to allow them to produce a formal description of how this relates to other data models which they know about. At this point it becomes possible for an application to say `I know about data models A and B, but I don't know about this new one. However I see that someone has described how this new model is related, at least partially, to A's and B's models, so I can use that knowledge to process the new data'. The relationship of the new data model to the old one might have been described by the maintainers of models A or B, or by the providers of the new data, or even by a third party who has had to work it out either using published sources, or by conversation with the new data's providers. Of course, there are issues about the authority of these descriptions, and the handling of conflicting information, but these are interesting challenges.This approach builds on both well-established Artificial Intelligence (AI) and Knowledge Engineering (KE) work, as well as on the emerging technologies from the Semantic Web community. It separately builds on work from the Information Retrieval (IR) community on how to work with distributed, heterogeneous and uncertain data.",,
23,E7B61D0B-478C-4461-8E5A-42DED551BEB9,AEDUS2: Adaptable Environments for Distributed Ubiquitous Systems,"This is request for renewal for a further 5 years for a Platform Grant to support personnel and travel for the Distributed Software Engineering Group within the Department of Computing, Imperial College London. This is a world-renowned group which combines practical work on building tools for design and implementation of adaptive ubiquitous and distributed systems and networks with more formal software engineering approaches to behaviour modelling, requirements specification and analysis, language semantics and type systems for distributed programming. This proposal will provide support for cohesion and continuity of funding for a number of experienced researchers as well as permitting short-term evaluation of new ideas and experiments. Projects will focus on a sound Software Engineering approach to adaptive software environments for autonomic systems, policy-based trust, privacy and security, as well as requirements engineering for ubiquitous systems. The proposal will also fund an experienced researcher to provide overall project management and help coordinate the effort required for packaging and maintaining experimental software to make it more accessible to external academic and industrial users.",,
24,E45B6C47-09A6-424E-9FAE-EF8D2C956A47,D-SCENT: Raising challenges to deception attempts using data scent trails,"Since 9-11 and 7-7, terrorism has been a major public concern. To ensure public safety and to protect the UK economy, research is needed that offers new methods to foil attacks before they are executed, to identify people and networks who might be preparing for or undertaking an attack, and to provide clear evidence that can be used to justify questioning, arrests and prosecutions. In this study, we will investigate whether deception can be identified and proved from 'scent trails', that is, coherent accounts of suspects' activities over time compiled from tracking their movements, communications and behaviours. We will develop software to derive inferences about what activities are consistent with suspects' scent trails and what are ruled out. These inferences will allow investigators to challenge suspects, both in real time (e.g., to encourage suspects to abandon an ongoing attack) and during interviews (e.g., to point out inconsistencies between a suspect's account and scent trail evidence that might change the course of an interview). The project will investigate scent trails in the context of people undertaking deceptive activities to gain advantage in adversarial 'treasure hunt'-type games. The games will be developed in consultation with stakeholders to provide a non-sensitive analogy to counter-terrorism contexts. Players, typically undergraduate students paid for participation, will be monitored during games via positional and communication data obtained from mobile devices enabled with geospatial positioning devices. Novel software for integrating these data will be developed to build up scent trails of players' activities during game play. Methods of artificial intelligence will be combined to derive inferences from the scent trails about what kinds of activity are possible and impossible given a player's location, trajectory, activities and links with others. We envision games with 3 teams: Team A represent the adversary, Team B the police or general public, and Team C the intelligence services. Team A scores points by visiting target locations within a time limit under a set of game rules that they must violate if they are to win. They must try to hide rule violations from Team B, who score points by preventing or identifying Team A's deceptions successfully. Team C can challenge Team A by sending them indications of the scent trails that are held or can feed Team B intelligence information. Moreover, the inferences from scent trails will support Team C in deciding how best to prove or falsify a suspicion during an interview with Team A players at key points during the games. By conducting observation of players during games, we can investigate how people change their behaviours when they are confronted with evidence that reveals their deceptions. We will also interview players at key points during games as a simulation of interviews with suspects, eliciting from players accounts of their activities before presenting them with challenges based on their own scent trails that are either consistent or inconsistent with legal game playing. This will allow interview and analysis techniques to be improved and will provide clues as to how people subsequently change their behaviour after they have been confronted with their deception. The results will also allow us to test between hypotheses deriving from forensic psychology as to how best to detect deception. The research also allows us to explore public awareness of, and response to, monitoring and surveillance in counter-terrorism. With an advisory panel of stakeholders and subject specialists representing key public and academic bodies, we will identify ethical and legal issues associated with collecting and using data on peoples' movements through public spaces. We will also conduct questionnaire studies with game players and others not involved in the games, to measure attitudes to monitoring and surveillance in game-playing and other contexts.",,
0,FFE6D8EA-D9C4-41F3-8BBA-A7DD06113CA0,Durham Agents 2007: 9th EASSS and 1st MALLOW,"This is a case for support for an academic event in the area of multi-agent systems. The event is called ``Durham Agents'007'' and will take place from 27th of August to 7th of September, 2007. In the first week, the 9th European Agent Systems Summer School will take place. In the second week, 5 workshops on various areas of research in multi-agent systems will be run as part of an event called MALLOW. We request support to reduce the price of registration for post-graduate students attending the summer school and for travel costs for the MALLOW invited speakers.",,
1,C53421EA-E24B-4F83-AC65-E04BDAAA054A,International Workshop on Self Organizing Systems (IWSOS) 2007 - Conference Organisation &amp; Travel Grants for International Speakers &amp; Panel Members,"The Computing Department at Lancaster University is organizing the 2nd International Workshop on Self-Organizing Systems (IWSOS 2007) in the English Lake District from 11th / 13th September 2007. The workshop theme is New Trends in Network Architectures and Services , and will be mainly concerned with the different aspects linked to self-organized systems, networks and architectures (additional details can be found on the workshop Web site at http://iwsos.comp.lancs.ac.uk/). This workshop is a major international event, and is being held in the UK for the first time this year. It has historically attracted over 70 participants from academia, industry and government organisations from all over the world, with a similar number are expected to attend this year's event. This EPSRC Workshop grant will enable us to enhance the programme of the workshop with a selection of international keynote speakers, each a renowned expert in the field of self-organizing systems, thus in turn, exposing UK researchers to the current state of the art research at an international level.",,
2,684282D0-957E-4C4F-90CA-6B7C7157E4B8,Reasoning Infrastructure for Ontologies and Instances,"Ontologies help both humans and computer applications to communicate by providing a vocabulary of terms together with formal and computer-processable descriptions of their meanings and the relationships between them. They play a major role in the next-generation World Wide Web (known as the Semantic Web), where they are used to describe the content of Web resources, with the aim of both improving search for human users and making it easier for computer programs to exploit the vast range of information that is available on the Web. Ontologies are also widely used to define specialised vocabularies for use in medicine, biology and other scientific disciplines.Ontologies are usually developed by human experts, but even for experts the job of defining all the relevant terms is a difficult and time consuming one. It is therefore essential to provide intelligent tools that support ontology designers. For this reason, many ontology languages, including OWL (the standard language used for Semantic Web ontologies), are based on Description Logics (DLs). This provides a formal specification of the meaning of the language and allows tools to use automated reasoning systems, e.g., to check that interactions between descriptions do not lead to logical contradictions. Reasoning systems are also needed when ontologies are deployed in applications, where they are used, e.g., to answer queries that use terms defined in an ontology.The central role of ontologies in the above mentioned applications brings with it, however, requirements for expressive power and reasoning support which are beyond the capabilities of existing ontology languages and reasoning systems. For example, OWL cannot express the fact that the brother of a person's father is also their uncle, and even for OWL, reasoning is very hard: existing reasoning systems often have difficulties dealing with the very large ontologies that are needed in many realistic applications. The research described in this proposal aims at bridging this gulf between requirements and capabilities; its ultimate goal is the development of a reasoning system that significantly extends the current state of the art, with respect to both scalability and the expressive power of the ontology language supported.The research programme will be made up of three main strands: The first strand will focus on reasoning about the structure of the domain as described in an ontology, and aims to develop a highly optimised class reasoner for the expressive description logics needed to provide reasoning support for applications using existing ontology language standards and proposed extensions. The second strand aims to combine a DL reasoner with a database in order to provide scalable reasoning for large volumes of data that are are described using terms from an ontology. The third strand aims at collaborating with ontology developers and users in order to evaluate the effectiveness of the above systems using data from their ontologies and applications, in particular the Gene Ontology (and other large biomedical ontologies) and large volumes of gene product data annotated with terms from the Gene Ontology.",,
3,6D645E22-7B34-464B-B644-89B872B1B146,(Semi)Formal Artificial Life Through P-systems &amp; Learning Classifier Systems: An Investigation into InfoBiotics,"Artificial Life (ALife) has advanced enormously since A. Turing proposed in the early 50s models of pattern formation in living systems. It was Turing who first demonstrated how a simple system of coupled reaction-diffusion equations could give rise to spatial patterns in chemical concentrations through a process of chemical instability. J. von Newman, later, demonstrated that it was possible to build self-replicating abstract machines while A. Lindenmayer introduced L-systems for modelling artificial plants. The bulk of ALife research in the last 20 years has been done with a more ad-hoc bottom-up engineering approach by designing or evolving the rules that govern the local interactions of the entities in the system as to produce certain emergent behaviour. Emergence in this context is interpreted as a process within the system that could not have been predicted from merely inspecting the rules but that it is observed only by running the simulation. Some of the earliest landmarks in ALife were T. Rays' Tierra, J. Holland's Echo and L. Yaeger's Polyword. These early systems were all based on an individual based modelling framework, which were highly abstract and quite limited in the simulated details (i.e. physical and chemical laws) of the environment where the agents performed their interactions. K. Sims's virtual creatures and research like framsticks or swimmers incorporated a more accurate (albeit still arbitrary) physical reality into the ALife system. In turn, this increase in the detail of the environmental interactions allowed richer emergent processes to be observed. More recent work incorporated a more detailed biology through the addition of developmental processes, differential gene expression and genetic regulatory networks endowing ALife simulations with greater realism. Thus, as computing resources became more accessible and our biological knowledge deepened, more and more levels of biological, chemical and physical details were included in a bottom-up fashion into ALife simulations. Recent advances in analytical biotechnology, computational biology, bioinformatics and micro-biology are transforming our views of the complexity of biological systems, particularly the computations they perform (i.e. how information is processed, transmitted and stored) in order to survive, adapt and evolve in dynamic and sometimes hostile environments. We propose to capture some of these more recent biological insights, in particular those related to cell biology, as to develop sophisticated ALife simulations of cellular-like systems. Furthermore, while we propose to stick to the traditional engineering approach of building ALife systems from the bottom-up we would like to extend current research practice towards a more computationally formal and rigorous approach to the design and implementation of ALife research. In this proposal we seek a fundamental rethink on the way bottom-up Artificial Life research is conducted. Until now, much of this research has had a strong ad-hoc component with very little formalisations. We propose a new (semi) formal cellular Artificial Life methodology, which we call InfoBiotics. InfoBiotics proposes that a synergy between formal informatics methods, evolution and learning and biological and biochemical insights are a pre-requisite for a more principled practice of ALife research. The driving research issues behind this proposal are:i. What combinations of formal informatics, evolutionary and learning paradigms and biochemical insights are needed for a successful development of InfoBiotics as a principled approach to Artificial Cellular Life research? ii. What is the balance of each of the former that is needed in order to ask and, be able to, answer scientifically relevant and meaningful ALife questions from an InfoBiotics perspective?",,
4,B69F6F79-DAD0-4861-B673-6FE26214E53D,Symposium on Frontiers of Combining Systems and Workshop on First-Order Theorem Proving,"This is a proposal for support for the 6th Symposium of Frontiers of Combining Systems (FroCoS)and the 6th International Workshop on First-order theorem Proving (FTP) which will be held on10th-13th September 2007 at the University of Liverpool.Founded in 1996, FroCoS has become the main scientific meeting devoted to research on combining logics and systems in computer science and artificial intelligence. Its aim is to bring together researchers from various areas working on combination problems in order to promote fruitful interaction and development of generic techniques and methods for combination andintegration of special formally defined systems and logics, as well as for the analysis and modularisation of complex systems. Examples of topics presented at FroCoS are combinations of constraint systems and decision procedures, combinations of modal, temporal and epistemic logics and their applications indistributed and multi-agent systems, modularity in software specification and ontologies, and satisfiability modulo theories.The aim of the FTP workshop series is to bring together researchers interested in all aspects of first-order theorem proving, who will present work at the cutting edge of their field. Given the various model theoretical, proof theoretical and practical aspects of first-order theorem proving as well as its many areas of application, FTP covers a wide range of topics. For example, reasoning methods for first-order logic, decision procedures and model building, combinations of logics and systems, reasoning modulo theories, and applications and systems.As is evident from the description of the topics addressed by the two series of events, common interests of FroCoS and FTP are the combination of logics, theories and decision procedures as well as satisfiability modulo theories. It is these common interests that motivate us to co-locate the events.",,
5,C2C212E4-6175-4D92-915F-60EF533A7C3B,Computational Logic of Euclidean Spaces,"Much of the spatial information we encounter in everyday situations isqualitative, rather than quantitative, in character. Thus, forinstance, we may know which of two objects is the closer withoutmeasuring their distances; we may perceive an object to be convexwithout being able to describe its precise shape; or we may identifytwo areas on a map as sharing a boundary without knowing the equationthat describes it. This observation has prompted the development,within Artificial Intelligence, of various formalisms for reasoningwith qualitative spatial information.Although substantial progress has been made in analysing themathematical foundations and computational characteristics of suchformalisms, most of that progress has centred on systems for reasoningabout highly abstract problems concerning (typically) arbitraryregions in very general classes of topological spaces. But of course,the geometrical entities of interest for practical problems are notarbitrary subsets of general topological spaces, but rathermathematically very well-behaved regions of 2 and 3-dimensionalEuclidean space; moreover, the geometrical properties and relationsthese problems are concerned with are typically not merely topological, butrather affine or even metric in character. Together, these factorsseverly limit the practical usefulness of current qualitative spatialreasoning formalisms. Overcoming this limitation represents anexciting mathematical and computational challenge.We propose to meet this challenge by drawing on developments inmathematical logic, geometrical topology, and algebraic geometry thatthe spatial reasoning literature in AI has so far failed fully toexploit. Specifically, we shall investigate the computationalproperties of spatial and spatio-temporal logics for reasoning aboutmathematically well-behaved regions of 2- and 3-dimensional Euclideanspace. We shall develop and implement algorithms for reasoning with these logics. This investigation will illuminate the important relationships betweenhitherto separate research traditions, provide new techniques foraddressing challenging problems in the mathematical geometry, andyield logics of direct relevance to practical spatial reasoningproblems.",,
6,8E6A6F15-1A81-4EF4-AC7D-F2ECA36CA63F,ReOPN: Retargetable Statistical Optimisation and Parallelisation for Network Processors,"Network processors are application specific devices optimised to support network protocols at the highest possible speed. Tight constraints on performance, cost, and energy consumption have led to the development of heterogeneous multi-core architectures with domain specific instruction set extensions and application specific processing elements. However, this adaptation has a strong impact on the programmability of these devices and makes software development a time-consuming and expensive task. This project investigates novel statistical approaches to retargetable and adaptive compiler directed optimisation and parallelisation for network processors to conquer a looming software development crisis caused by the rapidly increasing complexity of network protocols and applications. This research will enable the efficient implementation of high-level programming languages and increase programmer productivity in a highly cost and performance sensitive embedded computing domain.",,
7,902213CB-A0B7-4179-A1AE-35294B47CC9A,Investigating Ontologies for Discrete Event Simulation,"Discrete-event simulation (DES) is a technique that is widely used in the UK to analyse problems in diverse areas such as commerce, defence, manufacturing and transportation. Commercial-off-the-shelf Discrete-Event Simulation Packages (CSPs) are software tools that support the development of discrete-event models of systems and their simulation. A typical simulation study will involve data from several sources, one or more verified and validated models (and their revisions), experiments, results and their interpretation, reports and, perhaps most importantly, people. DES consultants, consultancies and departments within organisations will have many combinations of these that have accrued over the years. The problem is how to effectively organise all these artefacts of DES projects so that their meaning and context within a project can be preserved. The proposed visit by Professor Paul Fishwick presents a unique opportunity to establish the foundations of a novel approach to this problem by investigating the appropriateness of simulation ontologies developed in the USA (Fishwick and Miller, 2004) to the principles and practice of DES. It is anticipated that research issues of simulation interoperability, composability and reuse, and future productivity tools such as search engines for simulation projects will be facilitated by this research.",,
8,C70FBF3B-F2E9-463B-846F-A65B5250D831,Logic for Automated Mechanism Design and Analysis (LAMDA),"In recent years, there has been a dramatic increase of interest in thestudy and application of *economic mechanisms* in computerscience, particularly social choice mechanisms such as votingprocedures, which are used by a group of self-interested agents toselect an outcome from some set of candidates. The *primary aim*of this project is to develop formalisms and associated techniques toassist in the automated verification and synthesis of social choicemechanisms. To this end, we will first develop a social choicelogic, (SCL), intended for the formal specification of socialchoice mechanisms, combining features from several formalisms that wehave previously developed for the specification of cooperativesystems. We will explore the properties of SCL, and thendevelop a social choice mechanism language, (SCML),intended to allow the programmatic modelling of particular socialchoice mechanisms. We will then develop techniques for model checkingSCL-specified requirements against SCML-definedmechanisms, and investigate the complexity of this problem. Todemonstrate the viability of the approach, we will apply our formalismand modelling language to standard social choice mechanisms from theliterature. Finally, we will investigate the synthesis of mechanismsfrom specifications, by means of constructive satisfiability testing.Overall, the project has the potential to bring the samebenefits to the design and analysis of social choice mechanisms thatthe use of temporal logics and model checking have brought to reactivesystems specification and analysis.",,
9,CE4B0FEE-587B-4322-95A3-F1AF8F45E902,Composing and decomposing ontologies: a logic-based approach,"In computer science, ontologies are used to provide a common vocabulary for a domain ofinterest together with descriptions of the meaning of terms built from the vocabulary and relationships between them, like in an encycopedia. Ontologies in this sense are increasingly used in knowledge management systems, medical and bio-informatics and are set to play a key role in the semantic web and grid. In order to be computer-accessible, modern ontologies are formulated in an ontology language based on description logics (DLs) such as OWL. Current applications are leading to the development of large and complex ontologies (sometimes more than 300,000 different terms). Engineering and maintaining such ontologies is a complex task, and it has to be carried out with care for the ontology to be of use. It may involve a group of ontology engineers and domain experts co-operating in order to design the ontology, update it to reflect changes/developments in the domain, and integrate it with other ontologies so as to cover larger domains. Using a DL based ontology language such as OWL has two advantages: (1) they have an unambiguous semantics , which means that the meaning of terms is specified in unambiguous way, thereby enabling shared understanding of an ontology; and (2) we can make use of reasoning services of DL reasoners for ontology engineering and usage. The availability of these services has already changed how ontology engineers work. However, these services are not sufficient for engineering and maintaining large ontologies, especially in the collaborative case. Local changes to an ontology, and interactions between such changes, can have highly non-local effects that are currently unpredictable. The only time to examine these effects is after the changes have been made, in the light of all the proposed changes. And even then there are changes whose impact is not detectable using the current suite of reasoning services. To sum up, the state of ontology engineering is very similar to the state of software engineering before the advent of structured programming techniques: ontologies cannot be decomposed intosemantically distinct components, we cannot predict the scope of a (local) change, and how to re-use parts of ontologies or safely compose them are open problems. In software engineering,human documentation and rigorous process restrictions were put into place, as well as preliminary mechanisms for structuring programs. As these mechanisms have grown more sophisticated, they have led to new automated techniques for transforming programs for performance, understanding, and re-use. It has been convincingly argued that methodologies and algorithmic support for composing anddecomposing ontologies in a controlled way will be the key to supporting collaborative ontology engineering and re-use. More precisely, we plan to develop methodologies and algorithmic support for (T1) developing ontologies with interfaces (and acceptable restrictions on their usage) which guarantee that, if such an ontology is composed with other ontologies, it neither corrupts nor is corrupted by the ontologies they are composed with; (T2) evaluating the consequences of the composition of a set of given ontologies which may have been built in a completely unrestricted way; (T3) decomposing a large ontology into modules that can be edited in a controlled way.Above, terms such as ``in a controlled way'', ``corrupted ontology'', and ``consequence'' are deliberately left rather vague. Indeed, one goal of this project is to provide rigorous but practical and useful formal specifications of (T1)--(T3) above. It is only very recently that research in this direction has been carried out and the proposers have already made some pioneering contributions towards their actual development. The proposed approach is a continuation of this line of work, i.e., it is logic-based and founded on the notion of conservative extensions .",,
10,AF98B59A-A54A-4973-9992-1CA54F060FCF,Neuromorphic Sensorimotor Integration for Legged Locomotion (NSILL),"During walking, a robot must use several senses to guide its movements. For example, it may need to avoid obstacles, to lift its legs over bumps, or to take a longer stride to avoid a gap. Basic reflexes respond to direct contact (or lack of contact) with uneven ground, but walking would be aided by sensing the ground surface variation or obstacles in advance. It would help to literally see what is ahead . In living animals, walking patterns often depend on a Central Pattern Generator (CPG) to generate a regular gait. The CPG output must, however, learn to control walking on complex and unfamiliar terrain - it must use the animal's senses to adjust walking patterns. Alternatively, the patterns might emerge from distributed control in which each leg influences its neighbours. This approach might be advantageous when the 'pattern' becomes very irregular due to complex terrain.These biological solutions to the problem of walking suggest engineering solutions for walking robots. We will design and build biologically inspired systems using analogue/digital silicon chips, controlling a 6-legged robot. There are three research strands. 1) A CPG chip that learns to generate adaptive walking patterns. Biological CPGs are extremely flexible for producing different rhythms. Neuromodulators, central commands and input signals all influence the pattern produced by a CPG. They do so by altering both the electrical and chemical properties of individual neurons and the coupling between different groups of neurons. For example, visual inputs can cause an animal to break into a gallop. We have previously developed a CPG circuit capable of producing a wide range of animal gaits. We will now build this novel CPG in silicon and test it on a 6-legged insect-like robot, designed and built during the project. We will then evaluate its ability to produce sufficiently flexible output to deal with complex terrain, and compare it to a more distributed control system, or to a suitable hybrid of these two forms of walking control.2) A vision chip that estimates the distance to objects in an image. Neuromorphic vision sensors have been built for edge detection, velocity- and depth-sensing. These are, however, isolated case studies that do not integrate vision with other senses for robotic applications. We will design a silicon chip that implements a novel vision depth sensing method, developed under a previous grant and based on a spiking neuronal model. This stylised early vision model will consist of a 2-D array of light-sensitive transistors with the ability to perform both edge sensing and depth detection. 3) A biologically-inspired chip that integrates senses to make decisions. This chip will combine the output of the visual processor (detected objects and their depth) with additional senses (touch and joint angle sensors) to adjust the walking patterns to achieve smooth, stable movement across rough ground. In particular it will learn to use the visual depth information to anticipate movements required to avoid collisions and maintain stable footing. We will test the methods in simulation, then use software on microprocessors to test the results on real robots. We will then create a chip which will translate the sensors' view of the world into appropriate modulation of the walking controller. The final stage of the project will be the integration of all three chips on to the 6-legged robot to produce a new walking robot with explicit biological antecedents. The project will thus advance research in neuromorphic VLSI and sensor-motor integration, with direct applications in mobile robots for domestic applications and work in hazardous, difficult and unknown environments. It will also help to identify some of the basic computing and control principles in the nervous system.",,
11,A9F89E5A-A988-45AE-BCAC-C7ED2A1FCDAB,Verification of security protocols: a multi-agent systems approach,"The project investigates automatic verification of computer security protocols by using formal tools and techniques drawn from the areas of multi-agent systems, automatic verification and formal logic. Upon succesful completion of the project, a range of security protocols will be verified automatically by computer tools.",,
12,E494751A-66AA-4010-87EF-E82841694D1E,Using Learning to Support the Development of Embedded Systems,"Reasoning about the timing properties of many modern systems is crucial. Examples include anti-lock braking systems, air traffic control systems, and even medical applications such as X-ray dosage delivery equipment. Reasoning about response times of such systems has been the subject of much research. In particular, a great deal of scheduling theory has been developed to provide bounds on worst-case response times. Such work assumes the timing properties of individual components in the system are well understood. In particular, the Worst Case Execution Time (WCET) for an individual task is an input to all forms of real-time scheduling theory. The derivation of such WCETs therefore underpins our efforts to guarantee response times in critical systems. The real-time systems community recognizes this as a major challenge. Approaches developed for WCET analysis must give conservative bounds but the degree of pessimism is typically too great for many purposes. All such analysis tools must ultimately rely on information about hardware instruction execution. However, modern processors are becoming very complex and the information needed to calculate WCETs is typically unavailable (e.g. internals of processors are commercially confidential). Where such information is provided, it has often proved to be wrong. In many cases instruction WCET is of secondary interest to the manufacturers and WCET techniques developers are not the target audience of the information they make available. Several researchers have identified measurement-based approaches as a promising candidate to cope with modern-day engineering demands. However, relying only on measurements to infer WCET bounds in a black box approach is regarded as unsound by most researchers. We need information to reason effectively about WCETs, but this is not readily available. Measurements however, can be taken freely. The weakness of measurement is ensuring the results are safe. Thus, rather than directly inferring bounds on WCETs from execution trace timings, why not use the measurements to infer a model of the underlying system that can form an input into further WCET calculations? Our proposal addresses this very question. Since the problem is in essence a learning problem, we propose to investigate how well leading edge machine learning approaches can be adopted or adapted to this end.",,
13,CB92C67A-45A4-432D-89F1-C26DA1749756,Applying computational semantics,"This project is a collaboration between researchers who are interested in automatically extracting meaning representations from text. We are specifically interested in the problem of capturing those aspects of meaning which can be deduced from syntactic and morphological information. Many researchers are sharing work on semantic representation: however, it is necessary to improve the utility of this work for application developers. We will approach this by developing new techniques and objectives for collaborative development of semantic analyses. This project involves several international research groups, and the collaboration includes multilingual work.",,
14,9B768CDE-B75C-4936-91A4-2CE22407976E,Accurate and Efficient Parsing of Biomedical Text,"Natural Language Processing is a branch of Artificial Intelligence concerned with using computers to automatically process and understand natural languages. Natural language refers to languages such as English, French, German, etc., rather than artificial computer programming languages. There are a number of reasons why this will be an important technology in the 21st century. First, computers are gaining increasing importance in our society, and being able to communicate with them in a natural way, using spoken and written language, will become more desirable. Second, we are producing very large amounts of online electronic information; we require tools which can automatically process this information, to summarise it, to answer questions about it, to translate it, to find relevant documents within it. The staggering rise of Google demonstrates the importance of this kind of technology.The proposed research concerns the processing of a particular kind of text, namely the scientific articles produced by the biological research community. Biology produces an enormous number of new articles each year, far too many for any one individual to keep up to date with. Automatic computer tools are required which can process this information. For example, a biologist might want to know whether there is a paper on the Web answering a particular question about some gene.Sophisticated text processing, such as translating a document from one language to another, summarising documents, or answering questions, requires sophisticated language processing tools. A very useful tool for these kinds of tasks is a parser , which automatically determines the grammatical structure of a sentence and how the words in the sentence are related. For example, it would determine the verbs in the sentence, and how the nouns are related to the verbs. This information is needed if a computer is to be able to understand the text.The Natural Language Processing community now has very good parsing technology. However, the existing parsers are good at analysing certain kinds of text, such as newspapers, but not so good at other kinds of text, such as biology research papers. The reason is that the parsers have learned about language from linguistic resources created by humans, and the resources are based on newspaper text. Creating these resources from scratch for biology would take too long, and so the proposed research will investigate ways in which parsers tuned for newpaper text can be ported to handle biological text.",,
15,73B6E303-1487-49E7-BBA0-74E0FA2851CC,D-SCENT: Raising challenges to deception attempts using data scent trails,"Since 9-11 and 7-7, terrorism has been a major public concern. To ensure public safety and to protect the UK economy, research is needed that offers new methods to foil attacks before they are executed, to identify people and networks who might be preparing for or undertaking an attack, and to provide clear evidence that can be used to justify questioning, arrests and prosecutions.In this study, we will investigate whether deception can be identified and proved from 'scent trails', that is, coherent accounts of suspects' activities over time compiled from tracking their movements, communications and behaviours. We will develop software to derive inferences about what activities are consistent with suspects' scent trails and what are ruled out. These inferences will allow investigators to challenge suspects, both in real time (e.g., to encourage suspects to abandon an ongoing attack) and during interviews (e.g., to point out inconsistencies between a suspect's account and scent trail evidence that might change the course of an interview).The project will investigate scent trails in the context of people undertaking deceptive activities to gain advantage in adversarial 'treasure hunt'-type games. The games will be developed in consultation with stakeholders to provide a non-sensitive analogy to counter-terrorism contexts. Players, typically undergraduate students paid for participation, will be monitored during games via positional and communication data obtained from mobile devices enabled with geospatial positioning devices. Novel software for integrating these data will be developed to build up scent trails of players' activities during game play. Methods of artificial intelligence will be combined to derive inferences from the scent trails about what kinds of activity are possible and impossible given a player's location, trajectory, activities and links with others.We envision games with 3 teams: Team A represent the adversary, Team B the police or general public, and Team C the intelligence services. Team A scores points by visiting target locations within a time limit under a set of game rules that they must violate if they are to win. They must try to hide rule violations from Team B, who score points by preventing or identifying Team A's deceptions successfully. Team C can challenge Team A by sending them indications of the scent trails that are held or can feed Team B intelligence information. Moreover, the inferences from scent trails will support Team C in deciding how best to prove or falsify a suspicion during an interview with Team A players at key points during the games.By conducting observation of players during games, we can investigate how people change their behaviours when they are confronted with evidence that reveals their deceptions. We will also interview players at key points during games as a simulation of interviews with suspects, eliciting from players accounts of their activities before presenting them with challenges based on their own scent trails that are either consistent or inconsistent with legal game playing. This will allow interview and analysis techniques to be improved and will provide clues as to how people subsequently change their behaviour after they have been confronted with their deception. The results will also allow us to test between hypotheses deriving from forensic psychology as to how best to detect deception.The research also allows us to explore public awareness of, and response to, monitoring and surveillance in counter-terrorism. With an advisory panel of stakeholders and subject specialists representing key public and academic bodies, we will identify ethical and legal issues associated with collecting and using data on peoples' movements through public spaces. We will also conduct questionnaire studies with game players and others not involved in the games, to measure attitudes to monitoring and surveillance in game-playing and other contexts.",,
16,66AB2B88-DA93-4816-B04E-401510E9985F,An Industry/Academia Research Network on Artificial Intelligence and Games Technologies,"As we finished writing this grant proposal, there was breaking news that games industry representatives had renewed their lobbying of Margaret Hodge, Minister for Creative Industries, for continued governmental support of the video games industry. While the UK games industry is strong at the moment, it is widely appreciated that it could suffer the same fate as the UK film industry, and lose its position as the third largest player in the world. An important way to maintain its strong position is for the games industry to tap into the wealth of knowledge in University computing research laboratories. There would be a double benefit to such a collaboration, as computing researchers are always looking for research problems to stretch their understanding of the computing world, and to enable them to produce general purpose software and hardware solutions.Unfortunately, academics have been slow to collaborate with games companies. In particular, while 40% of EPSRC funded projects have an industry collaboration, this is true of only 20% of those projects involving games and digital media. We propose to change this position by bringing together a network of academics and games industry professionals to exchange ideas, problems and outlooks, and ultimately to lead to research grant proposals on particular games-related topics being written.We propose to focus on the Artificial Intelligence (AI) sub-area of computing. Such a focus is needed in order to narrow down the topics under consideration, and to encourage fruitful collaboration. Our focus on AI is justified for two reasons:1. The manifestation of artificial intelligences inside video games has always been an important aspect of games, and has recently been portrayed as the next major market differentiator for games. Indeed, AI is already used as a marketing tool, and many next generation games are planned with enhanced intelligence. Moreover, as game worlds are increasingly large and complex, many games studios are increasingly using AI techniques to help them more efficiently design their products. 2. Since the earliest days of AI, games have been an important testbed for new ideas. Indeed, video games have been proposed by many AI researchers as the Killer App for AI research. This is because (i) getting game characters to act intelligently and provide real interaction is a very difficult problem for AI research, as is the problem of helping build game worlds (ii) game environments provide a unique opportunity for experimenting with intellligent techniques - not only is it a rich 3D environment, but intelligent behaviour of game characters is imperative if a game is to be believable, and there is instant feedback from players and developers, and (iii) there is much room for improvement, as most intelligent behaviour in games currently uses little or no sophisticated AI techniques.The network we propose has the written support of eight games companies, and the verbal support of a further six, in addition to LDA and TIGA (the games industry trade association) support. We also have selected and gained the support of 24 academics to act as the initial network members. In order to bring these parties together, we plan a series of network-level events and three subgroup events per year (focussed on AI for: avatars, non-player characters and game design). We also aim to encourage exchange visits between games industry professionals and AI academics. We will keep the network members fully appraised of developments with a website, mailing list and quarterly newsletter. We aim not only to bring together interested parties in the AI/game world, but also to shape the future of AI research related to games. To do this, we will produce a road map document and widely circulate it. Moreover, we will strongly encourage network members to collaborate on the submission of research proposals to the funding agencies.",,
17,EDAC4B5C-50B3-45CC-9811-0C939EA08C48,Evolution of group properties via individual level selection,"The overarching question that motivates this proposal is the following: Under which conditions can group properties emerge in an evolutionary process through individual based selection only. In particular, this project will concentrate on the evolution of individual level traits that are selectively neutral at the level of the individual but not at the level of the group.",,
18,DC016386-27B4-4BBB-AF87-E999FE28C273,The Manchester Centre for Interdisciplinary Computational and Dynamical Analysis (CICADA),"It is said that we live in a digital age. The availability of cheap, reliable microprocessors and dense, high-speed memory has led to the mass reduction of analogue information about the real world to strings of digits which can be processed rapidly and stored without error. For most people the most obvious manifestation of this is seen in the entertainment industry. Music, for instance, can be stored digitally on a cd or an iPod, copied without degradation, and can be bought and sold electronically. Less obviously, perhaps, we rely on digital systems in circumstances where failure might result in loss of life. Military aircraft are designed for manoeuvrability by making them inherently unstable. No human pilot could fly such an aircraft without the aid of an active fly-by-wire/generally digital/control system. Commercial airlines also operate fly-by-wire aircraft, mostly digitally controlled. However, in this case there is a major gap between designs we would trust with our lives and those based on the most technically advanced solutions. We have little confidence in the latter due to their complexity and the lack of appropriate testing tools.As a general issue, whenever an embedded computer system, such as would be found in a digital controller, has to interact with the real world, we have what is known as a hybrid system. The difficulties associated with modelling and testing such systems arise because of the fundamental difference between analogue and digital systems. A digital system is finite in the sense that it is, at least in principle, possible to list/and therefore to test/all of the states it can be be found in. When a digital system begins to interact with the real world this is no longer the case and so new techniques must be devised that will allow us to predict the behaviour and test the designs of hybrid systems. The Manchester Centre for Interdisciplinary Computational and Dynamical Analysis (CICADA) will be an internationally-known centre which will bring together mathematicians, computer scientists and control theory engineers to work on this difficult, but deeply interesting and vital area. The Centre will attract internationally renowned scientists working in this field, and create a focus for research activity and, in addition, training for the next generation. A feature of the Centre will be the way it works by fostering a strong interaction between industry/where many of the hard problems are brought into sharpest focus/and academia/which has a wide range of new mathematical and computational techniques which can be brought to bear.",,
19,6CACFCFC-4C22-478B-9495-291394F48217,The Synthesis of Probabilistic Prediction &amp; Mechanistic Modelling within a Computational &amp; Systems Biology Context,"The synergistic advances that can be made by the multidisciplinary interplay between abstracted computational modelling and biological experimental investigation within a system biology context are poised to make major contributions to our understanding of some of the most important biological systems implicated in the genesis of many serious diseases such as cancer. However, due to the unavoidable inherent levels of uncertainty, noise and relative scarcity of biological data it is vital that sound evidential based scientific reasoning be enabled within a systems biology context by formally embedding mechanistic models within a probabilistic inferential framework. The synthesis of mechanistic modelling &amp; probabilistic inference provides outstanding opportunities to make further significant advances in understanding biological systems and processes at multiple levels, by defining system components and inferring how they dynamically interact. There is a major role that statistical machine learning methodology has to play in both computational &amp; systems biology research and a number of important methodological challenges are presented by applications working at this interface.However, one of the most important aspects of successful computational &amp; systems biology research is that it must be conducted in direct collaboration with world-class experimental biologists. An outstanding feature of this Fellowship is that it has set in place six exciting collaborations with internationally leading cancer researchers, proteomics technologists, biochemists and plant biologists who are all fully committed to successfully driving forward a potentially groundbreaking multidisciplinary systems biology research programme as detailed in this proposal. Three important application areas within biological science will shape and direct the research to be undertaken during this Fellowship. The applications are distinct, yet overlap in terms of the modelling &amp; inferential issues which each present and this is important in ensuring a consistent and coherent line of research. They have also been selected for their major importance in the study of cellular mechanisms which are fundamental to cell function, some of which are implicated in certain serious diseases. In addition, the applicant has substantive ongoing collaborations with world-class laboratories engaged in these biological investigations. This ensures the proposed research programme is focused on realistic methodological problems which will have a direct impact on the major scientific questions being asked within each area, as well contributing to the computational and inferential sciences. The first application will develop the inferential tools required by cancer biologists when reasoning about the structures underlying the observed dynamics of the MAPK pathway and these tools will be employed in a large scale study of this pathway in collaboration with the Beatson Institute of Cancer Research. The second application, to be conducted with the Plant Sciences group at the University of Glasgow, will seek to elucidate, in a model-based inferential manner, the remarkable observed phenomenon of organ specificity of the circadian clock in soybean and Arabidopsis, in addition a study of models of transcriptional regulation in the cell-cycle will be conducted. The final application will investigate a number of open issues associated with clinical transcriptomics and proteomics where the identification of possible target genes and proteins is of vital importance to cancer researchers in their studies of, in this case breast and ovarian cancer. This study will be conducted in direct conjunction with the Institute of Cancer Research where an ongoing study of BRCA1&amp;2 mutations implicated in breast and ovarian cancer is underway.",,
20,AD245383-D2AB-4CAE-A3FA-73614EE8463B,Visiting Fellowship - Ozsoy,"Text watermarking is an emerging technique in the intersection of natural language processing and the technologies of forensics and security. It works by embedding additional information in the format, structure or content of a text that is transparent with respect to the normal use of the text, but that can be detected when explicitly sought. Such information can be used, inter alia, to trace the origin, provenance, authenticity and use of the text itself. Rather than watermarking by format or layout that may be disrupted by changes in format, the present proposal seeks to manipulate grammatical aspects of the text, such as active/passive voice, that do not change meaning. The proposal seeks to develop such devices for Turkish, a morphologically rich language differing on many points from English, including the semantics of the passive, using grammar based techniques and statistical modeling.",,
21,BE603752-C3E5-4EC0-AF2A-F917CC8F8AC7,Practical competitive prediction,"The problem of prediction is central in many areas of science, and various general methods of prediction have been developed in machine learning, statistics, and other areas of applied mathematics. The proposed research project is in the machine-learning tradition.A common feature of the standard methods of prediction is their reliance on various stochastic assumptions made about the data-generating mechanism.For example, the currently dominant approach to prediction in machine learningis statistical learning theory. This theory gives interesting and useful performance guarantees for various prediction algorithms but makes a restrictive assumption on the way the data are generated: they are assumed to consist of independent and identically distributed labelled instances. Other possible assumptions are the stationarity or the Gaussian distribution of the data sequence.Competitive prediction does not make any assumptions on data generation but still produces strong performance guarantees in the on-line prediction framework;in many cases, these guarantees are as strong as those obtained in statistical learning theory. The role of stochastic assumptions is played by a benchmark class of prediction strategies: the predictor first decides what class of prediction strategies he or she wants to compete with, and methods of competitive predictionproduce predictions whose cumulative loss does not exceed the loss of any prediction strategy in the benchmark class plus a small overhead term. The task of competitive prediction is to design efficient algorithms that minimize the overhead term.An important development in machine learning in recent years has been the explosion of interest in kernel methods (started by the invention of support vector machines). The so-called kernel trick allows one to apply linear methodsto non-linear problems. The kernel trick has been applied to several algorithms of competitive prediction to obtain competitive loss bounds for benchmark classes that are Hilbert spaces of prediction strategies (basically, classes of prediction strategies that are equipped with the notion of scalar product). The prediction algorithms themselves work with kernels rather than Hilbert spaces directly, which makes them computationally efficient. Since a large number of kernels have been developed for various applications, such as pattern recognition, web, bioinformatics, linguistics, etc., this makes the methods of competitive prediction very widely applicable.The Hilbert spaces form a relatively small subclass of the Banach spaces , and the main goal of this project is to develop methods of competitive prediction for benchmark classes of prediction strategies that form Banach rather than Hilbert spaces. Why is this important? For us there are two main reasons.The main feature of Hilbert spaces is that they are equipped with the notion of scalar product, and the main feature of Banach spaces is that they are equipped with the notion of norm; once we have scalar product, norm is defined in a standard way, so all Hilbert spaces also qualify as Banach spaces. The overhead term in competitive loss bounds involves the norm of the prediction strategy in the benchmark class we are competing with: it can be shown that there are no uniform bounds, and the prediction algorithm must be given a start on the far-away strategies. Therefore, norm of prediction strategies plays a fundamental role in competitive prediction, whereas scalar product appears extraneous (albeit important for some algorithms and methods of their analysis).From the pragmatic point of view, Banach function spaces provide many new interesting and practically important examples of benchmark classes of prediction strategies. For example, some benchmark classes are just too big to be equipped with scalar product. We believe that Banach-space methods have potential to extend further the domain of applicability of competitive prediction.",,
22,20DD7D31-37F6-439F-8A31-053765408428,"Evolutionary Algorithms for Dynamic Optimisation Problems: Design, Analysis and Applications","Evolutionary algorithms (EAs) have been applied to solve many stationary problems. However, real-world problems are usually more complex and dynamic, where the objective function, decision variables, and environmental parameters may change over time. In this project, we will investigate novel EA approaches to address dynamic optimisation problems (DOPs), a challenging but very important research area. The proposed research has three main aspects: (1) designing and evaluating new EAs for DOPs in collaboration with researchers from Honda Research Institute Europe, (2) theoretically analysing EAs for DOPs, and (3) adapting developed EA approaches to solve dynamic telecommunication optimisation problems. In this project, we will first construct standardised, both discrete and continuous, dynamic test environments based on the concept of problem difficulty, scalability, cyclicity and noise of environments, and standardised performance measures for evaluating EAs for DOPs. Based on the standardised dynamic test and evaluation environment, we will then design and evaluate novel EAs and their hybridisation, e.g., Estimation of Distribution Algorithms (EDAs), Genetic Algorithms, Swarm Intelligence and Adaptive Evolutionary Algorithms, for DOPs based on our previous research. A guiding idea here is to improve EA's adaptability to different degrees of environmental change in the genotypic space, be it binary or not. Systematically and adaptively combining dualism-like schemes for significant changes, random immigration-like schemes for medium changes, and general mutation or variation schemes for small changes, is expected to greatly improve EA's performance in different dynamic environments. And memory schemes can be used when the environment involves cyclic changes. In order to better understand the fundamental issues, theoretical analysis of EAs for DOPs will be pursued in this project. We will apply drift analysis and martingale theory as the starting point to analyse the computational time complexity of EAs for DOPs and the dynamic behaviour of EAs for DOPs regarding such properties as tracking error, tracking velocity, and reliability of arriving at optima. Based on the above EA design, experimental evaluation, and formal analysis, we will then develop a generic framework of EAs for DOPs by extracting key techniques/properties of efficient EAs for DOPs and studying the relationship between them and the characteristics of DOPs being solved with respect to the environmental dynamics in the genotypic space. Another key aspect of this project is to apply and adapt developed EAs for general DOPs to solve core dynamic telecommunications problems, e.g., dynamic frequency assignment problems and dynamic call routing problems, in the real world. We will closely collaborate with researchers from British Telecommunications (BT) to extract domain-specific knowledge and model dynamic telecommunication problems using proper mathematical and graph representations. The obtained domain knowledge will be integrated into our EAs for increased efficiency and effectiveness. All algorithms and software developed in this project will be made available publicly to benefit as many users as possible, whether they are from academe or industry.",,
23,981BF3A6-4ADF-4E55-AA75-0D174856667E,"A NEW FRAMEWORK FOR HYBRID THROUGH-PROCESS MODELLING, PROCESS SIMULATION AND OPTIMISATION IN THE METALS INDUSTRY","IMMPETUS (Institute for Microstructural and Mechanical Process Engineering: The University of Sheffield) was founded in 1997 to undertake truly integrated interdisciplinary research across the disciplines of systems, mechanical and metallurgical engineering, addressing key issues in the metals processing industry. Over the last ten years the unique inter-disciplinary research produced by IMMPETUS has secured national and international acclaim for its systems driven approach to process and property optimisation of a wide range of metals process routes. Using systems engineering we target and optimise experiments to develop basic physical metallurgy in specific areas where knowledge is incomplete, to inform model elicitation, testing and validation. For the complex industrial processes we investigate, there is insufficient basic knowledge to construct true through-process physically based models. In order to cover the intractable factors not adequately described by the existing physically based models, we use hybrid models that merge discrete data, knowledge-based and physically-based models in a unique manner to give unprecedented precision in predictive model capability. All the modelling is verified through the use of a world class array of experimental techniques. The proposal comprises 12 projects which have been constructed in conjunction with our industrial collaborators in order to answer the following questions: 1. How do we formulate a 'generic' framework for 'through-process' modelling to achieve 'right first-time' production of metals?2. Which of the metallurgical and thermomechanical variables affect the microstructure and therefore the final properties of metals, but are not yet fully described by existing models?3. How do causalities (deterministic behaviours) as well as uncertainties (heterogeneities, random behaviours) influence the processing and affect the final properties of metals?4. What are the specific modelling strategies 'best' suited for answering 1, 2, and 3 above?5. Using the elicited models in 4, can we identify the achievable properties for a given process route, and what to do if a particular property is not achievable?6. Using 5, how do we optimise the process route?The programme of work is presented as four themes, all of which are inter-dependent and interwoven. PHYSICAL SYSTEMS will be aimed at developing basic physical metallurgical understanding where knowledge is inadequate, in areas including microstructural heterogeneities, and process conditions that are dynamic and non-linear. In MODELLING SYSTEMS, the physical metallurgy, mechanical engineering and systems engineering will be fully integrated, both through the development of new modelling approaches, and the coupling of existing state-of-the-art modelling that in itself produces new methodologies. PROCESS SIMULATION will involve the upscaling of focused laboratory experiments to accurately and completely simulate the relevant industrial process routes and validate them through appropriate mill trials. SYSTEMS OPTIMISATION will act as a powerful vehicle for integrating these themes and via a careful tuning of model structures/parameters will be core to our technology transfer to our will target specific industrial sponsors and to the wider academic community.",,
24,AF98CFC0-BF30-40DE-8BE5-0D638916BB25,D-SCENT: Raising challenges to deception attempts using data scent trails,"Since 9-11 and 7-7, terrorism has been a major public concern. To ensure public safety and to protect the UK economy, research is needed that offers new methods to foil attacks before they are executed, to identify people and networks who might be preparing for or undertaking an attack, and to provide clear evidence that can be used to justify questioning, arrests and prosecutions. In this study, we will investigate whether deception can be identified and proved from 'scent trails', that is, coherent accounts of suspects' activities over time compiled from tracking their movements, communications and behaviours. We will develop software to derive inferences about what activities are consistent with suspects' scent trails and what are ruled out. These inferences will allow investigators to challenge suspects, both in real time (e.g., to encourage suspects to abandon an ongoing attack) and during interviews (e.g., to point out inconsistencies between a suspect's account and scent trail evidence that might change the course of an interview). The project will investigate scent trails in the context of people undertaking deceptive activities to gain advantage in adversarial 'treasure hunt'-type games. The games will be developed in consultation with stakeholders to provide a non-sensitive analogy to counter-terrorism contexts. Players, typically undergraduate students paid for participation, will be monitored during games via positional and communication data obtained from mobile devices enabled with geospatial positioning devices. Novel software for integrating these data will be developed to build up scent trails of players' activities during game play. Methods of artificial intelligence will be combined to derive inferences from the scent trails about what kinds of activity are possible and impossible given a player's location, trajectory, activities and links with others. We envision games with 3 teams: Team A represent the adversary, Team B the police or general public, and Team C the intelligence services. Team A scores points by visiting target locations within a time limit under a set of game rules that they must violate if they are to win. They must try to hide rule violations from Team B, who score points by preventing or identifying Team A's deceptions successfully. Team C can challenge Team A by sending them indications of the scent trails that are held or can feed Team B intelligence information. Moreover, the inferences from scent trails will support Team C in deciding how best to prove or falsify a suspicion during an interview with Team A players at key points during the games. By conducting observation of players during games, we can investigate how people change their behaviours when they are confronted with evidence that reveals their deceptions. We will also interview players at key points during games as a simulation of interviews with suspects, eliciting from players accounts of their activities before presenting them with challenges based on their own scent trails that are either consistent or inconsistent with legal game playing. This will allow interview and analysis techniques to be improved and will provide clues as to how people subsequently change their behaviour after they have been confronted with their deception. The results will also allow us to test between hypotheses deriving from forensic psychology as to how best to detect deception. The research also allows us to explore public awareness of, and response to, monitoring and surveillance in counter-terrorism. With an advisory panel of stakeholders and subject specialists representing key public and academic bodies, we will identify ethical and legal issues associated with collecting and using data on peoples' movements through public spaces. We will also conduct questionnaire studies with game players and others not involved in the games, to measure attitudes to monitoring and surveillance in game-playing and other contexts.",,
0,B807434B-E9CA-41C7-B3AF-567C38589BAC,"Evolutionary Algorithms for Dynamic Optimisation Problems: Design, Analysis and Applications","Evolutionary algorithms (EAs) have been applied to solve many stationary problems. However, real-world problems are usually more complex and dynamic, where the objective function, decision variables, and environmental parameters may change over time. In this project, we will investigate novel EA approaches to address dynamic optimisation problems (DOPs), a challenging but very important research area. The proposed research has three main aspects: (1) designing and evaluating new EAs for DOPs in collaboration with researchers from Honda Research Institute Europe, (2) theoretically analysing EAs for DOPs, and (3) adapting developed EA approaches to solve dynamic telecommunication optimisation problems. In this project, we will first construct standardised, both discrete and continuous, dynamic test environments based on the concept of problem difficulty, scalability, cyclicity and noise of environments, and standardised performance measures for evaluating EAs for DOPs. Based on the standardised dynamic test and evaluation environment, we will then design and evaluate novel EAs and their hybridisation, e.g., Estimation of Distribution Algorithms (EDAs), Genetic Algorithms, Swarm Intelligence and Adaptive Evolutionary Algorithms, for DOPs based on our previous research. A guiding idea here is to improve EA's adaptability to different degrees of environmental change in the genotypic space, be it binary or not. Systematically and adaptively combining dualism-like schemes for significant changes, random immigration-like schemes for medium changes, and general mutation or variation schemes for small changes, is expected to greatly improve EA's performance in different dynamic environments. And memory schemes can be used when the environment involves cyclic changes. In order to better understand the fundamental issues, theoretical analysis of EAs for DOPs will be pursued in this project. We will apply drift analysis and martingale theory as the starting point to analyse the computational time complexity of EAs for DOPs and the dynamic behaviour of EAs for DOPs regarding such properties as tracking error, tracking velocity, and reliability of arriving at optima. Based on the above EA design, experimental evaluation, and formal analysis, we will then develop a generic framework of EAs for DOPs by extracting key techniques/properties of efficient EAs for DOPs and studying the relationship between them and the characteristics of DOPs being solved with respect to the environmental dynamics in the genotypic space. Another key aspect of this project is to apply and adapt developed EAs for general DOPs to solve core dynamic telecommunications problems, e.g., dynamic frequency assignment problems and dynamic call routing problems, in the real world. We will closely collaborate with researchers from British Telecommunications (BT) to extract domain-specific knowledge and model dynamic telecommunication problems using proper mathematical and graph representations. The obtained domain knowledge will be integrated into our EAs for increased efficiency and effectiveness. All algorithms and software developed in this project will be made available publicly to benefit as many users as possible, whether they are from academe or industry.",,
1,9F6BF6E3-4612-4D07-BBAC-BF4FB97FB3AB,Using Meta-Level Search for Efficient Optimal Planning,"Our current work, on which this proposal is based, suggests a way of guiding the search for a plan using meta-level information about the structure of the problem. Using an analysis of where the hard parts of the problem lie, we construct a meta-level CSP representation of the problem and use it to focus a SAT-solving search. In our published work we have shown that this approach is extremely successful for propositional planning, finding parallel optimal plans efficiently even for very large problem instances. We believe that it is possible to extend our approach beyond propositional planning and to show that optimality is realistically achievable, even for large problems, if appropriate meta-level information can be obtained. Our goal in this project is to extend our ability to analyse problem structure in order to identify powerful meta-variables in the non-propositional case, so that we can efficiently solve problems for which non-propositional expressive power is required. Our work has important potential application. Safety-critical decision-making in emergency egress planning, start-up and shut-down procedure planning in electrical power and chemical plants, docking procedures and surgical preparation planning and are all examples of problems that require non-propositional modelling and that need to be solved to optimality. In this project we will develop novel techniques to solve such mixed integer linear programming problems, efficiently and makespan-optimally. Currently, optimal planning is so far from application to real word problems that some fundamental developments need to be made before industrial scale problems can be addressed. By showing that optimal planning is feasible for large, non-propositional, planning problems we will make an exciting theoretical contribution to constraint reasoning research and also start to bridge the gap between optimal non-propositional planning theory and practice.",,
2,8943C545-6F74-43B0-A152-F20AE41AC5F4,High-throughput Differential Expression Proteomics,"In 2001, a major milestone was reached with the publication of the draft sequence of the human genome. It has now become apparent that there are far fewer protein-coding genes in the human genome than proteins in the human proteome. Whilst the genome is relatively stable, each tissue exhibits radically different protein expression that also changes dynamically over its life cycle and with environmental stimulus. Proteomics is therefore playing a major role in elucidating the functional role of many novel genes and their products, as well as in understanding their involvement in biologically relevant phenotypes both in normal cellular processes and disease. Differential proteomics has become a vital tool in the development of earlier and more accurate screening and diagnostic tests for the detection and treatment of disease. Protein biomarkers are discovered through determination of protein expression that changes uniquely through early progression of a disease state. These biomarkers can then be targeted in the development of non-invasive diagnosis, or used as indicators of the efficacy of new medications in drug discovery. The high-throughput discovery of protein biomarkers and the screening of all human proteins to ascertain their functions and interactions are the two major biology driven challenges in proteomics today.These large-scale challenges are too great for the resources of a single laboratory, so open international collaborations are essential and are being championed by the Human Proteome Organisation (HUPO - http://www.hupo.org/). HUPO is an international consortium that promotes the development and awareness of proteomics research and facilitates scientific collaborations between HUPO members and its initiatives. One such initiative is the Brain Proteome Project (BPP / http://www.hbpp.org/). The aims of the BPP are:- To analyse the brain proteome of human and mouse models in healthy, neurodiseased and aged states with emphasis on Alzheimer's and Parkinson's diseases.- To advance knowledge of neurodiseases and aging for developing new diagnostic approaches and medications.- To make neuroproteomic research and its results available in the scientific community and society.The brain is the most complex tissue of higher organisms, and therefore elucidating the protein complement of the brain is the upper limit of a significant challenge to today's current technologies in proteome analysis. The UK is playing a major role in HUPO, significantly through the HUPO Proteomic Standards Initiative (PSI - http://psidev.sourceforge.net/) hosted by the European Bioinformatics Institute, Hixton, Cambridge. However, the UK is under-represented in the BPP and notably in proteome informatics research as a whole. The two greatest technical barriers to large-scale proteomic analyses are:- The need for considerable expert manual interaction in differential expression proteomics. With conventional techniques errors propagate down the pipeline and so considerable expert manual validation is also required, which adds significant subjectivity.- Marked protocol variation in proteomic workflows between laboratories, leading to heterogeneity of results and therefore challenging results integration and cross-validation issues. To lift these barriers, the proposed fellowship aims to underpin proteomics research with an automated proteome informatics pipeline that:- Integrates the statistical power of multiple replicated experiments in order to elucidate all information, so that the accuracy of differential analysis and expression quantification increases to a level where full automation is possible and subjectively is removed.- Build up a statistical formation model of differential expression proteomics from a history of proteomics experiments, to compare and contrast the sensitivity of subtly different proteomic sample preparation, separation and identification protocols for use in subsequent experiment design.",,
3,E02D9527-5556-48AE-9294-4843EDF7534A,Linear-matrix-inequality-based stability analysis and performance design of fuzzy-model-based control systems,"A fuzzy model-based control system comprises a nonlinear plant represented by a fuzzy model and a fuzzy controller connected in closed loop. Fuzzy-model-based control approaches offer systematic design and analysis methodologies to tackle general nonlinear systems with fuzzy controllers. In this investigation, we shall further work on the results obtained in our years of research, and tackle the problems of system stability and performance of sampled-data and time-delayed fuzzy model-based control systems. The investigation will focus on the following areas. 1) To derive stability conditions that guarantee the system stability of sampled-data and time-delayed fuzzy-model-based control systems with or without parameter uncertainties. 2) To formulate the design of the two fuzzy controllers subject to system stability and performance as a linear matrix inequality problem that can be solved numerically using some convex programming techniques. 3) To develop a software package based on the obtained analysis results to aid the design of the fuzzy controllers.",,
4,F6734F95-6A19-436A-9514-2ADBEC0899C3,Advancing Machine Learning Methodology for New Classes of Prediction Problems,"The last few decades have seen enormous progress in the development of machine learning and pattern recognition algorithms for data classification. This has resulted in considerable advances in a number of applied fields, with some of these algorithms forming the core of ubiquitous deployed technologies. However there exist very many important applications, for example in biomedicine, which are highly non-standard prediction problems, and there is an urgent need to develop appropriate &amp; effective classification techniques for such applications. For example, at NIPS2006 Girolami &amp; Zhong reported state of the art prediction accuracy for a protein fold classification problem which stands at a modest 62%. While this may partly be due to overlaps between classes of fold, it is also clear that some of the fundamental assumptions made by most classification algorithms are not valid in this application. In particular, most algorithms make some assumptions on the structure of the data that are not met in reality: data (both training and test) is independent and identically distributed (i.i.d) from the same distribution, labels are unbiased (i.e. the relative proportions of positive and negative examples are approximately balanced) and the presence of labeling noise both on the input data and on the labels can be largely ignored. Recent advances in Machine Learning, such as kernel based methods and the availability of efficient computational methods for Bayesian inference, hold great promise that classification problems in non-standard situations can be addressed in a principled way. The development of effective classification tools is all the more urgent given the daunting pace at which technological advances are producing novel data sets. This is particularly true in the life sciences, where advances in molecular biology and proteomics are leading to the production of vast amounts of data, necessitating the development of methods for high-throughput automated analysis. Improving classification accuracy may lead to the removal of what is currently the bottleneck in the analysis of this type of data, leading to real impact in furthering biomedical research and in the life quality of millions of people. At present most classifiers used in life sciences applications, especially those deployed as bioinformatics web services, adopt &amp; adapt traditional Machine Learning approaches, quite often in an ad hoc manner, e.g. employing Artificial Neural Networks &amp; Support Vector Machines. However, in reality many of these applications are highly non-standard classification problems in the sense that a number of the fundamental underlying assumptions of pattern classification and decision theory (e.g. identical sampling distributions for 'training' and 'test' data, perfect noiseless labeling in the discrete case, object representations which can be embedded in a common feature space) are violated and this has a direct and potentially highly negative impact on achievable performance. To make much needed &amp; significant progress on a wide range of important applications there is an urgent requirement to systematically address the associated methodological issues within a common framework and this is what motivates the current proposal.",,
5,A7FBC55B-4E51-40D5-B8CB-70BF98634C0D,Beyond Clustering: Unsupervised Modeling with Complex Representations,"The field of Machine Learning strives to develop new theory and algorithms that improve the ability of computers to recognize patterns, make autonomous decisions, and make predictions based on data. New advances in Machine Learning have broad impact in other scientific fields, in commerce, and in the daily lives of individuals. For example, they can help neuroscientists analyze high-dimensional brain imaging data, improve online product recommendation systems, or help individuals automatically organize their digital photo albums.Clustering is an important unsupervised Machine Learning tool for a variety of problems. Abstractly, clustering is discovering groups of data points that belong together. As an example, if given the task of clustering animals, one might group them together by type (mammals, reptiles, amphibians), or alternatively by size (small or large). Automated clustering tools have been used to cluster gene expression data in order to elucidate gene function, automatically group news articles on the web by topic, automatically categorize music by genre, and spatio-temporally cluster climate data to improve climate prediction.While clustering is a wonderful tool for many applications, it is actually quite limited. In many situations the data being modeled can have a much richer and more complex hidden representation than the simple assignment of each data point to a cluster. For example, data points can actually belong to multiple clusters simultaneously (e.g. the movie Scream could belong to both the horror movie cluster and the comedy cluster). The hidden representation of the data could be structured, for example sentences can be represented by parse trees. The data being modeled might have multiple latent features (like images which can contain multiple objects). Moreover, the total number of latent features might not be known, and therefore should not be specified or limited a priori. This flexibility is provided by the use of nonparametric Bayesian methods, which will play a fundamental role in this proposal.My main goal is to advance the state-of-the-art for unsupervised machine learning, by developing principled, theoretically sound, probabilistic models and algorithms, which extend a clustering paradigm to problems which need richer representations. These richer and more complex representations for data provide the ability to model data well in the many situations in which clustering is not good enough. In addition to advancing the theory, I will also develop efficient learning and inference algorithms for the probabilistic models that use these representations.The starting point for much of my work will be nonparametric Bayesian methods, and in particular, the Indian Buffet Process (IBP). Nonparametric methods are designed to be very flexible, and can model data better than inflexible models with a fixed number of parameters. My methods will be able to automatically infer the correct model size (number of parameters) from the data. I will focus on six specific new contributions to unsupervised machine learning. First, I will develop probabilistic models in which each data point can simultaneously belong to multiple overlapping clusters. Second, I will extend the clustering-on-demand paradigm to relational data creating a method that will enable computers to perform simple forms of analogical reasoning. Third, I will develop efficient methods for learning and inference in IBPs. Fourth, using the IBP I will create a new approach to Independent Components Analysis (a widely-used signal processing method) making it possible to automatically learn the number of components in a signal. Fifth, I will develop new probabilistic unsupervised methods for computers to transfer what they have learned on one task to other tasks. Finally, I will explore new uses of advanced probability theory and stochastic processes in the design of practical nonparametric machine learning methods.",,
6,A603670F-D2C8-46DB-9615-0EF87643C764,REG Challenge 2008: A Shared Task Evaluation Event for Referring Expression Generation,"Natural Language Generation (NLG) is the subfield of Natural Language Processing (NLP) that is concerned with developing computational methods for automatically generating language, with the primary aims of economising text-production processes (for example producing drafts of manuals or letters), and improving access to non-verbal information (for example creating verbal descriptions for visually impaired users). Comparing how well alternative computational methods perform the same task (or 'comparative evaluation') is an important component of the consolidation of research effort and technological progress in general. Comparative evaluation initiatives with associated competitions and events have been common in many NLP fields for some time, where they have been seen to galvanise research communities, create valuable new resources, and lead to rapid technological progress.NLG has strong evaluation traditions, in particular in user evaluations of application systems, but also in embedded evaluation of NLG components against non-NLG baselines or different versions of the same component. However, what has largely been missing are comparative evaluation results for comparable but independentlydeveloped NLP systems and tools. Right now, there are only two sets of such results. Over the past two years, NLG researchers have become increasingly interested in comparative evaluation. We believe that comparative evaluation initiatives will have many beneficial effects for NLG, including creation of resources, focussing research effort on specific tasks and attracting new researchers to the field. This year, we organised the Attribute Selection for Generating Referring Expressions (ASGRE) Challenge, which was a pilot NLG shared-task evaluation event. Participation was high and reactions from NLG researchers have been enthusiastic. We are therefore planning a full-scale NLG evalution initiative, the Referring Expressions Generation (REG) Challenge, for 2008. Unlike the two leading evaluation intitiatives in the neighbouring fields of Machine Translation and Document Summarisation, which are funded and directed by US government agencies, the ASGRE and REG Challenges are community-led, UK-based evaluation initiatives. This proposal requests funding for data preparation and evaluation activities in the 2008 REG Challenge, to enable us to extend the range of shared tasks and the evaluation programme, and to keep this initiative community-based and UK-led.",,
7,8C54004A-983F-409D-890A-6A7165C1FA3B,Advancing Machine Learning Methodology for New Classes of Prediction Problems,"The last few decades have seen enormous progress in the development of machine learning and pattern recognition algorithms for data classification. This has resulted in considerable advances in a number of applied fields, with some of these algorithms forming the core of ubiquitous deployed technologies. However there exist very many important applications, for example in biomedicine, which are highly non-standard prediction problems, and there is an urgent need to develop appropriate &amp; effective classification techniques for such applications. For example, at NIPS2006 Girolami &amp; Zhong reported state of the art prediction accuracy for a protein fold classification problem which stands at a modest 62%. While this may partly be due to overlaps between classes of fold, it is also clear that some of the fundamental assumptions made by most classification algorithms are not valid in this application. In particular, most algorithms make some assumptions on the structure of the data that are not met in reality: data (both training and test) is independent and identically distributed (i.i.d) from the same distribution, labels are unbiased (i.e. the relative proportions of positive and negative examples are approximately balanced) and the presence of labeling noise both on the input data and on the labels can be largely ignored. Recent advances in Machine Learning, such as kernel based methods and the availability of efficient computational methods for Bayesian inference, hold great promise that classification problems in non-standard situations can be addressed in a principled way. The development of effective classification tools is all the more urgent given the daunting pace at which technological advances are producing novel data sets. This is particularly true in the life sciences, where advances in molecular biology and proteomics are leading to the production of vast amounts of data, necessitating the development of methods for high-throughput automated analysis. Improving classification accuracy may lead to the removal of what is currently the bottleneck in the analysis of this type of data, leading to real impact in furthering biomedical research and in the life quality of millions of people. At present most classifiers used in life sciences applications, especially those deployed as bioinformatics web services, adopt &amp; adapt traditional Machine Learning approaches, quite often in an ad hoc manner, e.g. employing Artificial Neural Networks &amp; Support Vector Machines. However, in reality many of these applications are highly non-standard classification problems in the sense that a number of the fundamental underlying assumptions of pattern classification and decision theory (e.g. identical sampling distributions for 'training' and 'test' data, perfect noiseless labeling in the discrete case, object representations which can be embedded in a common feature space) are violated and this has a direct and potentially highly negative impact on achievable performance. To make much needed &amp; significant progress on a wide range of important applications there is an urgent requirement to systematically address the associated methodological issues within a common framework and this is what motivates the current proposal.",,
8,EBDB4323-4907-4E2A-BFCB-E0AA252F3A39,Advancing Machine Learning Methodology for New Classes of Prediction Problems,"The last few decades have seen enormous progress in the development of machine learning and pattern recognition algorithms for data classification. This has resulted in considerable advances in a number of applied fields, with some of these algorithms forming the core of ubiquitous deployed technologies. However there exist very many important applications, for example in biomedicine, which are highly non-standard prediction problems, and there is an urgent need to develop appropriate &amp; effective classification techniques for such applications. For example, at NIPS2006 Girolami &amp; Zhong reported state of the art prediction accuracy for a protein fold classification problem which stands at a modest 62%. While this may partly be due to overlaps between classes of fold, it is also clear that some of the fundamental assumptions made by most classification algorithms are not valid in this application. In particular, most algorithms make some assumptions on the structure of the data that are not met in reality: data (both training and test) is independent and identically distributed (i.i.d) from the same distribution, labels are unbiased (i.e. the relative proportions of positive and negative examples are approximately balanced) and the presence of labeling noise both on the input data and on the labels can be largely ignored. Recent advances in Machine Learning, such as kernel based methods and the availability of efficient computational methods for Bayesian inference, hold great promise that classification problems in non-standard situations can be addressed in a principled way. The development of effective classification tools is all the more urgent given the daunting pace at which technological advances are producing novel data sets. This is particularly true in the life sciences, where advances in molecular biology and proteomics are leading to the production of vast amounts of data, necessitating the development of methods for high-throughput automated analysis. Improving classification accuracy may lead to the removal of what is currently the bottleneck in the analysis of this type of data, leading to real impact in furthering biomedical research and in the life quality of millions of people. At present most classifiers used in life sciences applications, especially those deployed as bioinformatics web services, adopt &amp; adapt traditional Machine Learning approaches, quite often in an ad hoc manner, e.g. employing Artificial Neural Networks &amp; Support Vector Machines. However, in reality many of these applications are highly non-standard classification problems in the sense that a number of the fundamental underlying assumptions of pattern classification and decision theory (e.g. identical sampling distributions for 'training' and 'test' data, perfect noiseless labeling in the discrete case, object representations which can be embedded in a common feature space) are violated and this has a direct and potentially highly negative impact on achievable performance. To make much needed &amp; significant progress on a wide range of important applications there is an urgent requirement to systematically address the associated methodological issues within a common framework and this is what motivates the current proposal.",,
9,9AEA42D5-5AE0-48B3-AE8E-3538F68C2FB1,AUTONOMIC SUPPLY CHAINS IN COMPUTATIONAL ECONOMIES,"This project will develop techniques and methods that enable the automatic establishment, maintenance and operation of supply chains in highly dynamic, multi-stakeholder environments. It will also focus on the associated supply chain business models for such agile and dynamic environments. In more detail, the various actors within the system, each with their own aims and objectives, will be represented as autonomous software agents that interact in a number of on-line markets in order to procure the goods and services they require in a timely fashion. The markets will also be represented as autonomous agents and so will adapt their offerings and their terms and conditions in order to attract traders and better differentiate themselves from similar competing markets. Given this, the ensuing supply chains will need to be autonomic / self-organizing, selfhealing, and self-optimizing / in order to cope with the high degrees of dynamism and uncertainty that are present in the system. Moreover, through its continual adaptation in response to change, the resulting computational economy will offer significant advantages to all its participants in terms of agility, lead-times, and profitability. To provide a specific illustration of this vision, this feasibility study will examine the supply chain associated with engine aircraft repair and overhaul in conjunction with end-users at Rolls-Royce.",,
10,939746BA-2213-43F1-9498-62D212E7E120,Multi-disciplinary Optimisation and Data Mining at Birmingham,"The University of Birmingham's commitment to collaboration across different disciplines is established through its Research Strategy and embodied in its Collaborative Research Networks (CRNs). The Strategy recognises that cross-School research is difficult to initiate, promote and ultimately to sustain. Building on the exceptional strengths within the disciplines, the Strategy identifies nine major cross-disciplinary research themes and four enabling technology themes (www.research.bham.ac.uk) and the use of Collaborative Research Networks to promote interdisciplinary working within each of the themes. Each CRN has an academic Champion who acts as advocate for the Network, promoting and fostering interdisciplinary research (IDR). Such a strategic approach to the promotion of multidisciplinary working provides a strong foundation and puts the University in a potentially unique position to take advantage of this opportunity. The CRNs and associated IDR activities underline the University's strategy and ability to support and deliver IDR and this proposal seeks funds to develop routes by which deeper and sustainable collaborations can be built upon such a foundation among the disciplines of Computer Science, Mathematics, Engineering, Chemistry and Psychology, adding significant value to current and future initiatives. This proposal will support linkages between the Informatics, Communications Technologies, Molecules &amp; Materials, Transport Technology, Sustainable Environment, Energy and Resources, and Imaging and Visualisation CRNs. The focus of this proposal is on optimisation and data mining, which underly numerous real world challenges in different disciplines, especially as we collect more and more data due to the advance in technologies. Most of these optimisation and data mining problems cannot be tackled effectively by researchers from any single disciplines, because a good solution can only be found with in-depth knowledge in both the problem domain as well as the techniques/algorithms that are applicable. At least two different disciplines need to be involved.This proposal will bring people together, enable them to communicate with other, make available time and space for them to interact intensively, provide forums from them to identify long-term and fundamental research challenges, create an environment and culture that foster multidisciplinary research, and enables them to develop sustained research programmes and write high-quality grant proposals.",,
11,837B5C98-1AA7-44F6-852C-3385B3C3908A,Chain Event Graphs - Semantics and Inference,"The Bayesian Network (BN) has proved very useful in Bayesian modelling, but parallel with the growth in the use of models utilising BNs, concerns have arisen about the scope and efficacy of this model class. In many applications dependence between variables has been found to be context specific. Also, as evidenced for example in analysis of forensic evidence, emergency support systems and biological regulation, the product sample space structure intrinsic to the efficiency of BN learning, is not universal. Much criticism has also been levelled at Causal BNs.Alternative representations have consequently appeared, such as case factor diagrams, each with their own theory and methods, often coding supplementary information in terms of a tree or probability tables. None of these alternatives demonstrates the versatility of the BN, and there is ample scope for a single graphical structure with which to model and analyse discrete asymmetric processes.The Chain Event Graph (CEG) has been devised to meet this need. Significant progress has already been achieved in examining how causal hypotheses can be expressed and examined, in developing propagation algorithms, and in developing methodology for eliciting models of this type in biological systems.The proposed research aims to develop a technology that supports the analysis of asymmetric models which is directly analogous to that provided by Bayesian Networks for supporting more symmetrical models. The research divides into theoretical aspects such as the discovery and characterisation of equivalence classes, devising analogues of the d-separation theorem for BNs, and analysis of causal manipulated systems; more applied statistical modelling including algorithms for propagation, dynamic algorithms, and the process of Learning CEGs; and using the theoretical aspects to develop, for example, methods for expressing and feeding back information provided by an experimenter or expert.",,
12,E38EFA2A-7A69-447A-8A16-144E12BCE9D9,"Visiting Fellowship: Bringing contemporary biology into Evolutionary Computation: Plasticity, hierarchy, and genetic re-use","Evolutionary computation is a branch of computer science using ideas from Darwinian evolution and genetics to design systems and algorithms. Starting from a primordial ooze of random tentative solutions to a problem, generation after generation, evolutionary algorithms refine them using genetic mutations and recombinations and the principle of survival of the fittest. This ultimately leads to the creation of highly fit novel solutions to the problem at hand. Evolutionary computation has provided tens of remarkable human-competitive results. However, there are clear limits in terms of the size and complexity of the kinds of solutions we are able to currently evolve. The main problems are: * limited modularity (modularity is the ability to build large systems out of smaller, semi-independent ones), * limited scalability (scalability refers to how rapidly the computational effort of running a problem-solver increases with the size of the problems),* lack of plasticity (plasticity refers to the ability for a solution to change its behaviour in response to external conditions and events)This project will tackle all three of these significant problems in evolutionary computation with a radically new approach, inspired by biology that has already solved these problems.",,
13,6CF4972F-1E02-4A08-BAF0-FF1384329A10,Opportunities and Challenges in the Digital Economy: an Agenda for the Next-generation Internet,"We propose a Research Cluster to explore the opportunities and challenges of the Digital Economy. The Internet is driving many powerful convergences in media, devices and infrastructure provision. These convergences hold the promise that the next-generation Internet could be a very powerful and universal platform where a great deal of economic and social activity could take place. Given the universal nature of the Internet this platform would break down the traditional distinctions between, say business and the general public, and anyone from any sector could be a provider or a user of these services.If properly realised the benefits from these developments could be considerable. However, they will not happen automatically, there are many issues that need to be tackled before they can be fully achieved. Given the nature of the Internet these issues are as much economic, social, legal and regulatory as they are technical and, critically, these issues have been tackled together to provide an holistic and complete solution.We have assembled a multi-disciplinary consortium that includes talented and experienced research workers in all the fields necessary to address these issues and have established relationships with major stakeholders in the next-generation Internet. The Cluster is led by Imperial College, the University of Oxford and the University of Southampton. The Research Cluster would conduct an open investigation to identify the topics that need to be addressed and produce a roadmap or research and development agenda to tackle them. The Cluster proposes to hold two open workshops at the beginning and end of the one year study to involve the community as much as possible and to create expert Working Groups to address the critical issues. All these deliberations will be conducted in an open manner using Web 2.0 community networking techniques.The outputs of these deliberations will a programme of linked actions to drive forward the development of the Digital Economy. These will comprise multi-disciplinary research programmes, commercial exploitations, social or legal actions or regulatory recommendations. The Reserch Cluster will also be used to identify the coalitions, again both research and commercial, best suited to take forward these proposals.",,
14,5743EE80-5F81-4536-87B2-3605CCFE81C7,Enhancing Objet Trouve Methods in Graphic Design (A Feasibility Study),"Software products such as Adobe Photoshop have revolutionised graphic design in the past decade, by enabling designers to be more creative and more efficient. Whenever a designer uses such software - whether it is for something simple like finding a suitable image, or something more complex, like making a piece of art or putting together an entire design - they will be making aesthetic judgements about the images/artworks/designs (which we call artefacts) that they are presented with. They will choose one image, but reject another; they will play around with image manipulation tools, rejecting many possibilities and developing others until they are happy with the end result; they will attempt a design then scrap it in favour of another.In theory, the aesthetic choices made by a designer could be analysed by the software they are using, and this information could be used to make the software better. This improvement could be in the form of greater efficiency, so that the designer can expect to finish their project quicker, or it could be in the form of improved creativity, so that the designer can expect to be able to try out more possibilities in the time they have available. While there have been a number of studies in this area, in general, graphics software does not utilise any aesthetic information from the user, and there is a great opportunity to bring in Artificial Intelligence techniques for this purpose.If we were to sit down and ask a designer to describe why certain artefacts are good or bad, and try and write down a method for approximating their choices, we would end up using mathematical concepts such as choosing the largest of two values, or taking averages, or composing two ideas together, etc. In effect, we would be mathematically modeling their aesthetic preferences. The purpose of this project is to test the feasibility of using our HR automated mathematical theory formation software for this modeling process. HR has had much success with the invention of mathematical concepts, and the discovery of theorems of genuine interest to mathematicians in number theory, graph theory and various algebraic domains. Moreover, it has recently been used to invent mathematical concepts which approximate the value of scenes for a computer art application. This small case study has highlighted the potential benefits of using HR in graphic design applications.We are proposing a two stage approach to integrating HR with graphic design software so that the user employs the software, oblivious to HR running in the background. We assume that the user starts the session with only a vague idea of what artefact they would like to produce, and are happy to work in an objet trouve (found art) manner. That is, they will look for artefacts within those presented to them for inspiration, and then pursue any ideas they have vigorously. In the first stage, we will replace the usual ad-hoc browsing of possibilities with a themed approach, where we will use HR to invent methods (fitness functions) for evaluating artefacts and then present the user with artefacts which score well within each theme. In the second stage, we will use closed-loop learning to mathematically model an approximation of the user's aesthetic. When the model is sufficiently good, it will be used to guide the user more quickly to a final artefact.We will test this approach in three application domains, namely evolutionary art, image retrieval and city design for video games. We have assembled a team of project partners with great experience in these areas, and we have software available with which to integrate HR. We plan to pay subjects to perform experiments, and we will test whether they are more efficient and/or more creative with the enhanced application software. This is a risky project, but if it is successful, it may lead to a radical change in the way designers take advantage of intelligent software in the creative process.",,
15,898B7B54-BD54-422E-A4E7-BBFF0CC31ECB,Ensemble Classifier Design applied to face expression classification,"Pattern classification involves assignment of an object to one of several pre-specified categories or classes, and is a key component in many data interpretation activities. The proposed approach focuses on classifiers that learn from examples, and it is assumed that each example pattern is represented by a set of numbers, which are known as the pattern features. In the case of face expression classification (for example distinguish between a smiling and frowning face), these features could consist of numbers representing different aspects of facial features. In order to design a system it is customary to divide the example patterns into two sets, a training set to design the classifier and a test set which is subsequently used to predict the performance when previously unseen examples are applied. A problem arises when there are many features and relatively few examples, and the classifier can learn the training set too well, known as over-fitting so that performance on the test set decreases. The field of ensemble classifiers has been developed to address the problem of achieving the best pattern classification performance using a combination of relatively simple classifiers. It has been found that the combination has the advantage that it is less likely to over-fit. However, there is still the difficulty of tuning the individual classifiers, a process that is normally performed using classifier parameters (for example complexity of a neural network classifier). The common approach is to further divide the training set to produce a validation set that can be used to adjust appropriate parameters. However, when the number of examples is in short supply theses techniques are either inappropriate or very time-consuming.In recent work, the Principal Investigator has developed an ensemble class separability measure that is computed on the training set and that can detect over-fitting. Therefore there is no need for a validation set, thereby making more data available for training. The project proposal is to test the method on real data and confirm the results that have been obtained previously on benchmark data.The technique was proposed for two-class problems, and the proposal is to develop the method for multi-class problems using Error-Correcting-Output-Coding (ECOC). ECOC is an ensemble technique that works by decomposing a multiclass problem into two-class sub-problems. In this proposal the aim is to understand why the technique works well, and to propose a design methodology with the aim of applying it to problems in face expression classification. A further objective is to apply the method to predicting the optimal number of features in feature selection using only the training set. There has been for many years a great deal of effort in discovering the most relevant features, since the result has been shown to be more accurate and efficient classifers. Some early research indicates that using class separability measure, this is a feasible approach. The problem is particulary challenging when there are hudreds or thousands of features,as there are in certain biometric, bio-informatics and data mining applications.It is known that even a small improvement in performance of a pattern classification system can affect commercial viability, and the successful outcome of the project should impact other biometric, bio-informatics and data mining applications. The proposed research is relevant to the EPSRC mission since it is aimed at advancing knowledge and technology with practical application relevance. It is anticipated that the likely result of this research will be the enhancement of UK competitiveness through exploitation of the technology.With the help of project partner Mitsubishi Electric, the developed techniques will be applied to stress analysis for physical security systems and driver fatigue for automotive applications.",,
16,A52F7609-DCC6-4796-82B4-59128EBFE09E,A cognitive model of axiom formulation and reformulation with application to AI and software engineering,"Mathematical and scientific theories rest on foundations which areassumed in order to create a paradigm within which to work. Thesefoundations sometimes shift. We want to investigate where foundationscome from, how they change, and how AI researchers can use these ideasto create more flexible systems. For instance, Euclid formulatedgeometric axioms which were thought to describe the physicalworld. These were the foundations on which concepts, theorems andproofs in Euclidean geometry rested. Euclidean geometry was latermodified by rejecting the parallel postulate, and non-Euclideangeometries were formed, along with new sets of concepts andtheorems. Another example of axiomatic change is in Hilbert'sformalisation of geometry: initially his axioms contained hiddenassumptions which were soon discovered and made explicit. Paradoxesfound in Frege's axiomatisation of number theory led to Zermelo andFraenkel modifying some of his axioms in order to prevent problem setsfrom being constructed. On a less celebrated, but equally remarkable,level children are able to formulate mathematical rules about theirenvironment such as transitivity or the commutativity of arithmetic,and to modify these rules if necessary. Recent work in cognitivescience by Lakoff and Nunez and in the philosophy of mathematics byLakatos suggests ways in which this may be done. We intend toconstruct and evaluate a computational theory and model of thisprocess and to explore the application of our model to AI and softwareengineering. This is an ambitious project, with the potential tobring together and deeply influence diverse fields including cognitivescience, automated mathematical reasoning, situated embodied agents,and AI problem solving and software engineering domains which wouldbenefit from a more flexible approach. Developing a set of automatedtechniques which are able to take a problem and change it into adifferent, more interesting problem could have great impact on thesedomains. In particular, we aim to explore the application of ourtheory and model to AI problem reformulation and softwarespecifications requirements. A general theory of how constraints,specifications or goals can be formulated and reformulated could leadto a communal set of powerful new AI techniques.",,
17,9BB2E8C2-0654-4814-B359-C8A27F642DE7,Alan Bundy Symposium,"The symposium will look at the developments in Informatics and Artificial Intelligencesince the 1970s and consider how research in the area may develop in the future.Specifically, it will look at research related to the work since the 1970s of Alan Bundy.Topics whose development will be reviewed include: * computer processing of written text * automated solutions to problems in physics * modelling by computer of the reasoning carried out by mathematicians * machine search for new results in mathematics * new forms of representing knowledge that allow non-experts to put together resources from the internet* support for ecological scientists to assemble computer models in a simple and transparent way.",,
18,971A5966-7BBE-4FDA-A64A-F48933928C82,Managing the Data Explosion in Post-Genomic Biology with Fast Bayesian Computational Methods,"Rapid technological advances in molecular biology are providing an unprecedented opportunity to investigate the basic processes of life. This `post-genomic' phase of molecular biology has resulted in an explosion of typically high dimensional structured data from new technologies for transcriptomics (microarrays), proteomics and metabolomics. Such data requires novel mathematical, statistical and computational methods for their interpretation and analysis. This proposal focuses on the development of statistical and computational methods for the analysis of such data, using novel approaches from the fields of machine learning and nonparametric Bayesian statistics. The project involves a close collaboration of scientists with expertise in machine learning and statistics, bioinformatics and molecular biology. The new software tools will be developed in the context of real-world scientific problems, such as: elucidating signalling networks in plant stress responses; metabolic regulation in the bacteria Streptomyces, major producers of antibiotics and delineating the molecular mechanisms contributing to mitochondrial dysfunction in obesity and diabetes. The scientific goal of the project will be to apply these novel methods to modelling bioinformatics data, but the methods developed will be broadly applicable across a number of fields.",,
19,3981E77B-5AA3-4A10-AC13-742D8EB5F74F,Managing the Data Explosion in Post-Genomic Biology with Fast Bayesian Computational Methods,"Rapid technological advances in molecular biology are providing an unprecedented opportunity to investigate the basic processes of life. This `post-genomic' phase of molecular biology has resulted in an explosion of typically high dimensional structured data from new technologies for transcriptomics (microarrays), proteomics and metabolomics. Such data requires novel mathematical, statistical and computational methods for their interpretation and analysis. This proposal focuses on the development of statistical and computational methods for the analysis of such data, using novel approaches from the fields of machine learning and nonparametric Bayesian statistics. The project involves a close collaboration of scientists with expertise in machine learning and statistics, bioinformatics and molecular biology. The new software tools will be developed in the context of real-world scientific problems, such as: elucidating signalling networks in plant stress responses; metabolic regulation in the bacteria Streptomyces, major producers of antibiotics and delineating the molecular mechanisms contributing to mitochondrial dysfunction in obesity and diabetes. The scientific goal of the project will be to apply these novel methods to modelling bioinformatics data, but the methods developed will be broadly applicable across a number of fields.",,
20,F6D97BE6-D9E4-4C2D-A074-7AB05BF59271,Managing the Data Explosion in Post-Genomic Biology with Fast Bayesian Computational Methods,"Rapid technological advances in molecular biology are providing an unprecedented opportunity to investigate the basic processes of life. This `post-genomic' phase of molecular biology has resulted in an explosion of typically high dimensional structured data from new technologies for transcriptomics (microarrays), proteomics and metabolomics. Such data requires novel mathematical, statistical and computational methods for their interpretation and analysis. This proposal focuses on the development of statistical and computational methods for the analysis of such data, using novel approaches from the fields of machine learning and nonparametric Bayesian statistics. The project involves a close collaboration of scientists with expertise in machine learning and statistics, bioinformatics and molecular biology. The new software tools will be developed in the context of real-world scientific problems, such as: elucidating signalling networks in plant stress responses; metabolic regulation in the bacteria Streptomyces, major producers of antibiotics and delineating the molecular mechanisms contributing to mitochondrial dysfunction in obesity and diabetes. The scientific goal of the project will be to apply these novel methods to modelling bioinformatics data, but the methods developed will be broadly applicable across a number of fields.",,
21,CF9723A7-9D40-4C49-A8B3-057A8C5C74F7,Conferences on Intelligent Computer Mathematics,"We are applying for funding to support organising the ``Conferences onIntelligent Computer Mathematics'' (CICM 2008), July 26 -- August 2, 2008 at theUniversity of Birmingham. CICM 2008 is a series of events that brings togetherthe three leading conferences in the field of Intelligent Computer Mathematicsas well as five collocated workshops addressing particular sub-areas of thefield. We therefore ask for funding to bring two internationally leading scientists asinvited speakers to the conference series. We also apply for funding to enableUK PhD students on EPSRC grants to attend the scientific part of the conference.",,
22,AC76A6C1-E852-49B7-A14A-62B379592FB9,Ontology Evolution in Physics,"A computer can be programmed to reason automatically by constructing an ontology in which to represent both some knowledge and the rules to derive new knowledge from old. An ontology is a mathematical formalism. Most ontologies are built manually for a particular reasoning task. Successful reasoning depends on striking a compromise between the expressiveness of the representation and the efficiency of the reasoning process. If either the reasoning environment or the goals subsequently change, then the reasoning process is likely to fail because the ontology is no longer well suited to its task. Many modern applications of automated reasoning need to work in a changing environment with changing goals. Their reasoning systems need to adapt to these changes automatically. In particular, their ontologies need to evolve automatically. It is not enough to remove from or add to the beliefs of the ontology. It is necessary to change its underlying formal language. Our group has pioneered work in this new area of research. Our techniques involve diagnosis of faults in an existing ontology and then repairing these faults. In this project we propose to apply and develop our techniques in the domain of Physics. This is an excellent domain because many of its most seminal advances can be seen as ontology evolution, i.e. changing the way that physicists view the world. These changes are often triggered by a contradiction between existing theory and experimental observation. These contradictions, their diagnosis and the resulting repairs have usually been well documented by historians of science, providing us with a rich vein of case studies for the development and evaluation of our techniques. We face some tricky technical challenges in (a) dealing with the large number of choices in diagnosis and repair and (b) filling in some undefined blanks in some of the repair operations. To solve these challenges we propose to compose together a number of diagnosis and repair operations into what we call repair plans. We have already experimented with two such repair plans, which we call Where's my stuff? and Inconstancy . The first works by dividing some stuff into visible, invisible and total stuff. We have applied Where's my stuff? to case studies as diverse as the discovery of latent heat and the speculation of dark matter. The second works by making some stuff dependent on a variable on which it was previously thought not to depend. This plan is being applied to Modified Newtonian Mechanics (MoND -- an alternative to dark energy) and to the gas laws. Our proposal is to extend this pilot study by looking at a much wider range of case studies, developing more repair plans and evaluating their performance on a test set of case studies.We hope to show that a small set of repair plans can successfully account for a large number of case studies. We will use this work as a basis to develop a theory of ontology evolution that we intend to be applicable outwith the Physics domain.",,
23,20542F55-DFBD-4E2C-AF15-AF95AB913DA1,Cooperatively Coevolving Particle Swarms for Large Scale Optimisation,"In this project, we will develop novel cooperative coevolutionary particle swarm algorithms for solving large scale optimisation problems, especially problems characterised by high dimensionality and non-separability. Although there have been some work on cooperative coevolutionary algorithms (CCEAs) for optimisation, these CCEAs break down quickly when dealing with non-separable high dimensional problems (e.g., with 100 or more real-valued variables). For this class of problems, more effective problem decomposition strategies are urgently needed. We will develop adaptive decomposition strategies capable of decomposing a large problem into subcomponents where the interdependencies among different subcomponents are kept at minimum. These more effective decomposition strategies will then be incorporated into a Particle Swarm Optimisation (PSO) algorithm to enhance PSO's ability in handling highdimensional non-separable problems, an area that PSO is currently very weak in.Classical PSO algorithms have been shown to perform well on low dimensionalproblems, but poorly on high dimensional problems. By combining a cooperativecoevolutionary framework with a PSO model, more effective PSO algorithms forlarge scale problems are expected to be developed. We will carry out in-depth theoretical analysis and computational studies of different adaptive decomposition strategies and cooperative coevolutionary PSO algorithms (CCPSO). Comprehensive comparisons between proposed CCPSO algorithms and other existing CCEAs will be carried out using both separable and non-separable benchmark functions with dimensions up to 1000 (real-valued variables). This will allow us to identify the strengths and weakness of our proposed algorithms in handling this particular class of problems. To evaluate further the performance of proposed CCPSO algorithms, a real-world application in shape optimisation will be used. The expected outcomes of this research will benefit not only researchers in the evolutionary computation and swarm intelligencecommunities, but also practitioners in real-world optimisation.",,
24,F88890C5-25C5-491B-B7CA-D1CA6E552B82,Real-time Intelligent Map-matching Algorithms for Advanced Transport Telematics Systems (RiMATTS),"A variety of transport applications and services such as pay-as-you-drive insurance scheme, navigation and route guidance, accident and emergency responses (enhanced 999 emergency services), bus arrival information at bus stops and fleet management require spatial and temporal location, and time information. One of the important components of such services is the navigation module which provides the required positioning data. Many commercial devices are available to support navigation modules of such transport systems. In recent years, most commercial devices use GPS technology for acquiring such positioning data. Since GPS suffers both systematic errors and noise, the required positioning accuracies of many transport services cannot be achieved by such devices. Moreover, such devices do not provide integrity (the level of confidence) of position solutions which is very important for liability and safety critical applications such as pay-as-you-drive insurance schemes (due to the possibility of billing incorrectly) and responses to emergency 999 calls. A map matching algorithm that integrates the locational data (from GPS or other sensors) with the spatial road network data needs to be employed. Map matching not only enables the physical location of the vehicle to be identified but also improves the positioning accuracy if a good digital map is available. Current map matching algorithms are not capable of supporting the navigation modules of certain transport systems in some operational environments (specifically in dense urban areas) due to the inherent limitations and constraints associated with them. In addition to this, a single map matching algorithm cannot optimally support the navigation module of a transport system in different operational environments. Therefore, there is a distinct need to select a set of representative map matching algorithms. The detailed characterisation of these algorithms through experiments is essential to evaluate their performance in the operational environments for which they were designed and to identify their limitations. This representative set of existing map matching algorithms with further enhancements, along with a new map matching algorithm that can take into account limitations and constraints of existing map matching algorithms, could optimally support the navigation modules of most transport systems in most operational environments. Therefore, the main objectives of this research project are to (1) identify a set of representative map matching algorithms from existing algorithms, (2) develop a new map matching algorithm and to address any gaps identified in objective 1 both in terms of applications and operational environments, (3) develop a knowledge-based intelligent map matching (iMM) technique to identify the best map matching algorithm (achieved in objectives 1 and 2 above) suitable for an operational environment, and (4) demonstrate a potential application of iMM in different operational environments. Several criteria will be defined for use with the iMM technique to select the best algorithm for a particular service in a given operational environment. Such criteria will include the geographic characteristics of the operational environment (such as land-use, road network density, and building height information) and others (such as complexity, and cost) if required. The exploitation of this proposed research would be in two levels: (1) the algorithms, (2) the actual navigation system which incorporates the algorithms and the navigation sensors. The expectation is that the cost associated with the actual navigation system will be relatively low (at the level of 500 per unit). This is expected to fall as the price of navigation sensor chips and MEMS technology-based sensors reduce over time.",,
0,EF87F3D0-CA75-4EBB-BD4B-DD53AF1BCFD9,ADEPT: Adaptive Dynamic Ensemble Prediction Techniques,"Predicting unknown quantities is a fundamental part of science andengineering. For example, in medicine one might wish to predict whether aperson has a cancerous tumour or not based on a scan; or in manufacturing, whether an industrial machine is producing faulty devices or not. The field of Artificial Intelligence has studied many techniques to produce good predictors. The last decade of research has seen the development of population-based techniques. Instead of using a single predictor, these build teams of predictors and combine the decisionsof the individuals through a voting or averaging process. Both theory andexperiments show this reliably improves upon using a single predictor --as they say two heads are better than one. A nice feature is that thesemethods are predictor-independent, meaning they can combine any kind ofpredictors (e.g. neural networks, decisions trees) into a team.This project aims to unify two sub-fields of Artificial Intelligence thatdeal with these population-based predictor-independent techniques:Ensemble Methods and Learning Classifier Systems.Ensemble Methods have produced some of the most powerful predictors ofthe last decade; the most well-known is called AdaBoost , and has beendubbed the best off-the-shelf predictor in the world (Professor LeoBreiman, University of California at Berkeley). These methods have beenwidely applied in many areas; however, one important area not yetinvestigated is multi-step problems. These are problems where decisionsin thepast and present can affect what the best decisions in the future willbe---for example choosing to play a certain opening strategy in chessmeans certain moves are less favourable later on in the game. Our mostdifficult multi-step problem will be optimising elevator scheduling tominimise the amount of time between pressing an elevator call button andthe arrival of the elevator. It is surprisingly difficult to optimise themovement of elevators in a large building. For one thing, a building with5 elevators and 30 floors has more possible configurations than thereare grains of sand on all the beaches in the world. Most ensemble methodscannot be directly applied to this kind of problem.Learning Classifier Systems are a class of nature-inspired algorithms, thatcan dynamically generate and adjust sets of predictors, and are capableof tackling these multi-step problems. Traditional ensemble methodshave not considered the multi-step domain, but have strong theoreticalfoundations to build upon. Learning Classifier Systems do not have sucha strong theory base, but have been intensely studied on multi-step problems.This project will create hybrid methods using theory and practice fromthese two quite disparate fields. We will advance the state-of-the-artin both fields and increase research capacity for tackling several problemclasses, focusing in particular on multi-step problems.",,
1,D7E1F8E9-B90A-4E3C-8568-7020CD6F3072,ADEPT: Adaptive Dynamic Ensemble Prediction Techniques,"Predicting unknown quantities is a fundamental part of science andengineering. For example, in medicine one might wish to predict whether aperson has a cancerous tumour or not based on a scan; or in manufacturing, whether an industrial machine is producing faulty devices or not. The field of Artificial Intelligence has studied many techniques to produce good predictors. The last decade of research has seen the development of population-based techniques. Instead of using a single predictor, these build teams of predictors and combine the decisionsof the individuals through a voting or averaging process. Both theory andexperiments show this reliably improves upon using a single predictor --as they say two heads are better than one. A nice feature is that thesemethods are predictor-independent, meaning they can combine any kind ofpredictors (e.g. neural networks, decisions trees) into a team.This project aims to unify two sub-fields of Artificial Intelligence thatdeal with these population-based predictor-independent techniques:Ensemble Methods and Learning Classifier Systems.Ensemble Methods have produced some of the most powerful predictors ofthe last decade; the most well-known is called AdaBoost , and has beendubbed the best off-the-shelf predictor in the world (Professor LeoBreiman, University of California at Berkeley). These methods have beenwidely applied in many areas; however, one important area not yetinvestigated is multi-step problems. These are problems where decisionsin thepast and present can affect what the best decisions in the future willbe---for example choosing to play a certain opening strategy in chessmeans certain moves are less favourable later on in the game. Our mostdifficult multi-step problem will be optimising elevator scheduling tominimise the amount of time between pressing an elevator call button andthe arrival of the elevator. It is surprisingly difficult to optimise themovement of elevators in a large building. For one thing, a building with5 elevators and 30 floors has more possible configurations than thereare grains of sand on all the beaches in the world. Most ensemble methodscannot be directly applied to this kind of problem.Learning Classifier Systems are a class of nature-inspired algorithms, thatcan dynamically generate and adjust sets of predictors, and are capableof tackling these multi-step problems. Traditional ensemble methodshave not considered the multi-step domain, but have strong theoreticalfoundations to build upon. Learning Classifier Systems do not have sucha strong theory base, but have been intensely studied on multi-step problems.This project will create hybrid methods using theory and practice fromthese two quite disparate fields. We will advance the state-of-the-artin both fields and increase research capacity for tackling several problemclasses, focusing in particular on multi-step problems.",,
2,124CFF34-C2D3-45E1-8813-955A3ACB9AB6,Machine Listening using Sparse Representations,"My aim for this Fellowship is to undertake a concerted programme of research in machine listening, the automatic analysis and understanding of sounds from the world around us. Through this research, and in collaboration with other international researchers, I aim to establish machine listening as a key enabling technology to improve our ability to interact with the world, leading to advances in many areas such as health, security and the creative industries.Human listeners have many capabilities a machine listening system should ideally have: to recognize a wide range of sounds; to segregate one sound source from a mixture of many sound sources; to judge complex attributes of sound such as rhythm and timbre (sound quality). Most human listeners take these abilities for granted, yet it has proved extremely difficult for conventional audio signal processing methods to tackle many of these tasks. Even currently successful tasks, such as automatic speech recognition, have typically led to very specialized techniques which cannot easily be applied to other domains. I propose to introduce new methods for machine listening of general audio scenes.As part of this work, I also will develop new interdisciplinary collaborations with both the machine vision and biological sensory research communities toinvestigate and develop general organizational principles for machine listening. One such principle that currently looks very promising is that of sparse representations. New theoretical advances and practical applications mean that sparse representations has recently emerged as a new and powerful analysis method, based on the principle that observations should be represented by only a few items chosen from a large number of possible items. This approach now has great potential for analysis and measurement of audio as well as other sensory signals. I also plan to use sparse representations to explore new biologically-inspired machine listening methods, and in turn to improve our understandingof biological hearing systems.Success in this research will open the way for new devices and systems able to process, identify and respond to a wide range of sounds, with diverse applications including: audio searching for the music and video industry; advances in hearing aids and cochlear implants; and incident detection for improved public safety on stations, roads and airports.",,
3,A14A2BD4-4E86-406A-84E4-E02905C67726,HermiT: Reasoning with Large Ontologies,"Ontologies are formal vocabularies of terms, often shared by a community of users. One of the most prominent application areas of ontologies is medicine and the life sciences. For example, the Systematised Nomenclature of Medicine Clinical Terms (SNOMED CT) is a clinical ontology which is being used in the UK Health Service's National Programme for Information Technology (NPfIT). Other examples include GALEN, the Foundational Model of Anatomy (FMA), the National Cancer Institute (NCI) Thesaurus, and the OBO Foundry -- a repository containing about 80 biomedical ontologies.These ontologies are gradually superseding existing medical classifications and will provide the future platforms for gathering and sharing medical knowledge. Capturing medical records using ontologies will reduce the possibility for data misinterpretation, and will enable information exchange between different applications and institutions. Medical ontologies are strongly related to description logics (DLs), which provide the formal basis for many ontology languages, most notably the W3C standardised Web Ontology Language (OWL). All the above mentioned ontologies are nowadays available in OWL and, therefore, in a description logic. The developers of medical ontologies have recognised the numerous benefits of using DLs, such as the clear and unambiguous semantics for different modelling constructs, the well-understood tradeoffs between expressivity and computational complexity, and the availability of provably correct reasoners and tools.The development and application of ontologies crucially depend on reasoning. Ontology classification, i.e., organising classes into a specialisation/generalisation hierarchy, is a reasoning task that plays a major role during ontology development: it provides for the detection of potential modelling errors such as inconsistent class descriptions and missing sub-class relationships. For example, about 180 missing sub-class relationships were detected when the version of SNOMED CT used by the NHS was classified using the DL reasoner FaCT++. Query answering is another reasoning task that is mainly used during ontology-based information retrieval; e.g., in clinical applications query answering might be used to retrieve all patients that suffer from nut allergies . Despite the impressive state-of-the-art, modern medical ontologies pose significant challenges to both the theory and practice of DL-based languages. Existing reasoners can efficiently deal with some large ontologies, such as NCI, but many important ontologies are still beyond the reach of available tools. For example, none of the existing reasoners can successfully classify either GALEN or FMA. Applications currently need to work around these limitations, e.g., by using subsets of ontologies that can be successfully processed. For example, the version of GALEN typically used in practice contains only about 20% of the axioms of the full version; this reduces the interaction between concepts and thus makes the ontology processable . This is, however, highly undesirable in practice, because it reduces coverage, weakens the conceptualisation of the domain and may prevent the detection of modelling errors.Furthermore, the amount of data used with ontologies can be orders of magnitude larger than the ontology itself. For example, the annotation of patients' medical records in a single hospital can easily produce data consisting of hundreds of millions of facts, and aggregation at a national level might produce billions of facts. Existing reasoners cannot cope with such data volumes, especially not if ontologies such as GALEN and FMA are used as schemata.The goal of this project is to develop scalable reasoning algorithms and a prototypical implementation that can efficiently deal with large and complex ontologies and large data sets. Developing such a reasoner will be critical to the success of many ontology based applications.",,
4,C64BD941-098C-4B81-B751-8786E5FA5321,ADEPT: Adaptive Dynamic Ensemble Prediction Techniques,"Predicting unknown quantities is a fundamental part of science andengineering. For example, in medicine one might wish to predict whether aperson has a cancerous tumour or not based on a scan; or in manufacturing, whether an industrial machine is producing faulty devices or not. The field of Artificial Intelligence has studied many techniques to produce good predictors. The last decade of research has seen the development of population-based techniques. Instead of using a single predictor, these build teams of predictors and combine the decisionsof the individuals through a voting or averaging process. Both theory andexperiments show this reliably improves upon using a single predictor --as they say two heads are better than one. A nice feature is that thesemethods are predictor-independent, meaning they can combine any kind ofpredictors (e.g. neural networks, decisions trees) into a team.This project aims to unify two sub-fields of Artificial Intelligence thatdeal with these population-based predictor-independent techniques:Ensemble Methods and Learning Classifier Systems.Ensemble Methods have produced some of the most powerful predictors ofthe last decade; the most well-known is called AdaBoost , and has beendubbed the best off-the-shelf predictor in the world (Professor LeoBreiman, University of California at Berkeley). These methods have beenwidely applied in many areas; however, one important area not yetinvestigated is multi-step problems. These are problems where decisionsin thepast and present can affect what the best decisions in the future willbe---for example choosing to play a certain opening strategy in chessmeans certain moves are less favourable later on in the game. Our mostdifficult multi-step problem will be optimising elevator scheduling tominimise the amount of time between pressing an elevator call button andthe arrival of the elevator. It is surprisingly difficult to optimise themovement of elevators in a large building. For one thing, a building with5 elevators and 30 floors has more possible configurations than thereare grains of sand on all the beaches in the world. Most ensemble methodscannot be directly applied to this kind of problem.Learning Classifier Systems are a class of nature-inspired algorithms, thatcan dynamically generate and adjust sets of predictors, and are capableof tackling these multi-step problems. Traditional ensemble methodshave not considered the multi-step domain, but have strong theoreticalfoundations to build upon. Learning Classifier Systems do not have sucha strong theory base, but have been intensely studied on multi-step problems.This project will create hybrid methods using theory and practice fromthese two quite disparate fields. We will advance the state-of-the-artin both fields and increase research capacity for tackling several problemclasses, focusing in particular on multi-step problems.",,
5,AA657A9E-B266-4DFE-B123-52A22E1797F8,Modular Neural Simulation with Reconfigurable Hardware,"This project aims to: (1) provide a development environment for the modular design of complex, large scale neural networks, either for use in computational neuroscience, or for engineering major neural network applications, (2) explore techniques for efficiently and productively mapping such neural networks onto reconfigurable hardware by exploiting parallelism, reconfigurability and design re-use, and (3) demonstrate the resulting hardware and software capabilities by scaling up and accelerating a recently developed brain-inspired control architecture for cognitive robotics, as well as other applications.",,
6,93C4EB2A-BC58-4A27-B241-E9E963D29F0E,Machine Learning for Thread Level Speculation on Multicore Architectures,"Computer hardware has arrived in the era of multi-core systems. Processors with 2 and 4 cores are already in the high-street. Chip manufacturers promise to deliver many more cores per chip in the coming years. The big research challenge is: how can we make best use of all these resources? Existing programs and programming styles are unable to take real advantage of this hardware concurrency. Thread-Level speculation is one viable solution. TLS works by making predictions about future computations, proceeding to execute programs `speculatively' as if these predictions were true. As a backup, it checks the predictions, in parallel with the speculative computation. If the predictions turn out to be correct, then the computer has done useful work earlier than it could have done otherwise - ultimately meaning your programs run faster. On the other hand, if the predictions are false, then the system has to throw away results, and the speculative work is wasted.There are many different factors to consider in this new paradigm. TLS influences different parts of the system, including processor, memory, operating system, programming language and compiler. At each of these different levels, there are various policies and heuristics to set. These affect things like how make predictions about the future, how to stop different computational tasks from interfering with each other, how to decide which threads are more important, and how existing optimization techniques interact with speculation. This research project will explore these factors using Machine Learning. We will use state of the art feature selection and online machine learning techniques, developing the field where necessary, with the ultimate goal of creating a computer system that can automatically tune itself to run its programs as fast as the physical resources will allow.",,
7,85AB987A-F749-4058-ACF4-136AD990F21C,Sixth European Multi-Agent Systems Meeting,"This is a proposal for EPSRC support for the the Sixth European Multi-Agent Systems Meeting (EUMAS08), to be held on 18 and 19 December 2008, at the University of Bath.After the first EUMAS meeting in Oxford in 2003, and following events in Barcelona (2004), Brussels (2005), Lisbon (2006) and Hammamet (2007), the meeting has come back to the UK, one of the leading countries in the agent community. The decision to site the meeting in Bath is the result of an annual tendering process and thus may be seen as recognition of the UK's place in international agent research.EUMAS is probably the largest agents meeting held annually in Europe. It also has a distinctive profile through being held in the winter, away from the other main conferences, and by its particular emphasis not only on novel work, but also its encouragement of research student submission/presentation, and the explicit invitation of already-published work for re-presentation in a broad European forum.The proposal requests support for five aspects of the EUMAS08 meeting:(i) invited speakers costs(ii) subsistence bursaries for EPSRC-funded students(iii) local organization time and costs,(iv) support staff time, and(v) venue hire.",,
8,B3764C90-33C8-4887-B363-A9A816897A07,Network Coding via Evolutionary Algorithms,"Communication in the Engineering sense refers to the process of transferring information between two or more places in a form that can be processed by communications equipment. Communication systems convey information, which is precisely defined for mathematical analysis but may be taken loosely to mean the symbols that make up the message. At the physical level, the information is often conveyed by a sequence of ones and zeros, forming a binary signal composed of single binary digits (bits). A transmission system is of little use if it cannot transmit information reliably, to help in this process a set of input bits is processing it in some way to produce a different set of output bits, known as coding. Often there are more output bits than input bits because redundant information has been added to help in message recovery. Coding has also been used to compress computer files and for encrypting messages. In 2000, a new use for coding was proposed: to increase the efficiency of message transmission through a computer network. Nodes in a traditional communications network pass on information unaltered if they are not the intended recipient, and all coding takes place at the transmitter and the receiver. In network coding, by allowing the intermediate nodes to modify messages, it has been shown that more information may be sent through a network in a given time in comparison with just forwarding the messages unaltered. Finding the right network codes is not easy, nor is deciding which network nodes should carry out the coding. In fact, some of the problems are of a type that is very difficult to solve in any reasonable time. In recent years the employment of natural processes to inspire solutions to such complex and ill-defined problems has found wide acceptance. This work intends to apply methods derived from nature, known as evolutionary algorithms, to some currently interesting problems in network coding. Two types of evolutionary technique will be employed, genetic algorithms and genetic programming. The former mimic natural processes and work by evolving a population of solutions and improving it in each generation through the means of selection, mating and mutation. The latter generates computer programs from high-level problem descriptions to solve the problems in question. These methods have been very successful in solving many problems for which mathematical expressions for optimisation do not exist or are too complicated to be of use. This project proposes to investigate the employment of evolutionary computing to the search for network codes. The starting point is the selection of nodes for coding via genetic algorithms, which has only recently been considered in the literature. Here, this will first be extended to scenarios outside that in the work to date, which considers one transmitter broadcasting to several receivers in an idealised network. The performance of genetic algorithms will be compared with genetic programming for this problem for the first time. Following this, the implementation of secure network coding in the framework developed will be addressed. This will lead to a secure network coding scheme that takes account of the limited resources available in real networks. In parallel, the dynamic nature of modern communication networks will also be considered. Evolutionary algorithms that function in the presence of changes in network topology will be selected and implemented. Through this phase of the work, network codes will be shown to be practical for future communication networks. Also, valuable insight into the adaptation of evolutionary algorithms for an important area that includes environmental change will be gained.Network coding is an important development in modern communications and combining it with evolutionary algorithms will have a significant impact on the development of new codes for the data-hungry world of today and tomorrow.",,
9,2345C062-095A-4A7A-82B0-EBDF64380F1F,A cognitive model of axiom formulation and reformulation with application to AI and software engineering,"Mathematical and scientific theories rest on foundations which areassumed in order to create a paradigm within which to work. Thesefoundations sometimes shift. We want to investigate where foundationscome from, how they change, and how AI researchers can use these ideasto create more flexible systems. For instance, Euclid formulatedgeometric axioms which were thought to describe the physicalworld. These were the foundations on which concepts, theorems andproofs in Euclidean geometry rested. Euclidean geometry was latermodified by rejecting the parallel postulate, and non-Euclideangeometries were formed, along with new sets of concepts andtheorems. Another example of axiomatic change is in Hilbert'sformalisation of geometry: initially his axioms contained hiddenassumptions which were soon discovered and made explicit. Paradoxesfound in Frege's axiomatisation of number theory led to Zermelo andFraenkel modifying some of his axioms in order to prevent problem setsfrom being constructed. On a less celebrated, but equally remarkable,level children are able to formulate mathematical rules about theirenvironment such as transitivity or the commutativity of arithmetic,and to modify these rules if necessary. It is astonishing that humansare able to form mathematical concepts, to abstract mathematicalrules, to explore the space that these rules define, and to modify therules in the face of counterexamples or other problems. Recent workin cognitive science by Lakoff and Nunez and in the philosophy ofmathematics by Lakatos suggests ways in which this may be done. Weintend to construct and evaluate a computational theory and model ofthis process and to explore the application of our model to AI andsoftware engineering. This is an ambitious project, with thepotential to bring together and deeply influence diverse fieldsincluding cognitive science, automated mathematical reasoning,situated embodied agents, and AI problem solving domains which wouldbenefit from a more flexible approach. Developing a set of automatedtechniques which are able to take a problem and change it into adifferent, more interesting problem could have great impact on thesedomains. In particular, we aim to explore the application of ourtheory and model to constraint satisfaction problems and softwarespecifications requirements. A general theory of how constraints,specifications or goals can be formulated and reformulated could leadto a communal set of powerful new AI techniques",,
10,6FD7B1D5-B5DF-4A33-9E4D-DD5C382A87BA,SUAAVE: Sensing Unmanned Autonomous Aerial VEhicles,"The SUAAVE consortium is an interdisciplinary group in the fields of computer science and engineering. Its focus is on the creation and control of swarms of helicopter UAVs (unmanned aerial vehicles) that operate autonomously (i.e not under the direct realtime control of a human), that collaborate to sense the environment, and that report their findings to a base station on the ground.Such clouds (or swarms or flocks) of helicopters have a wide variety of applications in both civil and military domains. Consider, for example, an emergency scenarion in which an individual is lost in a remote area. A cloud of cheap, autonomous, portable helicopter UAVs is rapidly deployed by search and rescue services. The UAVs are equipped with sensor devices (including heat sensitive cameras and standard video), wireless radio communication capabilities and GPS. The UAVs are tasked to search particular areas that may be distant or inaccessible and, from that point are fully autonomous - they organise themselves into the best configuration for searching, they reconfigure if UAVs are lost or damaged, they consult on the probability of a potential target being that actually sought, and they report their findings to a ground controller. At a given height, the UAVs may be out of radio range of base, and they move not only to sense the environment, but also to return interesting data to base. The same UAVs might also be used to bridge communications between ground search teams. A wide variety of other applications exist for a cloud of rapidly deployable, highly survivable UAVs, including, for example, pollution monitoring; chemical/biological/radiological weapons plume monitoring; disaster recovery - e.g. (flood) damage assessment; sniper location; communication bridging in ad hoc situations; and overflight of sensor fields for the purposes of collecting data. The novelty of these mobile sensor systems is that their movement is controlled by fully autonomous tasking algorithms with two important objectives: first, to increase sensing coverage to rapidly identify targets; and, second, to maintain network connectivity to enable real-time communication between UAVs and ground-based crews. The project has four main scientific themes: (i) wireless networking as applied in a controllable free-space transmission environment with three free directions in which UAVs can move; (ii) control theory as applied to aerial vehicles, with the intention of creating truly autonomous agents that can be tasked but do not need a man-in-the-loop control in real time to operate and communicate; (iii) artificial intelligence and optimisation theory as applied to a real search problem; (iv) data fusion from multiple, possibly heterogeneous airborne sensors as applied to construct and present accurate information to situation commanders. The SUAAVE project will adopt a practical engineering approach, building real prototypes in conjunction with an impressive list of external partners, including a government agency, the field's industry leaders, and two international collaborators.",,
11,DB288EA9-75E6-4675-A231-6146390359A6,A cognitive model of axiom formulation and reformulation with application to AI and software engineering,"Mathematical and scientific theories rest on foundations which areassumed in order to create a paradigm within which to work. Thesefoundations sometimes shift. We want to investigate where foundationscome from, how they change, and how AI researchers can use these ideasto create more flexible systems. For instance, Euclid formulatedgeometric axioms which were thought to describe the physicalworld. These were the foundations on which concepts, theorems andproofs in Euclidean geometry rested. Euclidean geometry was latermodified by rejecting the parallel postulate, and non-Euclideangeometries were formed, along with new sets of concepts andtheorems. Another example of axiomatic change is in Hilbert'sformalisation of geometry: initially his axioms contained hiddenassumptions which were soon discovered and made explicit. Paradoxesfound in Frege's axiomatisation of number theory led to Zermelo andFraenkel modifying some of his axioms in order to prevent problem setsfrom being constructed. On a less celebrated, but equally remarkable,level children are able to formulate mathematical rules about theirenvironment such as transitivity or the commutativity of arithmetic,and to modify these rules if necessary. Recent work in cognitivescience by Lakoff and Nunez and in the philosophy of mathematics byLakatos suggests ways in which this may be done. We intend toconstruct and evaluate a computational theory and model of thisprocess and to explore the application of our model to AI and softwareengineering. This is an ambitious project, with the potential tobring together and deeply influence diverse fields including cognitivescience, automated mathematical reasoning, situated embodied agents,and AI problem solving and software engineering domains which wouldbenefit from a more flexible approach. Developing a set of automatedtechniques which are able to take a problem and change it into adifferent, more interesting problem could have great impact on thesedomains. In particular, we aim to explore the application of ourtheory and model to AI problem reformulation and softwarespecifications requirements. A general theory of how constraints,specifications or goals can be formulated and reformulated could leadto a communal set of powerful new AI techniques.",,
12,E18A5C97-F2CF-40C9-9C00-CB9BABFE1C80,Machine Learning Methods for Predicting Phospholipidosis,"Phospholipidosis is the accumulation of excessive quantities of fatty material (specifically phospholipids) within cells, which can occur in many different organs and cell types. Effects have been noted in the nervous system, lymphatic system, liver, kidneys, eyes and lungs. Phospholipidosis is of great concern to the pharmaceutical industry, especially in the context of the nervous system, where phospholipidosis in neurons can disrupt cell signalling.Since the development of medicines is such an enormously expensive process, it is extremely important to be able to predict adverse effects from chemical structure in advance of synthesis. Ideally, predictions of toxicity should be made at a very early stage in the design of new medicines, hence minimising the expense and time wasted on medicines that turn out to be unsafe or ineffective.In this project, we will produce predictive computer models of the phospholipidosis inducing potential of substances that might possibly be developed into medicines. These models will be substantially more sophisticated and accurate than the models that have previously appeared in the scientific literature. The main method we will use is called Random Forest. The forest is a set of several hundred decision trees , each of which is basically a flow diagram. We will train them to learn patterns in the known properties of existing medicines, and failed candidates, and their tendencies to induce phospholipidosis. However, the way in which we will generate the trees involves computer-simulated dice-rolling. This will ensure that they are all different, though based on the same underlying information. The decision trees then behave like jury members, voting on whether each new substance should be classed as safe or unsafe.The work proposed here is a cost-effective project with a very high probability of successfully predicting phospholipidosis inducing potential. It uses state-of-the art computer-based chemistry and machine learning methods to address a major current problem in designing and developing medicines. More generally, this work is at the cutting edge of the developing field of computational toxicology. For social and political reasons, this is almost certain to become a hot area as concerns about the environmental and health effects of chemicals and medicines mount, at the same time as animal experiments are likely to be increasingly phased out.",,
13,EC25B8C3-75DA-4188-8317-ECFE5A0A2CD7,Gene Expression Programming - a new machine learning technique for supervised and unsupervised classification,"Many scientific, engineering and business fields such as genetics, medicine, environment science and engineering, physics, astronomy, finance, and marketing are facing common challenges in dealing with complex data for extracting field-specific knowledge. Efficient data analysis techniques are needed in order to intelligently assist the user in extracting this knowledge. This project will address this need using the basic ideas of a recently developed computer algorithm, Gene Expression Programming, for the development of novel evolutionary algorithms techniques and novel supervised and unsupervised data classification algorithms. The project will develop and exploit novel homologous genetic operators, and mechanisms to control the redundant information in the solutions provided by the algorithm in order to increase its efficiency. These developments will be combined with state-of-the-art statistical methods such as boosting learning in order to create efficient data classification algorithms.The methods and algorithms developed in the project will be implemented in software applications made available as open-source in order to maximize the spectrum of the beneficiaries of the project outcomes.",,
14,D70F2E8B-1D46-45DC-B5A1-3382C97F1642,Linguistic and direct transmission of concepts in robot-human networks,"Human intelligence relies on concepts. Anything we talk about is associated with concepts: each word is connected to a concept in our brain, and saying that word evokes a similar concept in anyone within earshot. Concepts however do not only serve language, they also help us structure our thoughts and make plans. They are fundamental to human intelligence, so much that when recreating human-like intelligence on a robot, the robot will need concepts that are similar to those of humans or that are coordinated with human concepts. It is impossible to program concepts for a robot, not only because there are too many concepts, but also because concepts are notoriously hard to describe in a programming language. Another problem is that concepts need to be grounded physical reality: a robot needs to experience a concept through its sensors for the concept to become meaningful. Perhaps a better approach is to let a robot learn concepts just like people do. A number of concepts are learned by exploring our environment, but most of our concepts have been taught to us by our caretakers. Recently it has become clear that language plays a crucial role in concept learning, both for young children and for adults: language provides additional information which aids concept learning, for example, it delineates concepts and helps make distinctions between concepts that are otherwise hard to differentiate. In this project we will build two robots that will learn the meaning of words through interacting with people, much in the same way that young children learn conceptual knowledge from hearing adults speak to them about objects, relations and actions. It takes children almost three years to master a few hundred word and related concepts, as long as the duration of this project. However, we could speed up the process of word-concept learning by using training more than one robot, thus reducing the training time needed, and then downloading the missing knowledge from one robot to the other. Such telepathic access to concepts is impossible for humans: we need to resort to pointing out examples of concepts and speaking about them, but direct transfer should be easy to arrange for robots. However, bluntly copying information from one robot to another will most certainly upset the conceptual knowledge already present in the receiving robot. To avoid this, direct transfer of conceptual knowledge needs to proceed with care in order to not disturb already present knowledge.The project has two major aims. One is to study how a robot needs to behave in order to elicit conceptual knowledge from people. Therefore we will build a robot face, containing cameras and microphones, on a long articulated neck. The neck allows to robot to look around the room, but also allows it to scrutinise objects laid out on a table in front of the robot. The robot will be able to seek eye contact, engage in joint attention and interpret gestures related to concept learning. It will engage in activities, such as asking its human teacher to confirm a word or play a round of spot the X , to check its knowledge and, if necessary, adapt it. The second major aim of the project is designing computer algorithms that efficiently learn concepts from interaction involving real-world scene and words. Children are particularly good at this, and the reason for it is that they use a number of constraints to help their learning. We want to program these constraints into our robot learning mechanisms. Finally, we want to study the fast direct exchange of knowledge between robots, and we believe that we can reuse the aforementioned algorithms to allow robots to teach each other new concepts and words. The robots will use the internet as a medium to interact and are no longer limited by the slow real world to do show and tell teaching. Learning thousands of concepts might, instead of the years it takes children, now take only a few minutes.",,
15,9953C8EE-3B8E-41E6-A67B-A0A6AD3D205C,Graphical Models for Relational Data: New Challenges and Solutions,"Data often come under the form of objects and relationships: forinstance, a library consists of books that cite each other; proteinsbind to other proteins according to a variety of patterns; a networkof online customers is formed by people that indicate which othercustomers give reliable product recommendations. Such relationshipscan be used to predict the behavior and properties of each object. Forinstance, if a particular news article cites several sport articles,this is evidence that the particular article is likely to be aboutsports. We propose novel ways of exploring this relationalinformation. The first task is precisely how to predict the propertiesof an object (e.g., the class of a news article) based on otherobjects that that share a relationship with it (e.g., the otherarticles that are cited by or cite our target). We show that thereare important forms of relationship that are not properly treated bycurrent methods, and propose a new methodology to account for suchrelations. The second task focuses on ways to measure similarity ofrelational structures. For instance, if we know that two proteinsphysically interact inside a yeast cell, can we infer which otherpairs of proteins are linked in a similar way? We show how toformulate problems like this using probabilistic models, and developnovel ways of discovering patterns in relational data withapplications to a variety of real-world problems.",,
16,85F02ED5-A837-4787-AA90-233C53760698,PDP-squared: Meaningful PDP language models using parallel distributed processors.,"Parallel Distributed Processing (PDP) is a form of computation where a large number of processing units performing simple calculations can be employed all together to solve much more complex problems. Perhaps the best example of this is the human brain, which contains approximately one hundred billion neurones. Individually these neurones simply have to decide whether to fire or not, and they do this based upon how many other neurones that are connected to them have fired recently. When this simple local computation is distributed over billions of neurones it is capable of supporting all the extremely complex behaviours that humans exhibit / talking, reading, walking, running etc / behaviours that are well beyond the abilities of more traditional computers. For this and other reasons, many psychologists believe that PDP models are the best way of describing human cognition. Unfortunately, at the moment these models are invariably simulated using standard PCs, which means that each unit in the model has to be dealt with one after the other in a serial process. This serial processing imposes severe limitations upon the complexity of problems that can be tackled. Our goal is to us to understand how the brain supports language function, how this breaks down after brain damage and the mechanisms that support recovery/rehabilitation. This will require a model of language that is capable of simulating speech, repetition, comprehension, naming and reading. To train such a model using existing pc-based simulators would take far too long /possibly more than a lifetime. So the first objective of this project is to produce a parallel distributed processing machine that is truly parallel (PDP-squared). We intend to use an array of 10,000 ARM processors incorporated into a machine that will be able to run our simulations of human behaviour 500-1000 times faster than is currently possible on a single pc. Once we have successfully produced this machine (Phase1 of the project), we will use it to build a model of normal human language function that can support reading (both aloud and for meaning), comprehension, speech, naming and repetition for all of the single monosyllabic words in English. We will validate this model by showing that damaging it can lead to the same patterns of behaviour as found in brain damaged individuals (Phase 2). Finally we will use the model to predict the results of different speech therapy strategies and will test these predictions in a population of stroke patients who have linguistic problems.",,
17,ED3639C6-EE71-417C-9B6E-D4ACF9302E92,Machine Learning for Thread Level Speculation on Multicore Architectures,"Computer hardware has arrived in the era of multi-core systems. Processors with 2 and 4 cores are already in the high-street. Chip manufacturers promise to deliver many more cores per chip in the coming years. The big research challenge is: how can we make best use of all these resources? Existing programs and programming styles are unable to take real advantage of this hardware concurrency. Thread-Level speculation is one viable solution. TLS works by making predictions about future computations, proceeding to execute programs `speculatively' as if these predictions were true. As a backup, it checks the predictions, in parallel with the speculative computation. If the predictions turn out to be correct, then the computer has done useful work earlier than it could have done otherwise - ultimately meaning your programs run faster. On the other hand, if the predictions are false, then the system has to throw away results, and the speculative work is wasted.There are many different factors to consider in this new paradigm. TLS influences different parts of the system, including processor, memory, operating system, programming language and compiler. At each of these different levels, there are various policies and heuristics to set. These affect things like how make predictions about the future, how to stop different computational tasks from interfering with each other, how to decide which threads are more important, and how existing optimization techniques interact with speculation. This research project will explore these factors using Machine Learning. We will use state of the art feature selection and online machine learning techniques, developing the field where necessary, with the ultimate goal of creating a computer system that can automatically tune itself to run its programs as fast as the physical resources will allow.",,
18,5D368D3D-D6EA-4189-B981-FAF380B19F9B,SUAAVE: Sensing Unmanned Autonomous Aerial VEhicles,"The SUAAVE consortium is an interdisciplinary group in the fields of computer science and engineering. Its focus is on the creation and control of swarms of helicopter UAVs (unmanned aerial vehicles) that operate autonomously (i.e not under the direct realtime control of a human), that collaborate to sense the environment, and that report their findings to a base station on the ground.Such clouds (or swarms or flocks) of helicopters have a wide variety of applications in both civil and military domains. Consider, for example, an emergency scenarion in which an individual is lost in a remote area. A cloud of cheap, autonomous, portable helicopter UAVs is rapidly deployed by search and rescue services. The UAVs are equipped with sensor devices (including heat sensitive cameras and standard video), wireless radio communication capabilities and GPS. The UAVs are tasked to search particular areas that may be distant or inaccessible and, from that point are fully autonomous - they organise themselves into the best configuration for searching, they reconfigure if UAVs are lost or damaged, they consult on the probability of a potential target being that actually sought, and they report their findings to a ground controller. At a given height, the UAVs may be out of radio range of base, and they move not only to sense the environment, but also to return interesting data to base. The same UAVs might also be used to bridge communications between ground search teams. A wide variety of other applications exist for a cloud of rapidly deployable, highly survivable UAVs, including, for example, pollution monitoring; chemical/biological/radiological weapons plume monitoring; disaster recovery - e.g. (flood) damage assessment; sniper location; communication bridging in ad hoc situations; and overflight of sensor fields for the purposes of collecting data. The novelty of these mobile sensor systems is that their movement is controlled by fully autonomous tasking algorithms with two important objectives: first, to increase sensing coverage to rapidly identify targets; and, second, to maintain network connectivity to enable real-time communication between UAVs and ground-based crews. The project has four main scientific themes: (i) wireless networking as applied in a controllable free-space transmission environment with three free directions in which UAVs can move; (ii) control theory as applied to aerial vehicles, with the intention of creating truly autonomous agents that can be tasked but do not need a man-in-the-loop control in real time to operate and communicate; (iii) artificial intelligence and optimisation theory as applied to a real search problem; (iv) data fusion from multiple, possibly heterogeneous airborne sensors as applied to construct and present accurate information to situation commanders. The SUAAVE project will adopt a practical engineering approach, building real prototypes in conjunction with an impressive list of external partners, including a government agency, the field's industry leaders, and two international collaborators.",,
19,90B23CB8-2AFC-4B81-BAA0-C3FC8217118A,Designing Mechanisms for Automated Resource Allocation: A Case for Support,"The proposed project aims to develop effective mechanisms for automating resource allocation (RA). The problem of resource allocation deals with the design of mechanisms for optimal allocation of a set of available resources to a set of participating agents that compete for them. These agents are self-interested and act strategically. This strategic behaviour affects the allocation of resources. Mechanisms must therefore be designed such that the allocation is optimal from a system wide perspective, despite the self-interest of the participants. RA has long been studied in game theory and economics, and mechanisms such as bargaining, auctions, and voting have been developed for optimal allocation of resources. However, because of new applications such as electronic trading that have arisen from the Internet, this problem is now receiving increasing attention in computer science. When considered in computer science, the goal is typically to automate the process of RA by implementing game theoretic mechanisms on computerized agents. However, this implementation is not straightforward. There are two main reasons for this. First, game theoretic mechanisms assume that the agents are perfectly rational (i.e., have unlimited computing power). However, in the context of automated RA that this project aims to focus on, the agents are computerized agents and have limited computational capabilities (i.e., they are boundedly rational). Moreover, these agents may have different computational capabilities (i.e., different rationalities). Hence, although game theoretic mechanisms are optimal in terms of allocation, they are hard to implement on machines with computational limitations. Second, game theoretic mechanisms are designed for scenarios in which RA is viewed as a one time activity in static environments. However, in many practical cases, RA must be done repeatedly in dynamic environments (i.e., in environments where the available resources and participants change with time). Against this background, the proposed research aims to design effective mechanisms that overcome these limitations. In order to achieve this goal, we aim to use game theory as a design principle but the primary emphasis will be on devising approximation algorithms for solving computationally hard RA problems. The focus will be on two types of mechanisms: bargaining (that involves two participant agents), and auctions and coalitions (that involve more than two participants). Within this context, the proposed project has the following main objectives: i) to design mechanisms that are approximately optimal in terms of their allocation but are computationally feasible, and ii) to investigate the economic consequences of computational limitations (i.e., how approximations affect the economic properties - both system level and individual level - with respect to the exactly optimal game theoretic mechanisms). Overall, this project has the potential to make it more attractive to use game theory as the basis for computing solutions to a range of practical problems such as electronic trading, and networks of distributed systems such as computational grids.",,
20,DBE78F03-15AF-4392-99FE-1DFB07940F06,CAD-GAME: Computer-Aided Game Design,"As a form of media - increasingly, mass media - games have much in common with film, particularly in terms of business models. One reason for the high levels of risk in film is that only limited market testing can be done prior to product release, and the product is essentially fixed from then on. Games have traditionally followed this model, but it is important to remember that games are not just media - they are also pieces of software. One major lesson from the software industry is that capturing users' interactions with a product can be used to refine market offerings in several ways: by adapting it after release (e.g. adaptive interfaces), by using that data to iteratively refine products in future releases (e.g. error reporting in Microsoft and Apple), and to understand the various user groups in more depth. In essence, we are suggesting that this approach can be directly applied to games. The difficulty, however, is that we cannot use simple measures of task completion, because there is no task: we are simply trying to entertain. The main research challenge is to understand better just what the word 'entertain' really means in games, and how this might be reflected in patterns of player behaviour. To do this, we will undertake a study of enjoyment and immersion in video games, in order to derive a concrete methodology for closely estimating the amount of enjoyment and immersion a player has, at fairly fine-grained intervals of a game. We will also capture a multitude of different forms of data from players actually playing Rebellion's games, and we will use advanced techniques from Artificial Intelligence to search for correlations with the player's overall enjoyment. These techniques could be used during design to speed up the process of tweaking the various game parameters - where the enemies are, how effective weapons are, etc. More interesting, however, is the idea that data analysis tools could run within the game itself, adapting the game in response to the player's behaviour. In this way, we hope to pioneer a new age of user-adaptive video games, within a one game, many gameplays paradigm, i.e., where each player has a unique game experience which is tailored to their personality, level of experience, playing style and mood.",,
21,DE3C5E35-0B74-41E4-84DF-A58D44CA7A97,Automatic Adaptation of Knowledge Structures for Assisted Information Seeking (AutoAdapt),"A massive number of electronic document collections exist within companies, universities and other institutions. Two common forms of information seeking are searching and exploring (browsing) the collections. However, finding relevant information within such collections can be difficult. This is true for searching with poorly formulated and less specific queries as well as for browsing where the user may not have a specific target to search. The user's information seeking could be assisted by well-structured knowledge about the search domain, which we refer to as domain model. A domain model is effectively a structure that people impose on data to support them in information seeking. We can now derive query modification or browsing suggestions directly from the domain model. To illustrate the point using a realistic example, assume a user of the University of Essex intranet started by searching for union . This query would trigger the search system to offer query refinement terms such as students union and european union . Indeed, all local Web sites, intranets and similar collections do contain a huge amount of valuable domain knowledge that is encoded implicitly. The challenge is to automatically acquire a domain model and then make it usable by assisting users in information seeking tasks such as searching or browsing. An even bigger challenge is to evolve this domain model automatically. The novelty of this proposal lies in evolving automatically acquired domain knowledge by observing users' usage of it and altering it accordingly. We hypothesize that the submitted user queries and the dialogues between users and search system can be monitored and used to improve the domain model over time. A user's selection of a query modification suggestion is taken as an indication of relevance. This can then be used to update the domain knowledge and thus help the next user with a similar query by presenting updated query modification suggestions.This project aims to develop and evaluate methods for adapting automatically constructed domain models to the population of users' search or browsing behaviour. Application and large-scale evaluation of the developed methods in two information seeking scenarios - namely, interactive search and browsing - will be performed on a number of domains including the intranets of the Essex University, the Open University and our industrial partners.",,
22,596A7B65-AFA2-499B-82CB-E8D8F8A938A4,Automatic Adaptation of Knowledge Structures for Assisted Information Seeking (AutoAdapt),"A massive number of electronic document collections exist within companies, universities and other institutions. Two common forms of information seeking are searching and exploring (browsing) the collections. However, finding relevant information within such collections can be difficult. This is true for searching with poorly formulated and less specific queries as well as for browsing where the user may not have a specific target to search. The user's information seeking could be assisted by well-structured knowledge about the search domain, which we refer to as domain model. A domain model is effectively a structure that people impose on data to support them in information seeking. We can now derive query modification or browsing suggestions directly from the domain model. To illustrate the point using a realistic example, assume a user of the University of Essex intranet started by searching for union . This query would trigger the search system to offer query refinement terms such as students union and european union . Indeed, all local Web sites, intranets and similar collections do contain a huge amount of valuable domain knowledge that is encoded implicitly. The challenge is to automatically acquire a domain model and then make it usable by assisting users in information seeking tasks such as searching or browsing. An even bigger challenge is to evolve this domain model automatically. The novelty of this proposal lies in evolving automatically acquired domain knowledge by observing users' usage of it and altering it accordingly. We hypothesize that the submitted user queries and the dialogues between users and search system can be monitored and used to improve the domain model over time. A user's selection of a query modification suggestion is taken as an indication of relevance. This can then be used to update the domain knowledge and thus help the next user with a similar query by presenting updated query modification suggestions.This project aims to develop and evaluate methods for adapting automatically constructed domain models to the population of users' search or browsing behaviour. Application and large-scale evaluation of the developed methods in two information seeking scenarios - namely, interactive search and browsing - will be performed on a number of domains including the intranets of the Essex University, the Open University and our industrial partners.",,
23,1B618367-A9CE-4821-B506-6CB3DA4DA3D5,Automated Modelling and Reformulation in Planning,"Although AI Planning and Constraint Programming share many techniques and approaches, an important difference lies in the approach to modelling. In CP and also in Operations Research, modellers spend considerable time and effort evaluating alternative models and selecting representations of a problem that will make it most amenable to solution by existing technology. In Planning, researchers typically spend little time considering alternative models and are content to work with the first model they construct, working instead on improving the planning technology to try to tackle the problem, whatever its form. The reason for the strategy of planning researchers is that the intention is to avoid the need for expert planning knowledge in order to exploit a planner. However, the price for this strategy is that there is very little accumulated research expertise in the problem of modelling and no systematic comparison of the performance of planners using alternative models of the same problem. Although avoiding the need for expert planning knowledge in order to use a planner is an important goal, there is clearly a lost opportunity to identify ways in which models might be structured to be most amenable to solution. We propose to combine these strategies by exploring the automatic reformulation of planning problems in order to better exploit the existing planning technology by restructuring models to expose the information that can make a planner make more intelligent choices.",,
24,F51DEC0E-FAB2-448B-8A38-23573FF4EF18,Engineering Autonomous Space Software,"This proposal is based on two premises: that (1) increased autonomy is essential for future space exploration; (2) that existing programming methods are tedious to apply to autonomous components that have to handle an environment with continuous state variables. For well defined discrete-event environments the above rational agent approach is well developed; for a continuous environment, however, perception processes need to be linked with abstractions forming the basis of behaviour. As the environment changes, the abstracted models may also change. Hence, agents are needed that can use these abstractions to aid their decision making processes, use these in the predictive modelling of a continuous world, and connect these abstractions to both planning and goal achievement within rational agents.This project also intends to replace the current complex programming techniques, used for autonomous spacecraftcontrol, with simpler declarative programming. High-level, declarative agent programming languages have been investigated at Liverpool and such theories and languages will be developed further for agents that require predictive modelling capabilities. The Southampton team is experienced both in the formal handling of analytical and empirical models for control and prediction, and in developing control software for real satellites. The merging of these themes is very promising. Although the results will be transferable to ground vehicles and robots, this project will particularly illustrate the new methods in space applications, both in simulation and laboratory hardware demonstrations.",,
0,C4CDA3D1-C51A-4241-ABB2-F401BC722C9D,Engineering Autonomous Space Software,"This proposal is based on two premises: that (1) increased autonomy is essential for future space exploration; (2) that existing programming methods are tedious to apply to autonomous components that have to handle an environment with continuous state variables. For well defined discrete-event environments the above rational agent approach is well developed; for a continuous environment, however, perception processes need to be linked with abstractions forming the basis of behaviour. As the environment changes, the abstracted models may also change. Hence, agents are needed that can use these abstractions to aid their decision making processes, use these in the predictive modelling of a continuous world, and connect these abstractions to both planning and goal achievement within rational agents.This project also intends to replace the current complex programming techniques, used for autonomous spacecraft control, with simpler declarative programming. High-level, declarative agent programming languages have been investigated at Liverpool and such theories and languages will be developed further for agents that require predictive modelling capabilities. The Southampton team is experienced both in the formal handling of analytical and empirical models for control and prediction, and in developing control software for real satellites. The merging of these themes is very promising. Although the results will be transferable to ground vehicles and robots, this project will particularly illustrate the new methods in space applications, both in simulation and laboratory hardware demonstrations.",,
1,52A98E29-A41F-4701-94BE-0EB88FF29DA3,SUAAVE: Sensing Unmanned Autonomous Aerial VEhicles,"The SUAAVE consortium is an interdisciplinary group in the fields of computer science and engineering. Its focus is on the creation and control of swarms of helicopter UAVs (unmanned aerial vehicles) that operate autonomously (i.e not under the direct realtime control of a human), that collaborate to sense the environment, and that report their findings to a base station on the ground.Such clouds (or swarms or flocks) of helicopters have a wide variety of applications in both civil and military domains. Consider, for example, an emergency scenarion in which an individual is lost in a remote area. A cloud of cheap, autonomous, portable helicopter UAVs is rapidly deployed by search and rescue services. The UAVs are equipped with sensor devices (including heat sensitive cameras and standard video), wireless radio communication capabilities and GPS. The UAVs are tasked to search particular areas that may be distant or inaccessible and, from that point are fully autonomous - they organise themselves into the best configuration for searching, they reconfigure if UAVs are lost or damaged, they consult on the probability of a potential target being that actually sought, and they report their findings to a ground controller. At a given height, the UAVs may be out of radio range of base, and they move not only to sense the environment, but also to return interesting data to base. The same UAVs might also be used to bridge communications between ground search teams. A wide variety of other applications exist for a cloud of rapidly deployable, highly survivable UAVs, including, for example, pollution monitoring; chemical/biological/radiological weapons plume monitoring; disaster recovery - e.g. (flood) damage assessment; sniper location; communication bridging in ad hoc situations; and overflight of sensor fields for the purposes of collecting data. The novelty of these mobile sensor systems is that their movement is controlled by fully autonomous tasking algorithms with two important objectives: first, to increase sensing coverage to rapidly identify targets; and, second, to maintain network connectivity to enable real-time communication between UAVs and ground-based crews. The project has four main scientific themes: (i) wireless networking as applied in a controllable free-space transmission environment with three free directions in which UAVs can move; (ii) control theory as applied to aerial vehicles, with the intention of creating truly autonomous agents that can be tasked but do not need a man-in-the-loop control in real time to operate and communicate; (iii) artificial intelligence and optimisation theory as applied to a real search problem; (iv) data fusion from multiple, possibly heterogeneous airborne sensors as applied to construct and present accurate information to situation commanders. The SUAAVE project will adopt a practical engineering approach, building real prototypes in conjunction with an impressive list of external partners, including a government agency, the field's industry leaders, and two international collaborators.",,
2,F89377D4-6132-4F47-8727-A03932DCBD71,3D World,"This project will deliver a cost-effective method of significantly improving the realism in the enhanced 3D online gaming platform under development by Realtime Worlds (RTW), one of Europe's largest independent computer games developers. The project will utilise sensor-derived data, location-based technologies, image processing techniques and user-generated inputs to create automatic mechanisms for the platform to integrate many 'real-world' elements, greatly improving the realism of the immersive experience.This represents a radical departure from traditional methods of development, reducing production costs and significantly improving return on investment. The improved 3D World Platform (3DWP) will target industries requiring a superior platform for highly-realistic, immersive multiplayer online game development. This will penetrate the fastest growing sector of the $3.7 billion worldwide video game market - expected to triple to $12.2 billion within five years, and enhance UK production capabilitiesin this sector. The Mobile Computing and Smart Sensors (MoCASS) Group at the University of Dundee will provide sensor and wireless-location technologies to capture real-time traffic flow data for the virtual world, and image processing and other expertise to enable the estimation of building footprints, heights and form factors from satellite (geomatic) data. The Smart Systems Group at the University of Abertay Dundee will also contribute expertise in sensors, image procesing and database management.",,
3,FD735019-4369-483D-BC58-CBD7F20E2968,Towards a next-generation computational neuroscience,"What don't we know? Recently, the journal Science selected 25 of the biggest unanswered questions facing scientists over the next 25 years. Number two on the list, right after figuring out the composition of the universe, is: What is the biological basis of consciousness? This indeed is a big question. Scientific descriptions of conscious experience, volition, and subjectivity will follow in the footsteps of Copernicus and Darwin by restructuring our relationship with each other and with nature, and many clinical and technological applications will follow.A scientific account of consciousness will not arrive fully formed in a 'Eureka' moment. What is needed is a multidisciplinary, integrative approach combining theory and experiment and exploiting the interchange between the information/computation sciences and the neural, psychological, and medical sciences. At the front-line of this interchange, computational neuroscience (CN) uses computational approaches to model intricate brain processes in much the same way that meteorology uses computers to forecast the weather. In this view and in contrast to early approaches to 'artificial intelligence' (AI), brains are not computers, and intelligent behavior and conscious experience arise from complex brain-body-environment interactions unfolding in temporally precise ways. Much current CN focuses on single levels of description of neural systems (e.g., how neural activity affects connections among neurons) and neglects the multi-scale relations that connect brains, bodies, and behavior. Moreover, current CN is also surprisingly silent with regard to consciousness itself. By targeting and overcoming these limitations, our research will deliver new insights into the neural mechanisms underlying adaptive behavior and conscious experience. We will follow three interacting themes: (i) design and analysis of large-scale CN models to explore how multi-scale neural interactions shape and are shaped by brain-body-environment interactions; (ii) development of new theory to identify causal interactions in complex networks (what we call 'causal network analysis'), and (iii) creation of CN models that account for functionally significant aspects of consciousness, for example that each conscious experience integrates diverse information sources into unified scenes. Theoretical work in the above themes will interact with experimental data from multiple sources. At a fine-grained level we will characterize causal interactions in the intact brain of a pond snail, shedding light on the integrated neural function of a simple (non-conscious) organism as it interacts with its environment. Zooming out, we will apply causal network analysis to brain-imaging data acquired from humans in various states of consciousness, to test predictions based on CN models, and to guide the design of new models. Insights at the fine-grained level will scaffold our understanding of the more complex mechanisms underlying consciousness, with causal networks cross-cutting brains, bodies and environments providing a common theoretical framework. Taken together, these research strands will catalyze an important shift from correlation to explanation in consciousness science.As well as advances in basic science, our research will have important practical benefits at the interface of the biological and information sciences. These will include new design principles for AI/robotic devices, new insights for the design and control of complex technological networks, and new tools for the management of large-scale datasets. A next-generation CN will also underpin new clinical approaches. Many brain-related health problems, from coma to depression to insomnia, can be understood as expressions of disordered consciousness, and many existing clinical approaches are palliative and lacking in theoretical foundation. Our research will provide a theoretical basis for a new generation of effective clinical interventions.",,
4,E21D88B5-FFFF-4DAD-8C51-C7DCAF228BCA,KT-EQUAL: Putting ageing and disability research into practice,"Research into ageing aimed at improving the lives of older and disabled people has received, over the last 11 years, significant financial support from the Engineering and Physical Sciences Research Council (EPSRC). In particular, the EQUAL (extending quality of life of older and disabled people) programme and associated network has encouraged a wide range of research; including design of the built environment to encourage safe and enjoyable use by older people, inclusive products which can be used by everyone including older people, and technological applications in the home to maintain independence in later life. The research has had a significant impact upon a range of groups; for example it has led to major changes to building regulations and to housing corporation specifications and has also identified best practice in a range of social, health, planning and design professions. A number of trial products have also been developed. With the increasing proportion of older people in the population, there is a growing urgency for evidence and knowledge to inform solutions to enable older people, for example, to maintain their independence, to continue to be active in the workplace for as long as they choose, and to benefit from emerging technologies. Meeting these and other challenges will be the focus of the work of the KT EQUAL consortium which brings together experts in engineering, construction, architecture, participatory and inclusive design, rehabilitation, psychology, change management and public engagement to work collaboratively with each other and with older people to promote knowledge transfer in innovative and effective ways. From the outset, EQUAL recognised the importance of involving older people so that they can both inform what research is undertaken in this area, and help to determine how that research is conducted. This will characterise the approach and working methods of the KT EQUAL Consortium. We will consult with older people, their carers, those that work with older people, with policy makers and with others. Older people will be invited onto project groups to directly influence the work programme of the Consortium. Views of a wider group of participants (including older people) will be sought through a variety of activities which will take place in different venues across the country. In these ways, older people will help us to identify the research topics and research results which are of importance to them, and what should happen as a result. A priority for the KT EQUAL consortium will be to actively draw the attention of industry and others to the needs of the ageing population and to the outcomes and impact of the EQUAL and SPARC programmes which offer great potential benefits for society, including excellent investment opportunities for industry. This will be achieved in part through events to raise awareness and a high profile in the media. We will work with stakeholders such as manufacturers, technologists, designers, as well as those responsible for delivering public services throughout. The Consortium will also encourage researchers to publicise their work in ways that will be understood by a wide audience so that new ideas might be taken forward by older people themselves, by professions working directly with older people and by those developing services/ products for older people.The KT EQUAL Consortium will also take over from SPARC to support the career development of researchers who are interested and committed to research concerning older people. We need to support the workforce that has already been funded through the EQUAL programme as well as identifying new researchers. We will also address the pressing need to develop the capacity of older people to engage and participate in ageing research and in the application of its findings..",,
5,B3BB81B6-3CF6-4D4A-8DD4-288AA89FB039,KT-EQUAL: Putting ageing and disability research into practice,"Research into ageing aimed at improving the lives of older and disabled people has received, over the last 11 years, significant financial support from the Engineering and Physical Sciences Research Council (EPSRC). In particular, the EQUAL (extending quality of life of older and disabled people) programme and associated network has encouraged a wide range of research; including design of the built environment to encourage safe and enjoyable use by older people, inclusive products which can be used by everyone including older people, and technological applications in the home to maintain independence in later life. The research has had a significant impact upon a range of groups; for example it has led to major changes to building regulations and to housing corporation specifications and has also identified best practice in a range of social, health, planning and design professions. A number of trial products have also been developed. With the increasing proportion of older people in the population, there is a growing urgency for evidence and knowledge to inform solutions to enable older people, for example, to maintain their independence, to continue to be active in the workplace for as long as they choose, and to benefit from emerging technologies. Meeting these and other challenges will be the focus of the work of the KT EQUAL consortium which brings together experts in engineering, construction, architecture, participatory and inclusive design, rehabilitation, psychology, change management and public engagement to work collaboratively with each other and with older people to promote knowledge transfer in innovative and effective ways. From the outset, EQUAL recognised the importance of involving older people so that they can both inform what research is undertaken in this area, and help to determine how that research is conducted. This will characterise the approach and working methods of the KT EQUAL Consortium. We will consult with older people, their carers, those that work with older people, with policy makers and with others. Older people will be invited onto project groups to directly influence the work programme of the Consortium. Views of a wider group of participants (including older people) will be sought through a variety of activities which will take place in different venues across the country. In these ways, older people will help us to identify the research topics and research results which are of importance to them, and what should happen as a result. A priority for the KT EQUAL consortium will be to actively draw the attention of industry and others to the needs of the ageing population and to the outcomes and impact of the EQUAL and SPARC programmes which offer great potential benefits for society, including excellent investment opportunities for industry. This will be achieved in part through events to raise awareness and a high profile in the media. We will work with stakeholders such as manufacturers, technologists, designers, as well as those responsible for delivering public services throughout. The Consortium will also encourage researchers to publicise their work in ways that will be understood by a wide audience so that new ideas might be taken forward by older people themselves, by professions working directly with older people and by those developing services/ products for older people.The KT EQUAL Consortium will also take over from SPARC to support the career development of researchers who are interested and committed to research concerning older people. We need to support the workforce that has already been funded through the EQUAL programme as well as identifying new researchers. We will also address the pressing need to develop the capacity of older people to engage and participate in ageing research and in the application of its findings..",,
6,206E7DA7-DD2E-46E6-BF81-5AA28EA96E5B,AI Social Agents,"Emote Games are developing a social networking platform upon whichgames will be played by a network of people. Such networks areenhanced by the presence of Artificial Intelligence (AI) members,which can direct human members to perform certain activities, offertutorial support, set up subgroups, etc. State of the art AI socialagents are fairly unimpressive - they are reactive rather thanproactive, and they tend to react in scripted and largelyuninteresting ways. It is fair to say that such agents are viewed asutilitarian in nature and not particularly interesting to interactwith. There is therefore much room for improvement, and to becomemarket leaders, Emote Games want their AI agents to add greater valueto the society. However, building agents to act intelligently (andperhaps moderately creatively), is a non-trivial problem which willrely on the use of various AI methods, including constraint solving,multi-agent systems, machine learning, planning and natural languageprocessing.Emote will iteratively build increasingly smarter AI agents for theirnetwork, with functionality that transfers from game to game. Each newrelease of the agents will perform more tasks, and perform existingtasks in more intelligent ways. To do this, each new release will beinformed by systematic experimental testing, which will be carried outat Imperial College. The Combined Reasoning Group at Imperial(www.doc.ic.ac.uk/crg) has much experience of building integratedsystems that deliver the kind of multifaceted intelligent behaviourrequired for this project. At the top level, we will build amulti-agent system architecture based on the beliefs, desires andintentions (BDI) model of agent behaviour. BDI agents are ideallysuited for situations where they are embedded, social, reactive, andgoal directed. This is a partial match to our requirements, and wewill experiment with various extensions which may be more suitable formore pro-active situations. We intend to determine the bestcommunication and behaviour framework to control the interaction ofthe agents within the network.Each agent will be semi-autonomous and will, like any other networkmember, have tasks to perform with respect to the game context. Wewill experiment with various planning approaches to control theoverall execution of the task, which will involve appealing to variousAI methods. As an example, an AI agent might be required to broadcasta piece of news to the right kind of network members, receive feedbackfrom the network members, and learn to do the same kind of task betterin future. Within an overall plan of action, determining which membersof the network to inform will be solved using constraint satisfactiontechniques. The communication of the news and the parsing of feedbackcomments will involve natural language processing techniques. Finally,given positive and negative feedback from the network members, theagent will set up a machine learning problem to generate a classifierwhich can be used in future to refine the constraint-based search fornetwork members to communicate news to.Emote Games will build the overall social network and the AI agents in it. They will specify the kinds of activities that agents will undertake, and write simple static scripts to do this. Imperial will beresponsible for suggesting planning methods and AI techniques toundertake the tasks in a more believable, intelligent and engagingfashion. Moreover, Imperial will deliver experimental results whichdemonstrate which approach(es) are the best for particular tasks. Toperform these kinds of experiments, we plan to work in controlledsituations where we gain feedback from subjects about theirsatisfaction with their interactions with the AI agents. In caseswhere the performance of the agents is sub-optimal, Imperialresearchers will research novel AI methods to improve matters.",,
7,F21ABC0A-AC32-4DF5-90F4-C6F224F86ED7,Qualitative Performance Assessment of Adaptive Filtering and Machine Learning Algorithms,"Signal modality characterisation, that is, the assessment of the linear, nonlinear, deterministics and stochastic signal content, is becoming an increasingly important area of multidisciplinary research. These ideas arose in Physics in the mid-1990s, however, the applications in machine learning and signal processing are only recently becoming apparent. As changes in the signal nature from, say, linear to nonlinear, can reveal e.g. health hazard, the signal processing framework should be chosen so as to preserve this critical information. However, standard learning algorithms are typically based on second order statistics, and will linearise naturally nonlinear phenomena.This proposal aims to provide a novel theoretical and computational framework for the design of learning algorithms with enhanced qualitative performance. Standard, second order statistics based adaptive filtering and machine learning algorithms are designed to optimise quantitative performance, and useful information is often lost. This type of problem arises typically in biomedical applications, for example, the change in the nature of brain electrical recordings from linear stochastic (ARMA) to nonlinear deterministic (chaotic) can indicate health hazard. The fundamental novelty of this work is a recently proposed, but not fully tested, delay vector variance (DVV) method which examines the local predictability and determinism of a signal in phase space, and provides a measure for the degree of linear, nonlinear, deterministic, and stochastic signal natures. This will serve as a framework to analyse the changes that signal processing and machine learning make to signal natures, and as a basis for the development of novel optimisation criteria which will both provide the required quantitative performance and preserve the fundamental signal nature to the desired degree. The team at Imperial have performed conceptual work related to this proposal, but no rigorous statistical evaluation or relavance analysis of the underlying state space features. The proposed research will perform comprehensive testing in order to provide enhanced understanding and insight into the qualitative peformance of learning algorithms used in biomedical applications. This will also lead to the design of novel adaptive learning algorithms capable of preserving the signal nature to a desired extent, a critical issue in several emerging applications.Solutions to these problems open new possibilities for advances in biomedical engineering, which underpins this research proposal, based at Imperial College and in collaboration with a leading applied biomedical group from Germany.",,
8,20A969D6-7C07-4AA1-A83C-4A7AF64AC7BB,CAD-GAME: Computer-Aided Game Design,"As a form of media - increasingly, mass media - games have much in common with film, particularly in terms of business models. One reason for the high levels of risk in film is that only limited market testing can be done prior to product release, and the product is essentially fixed from then on. Games have traditionally followed this model, but it is important to remember that games are not just media - they are also pieces of software. One major lesson from the software industry is that capturing users' interactions with a product can be used to refine market offerings in several ways: by adapting it after release (e.g. adaptive interfaces), by using that data to iteratively refine products in future releases (e.g. error reporting in Microsoft and Apple), and to understand the various user groups in more depth. In essence, we are suggesting that this approach can be directly applied to games. The difficulty, however, is that we cannot use simple measures of task completion, because there is no task: we are simply trying to entertain. The main research challenge is to understand better just what the word 'entertain' really means in games, and how this might be reflected in patterns of player behaviour. To do this, we will undertake a study of enjoyment and immersion in video games, in order to derive a concrete methodology for closely estimating the amount of enjoyment and immersion a player has, at fairly fine-grained intervals of a game. We will also capture a multitude of different forms of data from players actually playing Rebellion's games, and we will use advanced techniques from Artificial Intelligence to search for correlations with the player's overall enjoyment. These techniques could be used during design to speed up the process of tweaking the various game parameters - where the enemies are, how effective weapons are, etc. More interesting, however, is the idea that data analysis tools could run within the game itself, adapting the game in response to the player's behaviour. In this way, we hope to pioneer a new age of user-adaptive video games, within a one game, many gameplays paradigm, i.e., where each player has a unique game experience which is tailored to their personality, level of experience, playing style and mood.",,
9,2B10BC58-5287-4CA0-A016-3DEF46B834B4,Automatic ontology augmentation: evaluation issues,"New scientific knowledge in areas such as biology and biochemistry is being discovered at an unprecedented rate. In some cases, structured knowledge sources are available, in the form of semantic web markup or databases, but most scientific discoveries are still reported in thetraditional way, as journal articles or conference proceedings. Managing this vast amount of information, whether structured or unstructured, requires mapping between disparate knowledge sources, involving different nomenclature and relationships. Ontologies have played a critical role in addressing the challenge of semantic integration of such knowledge. Constructing ontologies is an extremely laborious effort. Not only must researchers agree on the concepts and relationships needed for a domain of knowledge, but they must also do so in a way that minimizes errors and is easy to update and maintain. There is therefore considerable interest in creating or augmenting ontologies automatically by analysing text. However, none of the research in this area has yet had a significant impact on the process of creating ontologies for scientific domains. In this short project, we intend to collaborate with a visiting researcher, Dr Inderjeet Mani, who is a leader in this field to look specifically at the issue of evaluatingautomatically created ontologies. Research in language processing in general requires good evaluation techniques to be agreed on by the relevant community. Without such techniques, it is impossible to replicate results and build on previous work in a motivated fashion. Evaluation of automatically created ontologies is in its infancy, which is hampering research in the area. The most effective way to make progress is by intensive discussion between different groups,backed up with small scale experimentation. Dr Mani's visit will allow us to improve on existing evaluation practice.",,
10,E0262FB4-4060-4FC5-8AD0-A162CEDEDCE2,EnAKTing the Unbounded Data Web: Challenges in Web Science,"The EPSRC funded Advanced Knowledge Technologies Interdisciplinary Research Collaboration (AKT IRC) has been a significant success in terms of papers published, grants awarded, students trained, and international impact. The Review Panel rated the project as outstanding scoring 34 out of a maximum possible 35 on the seven review criteria used to assess the results of projects by the EPSRC. The purpose of this proposal is to take some of the most important results from AKT and organise a next stage of research. This in turn will serve as a precursor to a longer-term ambition; the establishment of Web Science as a discipline. This initiative we are undertaking with the Web's inventor Professor Sir Tim Berners-Lee and MIT. The development of new Semantic Web technologies (many developed and researched in the AKT IRC) points to a new generation of Web capability that can explore and query, assemble and integrate content in a context-aware, focused fashion. The basic idea is that we move from a document centric view of the Web to one in which data and information are the principle objects of interest. This data may relate to people, scientific structures, financial transactions or any domain that can be represented on the Web. With the emergence of a Web of data it is essential to address three key research problems; (1) how to build ontologies quickly that are capable of exploiting the potential of large-scale user participation, (2) how we query an unbounded web of linked data, (3) how to visualise, explore, browse and navigate this mass of data.The proposal is to undertake fundamental research in the areas 1-3 identified above. This fundamental research is supported via two application domains; one in the area of public sector information, a second in the domain of transport. The application domains will provide the context in which to gather realistic requirements, understand the social aspects that determine the success or otherwise of the systems constructed, test the adequacy of solutions, and showcase the promise of the results obtained in pursuing the research objectives outlined.",,
11,97DE34A5-C18A-452D-A379-F179517909DB,CHIME: Computational Hearing in Multisource Environments,"In everyday environments it is the norm for there to exist multiple sound sources competing for the listener's attention. Understanding any one of the jumble of sounds arriving at our ears requires being able to hear it separately from the other sounds arriving at the same time. For example, understand what someone is saying when there is a television on in the same room requires separating their voice from the television audio. The lack of an adequate computational solution to this problem prevents hearing technologies from working reliably in typical noisy human environments -- often the situations where they could be most useful. Computational hearing algorithms designed to operate in multisource environments would enable a whole range of listening applications: robust speech interfaces, intelligent hearing aids, audio-based monitoring and surveillance systems.The CHIME project will develop a framework for computational hearing in multisource environments. Our approach operates by exploiting two levels of processing that combine to simultaneously separate and interpret sound sources. The first processing level exploits the continuity of sound source properties, such as location, pitch, and spectral profile, to clump the acoustic mixture into pieces (`fragments') belonging to individual sources. Such properties are largely common to all sounds and can be modelled without having to first identify the sound source. The second processing level uses statistical models of specific sound sources expected to be in the environment. These models are used to separate fragments belonging to the acoustic foreground (i.e. the `attended' source) from fragments belonging to the background. For example, in a speech recognition context, this second stage will recruit sound fragments which string together to form a valid utterance. This second stage both separates foreground from background and provides an interpretation of the foreground.The CHIME project aims to investigate and develop key aspects of the proposed two-level hearing framework: we will develop statistical models that use multiple signal properties to represent sound source continuity; we will develop approaches for combining statistical models of the attended `foreground' and the unattended `background' sound sources; we will investigate approximate search techniques that allow acoustic scenes containing complex sources such as speech to be processed in real-time; we will investigate strategies for trying to learn about individual sound sources directly from noisy audio data. The results of this research will be built into a single demonstration system simulating a home-automation application with a speech-driven interface that will operate reliably in a noisy domestic environment.",,
12,B26DB18B-BBB0-4073-8061-77F871EE3DF5,ConDOR: Consequence-Driven Ontology Reasoning,"Ontologies, and ontology based vocabularies, are becoming increasingly important. They provide a common vocabulary together with computer accessible descriptions of the meaning of relevant terms through relationships with other terms. For example, in an ontology describing human anatomy the vocabulary could include terms such as [Organ], [Circulatory System], [Heart], etc., and one can define a term [Muscular Organ] as an [Organ] that is a part of the [Muscular System] and a term [Heart] as a [Muscular Organ] that is a part of the [Circulatory System].Ontologies play a major role in the Semantic Web and in e-Science where they are widely used in, e.g., bio-informatics, medical terminologies and other knowledge management applications. One of the most important aspects of ontologies is that they contain knowledge structured in a special way. The users of ontologies are typically interested in obtaining information about relationships between concepts described in ontologies and querying the ontologies. Both tasks require reasoning tools that can derive new knowledge from the knowledge explicitly stated in ontologies. For example a reasoning tool should be able to derive that [Heart] is a part of the [Muscular System] which is not explicitly stated in the anatomical ontology but is a logical consequence of the above definitions for [Heart] and [Muscular Organ].Most existing ontology reasoners do not derive logical consequences of ontological axioms explicitly, but instead they check whether it is possible to construct a model of the ontology where the target consequence does not hold, e.g., they try to construct a situation where [Heart] would be a part of the [Circulatory System] but not a part of the [Muscular System]. If such a situation is not possible, then it is concluded that the target consequence follows from the axioms in the ontology. One problem with this technique is that when an ontology expresses long and possibly cyclic dependencies between terms, e.g., [Heart] is a part of [Circulatory System] which has a part [Lung] which is a part of [Respiratory System] which has a part [Trachea], etc., then the reasoner has to construct very large models. For some existing medical ontologies, the models are so big that they do not fit into the main memory of a computer. Another problem is that the ontology may potentially have a large number of different models, each of which must be independently explored by the reasoner. Ontology languages provide for constructors called 'number restrictions', which result in a particularly large number of models. Number restrictions are used to specify quantitative information in ontologies and are often used in bio-chemical ontologies, for example to express that a molecule of [Ethanol] contains {exactly 6} [Hydrogen Atoms]. These limitations of model-building reasoners, therefore, pose a serious problem for the development of large medical and bio-chemical ontologies---without efficient reasoning tools, for example, the users of such ontologies may not be able to obtain the information that they are interested in.In this project we investigate alternative consequence driven reasoning procedures that do not build models but explicitly derive logical consequences of ontological axioms. Our preliminary investigations suggest that both problems mentioned above can be avoided for consequence-driven reasoning procedures: there is no need to keep track of large models, and the number of logical consequences of ontological axioms is typically much smaller than the sizes and the number of the models.",,
13,7369613E-F424-4E0E-9876-F2F522F3F6B8,"Support for the 13th Ph.D. Summer School (Advanced Course on Artificial Intelligence - ACAI2009) in Northern Ireland, UK.","The School of Computing and Mathematics has won a bid to organize the 13th Ph.D. Summer School Advanced Course on Artificial Intelligence (ACAI2009) in Northern Ireland. This prestigious Summer School is organized biannually by the European Coordination Committee on Artificial Intelligence, each year in a different European country. This is the first time that ACAI is organized in the Britain or Ireland. ACAI'09 will focus on methods and tools available for the development of Intelligent Decision Support Systems. These systems can take decisions by themselves or by interacting and cooperating with humans. As such they have a wide range of important applications in areas like healthcare, finance, emergency response, etc. The Summer School will include courses delivered by distinguished academics and industrial representatives with extensive experience in the development and application of such systems. The courses are designed to be interactive and will consist of a mixture of lectures and hands-on practice.",,"Beneficiaries - UK researchers including PhD students and advanced MSc students Benefits/Impact It provides an opportunity for UK researchers to interact with some of the world leaders in IDSS research. As ACAI'09 will be hosted in the UK, we expect it to attract a larger number of UK researchers, PhD students in particular, than are typically able to attend other summer schools and major international conferences, say in the USA. We hope that this will result in more international collaborations in future years, which will help advance IDSS research in the UK. 2) It provides an opportunity for UK researchers to participate in an assessment of the state of IDSS research and an assessment of their role in shaping the future of IDSS research. This is significant if IDSS research in the UK is to stay at the forefront of this field of study. 3) It provides a unique training opportunity for PhD students. There is a large number of PhD students currently studying in fields related to IDSS in the UK, and we believe that they can benefit from interacting with experts in their PhD subject areas early in their studies. ACAI'09 will enable them to do so in an affordable way (The organising committee of ACAI'09 will endeavour to make the associated PhD forum as affordable as possible by seeking sponsorships for ACAI'09 ). 4) ACAI'09 can also indirectly benefit IDSS related industry in the UK. Today's IDSS research is diverse, and there is a strong feeling within the international IDSS community that one size does not fit all and new IDSS tools are needed. The range of topics covered by the invited speakers at ACAI'09 represents some of the emerging areas where novel products may be developed, and can therefore help inform the industry, through participants and their industrial collaborators, of opportunities for new product development in future years. We are also interested in reinforcing the link between academy an industry therefore we are keen on providing a space for companies using IDSS to participate of this event. SAP, an international company with a branch in Belfast, already signed up to provide a talk on how they use IDSS in their company. We will be contacting other companies using IDSS who wish to share their experience and interact with the participants of ACAI'09."
14,D7B5A6C3-E4CF-4CD0-A17D-E0A3E04905B8,Automated Discovery of Emergent Misbehaviour,"Computational models are essentially computer programs intended to simulate a natural system, such as an ant colony, the formation of skin tissue or the global economy. Scientists and industrialists use computational models to help develop their understanding of the natural system being modelled, to make forecasts, and to predict the impact of some change to the system. A topical example of an application of a computer model might be to predict the impact of nationalising the Northern Rock bank on the UK economy.Agent-based models are computational models which have been developed from the point of view of the main actors in the system, e.g. a terrorist in a model of civil uprising, or a voter in an election. Observations made from simulating the model can be understood and explained in terms of the individuals involved, answering questions such as 'why is there civil disturbance in this area of the country?', 'why is this political party popular here?' or 'why has a tumour developed here?'. Such inferences are arguably difficult to make from 'top-down' approaches to modelling, which are composed of a set of mathematical equations.As predictions and scientific discoveries are reliant on the models being implemented correctly, the consequences of not properly testing a model can be extremely serious, and have cost companies several millions of pounds in the past. However, traditional software testing strategies, which take a 'divide and conquer' approach, are difficult to apply to agent-based models. The interaction of agents in a simulation, often at random, produces complex patterns and behaviours. Thus, it is difficult to predict which causes will lead directly to which effects. The proposed research here intends to test agent-based models using intelligent search techniques, with the specific intention of 'homing in' on behaviours of the model that have not been exposed in previous simulation runs. In this way, the search process will encourage testing of the model in unlikely or ill-conceived situations, where the model's behaviour may diverge from that intended. In order to do this, the search process will build up an abstract picture of what the model is doing in simulation through extension of a technique known as invariant detection. Invariants are statements that are always found to be true. The search will essentially aim to falsify generated invariants generated from past simulations in order to demonstrate new behaviours of the model. These are likely to be rare or unexpected behaviours that may not have been previously tested, and thus possible instances where software errors may be lurking.",,
15,A1B05299-5438-45ED-9BBC-95983F6158B9,Lexical Acquisition for the Biomedical Domain,"Natural Language Processing (NLP) is now critically needed to assist the processing, mining and extraction of knowledge from the rapidly growing literature in the area of biomedicine. In recent years, considerable progress has been made in the development of basic NLP techniques for biomedicine. The current challenge is to improve these techniques with richer and deeper analysis capable of supporting a wide range of real-world tasks. High-quality lexical resources (e.g. accurate and comprehensive lexicons and word classifications) are critically needed for this. Most lexical resources used in current systems are developed manually by linguists. Manual work is extremely costly, and the resulting resources require extensive labour-intensive porting to new (sub-)domains and tasks. Automatic acquisition or updating of lexical information from repositories of un-annotated text (e.g. corpora of biomedical articles) is a more promising avenue to pursue. Since lexical acquisition gathers usage and frequency information directly from relevant data, it can considerably enhance the viability and portability of NLP technology. Research into automatic lexical acquisition is now starting to produce large-scale resources useful for practical NLP tasks. However, the application of such techniques to biomedical texts has been limited because many existing techniques require adaptation before they can perform optimally in this linguistically challenging domain. In this project, we will take existing techniques capable of acquiring basic syntactic-semantic information for verbs from corpus data and will adapt them to the biomedical domain. We will focus on verbal (i) subcategorization frames, (ii) selectional preferences, and (ii) lexical-semantic classes. This information, when tailored to the domain in question, can aid key NLP tasks such as parsing, anaphora resolution, Information Extraction (IE), and question-answering (QA). Building on our pilot studies and expanding on the adaptive, state-of-the-art text processing tools available to us, we will improve existing techniques further and extend them with novel unsupervised and semi-supervised methods capable of supporting efficient domain adaptation. We will evaluate and demonstrate the capabilities of our techniques directly and in the context of practical BIO-NLP tasks. We will use the final version of the system to acquire a substantial lexical database from a biomedical corpus. The resulting resource will be distributed freely to the research community, along with the software which can be used to tune the frequency information stored in the database to particular biomedical sub-domains/tasks.We expect this project to (i) advance BIO-NLP and improve its usefulness for practical tasks in biomedicine, (ii) advance NLP by improving the accuracy, robustness and portability of lexical acquisition to real-world tasks, and (iii) provide an important large-scale study of domain-adaptation in the critical area of lexical acquisition.",,
16,E6D57A5A-F711-467E-84C8-2B86A04B25E6,From Frequent Itemsets to Informative Patterns: Theory and Applications,"This project will address a problem that has recently been highlighted as a major research challenge in data mining: the fact that the output of many data mining algorithms is typically (and with the present technology often unavoidably) too large for human understanding and interpretation, and therefore of little use for many practical purposes. This is the case in particular for the output of the important and broad class of frequent pattern mining algorithms, which are in the focal point of this proposal. (Such patterns can be subsets in sets, subgraphs in graphs, or subsequences, substrings, or approximate substrings in strings, and much more.)To address this challenge, we will develop new data mining approaches for effective database summarization and understanding. We will achieve this by combining, integrating, and developing state-of-the art ideas from data mining, statistical modeling, and optimization theory.The solution of this problem is likely to boost the impact of all frequent pattern mining techniques, of which the development required a considerable community effort, but of which the uptake in applied research and industry has remained limited so far. Therefore, this project may have a significantly non-linear effect on the research community: it has the potential to unleash a large amount of data mining expertise for practical application.To underline the impact of our theoretical results, and to ensure that they are used in practice, in a second part of this project we will be the first ones to apply our results to a number of case studies. We have intentionally chosen these case studies from three diverse domains: bioinformatics, text mining, and marketing. Each of these domains has a large user base, all of whom will become beneficiaries of this project. By developing these applications, we will be able to demonstrate the use of our newly developed methods to these user groups. The bioinformatics application we will tackle will be the search for transcriptional modules of genes, which are regulated by the same regulatory program under certain conditions. The data driven nature of bioinformatics makes data mining approaches very well suited, and our new methodologies will ensure their successful application with a minimal amount of expert knowledge required. Note that this is just one example of a bioinformatics application -- frequent pattern mining (subsets, subgraphs, subsequences, substrings...) is of use in many other branches of bioinformatics.In the text mining application we will search for interesting sets of words, sequences of words, or approximate sequences (strings) of words, occurring in a corpus of text documents. These text documents will be sets of news articles over a certain time span. We should point out that we are currently already gathering thousands of news articles every day for subsequent text mining analyses, and this in the context of another project being developed together with Prof. Nello Cristianini in the same department. The integration of this result with that project will provide our new methodology with a unique showcase demonstration.For the marketing application, the most obvious application would be the search for items in a supermarket store that are often sold together. Marketers are interested in this information, for example to allow them to optimize their promotion strategies. For example, they may choose to reduce the price of one product, and increase the margin on other products that are strongly associated with the former. Our collaboration with Unilever, who have such retail transaction data available for use in this project, will enable us to successfully complete this application.",,
17,8749FB06-9FAE-4E1F-B5E3-2D59DF3099DA,Quality of Service Provision for Grid Applications via Intelligent Scheduling,"Grid computing can be defined as coordinated resource sharing and problem solving in dynamic, multi-institutional collaborations. The success of a Grid infrastructure is based on a number of fundamental requirements, including the ability to provide dynamic and efficient services. Underpinning such a system is the need to ensure that the Grid infrastructure is delivering the required Quality of Service to its users. Quality of Service (QoS) is the ability of an application to have some level of assurance that users' requirements can be satisfied. It can be seen as an agreement between a user and a resource provider to execute an application within a guaranteed time frame at a pre-specified cost. As a rule, the higher the cost paid by the user, the smaller the execution time a resource provider can ensure. The novel contribution of the proposed project is to produce a new type of Grid resource broker with an advanced scheduling component aimed at optimising both resource usage costs and applications' execution times to enforce QoS. It should combine the features of two types of brokers: system-centric and user-centric providing a transparent means of meeting users' requirements and at the same time optimising the usage of Grid resources on the provider's end. This proposal is timely in that it addresses the need for continued development of infrastructure support for Grid computing. It responds to the increased attention of the Grid community to QoS provision and higher expectations of Grid users to receive adequate services at an agreed price payable for the agreed execution time. Our research will take advantage of the achievements in the classical scheduling theory and the newly emerged Grid scheduling research and will advance the frontiers of both areas. Grid applications give rise to new enhanced scheduling models. These enhanced models generally cannot be handled by the existing scheduling techniques developed mainly for manufacturing applications. They are characterised by complex additional constraints including those related to data storage and data transfer, co-ordinating the execution of linked tasks and arranging the required data interchange. Further challenges are related to the dynamic nature of Grid systems with the changing availability and quality of resources. The new aspects of QoS provision introduce additional complexity to scheduling. The project will draw on expertise of two established research groups at the University of Leeds: Algorithms and Complexity Group and Collaborative Architectures and Performance Group. The Algorithms and Complexity Group performs multidisciplinary research in algorithms, combinatorics and optimisation. Inter alia, the group develops and analyses advanced mathematical techniques for solving complex optimisation problems including those related to the areas of scheduling and optimal resource allocation. Research of the Collaborative Architectures and Performance Group focuses on Intelligent Infrastructures for large-scale applications. In particular, research of the group brings together e-science, Grid and adaptive computing systems research.",,
18,F83982B3-BF02-448F-849A-636B3F900BBC,Imbalanced Data Set Modelling and Classification for Life Threatening/ Safety Critical Applications,"Machine learning from imbalanced data sets is related to a broad range of very important problems in many engineering and scientific disciplines, e.g. medical diagnostics, signal detection and machine/material fault detection. Apart from the highly practical value, data learning from imbalanced data sets is also of high theoretical interest. Because the performance metrics used in conventional classifier construction may break down when applied to the imbalanced data sets, this has motivated considerable researches in machine learning communities aimed at a variety of learning methodologies for the imbalanced data setsDespite significant research in machine learning for imbalanced data, there is still a need and/or a lack of general methodologies that are able to deliver the capability of knowledge discovery as demanded by many hugely important applications. For example, it is highly beneficial to discover new noninvasive biological markers from clinical data, which can improve early medical diagnostics results, in order to start early treatment of a cancer. The motivation of the proposed research can be illustrated by another example. In material science, suppose that new materials with exceptional properties, e.g. strength, are required for new mechanical structures, e. g. military vehicles. For this purpose, a sample of experimental trials is performed to obtain a new material together with the measurements of the properties. It is highly desirable that the properties/behaviours could be discovered, by resort of data modelling using a small sample, rather than performing many more unnecessary and very expensive engineering experiments (large sample).This proposal is concerned with the development of a new modelling approach which builds upon the state-of-the-art nonlinear modelling methodologies and is specifically designed for pattern recognition using the imbalanced data sets. The objectives of the research include the modelling, classification, class probability (risk) prediction and knowledge discovery from the imbalanced data sets which are commonly found in many associated applications.",,
19,96398258-F662-46D8-9D31-9BF7CC9AFB52,Dialectical Argumentation Machines,"Humans use argument to express disagreement, to reach consensus and to both formulate and convey reasoning. The theory of argument has found wide application in artificial intelligence, providing mathematical structures for automated reasoning, communication protocols for distributed processing and linguistic models for natural language processing. A key stumbling block, however, has been joining together models that focus on abstract, mathematical relationships with those that focus on concrete, linguistic relationships. The first objective of this project is to develop for the first time a theoretical account that connects static, monologic argumentwith dynamic, multi-person, dialogic argument and ties together abstract, mathematical models with concrete, linguistic representations.Furthermore,models of argument have been predominantly confined to the lab. Our goal is to translate the research advances into high profile, large scale deployments using partners with enormous user bases. Prototype systems in this area have been sufficient to demonstrate the unique advantages of practical argumentation systems to potential users of this research such as those within the broadcasting domain. There is a demonstrated public demand for argument-based exploration of current issues with complex scientific and ethical dimensions, demonstrated, for example, by the longevity and success of high profile programming featuring topical issues discussed in a stylised argumentative debate format. The second objective of this project is to develop the theory into implemented components that can form a foundation for application development to support actual programmes with prototype testing Unique advantages afforded by the technology will allow users to interact with the programme material as if they were themselves contributors, allowing arguments to be probed, tested and extended, and the distinction between in-programme and post-programme content to be blurred. The interaction metaphor shifts from 'message-then-next-message' to 'question-answer-riposte-challenge...'.The rich structure is natural for users, and provides rich metadata for programme-makers. Finally, in 2007 an exciting vision of the world-wide argumentation web (WWAW) was laid out, in which systems such as those constructed to work alongside practical prototypes could interact, both with each other and with other debate and argumentation systems, both populist and academic.Argument fragments, expressed as resources on the Semantic Web, can cross-refer, allowing different debating systems to navigate the WWAW according to various rules of dialogue captured by dialectical games. To bring this vision of the WWAW into reality, the third and final objective of the project is to allow execution of arbitrary dialogue games on a platform that provides interfaces for human players, and both interfaces and control for computer players of dialogue games. In this way, we want to harness the enormous channel to market and the high-profile reference case that is offered by collaboration within broadcasting. At the same time, the project will be developing platform technology that can support exploitation in other areas. During the project, we will work with the Scottish Mediation Network in the context of mediation tools, with the Ontario courts in the context of judicial summaries, and with the Universities of Lugano and Groningen in the context of legal education to identify exploitation routes for the technology.",,
20,9CDC7D38-14F4-4268-AABA-464207E32A5F,University of Bristol Bridging the Gaps Cross-Disciplinary Feasibility Account,"A dynamic, flexible approach will be taken to managing a fund for cross-disciplinary feasibility projects in areas that have emerged from the interactions that took under the Bristol University's EPSRC Bridging the Gaps award. Five initial ideas are : Integrated Uncertainty in Environmental Modelling; Fracture in Engineering and Geology; Pure Mathematics and Algorithms; Complex Networks and Optimal Search Strategies; Pattern Analysis and Global News Mining. The grant will be run like a mini-research council, seeking to fund short-term, high risk feasibility studies within the remit of the call. Given the short time available, the first 5 studies (outlined above) will be funded immediately for 6 months. Two further calls will then seek fresh proposals or continuation funding for existing studies. The BTG committee will be continued, but its membership widened to six academics to cover the wider remit of the feasibility account. It will report to the University Research Committee. Lessons learned from BTG suggest 20% of project management support will be required, and 20% of administrative support to manage the resources, to engage with researchers and to maintain publicity and dissemination via the exisiting BTG website. The situation of the project manager within Research and Enterprise Development will maximise opportunities to seek follow-on support.",,"The dynamic, flexible nature of the way this grant will be run makes it hard to be specific about particular impacts of the work. The fact that we are specifically encouraging feasibility studies that engage with collaborators outside the traditionally EPSRC-funded areas will increase the likelihood of impact. The proposed work plan with 5 research areas funded immediately from the start is a testimonial of the effectiveness of the management plan. Competition for the available resources will drive the results forward and will engage the various users. When results are particularly encouraging, we might choose to engage the policy-makers, the public sector, or the wider public through training materials or software when appropriate. Part of the role of the project manager will be to engage with the investigators of each of the funded projects and to liaise with other colleagues with the University Research and Enterprise Development Office (RED) to maximise these opportunities. In terms of the 5 initially funded studies potential impacts are as follows: Fracture in Engineering and Geology: Impacts in aerospace,mechanical, civil and marine engineering as well as in seismology will be sought by inviting industry representatives to the proposed workshops. Pattern Analysis and Global News Mining. A final workshop at the end of the feasibility study will initially seek to engage with further social scientists about the possibilities of these tools. Regular contact will be maintained with RED to keep in mind at every stage of the research all the various mechanisms available for commercialisation. Integrated Uncertainty in Environmental Modelling: In the area of flood risk modelling the University has led the national NERC-funded consortium in this area. Further opportunities for international dissemination are likely to come about through Prof Steve Sparks' Marie Curie large grant. Pure Mathematics and Algorithms: Applications of algorithms that enable energy efficient computation (so-called Green-IT) will be sought through industrial connections, for example with HP labs. Complex Networks and Optimal Search Stratgies: As well having better prediction on how and where an animal would move would have profound consequences for government agencies involved in preventing the spread of zoonotic diseases and could be used in taking conservation biology decisions by public sector agencies."
21,B6B1A776-9FD2-4D5D-B8AB-434745F73F54,Scaling up Statistical Spoken Dialogue Systems for real user goals using automatic belief state compression,"Spoken dialogue systems (SDS) are increasingly being deployed in avariety of commercial applications ranging from traditional CallCentre automation (e.g. travel information) to new ``troubleshooting''or customer self-service lines (e.g. help fixing broken internetconnections).SDS are notoriously fragile (especially to speech recognition errors),do not offer natural ease of use, and do not adapt to differentusers. One of the main problems for SDS is to maintain an accurateview of the user's goals in the conversation (e.g. find a good indianrestaurant nearby, or repair a broadband connection) underuncertainty, and thereby to compute the optimal next system dialogueaction (e.g. offer a restaurant, ask for clarification). Recentresearch in statistical spoken dialogue systems (SSDS) hassuccessfully addressed aspects of these problems but, we shall show,it is currently hamstrung by an impoverished representation of usergoals, which has been adopted to enable tractable learning withstandard techniques.In the field as a whole, currently only small and unrealistic dialogueproblems (usually less than 100 searchable entities) are tackled withstatistical learning methods, for reasons of computationaltractability.In addition, current user goal state approximations in SSDS make itimpossible to represent some plausible user goals, e.g. someone whowants to know about nearby cheap restaurants and high-quality onesfurther away. This renders dialogue management sub-optimal and makesit impossible to deal adequately with the following types of userutterance: ``I'm looking for french or italian food'' and ``NotItalian, unless it's expensive''. User utterances with negations anddisjunctions of various sorts are very natural, and exploit the fullpower of natural language input, but current SSDS are unable toprocess them adequately. Moreover, much work in dialogue systemevaluation shows that real user goals are generally sets of items withdifferent features, rather than a single item. People like to explorepossible trade offs between features of items.Our main proposal is therefore to: a) develop realistic large-scale SSDS with an accurate, extended representation of user goals, and b) to use new Automatic Belief Compression (ABC) techniques to plan over the large state spaces thus generated.Techniques such as Value-Directed Compression demonstrate thatcompressible structure can be found automatically in the SSDS domain(for example compressing a test problem of 433 states to 31 basisfunctions).These techniques have their roots in methods for handling the largestate spaces required for robust robot navigation in realenvironments, and may lead to breakthroughs in the development ofrobust, efficient, and natural human-computer dialogue systems, withthe potential to radically improve the state-of-the-art in dialoguemanagement.",,
22,552617C1-3206-4104-98BB-8F85FFA536BF,Scaling up Statistical Spoken Dialogue Systems for real user goals using automatic belief state compression,"Spoken dialogue systems (SDS) are increasingly being deployed in avariety of commercial applications ranging from traditional CallCentre automation (e.g. travel information) to new ``troubleshooting''or customer self-service lines (e.g. help fixing broken internetconnections).SDS are notoriously fragile (especially to speech recognition errors),do not offer natural ease of use, and do not adapt to differentusers. One of the main problems for SDS is to maintain an accurateview of the user's goals in the conversation (e.g. find a good indianrestaurant nearby, or repair a broadband connection) underuncertainty, and thereby to compute the optimal next system dialogueaction (e.g. offer a restaurant, ask for clarification). Recentresearch in statistical spoken dialogue systems (SSDS) hassuccessfully addressed aspects of these problems but, we shall show,it is currently hamstrung by an impoverished representation of usergoals, which has been adopted to enable tractable learning withstandard techniques.In the field as a whole, currently only small and unrealistic dialogueproblems (usually less than 100 searchable entities) are tackled withstatistical learning methods, for reasons of computationaltractability.In addition, current user goal state approximations in SSDS make itimpossible to represent some plausible user goals, e.g. someone whowants to know about nearby cheap restaurants and high-quality onesfurther away. This renders dialogue management sub-optimal and makesit impossible to deal adequately with the following types of userutterance: ``I'm looking for french or italian food'' and ``NotItalian, unless it's expensive''. User utterances with negations anddisjunctions of various sorts are very natural, and exploit the fullpower of natural language input, but current SSDS are unable toprocess them adequately. Moreover, much work in dialogue systemevaluation shows that real user goals are generally sets of items withdifferent features, rather than a single item. People like to explorepossible trade offs between features of items.Our main proposal is therefore to: a) develop realistic large-scale SSDS with an accurate, extended representation of user goals, and b) to use new Automatic Belief Compression (ABC) techniques to plan over the large state spaces thus generated.Techniques such as Value-Directed Compression demonstrate thatcompressible structure can be found automatically in the SSDS domain(for example compressing a test problem of 433 states to 31 basisfunctions).These techniques have their roots in methods for handling the largestate spaces required for robust robot navigation in realenvironments, and may lead to breakthroughs in the development ofrobust, efficient, and natural human-computer dialogue systems, withthe potential to radically improve the state-of-the-art in dialoguemanagement.",,
23,A8BE27AB-3DD3-43A3-931D-77FB553C2550,Automated Reasoning with Very Large Theories,"Our proposal focuses on first-order reasoning with Very Large Theories (VLTs).A VLT is a collection of formalised knowledge expressed in a logical language.For example, such a VLT can be extracted by programs from largecollections of documents in some domain, such as biology, or from a largecollection of Web sites.Reasoning with such theories means answering queries based on thelogical semantics of the knowledge as opposed to the keyword search.If the project turns out to be successful, it may give rise to new waysof Web search where search for user's queries will be based onsemantics and reasoning.",,"In the short term the users and beneficiaries of the research are in the academic research community. The research area is too young to have immediate short-term impact outside the community. Let us point out potential longer-term impact. The very nature of our project is answering in real time queries to very large collections of knowledge. This is not very different from what Web search engines do. What is different is the way HOW queries are answered: in our case answering is based on semantics and reasoning. The apparent connection between search engine query answering and reasoning with Very Large THeories (VLTs) suggests that the potential impact of our project goes far beyond automated reasoning and that in the long term potential beneficiaries of our project include nearly everybody using computers. To realise this longer-term impact we intend to maintain collaboration with the developers of VLTs. The collaboration will be of the following kind: (1) We will use their VLTs for developing and benchmarking our system. (2) We will try to integrate the system in their Web interfaces and use it as (one of) mechanisms for query answering. This will enable us to understand what kinds of queries are posed by the users and benchmark our system against the queries. We are also going to analyse queries posed by users and answers to them. These queries and answers will be key to understanding how semantic Web search can be used when the standard Web search is inadequate. Another potential avenue to exploit is contacts with the natural language processing researchers. The idea is to try to extract formal knowledge from parsed natural language texts and use theorem provers to answer queries, check consistency, or extract new knowledge. This is a separate line of research that can complement this project and we plan a separate grant proposal for it. It is also important for us to present our results at world-leading conferences attracting industrial participants, such as WWW and IJCAI. We will submit publications resulting from this project to these conferences and collocated workshops. Visits to research centres developing VLTs and the conferences mentioned above are included in this proposal. Apart from the visits, there are no other resource implications resulting from implementing the impact activities."
24,DB0DD5EE-D9D3-4E0F-A3E2-6EB9FBBA5CA2,SANDPIT: Integrating and Automating Airport Operations,"This project emerged from the Research Councils' Energy Programme Sandpit in Airport Operations held at Shrigley Hall, Cheshire between 10-14 November 2008. It represents a wide ranging multi-disciplinary and cross-institutional initiative to exploit recent research advances in automated search methodologies and decision support techniques for air operations (and other related areas). This project will open up a range of exciting and ambitious research directions in the crucially important area of airport operations. The proposed programme of research will build integrated computational models of four key airport operations: Take-off scheduling, Landing Scheduling, Gate Assignment and Baggage flow. The project will explore how to build computational models that represent the integration of these problems and it will explore how to develop effective multi-objective search methodologies which will employ the models. At the moment, these operations are addressed in an isolated (and often manual) way. The ultimate goal is to develop innovative search methodologies which are able to operate from a global perspective in order to provide airport operators with a much higher level of computer aided decision support than is available today. Integrating these four operations and exploring new and exciting ways of generating high quality solutions to the integrated problem is the broad basic aim of the proposal. We will work closely with colleagues at Manchester and Zurich airports to ensure that we have continuous access to real world expertise and data. The proposal brings together a balanced inter-disciplinary team of scientists and engineers to investigate a series of novel research challenges with the overall goal of underpinning the development of tomorrow's airport operations decision support systems.",,
0,7F9070C2-196A-42CD-824C-5EB1AAAAF482,Advanced Algorithms for Neural Prosthetic Systems,"Our seemingly effortless ability to make coordinated movements belies the sophisticated computational machinery at work in our nervous system. In recent years, the field of neuroscience has been dramatically expanding the complexity of its data acquisition technologies and experiments. This technological development has created a preponderance of valuable experimental data, but the analytical methods required to deeply interrogate this data have not yet been developed. Simultaneously, the last decade has seen major advances in the fields of computational statistics, data analysis techniques, and machine learning. Research in these areas has enabled investigation into and understanding of previously uninterpretable data.This proposal seeks to bring together key research from these two fields to significantly advance the scientifically and medically important application of neural prosthetic systems, which seeks to improve greatly the quality of life of hundreds of thousands of severely disabled human patients worldwide. Debilitating diseases like Amyotrophic Lateral Sclerosis can leave a human without voluntary motor control. However, in most cases, the brain itself remains intact and has normal function. The same is true with spinal cord injuries that result in severe paralysis. In fact, tetrapalegic patients list ``regaining arm/hand control'' as the top priority for improving their quality of life, as regaining this function would allow significant patient independence. To address this priority, neural prosthetic systems seek to access the information in the brain and use that information to control a prosthetic device such as a robotic arm or a computer cursor. There are many medical, scientific, and engineering challenges in developing such a system, but all neural prosthetic systems share in common a decoding algorithm. Decoding algorithms map neural activity into physical commands such as parameters for controlling a robotic arm. Current decoding approaches have shown exciting proofs of concept, but there are a number of shortcomings that must be addressed before the field produces a clinically viable prosthetic device with speed and accuracy comparable to a healthy human arm. Our research programme will use advanced statistical and machine learning technologies to create algorithms that can decode neural activity with higher precision that previously seen. We have identified several opportunities for meaningful improvement, from incorporating the statistics of natural reaching to validating these algorithms in a realistic online setting. Taken together, these algorithmic developments should help create a much higher quality neural prosthetic device.",,"We believe this research will provide important benefit to society, both in terms of medical technology and economic opportunity, as well as to the academic community. We outline our plan to make impact in several broad categories (as identified by the Research Council) below. Societal Impact - Health and Quality of Life We believe the most important and widespread benefit of this research programme will be for the hundreds of thousands of people living with severe motor impairments. Debilitating diseases like Amyotrophic Lateral Sclerosis (ALS, often called Lou Gehrig's disease) can leave a human without voluntary motor control. However, in most cases, the brain itself remains in tact and has normal function. The same is true with spinal cord injuries that result in severe paralysis. In fact, tetrapalegic patients list ``regaining arm/hand control'' as the top priority for improving their quality of life, as regaining this function would allow significant patient independence. To address this priority, neural prosthetic systems seek to access the information in the brain and use that information to control a prosthetic device such as a robotic arm or a computer cursor. Such systems, if successful, would clearly have massive quality of life and health impact for the hundreds of thousands of people living with these conditions. While there are many challenges in delivering a clinically viable neural prosthetic, one critical and unsolved challenge is the algorithms that decode neural activity into movement. Our research programme will develop algorithms to drastically improve our ability to decode neural activity. Thus, we believe there is significant societal impact for decode algorithms that can translate proof-of-concept systems to systems that are clinically viable for this important medical application. Further, we believe that we are well positioned to make this impact. When these algorithms are demonstrated to provide superior decode performance (in offline data analysis, and online data analysis as contemplated with our collaborators at Cambridge and at Stanford), and when these algorithms become available to the broader community through academic publication, this impact will be quickly realised as a significant step towards the goal of a medically useful neural prosthetic system. Societal Impact - Economic and Industrial The benefits of these advances to patients will be accompanied by economic and industrial benefits for companies who make relevant medical device technologies. Currently only early human clinical trials are underway as academic proofs-of-concept. Nonetheless, companies have emerged around this young technology. Thus, it is reasonable to anticipate that drastically improving our ability to decode neural activity will remove a key obstacle to industrial and economic growth in neural prosthetic systems. We believe this impact will naturally be driven by device companies. However, we believe that our algorithmic work will act as a key enabler for these engineers, medical professionals, and business people to make their important economic impact. Academic Impact - Contributions to Knowledge In addition to the large societal impact opportunities, we believe there are valuable impact opportunities for the academic community. We detail those benefits in the Academic Beneficiaries section."
1,06F040AA-93EC-4947-BDC8-7B49BF169ADC,Refactoring and Neutrality in Genetic Programming,"Computer programming is difficult. Despite many years of experience, certain problems are very difficult for programmers to solve. To address this, researchers have developed methods where computers automatically create program code from a description of the problem to be solved. One of the most popular forms of automated program creation is called Genetic Programming (GP). In GP, a population of potential solutions is created, tested on the problem, and a new population created by processes inspired by biological evolution, such as genetic mutation and the crossover of genes between individuals. This process is repeated until an effective program is found.GP has proven effective in a number of areas: GP systems have created programs that are as good or better than those created by human programmers in a number of areas, such as robot control, bio-medical data analysis, and the design of electronic circuits. It has been applied in real-world technologies such as the design of antennas for satellites, the analysis of currency markets, improving the design of chemical engineering systems, and the detection of unexploded devices such as landmines. The aim of this project is to make GP more effective by including in the evolutionary process a technique that has risen to prominence in human-based software engineering, known as refactoring. Refactoring means changing the structure of computer programs without changing what they actually do. This is important in the development of software because it means that programs can be simplified and their structure made clearer before programmers work on changing the behaviour. By separating out these two aspects, the programming process is made clearer. These techniques have not been systematically applied to GP in the past.An important reason why this idea is likely to succeed is because it has already been shown that something called neutrality is important for evolution. Neutrality means that there are many more possible genes than there are proteins that can be created by those genes, and therefore one protein can be encoded for by many genes. During evolutionary history, many of the changes that occur are these neutral changes-changes to the genetic encoding, rather than the behaviour that results from that encoding. As part of the project we want to understand how this idea of neutrality can be used to understand the development of these computer programs during their evolution.Overall, we want to rigorously test whether this enhancement - refactoring - is capable of improving the efficiency and effectiveness of this increasingly important technology for the automated creation of computer programs.",,"Genetic Programming is at present the only technology that has demonstrated the ability to automatically generate software, and similar executable structures, across a wide variety of application domains. It has produced results comparable to that of human programmers and designers in a breadth of areas of application, and has been incorporated into, or used to design, a number of products that have been used to solve real problems or to produce commercial products. As problems get more complex, and as more problems require the production of bespoke on-demand software for areas where there is no expert knowledge that could guide human programmers, the demand for automated software creation is increasing. Genetic Programming is in a position to be the technology of choice for such applications. The development of better GP systems will therefore be of benefit to the creators of software, other executable structures, and software-hardware co-designs. As such technologies become mainstream, we expect the automated design of part of a software system to become a normal part of software design practice. Our plan for making GP the technology of choice for automated software creation consists of two elements. Firstly, we plan to incorporate our enhancements to GP into an existing open-source GP system, so that the improvements will be readily available. Secondly, we aim to publicize this technology to a wide audience within the traditional software engineering practice community, and open up a dialogue with that community to share ideas. This will be done by talks at relevant events, articles in practitioner-focused media, the development of online resources, and engagement in online communities."
2,4B9F6C9E-C0AB-4450-9772-E6F9623DCEDD,Evolutionary Optimisation of Self Assembling Nano-Designs (ExIStENcE),"The primary objective of this proposal is the development of novel evolutionary algorithms (EAs) and protocols, based on deeper principles than currently available, for the optimisation, design and exploitation of molecular self-assembly. Evolutionary algorithms are nowadays well established techniques that have shown their worth on a large variety of applications that range from timetabling and scheduling problems to robotics and space antenna design. Surprisingly, EAs have not yet been systematically analised in the context of molecular tile design. At the core of our approach lies the assumption that self-assembly can be understood as an information-driven process and hence be better exploited by directly linking it to computational phenomena. Taken as an operational hypothesis, which our research programme will analyse both theoretically and experimentally, this assumption implies that with suitable tools, desired emergent phenomena could in principle be programmed into self-assembling nanosystems. Our experimental target will be based around molecular tiles as these have been shown to be computationally complete [1,2,3]. Hence, they can potentially be programmed to perform any set of discrete information processing steps which in turn could induce a specific emergent pattern of complex behaviour. This project will seek to automate the process of programming molecular tiles using evolutionary algorithms. In an interview for Thomson Scientific's Science Watch newsletter [quoted in J.A. Pelesko, Self Assembly, The Science of Things that Put Themselves Together, Chapman &amp; Hall/CRC, 2007], G. Whitesides, one of the most prolific and highly cited chemists in the world, noted that the holy grail of his research was To be able to make complex systems, either structurally or functionally, by self-assembly...We would like to develop a synthesis technology that would enable the making of nanometer-scale... structures on surfaces with arbitrary chosen properties . Whitesides' challenge is at the heart of our research programme. We will seeks to leverage state-of-the-art research in Computer Science and Nanoscience to meet this challenge.",,
3,58C6258C-560C-4FC8-A31C-6BC9835F53F7,Towards More Effective Computational Search,"The ASAP group has set the international research agenda in exploring the development of computational systems that can automatically build decision support systems. The group addresses a broad range of scientifically challenging problems, many of which are drawn from the real world where the complexities of the problem have not been abstracted away in order to make the problem easier to model/solve.The group's key research goals include:- Automating the Heuristic Design Process: We lead the international community in hyper-heuristics (heuristics to choose heuristics) research, with the aim being to investigate the extent to which we can replace human decision making by computer algorithms.- Closing the gap between industrial and real world issues and academic practice: We aim to explore dynamic and complex computational modeling and intelligent decision support within the context of real world problems such as aircraft scheduling, timetabling, manufacturing, bioinformatics, production scheduling and healthcare rostering. ASAP aims to establish new decision support methodologies that explore the use of automated search methodologies and the complexity that they are able to handle.- Closing the gap between theoretical understanding and the construction of search methodologies: We aim to theoretically analyse complex real world scenarios with a view to deepening our understanding of search methodology development. The state of the art in theoretical study in this field tends to deal with models that are too simple to be placed into real world practice. We aim to study the theory of real world applications.Our core research on modeling and search methodologies has redefined the inter-disciplinary interface between Computer Science and Operational Research, while our grounding in diverse applications involves dialogue with many other disciplines spanning biomedical science (new computational methodologies in bioinformatics, systems and synthetic biology as well as in nanoscience) through to the built environment (search methodologies for office space allocation). In this renewal proposal to our current Platform award, we are requesting support for 132 months of research assistant funding (at varying levels of seniority), over a five year period. This would enable us to conduct (and continue) a programme of transformative and innovative research that is not only high risk and high return, but which also has a clear multi-disciplinary and industrial focus.A Platform award would enable the ASAP group to retain key personnel at the interface of Computer Science and Operational Research. The potential benefits in providing the grounding for tomorrow's decision support systems could be far reaching in laying the foundations for more efficient, effective, cheaper, easier-to-implement and easier-to-use systems across many industries and sectors.",,
4,A46E55EE-B5E8-4163-AF43-3B72402378F2,Synthetic Cognitive Systems,"The brain can be viewed as an immensely sophisticated computational system, the understanding of which presents us with a very challenging problem. Whilst much progress has been made in understanding how the brain functions, a fundamental question that remains to be resolved is how the brain encodes and processes information as spatiotemporal patterns of neural activity. This proposal will establish a core team of interdisciplinary researchers and the facilities required in order to address this important question. The key aim of the group's research program is to produce a synthetic system, incorporating living neurons, which is capable of cognition at a highly simplified, elemental, level. The system will incorporate a continuous, closed-loop, flow of information between the network and its environment analogous to that in a real organism and will ensure the system has some relevance to neural processing. It will result in a tool that will overcome current limitations; allowing a close correspondence between the experimental system and the mathematical models. This experimental tool will be made available to the neuroscience community at large and will enable researchers to develop the beginnings of a theory of neural processing. The initial system will undergo continual development; incorporating technical innovations and improvements to construct neural systems of ever increasing complexity and relevance.This proposal will provide an investment in the core activities of a new interdisciplinary facility that will provide an experimental platform for neuroscientists both from the UK and abroad. The expertise of the facility will be both in the development and production of synthetic cognitive systems as well as their application to fundamental, curiosity-driven, neuroscience research. The majority of output from this facility will come from funding attracted by the collaborative projects that arise both during the development and the application of this tool.The relatively simple initial system will undergo continual development; incorporating technical innovations and improvements. For example, larger networks will be produced, more sophisticated methods of monitoring and controlling biochemical pathways will be incorporated and networks of interacting networks will be produced to construct neural systems of ever increasing complexity and relevance.Ultimately these systems will allow us to develop models of information processing in the more complex networks found in living brains. These models may lead to treatments to correct the instabilities that underlie many brain diseases and disorders and may lead to advances in artificial intelligence.",,"The primary impact of this research program will be advances in fundamental science and the development of basic technology. Secondary activities including commercialisation of technology, education and training and outreach are anticipated to have flow on effects that will have wider, less well defined, social and economic impact. The multidisciplinary nature of this research is expected to result in incremental gains in basic knowledge and technology in several fields including physics, biology, medicine, neuroscience and artificial intelligence - ranging from imminent and highly probable to longer term and more speculative. A step change in neuroscience is also anticipated by providing a set of research tools that will allow fundamental questions of neural processing to be addressed from a novel, biologically inspired, philosophically informed perspective. The development of basic technology has the potential to lead to several products with commercial value. The support and involvement of industry from the early stages of this project will ensure the best chance of success in transforming basic technology into viable commercial products. It will also ensure researchers focus on research while any commercial activities are handled by industry. Traditional academic dissemination routes will be utilised to communicate research outputs in addition to a number of educational and outreach initiatives. This includes science festival booth construction and touring, school outreach events, projects with film makers and theatre companies and engagement in the policy making process. In summary, as well as advances in basic knowledge and technology and its dissemination via traditional routes there are many potential broader consequences that could result from this research program. It is anticipated that there will be flow on effects from the secondary activities of education and outreach, commercial development of technology and engagement in the policy making process that will have wider, less well defined, social and economic impacts."
5,60DFB6C9-FE10-49E6-B76A-414794B4CA34,Sketching Euler Diagrams,"The proposed research will develop the first techniques for recognizing sketches of Euler diagrams drawn by users. The proposal is for Dr Plimmer to travel to the UK team for 1 month as a Visiting Researcher and for Dr Stapleton to travel to New Zealand for one week. The research visits will facilitate the exchange of knowledge between experts in automated Euler diagram drawing (the UK team) and sketch recognition (Dr Plimmer, Auckland, NZ). This will allow an important exchange of information about relevant drawing and sketching techniques and result in further, significant, collaboration.A natural creation method for Euler diagrams is using a pen but no intelligent tool support exists for this mode of entry. Euler diagrams are a popular and frequently used visualization technique; in part, this popularity serves to motivate our selection of them for the proposed research, since the results are likely to have significant impact. Moreover, Euler diagrams form the basis of more expressive notations, built by augmenting them with graphs or arrows (or both), for instance. Example notations include spider diagrams, Euler/Venn diagrams, Venn-II diagrams, constraint diagrams, and ontology diagrams. The research results of the project provide a basis for developing sketch recognition tools for these more expressive notations. Extending the research results to these notations will be the subject of the future collaborations, between Dr Plimmer and the UK team, that will build on this project.",,"The project will develop original techniques that allow the sketch recognition and post-processing of Euler diagrams. The results will have impact in areas where one wishes to visualize relationships between sets of data. This is because the results will enable the automatic production of high-fidelity Euler diagrams from user sketches, which is not currently possible. Consequently, this will bring benefits by improving the communication of information for the public in general, including in a business context. In turn, this will lead to economic benefits through improved understanding and communication of information. Euler diagrams are useful information visualization tools, having uses in a variety of areas. This project will impact the ability of users to convey, visually, information about related sets of data and, so, enhance user understanding. This is because, at present, Euler diagrams must typically be drawn on paper (which results in low-fidelity diagrams that cannot be used in intelligent computer applications) or using diagram editing tools (which is time consuming and detracts from the cognitive processes one follows in order to produce the most effective diagram). Impact will be realised through a variety of dissemination activities, including published articles and freely available sketching software. In an industrial context, we will closely collaborate with our Project Partner, Nokia, who are keen to take up the results of the project."
6,D79088C8-E199-4339-988D-32666B2BB473,TradeTech2010 Showcase Workshop,"This proposal seeks 18,110 funding from EPSRC to support a showcase of EPSRC-funded talent (i.e., early-career researchers) and technology (i.e., research outputs) at TradeTech2010, the premier annual trading-technology summit for senior executives in the global financial markets, to be held in London in April 2010. As the global financial markets are clearly a major component of the current and future digital economy landscape both in the UK and overseas, such a showcase seems ideally-suited to funding from EPSRC's Digital Economy programme. TradeTech2010 will be held in the UK for the first time in its ten-year history, adding extra significance to a showcase of EPSRC-funded researchers and research-outputs. The cash value of in-kind contributions from WBR, the conference company that organizes the TradeTech summit series, is worth up to 37,400 and hence significantly exceeds the EPSRC funding requested here. The two authors of this proposal have a unique mix of experience, expertise, and contacts that make them ideally suited to deliver a high value return on the modest funds requested here.",,"By the usual standards of EPSRC grant applications, the amount of money requested here is quite small. Nevertheless, we believe that the degree of impact per pound spent will be very high (although of course it is famously difficult to precisely define what we mean by degree of impact , given the intangibility of some significant and desirable forms of impact). Specific knowledge-transfer, of particular ideas and practices, will occur in the keynote presentations, and in the rising-star sessions, in the sense that ideas currently being developed in academic research contexts will be exposed to an audience of senior, hardened, industry practitioners. The flow of knowledge will not be one-way though: the responses of the expert panellists to each of the rising-star presentations will allow the academics present to refine their understanding of what kind of research work is judged to be useful by industry practitioners, and what kind of research is not. More diffuse knowledge-transfer and impact will manifest themselves in the increased awareness among the TradeTech2010 delegates of the role of EPSRC in general, and the Digital Economy programme in particular, in funding advanced R&amp;D activities in UK universities that is likely to be of relevance and interest to industrial practitioners in the global financial markets. This may then in turn result in an increase in collaborative financial industry/academia grant-proposal submissions, or of other forms of financial-markets industrial participation in EPSRC's various funding activities. The development and publication of the designs for the lab-in-a-box experimental trading floor is intended to have an impact on the world community of researchers, academic or industrial, who have hitherto been prevented from conducting trading-floor experiments by the belief that they are only possible in expensively-equipped fixed laboratory installations. We fully expect that our methods will be copied, and adapted and extended, by researchers around the world, although it is of course not possible at this stage to predict which researchers will pick up on our work, nor how they will use it or extend it. Nevertheless, we feel optimistic that, if funded, our work as proposed here will lead to a noticeable increase in the use of rigorous laboratory experiments as a testing ground for new ideas in trading systems and market mechanisms. The impact of releasing the lab-in-a-box as open-source can be measured by citation counts after two or three years have elapsed (this being the lead-time from someone downloading our la-in-a-box, running their own experiments, and then having the results of those experiments published in peer-review outlets)."
7,66238327-33BE-4FD7-B9D4-6281D734E8C9,Advancing Machine Learning Methodology for New Classes of Prediction Problems,"The last few decades have seen enormous progress in the development of machine learning and pattern recognition algorithms for data classification. This has resulted in considerable advances in a number of applied fields, with some of these algorithms forming the core of ubiquitous deployed technologies. However there exist very many important applications, for example in biomedicine, which are highly non-standard prediction problems, and there is an urgent need to develop appropriate &amp; effective classification techniques for such applications. For example, at NIPS2006 Girolami &amp; Zhong reported state of the art prediction accuracy for a protein fold classification problem which stands at a modest 62%. While this may partly be due to overlaps between classes of fold, it is also clear that some of the fundamental assumptions made by most classification algorithms are not valid in this application. In particular, most algorithms make some assumptions on the structure of the data that are not met in reality: data (both training and test) is independent and identically distributed (i.i.d) from the same distribution, labels are unbiased (i.e. the relative proportions of positive and negative examples are approximately balanced) and the presence of labeling noise both on the input data and on the labels can be largely ignored. Recent advances in Machine Learning, such as kernel based methods and the availability of efficient computational methods for Bayesian inference, hold great promise that classification problems in non-standard situations can be addressed in a principled way. The development of effective classification tools is all the more urgent given the daunting pace at which technological advances are producing novel data sets. This is particularly true in the life sciences, where advances in molecular biology and proteomics are leading to the production of vast amounts of data, necessitating the development of methods for high-throughput automated analysis. Improving classification accuracy may lead to the removal of what is currently the bottleneck in the analysis of this type of data, leading to real impact in furthering biomedical research and in the life quality of millions of people. At present most classifiers used in life sciences applications, especially those deployed as bioinformatics web services, adopt &amp; adapt traditional Machine Learning approaches, quite often in an ad hoc manner, e.g. employing Artificial Neural Networks &amp; Support Vector Machines. However, in reality many of these applications are highly non-standard classification problems in the sense that a number of the fundamental underlying assumptions of pattern classification and decision theory (e.g. identical sampling distributions for 'training' and 'test' data, perfect noiseless labeling in the discrete case, object representations which can be embedded in a common feature space) are violated and this has a direct and potentially highly negative impact on achievable performance. To make much needed &amp; significant progress on a wide range of important applications there is an urgent requirement to systematically address the associated methodological issues within a common framework and this is what motivates the current proposal.",,
8,B9074552-41A4-4F42-87A2-7D3140C9058E,AI4FM: using AI to aid automation of proof search in Formal Methods,"Formal Methods bring to software engineering the level of scientific underpinning familiar in other engineering disciplines. Such methods use specification languages with precise meaning and thus open the possibility of proving that a design (ultimately the implementation) satisfies the specification.Such formal methods have come a long way since their inception and they are now used in applications far more common than the safety-critical systems where they were first deployed. Significant awareness of their potential has come from the use of push-button, post facto, methods that derive from ideas of model checking . The family of methods that can be thought of as top-down have more potential pay-off but are also more challenging for users. Any post-facto method has to face the prospect of incorrect programs - extracting their specifications can be unedifying. Furthermore, an enormous amount of the waste in software development derives from scrap and rework when design errors are discovered after their insertion (but possibly before there is even code to execute). Both post-facto and top-down approaches are important: we choose to address the latter and tackle a key cost in their deployment.In justifying a top-down step of design, the user has to discharge so-called proof obligations . These are small proofs that can often be discharged by an automatic theorem prover. But where they are not discharged automatically, an engineer is faced with the unfamiliar task of constructing a formal proof. Improvements in so-called heuristics can help increase the power of theorem provers. This project aims to use learning techniques from artificial intelligence to record and abstract how experts do proofs in order to increase the proportion of cases where proofs are constructed without (or with minimal) human intervention.",,"Who will benefit from this research?: The long-term, non-academic beneficiaries of this research will be all users of formal methods in ICT system development. In the shorter term, our prototype system will first be available to users of the Rodin Toolset, e.g. industrial members of the DEPLOY project, such as Bosch, SAP and Siemens. Assuming this leads to a greater uptake of formal methods for ICT system development, then customers of these systems will benefit from more dependable ICT products, i.e. systems that are delivered on-time, on-budget, that meet their specifications and are more easily maintained against an evolving specification. Considering the horrifying statistics associated with the failures of current ICT projects, this would be a major impact. How will they benefit from this research? The potential impact on the practical use of formal methods is huge. It is clear that recalcitrant POs are a major bottleneck in the use of formal methods even within a project like DEPLOY where the industrial partners have already made some commitment to their use. Our tools will increase the level of automation in the use of formal methods. This will result in both a decrease in the required skill level in their use and a decrease in the time take to apply them. Expert interaction will still be required to deal with proof obligations (POs) that are beyond the ability of current theorem proving systems, but this interaction will be limited to an exemplar PO within each family, where a typical family size is 20 POs. The other 19 POs will then be dealt with automatically using the source proof to guide the target proofs. Rare, expensive and highly-skilled proof expertise can then be spread more thinly across a wider range of projects, with less-skilled ICT developers dealing with the automated tools that will deal with the majority of the POs. The savings to users might well amount to a quarter of their costs. With the resulting reduction in costs and time-to-market of formal methods, we would expect a major barrier to their wider uptake to be lifted, and their use to become more routine. What will be done to ensure that they have the opportunity to benefit from this research? Our workplan involves close interaction with several industrial formal-methods users. We will harvest examples from these users to form our development and test sets. This will ensure that we are working on industrially relevant problems. We will embed our prototype in tools, such as the Rodin Toolset, that are being used by these industrial users. This will ensure that our contribution is readily usable by these industrial users. Most importantly, we will engage in a continuous dialogue with these industrial users to ensure that our tools will address their most significant problems. This dialogue will take place not only within this project, but also within sister projects, such as DEPLOY, and our membership of communities, such as UKCRC Grand Challenge 6, on Dependable Systems Evolution and the Verified Systems Initiative . In particular, GC6 and VSI will give us the opportunity to tension our research against related research on a common benchmark of problems. We will, of course, also use all the usual channels of research dissemination: conference and journal publication; seminar, workshop and conference presentations; making software and examples freely available to download; web page with links to publications, etc.; tutorials on our results during the later stages of the project; organising both UK and European workshops on our evolving research, e.g., at Schloss Dagstuhl, involving both industrial and academic researchers; integrating our research results into our teaching, in order to send out a generation of UG and MSc CS students much better equipped to employ FM in their future careers."
9,7B805DD4-61EC-455D-A36B-001101141B9F,AI4FM: using AI to aid automation of proof search in Formal Methods,"Formal Methods bring to software engineering the level of scientific underpinning familiar in other engineering disciplines. Such methods use specification languages with precise meaning and thus open the possibility of proving that a design (ultimately the implementation) satisfies the specification.Such formal methods have come a long way since their inception and they are now used in applications far more common than the safety-critical systems where they were first deployed. Significant awareness of their potential has come from the use of push-button, post facto, methods that derive from ideas of model checking . The family of methods that can be thought of as top-down have more potential pay-off but are also more challenging for users. Any post-facto method has to face the prospect of incorrect programs - extracting their specifications can be unedifying. Furthermore, an enormous amount of the waste in software development derives from scrap and rework when design errors are discovered after their insertion (but possibly before there is even code to execute). Both post-facto and top-down approaches are important: we choose to address the latter and tackle a key cost in their deployment.In justifying a top-down step of design, the user has to discharge so-called proof obligations . These are small proofs that can often be discharged by an automatic theorem prover. But where they are not discharged automatically, an engineer is faced with the unfamiliar task of constructing a formal proof. Improvements in so-called heuristics can help increase the power of theorem provers. This project aims to use learning techniques from artificial intelligence to record and abstract how experts do proofs in order to increase the proportion of cases where proofs are constructed without (or with minimal) human intervention.",,
10,4E01C64F-2B00-4930-834E-69CFA72C6916,Maximising Efficiency of Resource Usage Under Uncertainty in AI Planning,"In recent years planning technology has a enjoyed significant increase in real-world application, with industry and general science research benefiting from the great depth theoretical work done in this area over many decades. The core problem of deciding which activities to carry out and when occurs in a vast range of domains; the research area of planning is concerned with developing generic problem solving technology that automates the task of performing this core reasoning. Planning technology has been employed in a wide range of domains including controlling printing presses, power management for the UK National, train scheduling on the Hungarian Railway Network, scheduling aircraft landing in airports, and autonomous robotic control, both in space and in the oceans. Experience in these areas has given rise to two key observations. First, the existing theoretical work done in AI Planning has been extremely valuable, allowing planning technology to begin to solve real world problems. Planning is a fundamental component of intelligent autonomous behaviour and as such planning technology has real potential for application in many different areas, both now and in the future. The second is that whilst one can observe that planners can now begin to be applied to these problems, there is still a great need for improvement of the underlying technology, in terms of expressivity and performance, in order to be able to create greater autonomy by allowing reasoning about an uncertain world.At the heart of this lies deeply theoretical computer science research: a planner is a generic problem solving system, consisting of search algorithms and heuristics. Of particular interest is reasoning about time and resources, something key to many areas of computer science, from compilers and programming languages to web services and optimisation. In order to tackle application problems well, reasoning effectively about these is essential. Of specific interest here is uncertainty in time taken and resources consumed. This occurs in many application domains, and in each of these a similar approach is taken: conservatism about time and resource availability in order to guarantee success. This, however, comes at a cost. By way of example, when planning for autonomous Martian exploration, the models used by both the ESA and NASA are pessimistic, underestimating the amount of power the rover will receive from the sun, and overestimating the amount of energy and time each activity will take. The result is that the equipment is highly under-utilised, with fewer science targets being achieved than could have been with better on-board reasoning. Given the expense of placing rovers on Mars and the limited equipment lifespan, this is a great cost to mankind's exploration of space. A similar problem occurs when deploying renewable energy generation: wind farms are assumed to provide 10% of their maximum output, even thought the reality is almost always greater than this. This conservative assumption, there to ensure power is always provided, causes great environmental and economic cost, as extra production capacity must be available through other sources regardless of whether it is required.The major benefit of planning is in generating a generic problem solving technology. Developing several bespoke solvers would take many years, and incur great financial cost. By developing efficient planning systems, a single domain-independent problem-solving core is built, capable of solving many problems without the cost of developing a bespoke solver for each. The core of this research is addressing challenges in solving the general planning problem that will allow future application of planning, extending the range of problems to which this generic technology can be applied.",,"Here we consider the wider impact of this research on society and industry; this is subject to varying time-scales, with some areas expected to benefit within the lifespan of the project, and other benefits to become apparent in the longer term. Planning is generic problem solving technology, with the potential for impact on many different application areas. The UK has established itself as a world leader in planning technology on the international scene, and we are already exporting this technology to the US and Europe. This project will help the UK to maintain its leading role in applications of planning, by developing the technology to allow further pioneering deployment in a range of different areas. The fellowship provides the opportunity for a young researcher, who has shown great promise, to stay in the UK to pursue this world leading research, thus preventing brain drain and maintaining expertise within the UK. Supporting a young researcher will clearly have a great impact on career development, nurturing an independent research career. Environmental protection is a leading priority across the world at the current time, with governments, including that of the UK, aiming to meet emissions targets. This is the key reason I have chosen to focus on wind farm planning, an application in the area of smart grids. When planning power generation in a setting with renewable and non-renewable energy sources, many sources of power are available, from wind-farms, to hydroelectricity, to nuclear power, to coal/gas power stations. The challenge is to automatically adapt to meet customer demand under uncertain weather conditions whilst making the best use of renewable, low emission power sources. Research in this area can directly reduce the UK's use of non-renewable and environmentally damaging generation sources, helping the government to meet targets, and improve global environmental protection. A further potential application is in micro-generation --- effectively, small-scale power grids amongst networks of homes --- and smart electricity meters, running high energy appliances during times of surplus production. The work done in the project will be directly applicable to these problems. These applications in power engineering have clear potential to directly benefit UK industry. The power supply industry is one of the largest in the UK, with the relevant companies, e.g. National Grid PLC, standing to benefit, both by achieving environmental targets, and through cost reduction by efficient use of resources. This in turn benefits the UK, establishing a cheaper energy supply, helping all UK businesses, as well as establishing the UK as a technological leader in energy production and distribution. Optimisation planning under uncertainty is core to many businesses (for example logistics firms, oil supply companies, airports): making the best use of finite resources. In the longer term, small businesses will be able to benefit from planning research: these companies also need to optimise resource usage, but writing a bespoke problem solving system is expensive; using an off-the-shelf problem-independent technology allows this to be done at at much lower cost. Research being done into intuitive modelling technologies for planning, and development of planning technology, will make this possible in the future. Autonomy is important to allow tasks to be carried out that either pose too great a risk to human life, or simply that the costs of the manpower required to perform such tasks would be prohibitive. Examples of such tasks are nuclear decommissioning, search and rescue following disasters, space and oceanographic exploration and autonomous assistance to help our ageing population to remain independent for longer. These are long-term beneficiaries that will be reached through the continued efforts of the planning community, as well as improving technology."
11,D70C583A-FD82-4873-A15D-C334D638CD9F,AI4FM: using AI to aid automation of proof search in Formal Methods,"Formal Methods bring to software engineering the level of scientific underpinning familiar in other engineering disciplines. Such methods use specification languages with precise meaning and thus open the possibility of proving that a design (ultimately the implementation) satisfies the specification.Such formal methods have come a long way since their inception and they are now used in applications far more common than the safety-critical systems where they were first deployed. Significant awareness of their potential has come from the use of push-button, post facto, methods that derive from ideas of model checking . The family of methods that can be thought of as top-down have more potential pay-off but are also more challenging for users. Any post-facto method has to face the prospect of incorrect programs - extracting their specifications can be unedifying. Furthermore, an enormous amount of the waste in software development derives from scrap and rework when design errors are discovered after their insertion (but possibly before there is even code to execute). Both post-facto and top-down approaches are important: we choose to address the latter and tackle a key cost in their deployment.In justifying a top-down step of design, the user has to discharge so-called proof obligations . These are small proofs that can often be discharged by an automatic theorem prover. But where they are not discharged automatically, an engineer is faced with the unfamiliar task of constructing a formal proof. Improvements in so-called heuristics can help increase the power of theorem provers. This project aims to use learning techniques from artificial intelligence to record and abstract how experts do proofs in order to increase the proportion of cases where proofs are constructed without (or with minimal) human intervention.",,
12,DE0730C6-1884-45FC-ADA1-E95F744B06B0,Principled Application of Learning Classifier Systems to Large-Scale Challenging Datasets (LCSxLCD),"The goal of this project is to study the general applicability of Learning Classifier Systems (LCS) to large-scale challengingdata mining tasks. Data Mining and Knowledge Discovery have become crucial technologies for the advancement of manyscientific disciplines. Vast amounts of data are available thanks to initiatives such as the human genome project, thevirtual human physiome, etc. Successful data mining techniques have to scale accordingly to the volume of the data,extract accurate models out of (often) noisy and ambiguous datasets and provide new insight that enhances our understanding of complex problems. LCS are robust machine learning techniques with very high potential for data mining. The frontier of competence for LCS has been pushed forward in recent years with the help of advanced representations, better search mechanisms and theoretical analysis, as well as a few examples of their application to challenging real-world domains. This success notwithstanding, most if not all of the progress has been heuristically driven. In this project we will (1) develop theoretical models for the performance of LCS when applied to large volumes of data that can inform us of when and why LCS methodsare successful and also when do LCS fail; (2) afterwards, the insight gained from these models will help us design new LCS methods with improved performance and robustness. The end product of the project will be a framework containing allthe studied techniques with theory-based efficient implementations, adapted for their usage in high performance computingenvironments. Datasets known to be difficult to data mine will be used to validate the success of the developed techniques.",,
13,E7EA318C-2193-47FB-BA5C-B96872289F9A,Sandpit: SerenA - Chance Encounters in the Space of Ideas,"We live in an age of burgeoning information, and increasingly fast information access. The WorldWide Web has allowed us to make many positive changes in our society and environment, for example through social networking and e-publishing, but it also presents problems, by its very nature. There is now so much information being spread so quickly that it is becoming impossible for individuals to be aware of enough of it to enable them to take advantage of it.What is more, because of the information overload, we are having to rely more and more on search tools to find what we want. While existing search tools work quite well, after a bit of practice in using them, they are only able to give us information directly matching keywords in what we ask for. This is clearly useful, but its down side is that we are less likely than before to notice peripheral things, situations or people who are relevant to us, in the kind of serendipity or happy accident that led, for example, to the discovery of penicillin. It's becoming harder to notice such connections, partly because we are more narrowly focused in how we search for new knowledge, and partly because the search systems we use are very literal and not imaginative at all.This project aims to design a Serendipity Arena, called SerenA, which will proactively search information available in users' documents and on the Web to identify relevant knowledge and connections related to their work and their environment. The aim is not merely to search for shared keywords, like existing systems, but to use state-of-the-art technology from automated reasoning and computational creativity to identify things that users did not know they needed to know, using more advanced search based on metaphor and analogy. SerenA will be implemented as a physical presence in the working environment, and via personal technology, such as smartphones. With its users' permission, SerenA will proactively search for people and information in a user's local environment, both physically and virtually, allowing it, for example, to suggest that people who don't know each other might find some value in meeting (perhaps because they share an interest in particular aspects of the academic world), or to suggest a paper omitted by keyword search in a particular e-journal (because it has connections with other things of interest to the user who is searching).SerenA will have its own document-analysis technology, but it will also take advantage of the increasingly rich information available via the Semantic Web. The project will include development of a test-bed of information about music and musicology, as an exemplar of an academic field that can benefit from this enabling technology. Importantly, SerenA will have specially designed and carefully validated user interfaces, to make it intelligible to everyone interested in learning and discovery, of whatever kind.SerenA aims to draw man and machine closer together than ever before, enhancing its users' knowledge and their ability to interact with people likely to be important to them.",,"SerenA will provide impact to public sector organisations with research as a core activity (e.g. libraries), museums and galleries on a national and regional level. SerenA will facilitate public engagement with their contents and artefacts and create new links for users that would not otherwise have occurred. Commercial private sector beneficiaries will include businesses whose work is related to information search, visualisation or dissemination, such as professional research organisations. In addition, internet industries will benefit from the development of novel algorithms for proactive serendipity. Industries that develop and anticipate future technologies will benefit from using SerenA by establishing and extending links for those within their organisation to inspire the creation of novel interaction tools, systems and devices. The wider public will benefit as they view and user SerenA in public spaces. There will also be a range of indirect impacts as the algorithms developed in the project may lead to benefit in industries not directly concerned with research practices, such as the advertising industry or government agencies. The users of SerenA will benefit within a reasonably short timescale of the project start (i.e. one-three years) and the impact both of the algorithm development and the implementation of ideas generated by SerenA will endure for the longer term. Staff working on SerenA will develop skills ranging from computational algorithm development, integration of mobile and large screen technologies and networks, and application of evaluation methods in requirements specification and system design. The multidisciplinary team of researchers and investigators will all develop research skills in writing, presentation, preparation of public exhibits and technology integration. It is anticipated that all researchers working on the project will develop transferable skills that could be applied in industry, but that could also form the basis of an academic career. A series of engagement activities are integral to the SerenA project. SerenA will be implemented into two types of environments: Research environments (e.g. Horizon DTC) and Public spaces (e.g. Dundee Contemporary Arts, Sensation, Media City). These implementations will ensure engagement with SerenA during the lifetime of the project. We will also run specific dissemination activities with the public and with wider industry. We will run annual workshops that will showcase SerenA outputs, obtain key stakeholder input into SerenA developments, and obtain feedback from industry as to how best exploit SerenA results. In addition to engagement and dissemination activities, we will conduct specific dedicated activities to ensure that SerenA is publicised and delivered into the public and business community as quickly as possible. We will work with knowledge transfer experts at our own universities, with particular support from the Horizon digital economy hub. We will engage in media relations activities via our university press offices and also through a professional quality project website. All researchers on the project will be given the opportunity for training in public engagement, with those researchers who have a particular aptitude and interest in engaging with the public being provided with additional training. To maximise outreach activities we will work with two of the six Beacons of Public Engagement at UCL and Edinburgh. We have secured collaboration with Dundee Contemporary Arts and MediaCity and will work closely with KTN on Photonics and Plastic Electronics. The SerenA investigation team has extensive experience of liaising with the media, supporting start-ups and running conferences and workshops."
14,F6791E8F-A6C9-4A0A-87BB-D8C3EDDF67C6,Learning and computation in disordered networks of memristors: theory and experiments,"Memristor (memory resistor) is a device whose resistance changes depending on the polarity and magnitude of a voltage applied to the device's terminals and the duration of this voltage's application. The memristor is a non-volatile memory because the specific resistance is retained until the application of another voltage. A memristor implements a material version of Boolean logic and thus any logical circuit can be constructed from memristors. We propose to fabricate in laboratory experiments an adaptive, self-organized disordered network of memristors. This practical fabrication will be backed up by rigorous computer simulation experiments. The memristor network is comprised of a conglomerate of conductive polymer fibres interspersed with particles of solid electrolyte. The conglomerate is placed on a matrix of micro-electrodes capable of recording voltage and generating current sources and sinks. Machine learning techniques will be applied in order to design logical schemes and basic arithmetical circuits.",,
15,218F7553-FCFA-498F-97A3-40CAC7CEAA52,Providing better information for parents and babies in neonatal intensive care,"Parents whose baby is in a neonatal intensive care unit (NICU) usually are under a lot of stress. Much of this stress is unavoidable, but in some cases parents are under more stress than necessary because they do not understand what is happening to their baby. Although NICU medical staff of course do their best to keep parents informed, some parents may not fully understand the terminology used by doctors and nurses (and may be reluctant to admit this), and also some parents may not be able to physically visit the NICU and talk to medical staf because of other commitments such as caring for other children.In a PhD project associated with the EPSRC-funded BabyTalk project, we have developed a software system, BT-Family, which produces summaries of a baby's status for parents. BT-Family builds on the award-winning BabyLink parent-information system used in the Edinburgh NICU, primarily by using artificial intelligence and natural language generation technology to automatically analyse and summarise the information in the baby's electronic patient record.BT-Family has been developed in consultation with parents, but it has not actually been deployed and evaluated by parents; this was not possible in the time frame of the PhD project. The goal of this project is to enhance BT-Family and deploy it in the wild where parents of NICU babies can use it, and evaluate how useful it is and also find out how parents believe the system can be improved.Although our focus in this project is specifically on parents of NICU babies, if this project is successful we believe that our ideas can be generalised to other situations where a parent or carer is responsible for someone in hospital. We believe that providing better information to parents and carers can reduce stress in many contexts (not just NICU), and that this is a major opportunity to use advanced IT to enhance the quality of life of people in the unfortunate position of having a child or dependent in hospital.",,"The main beneficiaries of our research will be parents of babies in neonatal intensive care; our technology will help them cope with a very stressful situation. Since high stress after birth can adversely impact long-term parenting, our technology will also help the babies, and indeed other children of the parents. In general, while considerable research has been done on using advanced IT to help hospital patients, much less has been done on supporting parents and carers (and indeed friends and family more generally). We believe this is a significant opportunity here to genuinely help people in the difficult position of having a very sick baby in hospital. In the longer term, if we are successful we believe our ideas could be applied in many other contexts (besides NICU) where a parent or carer is responsible for someone in hospital; indeed we have already been contacted by staff from a pediatric intensive care unit who are interested in our ideas."
16,2C9214F3-2CFB-412F-8A53-A0F3A5000447,Workshop Proposal: Cancer Bioinformatics,"Currently the biomedical sciences are generating substantial amounts of data. Focussing specifically on the cancer context, the interpretation of these datasets draws on mathematical and computational skills beyond traditional cancer research. The proposed Workshop would stimulate interaction between researchers in this discipline and we will engage the interest of researchers in related disciplines such as statistics, machine learning and various areas of computer science.",,"The Workshop will provide the opportunity for networking between UK researchers interested in cancer bioinformatics with an opportunity to discuss future research aims, data availability and opportunities. The Workshop will also engage an interest from other disciplines, princi- pally researchers with a background in statistics, machine learning and computer science: the majority of invited speakers will have a general background in these areas giving the Workshop a strong analytic leaning. The aim is to stimulate collaborations between researchers. As a residential conference venue there are good opportunities for social interaction. In addition, the poster seesions will provide a more technical avenue for discussion betwen delegates. Additionally there will be two Open Discussion Forums and Breakout sessions on the last day."
17,E1B370FC-5463-48EC-8BD2-FB7122797B83,Rigorous Runtime Analysis of Nature Inspired Meta-heuristics,"A rigorous runtime analysis of different nature inspired meta-heuristics will be analysed in this projectin order to gain a deeper understanding of when and why a given meta-heuristic is expected to perform well or poorly. Various nature inspired meta-heuristics have been applied successfully to combinatorial optimisation in many scientific fields.However, their computational complexity is far from being understood in depth. It is still unclear how powerfulthey are for solving combinatorial optimisation problems, and where their real power is in comparison with the more traditional deterministic algorithms.Evolutionary Algorithms (EAs), Ant Colony Optimisation (ACO) and Artificial Immune System (AIS) algorithms will be studied in this project.Since the knowledge level of their computational complexity is at very different stages, two different types of results will be produced.One is the computational complexity results of realistic EAs, not (1+1)-EAs, on selected well-known combinatorial optimisation problems. A setup of complexity classes will be built revealing what classes of problems are hard (or easy) for which kind of EAs.The other is a setup of the first basis for a systematic computational complexity analysis of ACO and AIS other popular nature inspired meta-heuristics for which very few runtime results are available.The expected outcomes of this project will not only provide a solid foundation, but also insights and guidance in understandingwhich meta-heuristic should be preferred for a given problem and in the design of more efficient variants.",,"Nature inspired meta-heuristics are used in numerous practical applications that involve an optimisation process. These are wide spread with numerous applications. Industrial beneficiaries include the supply chain and logistics sector, the manufacturing sector, engineering sector, the health sector, etc. Through runtime analyses on specific problems, we will gain a deep understanding of which kind of problems a given meta-heuristic will perform well on, and on which it will perform poorly. Furthermore, the reasons for the good or bad performance will be highlighted. These kind of results, will allow practitioners to choose the best algorithm for their problem and effectively exploit the known information about the problem when setting the parameters of the algorithms. As a consequence, the results will help in considerably reducing the amount of resources required for a practical application, especially in terms of money and time. Most importantly they can tell a practitioner about the scalability of the algorithm to large problem sizes. The gained insights and guidance will lead to the design and application of better variants of the algorithms and to more efficient solutions to the problems. Due to the enormous spread of applications, the benefits of theoretical guidance should not be underestimated. Considering the involved fields, improvement in the quality of results of major companies and government organisations will definitely foster economic competitiveness, increase the effectiveness of public services and increase the quality of life and health. We will regularly disseminate our results through top international journals in evolutionary computation (EC), artificial intelligence and classical computer science. Results will also be presented at the top EC international conferences (GECCO, CEC, PPSN, FOGA, ANTS and ICARIS). Focused special sessions, workshops and tutorials will also be organised at the international conferences to foster discussion and exploitation of the latest results. Direct exploitation of our results in UK, Europe and USA will happen through the ``Centre of Excellence for Research in Computational Intelligence and Applications'' (CERCIA), The Max Planck Institute (MPI) of Saarbruecken, Germany and AT&amp;T Labs, New Jersey, USA. Results towards the better understanding of the studied algorithms will be immediately exploited in the practical applications involved in these research organisations. During these research visits, seminars and workshops will be also organised."
18,6E8389DB-1DDC-4671-9538-C40DC4036C53,Designing Effective Research Spaces Sandpit: SPIRES- Supporting People Investigating Research Environments and Spaces,"The EPSRC Digital Economy theme seeks to investigate how the use of innovative ICT can transform both lives and work. Research is vital to the UK economy. The UK research councils annually invest 2.8 billion into new research across a spectrum of academic disciplines. Total research and development spending by UK businesses was 16.1 billion per annum in 2007 . Research is also a significant part of the lives of the general public, whether researching their family history or contributing observational data to the BBC programme Springwatch. Given that research is so important to society, it is reasonable to ask how we can assist researchers to become more effective.SPIRES arises from a Digital Economy sandpit held in July 2009 on Effective Research Spaces that brought together an inter-disciplinary set of researchers interested in this theme. SPIRES - Supporting People Investigating Research Environments and Spaces - is a network proposal that aims to bring together currently disparate groups interested in the design of effective research spaces and environments in order to create a new research community. Through a set of focused workshop and outreach activities it will bring together the three perspectives of physical spaces, novel technology, and social interaction to support a new synthesis of ideas and new conversations between groups currently not in contact with each other. Its objectives are: - To form and support a multi-disciplinary community of researchers with expertise in enhancing physical, technological or social aspects of research environments; - To develop a deeper understanding of the ways in which effective research can be fostered; - To document and disseminate the current state of the art in nurturing effective research spaces, both from a UK and international perspective; - To stimulate the production of new methodologies for designing and evaluating effective research spaces.",,"SPIRES will actively seek to expand its membership base outside of academia, using its contact with the Edinburgh BELTANE (Beacon for Public Engagement) with their close links to public organisations such as Edinburgh Museums and Botanic Gardens. Equivalent public engagement organisations elsewhere in the UK will be sought to expand the network's influence into Museums and other public spaces where research is carried out by non-academics. Nesta FutureLabs will also be approached in order to link SPIRES to state-of-the-art work in more generic learning environments and to non-university educationalists. SPIRES has been asked not to approach the British Library at the proposal stage because they are involved as evaluators, but will do so as a priority if the network is funded both because they will be designing an innovative research space of their own and because they link to many different types of researchers whose research environment requirements may be very different from those of university researchers. SPIRES will also benefit industrial research centres by making available and publicising the most current work on effective research spaces as well as the theoretical basis for making research more effective. Industrial research centres such as Microsoft Cambridge and Google will be approached at the start of the network with a view to drawing them into membership. The Salford MediaCity will also be approached through SPIRES Salford members as a domain in which technology-rich research spaces are likely to be of immediate benefit to Media and Communications professionals. SPIRES will also target specific industrial sectors such as pharmaceuticals and genetic engineering where increasing the effectiveness of research has a direct impact on UK productivity and wealth and on the health of its population."
19,D5A3AF71-D4DA-4859-8017-A31C2D72F785,SEBASE: Software Engineering By Automated SEarch,"Current software engineering practice is a human-led search for solutions which meet needs and constraints under limited resources. Often there will be conflict, both between and within functional and non-functional criteria. Naturally, like other engineers, we search for a near optimal solution. As systems get bigger, more distributed, more dynamic and more critical, this labour-intensive search will hit fundamental limits. We will not be able to continue to develop, operate and maintain systems in the traditional way, without automating or partly automating the search for near optimal solutions. Automated search based solutions have a track record of success in other engineering disciplines, characterised by a large number of potential solutions, where there are many complex, competing and conflicting constraints and where construction of a perfect solution is either impossible or impractical. The SEMINAL network demonstrated that these techniques provide robust, cost-effective and high quality solutions for several problems in software engineering. Successes to date can be seen as strong pointers to search having great potential to serve as an overarching solution paradigm. The SEBASE project aims to provide a new approach to the way in which software engineering is understood and practised. It will move software engineering problems from human-based search to machine-based search. As a result, human effort will move up the abstraction chain, to focus on guiding the automated search, rather than performing it. This project will address key issues in software engineering, including scalability, robustness, reliability and stability. It will also study theoretical foundations of search algorithms and apply the insights gained to develop more effective and efficient search algorithms for large and complex software engineering problems. Such insights will have a major impact on the search algorithm community as well as the software engineering community.",,
20,924A70DE-105E-41C5-818E-BBC14EE4D0DE,Structured Sparsity Methods in Machine Learning an Convex Optimisation,"Over the past ten years theoretical developments in machine learning (ML) have had a significant impact in statistics, applied mathematics and other fields of scientific research. In particular, fruitful interactions between ML and numerical optimisation have emerged that are expected to lead to theoretical and algorithmic breakthroughs with the potential to render ML methodologies significantly more applicable to many problems of practical importance. The proposed project aims to make significant UK contributions at a crucial juncture in this emerging interdisciplinary field that has so far been dominated by the US and France. Many ML techniques can be cast as problems of minimising an objective function over a large set of parameters. Examples include support vector machines as well as more recent techniques for semi-supervised learning and multi-task learning. Often the objective function is convex. Consequently, ideas from convex optimisation are becoming increasingly important in the design, implementation and analysis of learning algorithms. Up to now, however, ML has almost exclusively resorted to off the shelf methods for convex optimisation, without substantially exploiting the rich theory which lies behind this field. A thesis of this proposal is that there is a need for a deeper interplay between ML and numerical optimisation. Ultimately, bridging the two communities will facilitate communication and the power of core optimisation will be more easily brought to bear in ML and lead to new frontiers in optimisation. An area in which the interplay between ML and optimisation has a particularly important role to play is in the use of sparsity inducing optimisation problems. A rationale that drives the use of sparsity-inducing models is the observation that when the number of model parameters is much larger than the number of observations, a sparse choice of parameters is strongly desirable for fast and accurate learning. Building on this success, we believe that the time is now right for the development of a new line of algorithms for matrix learning problems under structured sparsity constraints. This means that many of the components of the parameter matrix or a decomposition thereof are zero in locations that are related via some rule (e.g the matrix may be constrained to have many zero rows, many zero eigenvalues, to have sparse eigenvectors, etc.).Perhaps the most well-know examples in which structured sparsity has proven beneficial are in collaborative filtering, where the objective function is chosen to favour low rank matrices, and in multi-task learning where the objective function is chosen to favour few common relevant variables across different regression equations. These types of optimisation problems have only recently started to be addressed in ML and optimisation, and several fundamental problems remain open, most importantly the study of efficient algorithms which exploit the underlying sparsity assumptions and a statistical learning analysis of the methods.Our proposal is multidisciplinary and involves substantial exchange of ideas between Computer Science (Machine Learning) and Mathematics (Numerical Optimisation), with three main goals. Firstly, we aim to develop novel and efficient algorithms for learning large structured matrices; fast convergence of the algorithms should be guaranteed when applied to problem data that have a sparse solution. Secondly, in the cases where the assumed sparsity structure leads to NP-hard problems and the first goal is unachievable (this is often the case under low-rank assumptions), we aim to identify tractable convex relaxations and understand their impact on sparsity. Thirdly, we aim for models and algorithms that have a more natural interpretation than generic solvers (e.g., a minimax statistical justification), which should make it more likely that practitioners will embrace the new methodology.",,"As data collection continues, the design of methods for ''learning from data'' may have a major impact on the way our economy evolves, and may aid societal development. The methods developed in the proposed project may prove to be a valuable approach to handle large amount of data in a more efficient manner. We expect these methods to have a significant impact in improving current systems in several areas outside the academic community, including both the industrial and public sectors. Some of our matrix learning methods may be of significant practical value in the retail industrial sector, where data of customers' preferences to products abound. For example, we expect that our work may be valuable to companies such as GlaxoSmithKline, Unilever, and Fortent, with whom UCL have established links. At the same time our methodology has the potential of being used by health organisations in a variety of prediction problems. We believe that, by appropriately engaging potential users and making them aware of the novelty of our methodology, the proposed project may have a noticeable impact outside the academic community within the next 5 years. This impact may be measured in terms of transfer of knowledge, improved technology and job creation. In the longer term (more than 10 years), the methods and principles developed in this project may influence both social organisations and companies to redevelop their data analysis software and products, which may be crucially important in improving quality of life, health care and creation of wealth. In order to increase the impact of the proposed project in the real world, we shall promote our research findings in different ways. These include: consultation meetings, workshops, an interactive website, and publications for the wider public. In particular, we plan a two-day workshop in Oxford (at the beginning of the second year of the project) where people from industry will be encouraged to participate. This will ensure that the potential users become aware of our methodology and will facilitate transfer of knowledge outside the academic environment. Furthermore, the Centre for Computational Statistics and Machine Learning (CSML) at UCL provides an excellent means through which to pursue further contacts with industry and promote the new methodology in the wider industrial sectors. As a further means of dissemination by direct training, we aim to develop and offer a graduate course on sparsity inducing optimisation and its applications via the Oxford Taught Course Centre (TCC). Lectures will be videotaped and made available to the wider public via podcasts on a special project website. The website will be designed to have both material that is easy to understand and appeals to the mathematically untrained public, as well as mathematically rigorous material aimed at university students with a mathematical or technical background and at colleagues in the field. The material aimed at the general public could motivate high-school pupils and young adults to undertake studies in computer science or applied mathematics. The principal investigators already have relevant prior expertise in achieving successful knowledge exchange. Furthermore, UCL and Oxford have professional experts in place who can provide feedback on building up communication skills and writing to the wider public. Moreover, if needed, the postdocs and students will undertake skill developments courses, which are freely available at both institutions."
21,CE5FD347-6067-4C53-AA67-E4982A51E25D,CARDyAL: Cooperative Aerodynamics and Radio-based DYnamic Animal Localisation,"The primary scientific objectives of the CARDyAL project lie (i) in the challenges inherent in creating very lightweight sensing devices capable of accurate localisation; (ii) in the use of those devices in measuring the dynamically changing relative location of a cooperative group of animals in a range of different contexts for which GPS-based localisation techniques are inappropriate; (iii) relating that dynamic behaviour to scientific questions of cooperative fluid dynamics, energetics, and social biology using appropriate physiological data; and (iv) using the measured information about team organisation to inform a particle swarm optimisation model that will be used as a potential way of reducing energy consumption within and outside this application domain. The ability to make such measurements, and thus to undertake science in these areas, is currently very limited and the problem of achieving this is both challenging in research and engineering terms, and likely to be dependent on the environment for which the particular solutions are created. However, the applications for such technologies are numerous, varied and of significant scientific and strategic importance to the biological sciences community and to the public at large.The technology developed in this project is intended to be inherently transferrable: the hardware and software base for tags capable of relative localisation (a) indoors (b) in long-term deployments (c) in high dynamic environments has applications both to other animal models - understanding the dynamics of groups for purposes of conservation, ecology, welfare or epidemiology - and to the monitoring of humans for better facilities management, workplace design, emergency service and military use, amongst other things. The data reduction and analysis methodologies are applicable to a range of situations in which dynamic and social structure is important, and the relationship between true biological data and a derived PSO model is of substantial scientific interest, and of potential applicability to optimisation problems of many sorts, e.g. energy minimisation, in the computer graphics industry in terms of more accurately representing swarm movements, and in swarm robotics of various types, in path planning and control. In terms of the biology, the availability of this technology in a usable form factor allows research to be performed in a way that is not currently possible, and, consequently, allows the potential to ask scientific questions that are not currently capable of being answered.Thus CARDyAL involves research and engineering in the field of lightweight wireless sensing and wireless localisation that would lack specificity without the constraints and demands of a real application domain, and it involves research in the field of animal sciences that it would not be possible to conduct without the sensing devices and their associated algorithms. There are advances to be made in each of the fields that could not be made without the active engagement of the other and this co-dependency is innately both translational (high risk) and high reward since it is founded on a truly symbiotic programme of research in which neither of the constituent research fields dominates.",,
22,C6F35F14-A777-41AB-9F7C-8246570848B8,Performance based expressive virtual characters,"Creating believable, expressive interactive characters is one of the great, and largely unsolved, technical challenges of interactive media. Human-like characters appear throughout interactive media, virtual worlds and games and are vital to the social and narrative aspects of these media, but they rarely have the psychological depth of expression found in other media. This proposal is for the development of research into a new approach to creating interactive characters which identifies the central problem of current methods as being the fact that creating the interactive behaviour, or Artificial Intelligence (AI), of a character is still primarily a programming task, and therefore in the hands of people with a technical rather than an artistic training. Our hypothesis is that the actors' artistic understanding of human behaviour will bring an individuality, subtlety and nuance to the character that it would be difficult to create in hand authored models. This will help interactive media represent more nuanced social interaction, thus broadening their range of application. The proposed research will use information from an actor's performance to determine the parameters of a character's behaviour software. We will use Motion Capture to record an actor interacting with another person. The recorded data will be used as input to a machine learning algorithm that will infer the parameters of a behavioural control model for the character. This model will then be used to control a real time animated character in interaction with a person. The interaction will be a full body interaction involving motion tracking of posture and/or gestures, and voice input.In entertainment this method will enable more social genres and help improve the current limited demographic. It will also enable a number of new applications in education, rehabilitation, media and marketing. Putting actors in charge of creating character AI will also make production pipelines more efficient be requiring less input from programmers This project is timely in that it brings together a number of active and developing research fields including expressive virtual characters, motion capture based animation and machine learning. It has the potential to transform current research in expressive virtual characters and present new research problems for machine learning and motion capture based animation. It is novel in that it proposes a fundamentally new approach to creating interactive characters and it combines disciplines such as animation, statistical machine learning, performance, affective computing, human computer interaction and psychology. The use of both machine learning and performance for virtual characters is particularly novel.",,"Who will benefit? The primary beneficiaries outside of academia will be the creators of 3D interactive media, particularly computer games. This will also benefit the users of similar technology in specific areas such as education, psychotherapy and the arts. Academics in areas such as computer animation and virtual reality are also likely to benefit. How will they benefit? There are two major benefits of the proposed method: 1. Integrating artists more directly into the content pipeline for character AI, thus improving the efficiency of the production process 2. Creating more expressive and subtle behaviour of characters, thus increasing the range of genres and markets available to interactive media. What will be done to ensure they benefit? Our strategy will be 3 fold: 1. knowledge dissemination: primarily through academic journals and conferences, but also potentially directly through industry through training programmes (Gillies has participated in training schemes for Electronic Arts). 2. Further development: as the proposed research is still new, it is unlikely that the results will be ready for commercial exploitation by the end of this project. Therefore, if the results are positive, further funding will be sought to develop the proposed method in an academic research context 3. Direct exploitation: in the longer term, when the method is sufficiently developed, commercial exploitation will be sought. The most likely route will be through a partnership with an existing company working in games or interactive media via knowledge transfer or commercialization funding. Intermediary software may also be released to the community if this does not conflict with commercial exploitation."
23,B962EBD9-54E0-4D82-A2B8-88AB73202958,Structured Sparsity Methods in Machine Learning an Convex Optimisation,"Over the past ten years theoretical developments in machine learning (ML) have had a significant impact in statistics, applied mathematics and other fields of scientific research. In particular, fruitful interactions between ML and numerical optimisation have emerged that are expected to lead to theoretical and algorithmic breakthroughs with the potential to render ML methodologies significantly more applicable to many problems of practical importance. The proposed project aims to make significant UK contributions at a crucial juncture in this emerging interdisciplinary field that has so far been dominated by the US and France. Many ML techniques can be cast as problems of minimising an objective function over a large set of parameters. Examples include support vector machines as well as more recent techniques for semi-supervised learning and multi-task learning. Often the objective function is convex. Consequently, ideas from convex optimisation are becoming increasingly important in the design, implementation and analysis of learning algorithms. Up to now, however, ML has almost exclusively resorted to off the shelf methods for convex optimisation, without substantially exploiting the rich theory which lies behind this field. A thesis of this proposal is that there is a need for a deeper interplay between ML and numerical optimisation. Ultimately, bridging the two communities will facilitate communication and the power of core optimisation will be more easily brought to bear in ML and lead to new frontiers in optimisation. An area in which the interplay between ML and optimisation has a particularly important role to play is in the use of sparsity inducing optimisation problems. A rationale that drives the use of sparsity-inducing models is the observation that when the number of model parameters is much larger than the number of observations, a sparse choice of parameters is strongly desirable for fast and accurate learning. Building on this success, we believe that the time is now right for the development of a new line of algorithms for matrix learning problems under structured sparsity constraints. This means that many of the components of the parameter matrix or a decomposition thereof are zero in locations that are related via some rule (e.g the matrix may be constrained to have many zero rows, many zero eigenvalues, to have sparse eigenvectors, etc.).Perhaps the most well-know examples in which structured sparsity has proven beneficial are in collaborative filtering, where the objective function is chosen to favour low rank matrices, and in multi-task learning where the objective function is chosen to favour few common relevant variables across different regression equations. These types of optimisation problems have only recently started to be addressed in ML and optimisation, and several fundamental problems remain open, most importantly the study of efficient algorithms which exploit the underlying sparsity assumptions and a statistical learning analysis of the methods.Our proposal is multidisciplinary and involves substantial exchange of ideas between Computer Science (Machine Learning) and Mathematics (Numerical Optimisation), with three main goals. Firstly, we aim to develop novel and efficient algorithms for learning large structured matrices; fast convergence of the algorithms should be guaranteed when applied to problem data that have a sparse solution. Secondly, in the cases where the assumed sparsity structure leads to NP-hard problems and the first goal is unachievable (this is often the case under low-rank assumptions), we aim to identify tractable convex relaxations and understand their impact on sparsity. Thirdly, we aim for models and algorithms that have a more natural interpretation than generic solvers (e.g., a minimax statistical justification), which should make it more likely that practitioners will embrace the new methodology.",,
24,639D7213-0F3F-4865-A991-5D34B29D645C,Topology-based Motion Synthesis,"One of the major drivers of research in the area of humanoid robotics is the desire to achieve motions involving close contact between robots and the environment or people, such as while carrying an injured person, handling flexible objects such as the straps of a knapsack or clothes. Currently, these applications seem beyond the ability of existing motion synthesis techniques due to the underlying computational complexity in an open-ended environment. Traditional methods for motion synthesis suffer from two major bottlenecks. Firstly, a significant amount of computation is required for collision detection and obstacle avoidance in the presence of numerous close contacts between manipulator segments and objects. Secondly, any particular computed solution can easily become invalid as the environment changes. For instance, if the robot were handling an object such as a knapsack, even small deformations of this flexible object and minor changes in object dimensions (e.g., between an empty bag and a stuffed bag) might require complete re-planning in the current way of solving the problem. Similar issues arise in the area of computer animation, where there is a need for real-time control of characters - moving away from static sequences of pre-programmed motion. Although it may seem that this world is much more contained, as it is created by an animation designer, there is in fact a strong desire to create games and simulation systems where the users get to interact with the world continually and expect the animation system to react accordingly. This calls for the same sort of advances in motion synthesis techniques as outlined above.The fundamental problem lies in the representation of the state of the world and the robot. Typically, motion is synthesizes in a complete configuration or state space represented at the level of generalized coordinates enumerating all joint angles and their 3D location/orientation with respect to some world reference frame. This implies the need for large amounts of collision checking calculations and randomized exploration in a very large search space. Moreover, it is very hard to encode higher level, semantic, specifications at this level of description as the individual values of the generalized coordinates do not tell us anything unless further calculations are carried out to ensure satisfaction of relevant constraints. This is particularly inconvenient when searching for a motion in a large database. The focus of this research is to alleviate these problems by developing methods that exploit the underlying topological structure in these problems, e.g., in the space of postures. This allows us to define a new search space where the coordinates are based on topological relationships, such as between link segments. We refer to this space in terms of 'topology coordinates'. In preliminary work, we have shown the utility of this viewpoint for efficient motion synthesis with characters that are in close contacts. We have also demonstrated that this approach is more efficient for categorizing semantically similar motions. In this project, we will develop a more general framework of such techniques that will be applicable to a large class of tasks carried out by autonomous humanoid robots and virtual animated characters. Moreover, we will implement our techniques on industrially relevant platforms, through our collaborators at Honda Research Institute Europe GmbH and Namco Bandai, Japan.",,
0,DA8003D5-4D10-4224-8600-EB4A879254B8,Advanced Dynamic Energy Pricing and Tariffs (ADEPT),"This project addresses a crucial research question that must be answered in the near term is How complicated can, or should, a dynamic electricity tariff be? , such that it is accepted by the public and offers clear enhancements and incentives for reduction in energy demand? The 'can' and 'should' reflect the fact that any ubiquitous technical system is (primarily) designed and implemented by experts, but has to be accepted and operated by non-experts. This project looks at how the information potentially available from smart meters may be exploited to the advantage of both the distribution network operator and the customer. We are looking for the best overall outcome in terms of energy demand reduction, not the best 'engineering solution'. The driving forces towards the need for dynamic tariffs are strong: increased embedded generation, the introduction of plug-in electric vehicles, decreasing national generating capacity, further additions of medium and large scale variable generators, and the prospect of short-term load-shedding by suspending low priority consumption within commercial and domestic. This project aims to discover understanding of the whole interacting system. This project will take account of the smart metering and infrastructure options outlined in the recent Government consulation and response. Using High-Performance Computing to provide a scalable solution to large-scale data management for smart metering is especially timely as it addresses one of the main issues that was raised in the consultation. If, as a nation, we are to lower our overall energy demand, we will have to shift from fossil fuels to less carbon intensive supplies and optimise our energy consumption across all possible sources. This may mean that electricity demand may increase. At the same time, there is an imminent crisis in generating capacity (by whatever means), so we have to make significantly better use of the energy and the assets which make up the infrastructure. The meter is the interface between the consumer and the network operator, so in principle, a smart meter could manage and provide all of the information which describes the state of the network at that point at that time. Increasing data availability will bring benefits to both users and controllers - with detailed knowledge system behaviour in near-to-real-time at the lowest operational level, network operators have a better opportunity to balance the system load, and concurrently offer consumers much enhanced mechanisms for reducing their own power demand.",,"The proposed system has the potential to significantly lower residential electricity demand and improve power management in the distribution network, resulting in a more energy efficient and sustainable system. Network observability and stability can also be improved by forestalling peak demand though dynamic and time-of-use pricing. The end product from the consumer and supplier perspectives is an electricity supply system that: 1) offers opportunities to save both electricity and money, 2) is more flexible, and 3) is potentially more sustainable. High performance computing solutions to large-scale data problems are falling in cost and we expect will offer ubiquitous capabilities to assist with systems such as the supply of electricity, with benefits for both consumers and retailers."
1,F785F4E9-D9C6-4D99-A512-5CB1CD269676,Life-Long Infrastructure Free Robot Navigation,"In the future, autonomous vehicles will play an important part in our lives. They will come in a variety of shapes and sizes and undertake a diverse set of tasks on our behalf. We want smart vehicles to carry, transport, labour for and defend us. We want them to be flexible, reliable and safe. Already robots carry goods around factories and manage our ports, but these are constrained, controlled and highly managed workspaces. Here the navigation task is made simple by installing reflective beacons or guide wires. This project is about extending the reach of robot navigation to truly vast scales without the need for such expensive, awkward and inconvenient modification of the environment. It is about enabling machines to operate for, with and beside us in the multitude of spaces we inhabit, live and work. Even when GPS is available, it does not offer the accuracy required for robots to make decisions about how and when to move safely. Even if it did, it would say nothing about what is around the robot and that has a massive impact on autonomous decision-making.Perhaps the ultimate application is civilian transport systems. We are not condemned to a future of congestion and accidents. We will eventually have cars that can drive themselves, interacting safely with other road users and using roads efficiently, thus freeing up our precious time. But to do this the machines need life-long infrastructure-free navigation, and that is the focus of this work.We will use the mathematics of probability and estimation to allow computers in robots to interpret data from sensors like cameras, radars and lasers, aerial photos and on-the-fly internet queries. We will use machine learning techniques to build and calibrate mathematical models which can explain the robot's view of the world in terms of prior experience (training), prior knowledge (aerial images, road plans and semantics) and automatically generated Web queries. The goal is to produce technology which allows robots always to know precisely where they are and what is around them. Robots have a big role to play in our future economy, but underpinning this role will be life-long infrastructure-free navigation.",,"Impact Summary It is hard to understate the importance of the transport of goods and people in daily life. We depend on it totally. Any increase in efficiency, access, safety or reliability will have a major economic and societal impact. This proposal aims to achieve just those things. We will use information engineering, computing and robotics to provide a low cost underpinning for smart vehicles in civil, defence and industrial domains. Such vehicles offer the possibilty of end-to-end goods transportation - from raw materials to point of sale. They promise improved efficiency and safety on our roads. They give our aged, infirm and sensorially impaired citizens the hope of independent personal transport. Our aim is to enable this without requiring new navigation infrastructure. Car manufacturers have long been interested in improving driver experience and safety through sensing and processing. The vision of this proposal reaches beyond better parking sensors or lane detectors to real automotive autonomy and assisted driving. Robotics science has a massive role to play. In 2008, 2000 people were killed on UK roads due to driver error or concentration loss. We can address this by building cars which interpret their surroundings hundreds of times a second, never grow weary and access the cumulative experience of all cars. We need not wait for complete autonomy. The path to driverless transport is rich with exploitation opportunities. For example, smart cars can aid congestion control systems where humans currently respond to flow control signals in overly cautious and far from optimal ways. Cars that control themselves efficiently will improve flow, reduce congestion, pollution and transit times. There are energy considerations too. The CO2 cost of manufacture of the average sedan is 11% of its total life footprint. A fleet of autonomous cars operating ceaselessly in our cities would increase transport capacity and increase the person miles/kg CO2. Access to independent transport is another huge motivation. We insist, with good reason, that only the able can drive cars. We often preclude the infirm, partially-sighted and otherwise disabled, demanding that their access to roads be dependent on the munificence of others and public transport. Ultimately, autonomous cars, capable of operating on our roads as they exist now, operating beside and amongst traditional cars will extend the reach of transport to these disadvantaged citizens. This research is driven by this ultimate goal and the belief that this can be achieved by putting smarts in individual cars alone rather than on the roads themselves. The UK Department for Transport is a partner in this project and will ensure relevance to the UK's transport and logistics portfolio. Industry already benefits from precision robot navigation. For years now factories have been serviced by automated guided vehicles (AGVs) but they require awkward installation of beacons or buried wires. Using similar methods, ports in Australia and Singapore are fully automated. Infrastructure-free navigation will benefit AGV producers through market generation and users through increased efficiency. We will see increased coverage, cheaper operation and greater flexibility. There is a prima-facie need for autonomous vehicles in the defence sector. Our armed forces are keen to use smart robots for urban reconnaisance in anti-terrorist and rescue roles. The importance of mobile autonomy in national security is made clear by a US Congressional Mandate that an astounding one-third of all ground vehicles in the US Armed Forces will be unmanned by 2015. Impact will be driven by the involvement of partners representing automotive (Nissan), defence (BAe), industrial (Guidance Ltd), sensing (Navtech), international academic (MIT) and policy-making (DfT) sectors. It will be managed through licensing and inter-partner coorperation. It will be generated by first-class robotics research."
2,AA134693-274F-4597-B709-7372C0CB93F3,"Machine Learning Methods for Personalised, Abstractive Summarisation of Consumer-Generated Media","The success of Web 2.0 and CGM is based on tapping into the social nature of human interactions, by making it possible for people to voice their opinion, become part of a virtual community and collaborate remotely. If we take micro-blogging as an example, the growth in Twitter visits between 2008 and 2009 was over 1,000% and it is projected that by 2010 around 10% of all internet users will be on Twitter. This unprecedented rise in the volume and importance of online content has resulted in companies and individuals spending ever increasing amounts of time trying to keep up with relevant CGM. It is estimated that 700 person hours per year is the absolute minimum that companies and public services need to spend on CGM monitoring, online user engagement, and discovery of new information. This fellowship is about helping people to cope with the resulting information overload, through automatic methods that are capable of adapting to individual's information seeking goals and summarising briefly the relevant media and thus supporting information interpretation and decision making. Automatic text summarisation is key to our goal and consists of compressing the meaning of text documents while preserving the relevant information contained within them. While there has been a lot of research on well-authored texts such as news, summarisation of social media is still in its infancy, with research focused on product reviews. A key experimental finding has been that due to the characteristics of social media (product reviews in particular) it is better first to abstract the relevant information from the different documents and sites and then to use natural language generation to create a fluent text based on this information.In this fellowship I will investigate and evaluate new machine learning methods for personalised, abstractive multi-document summarisation across different social media. For example, diachronic summaries that combine Twitter posts, blog articles, and Facebook wall messages on a given topic. In contrast to previous work, we will pursue an inter-disciplinary approach, which will help us study the social dimension of CGM summarisation and establish actual user needs. The second research challenge is that the algorithms need to be robust in the face of this noisy, jargon-full and dynamic content, as well as needing models capable of representing the contradictory and strongly temporal nature of CGM. A key novel contribution of our work is personalising the summaries, based on a model of user interests, goals, and social context. Issues such as trustworthiness, privacy, and online communities (with their hubs and authorities) will also play an important role. The fourth research challenge is to generate personalised abstractive summaries that can help users with sensemaking and content interpretation. An exciting element of my research will be in studying the different kinds of summaries that are useful for a variety of real users (companies, journalists, and the general public) through multi-disciplinary collaborations with the Press Association, British Telecom, the Oxford Internet Institute, and Sheffield's Department of Journalism. A key project deliverable will be a publicly available browser plugin that provides easy access to the automatically generated summaries. This will allow me to evaluate the project results with real users, on a large scale. It will also provide a new evaluation challenge for the Natural Language Generation community, as researchers will be able to compare their summarisers against those delivered by our open-source algorithms. Last but not least, the fellowship covers not only foundational multi-disciplinary research but it also tests the results in several Digital Economy pilot experiments involving commercial partners (The Press Association, British Telecom, Fizzback).",,"Since consumer-generated media have revolutionised our personal lives, the economy, and society as whole, this research has wide-ranging implications and relevance. The five year duration of this fellowship will give us the necessary flexibility, time and resources to undertake dissemination, exploitation, and training activities aimed at the multiple beneficiaries detailed below. Firstly, in order to achieve maximum impact, project results will be made open-source, which will include both computational algorithms and the data collections on which they were tested and evaluated. Having open-source results is needed not only because we aim to create an active research community around the project results, but also since we want to promote use by non-governmental organisations, educational institutions, and other non-profit users. Another important project result will be a free tool to help users become aware of the dangers of social media and allow them to monitor continuously what is being disclosed about them online (by themselves, family, and friends). In terms of economic impact, we will target: (i) the areas used as Digital Economy pilots, namely digital journalism, voice-of-the-customer service providers, and online brand, product, and reputation management; (ii) companies providing internet privacy and security services; (iii) companies providing product comparison services to consumers. Major beneficiaries are the Digital Economy sectors identified above. In addition to the project partners (BT, PA, Fizzback, and Nokia), through UK and international research projects the PI has built successful industrial collaborations with other large companies (Yahoo, Atos, Dassault Aviation, Elsevier, MPS Bank, Creditreform, NetInfo) and SMEs (Garlik, Innovantage, Ontotext, Matrixware, Mondeca, Ontoprise, ISOCO), where our previous research showed the potential of intelligent summarisation technology for knowledge management and business intelligence. Further knowledge transfer opportunities arise through the Sheffield University connections to the digital and new media industries in the Sheffield city region, which are growing at a faster rate than anywhere else in the UK in terms of specialist companies and new jobs. A unique opportunity arises also from the 100m South Yorkshire Digital Region project, which will pilot the Next Generation Broadband and thus provide the required infrastructure for advanced digital economy applications. The research proposed here also has significant societal relevance. The first dimension is user privacy and trust, which are an important element of our work. We plan to promote the new technology to users and companies with identity theft products (e.g. Garlik, with whom we have worked in the past), in order to help people with tracking personal data divulged on public web sites and social networks. This research is relevant to policies such as: Digital Britain, EU's e2010, Global Focus on ICT in Development. We will reach out towards policy makers through the new UK Ethical ICT and young people stakeholder group and through Sheffield's Digital World outreach activities. Last, but not least, this fellowship will impact significantly the research careers of all team members. It will provide the PI with the unique opportunity to realise a step-change in her career, in order to join the 7% of female grade A staff in engineering and technology. The inter-disciplinary nature of our research team will help researchers to gain knowledge of fields complementary to their primary expertise and to develop new cross-disciplinary research skills. The PhD students will gain transferable research and presentation skills and be prepared for post-doctoral positions in academic or applied research."
3,95288505-5FAF-4481-B5D7-D5BF9B2BC4A7,UCT for Games and Beyond,"Artificial Intelligence (AI) research and the development of the multi-billion dollar video games industry have gone hand in hand for many years. Video games are by far the most prevalent way that the public encounter AI techniques on a day to day basis, and the desire for better video games has driven AI research in areas such as move/path planning, decision making, non-player character (NPC) behaviour and the automated generation of game content. A recent development of Monte Carlo methods called the Upper Confidence Bounds for Trees (UCT) method promises to have a profound impact on AI for games. Applications of UCT are not limited to games and have potential benefits for almost any domain where simulation and statistical modelling can be used to forecast outcomes, such as planning, decision support, economic modelling, behavioural analysis, and so on.Since it appeared in 2006/7, UCT has revolutionised the demanding problem of move planning for computer Go to produce artificial players able to beat professional players for the first time this year, a feat previously thought infeasible. UCT has also been successfully applied to the less specialised domain of General Game Playing (GGP) to produce the 2008 and 2009 world champion GGP programs. This success in Go, where substantial problem-specific knowledge is used, and in GGP, where it is impossible to use problem-specific knowledge, points to the tantalising possibility of the broad use of UCT between these two extremes. Game AI researchers are now starting to take such a great interest in UCT that we are seeing the birth of a new research field of Monte Carlo Tree Search (MCTS). However, there has been to date no unified effort to fully understand and exploit the UCT algorithm and related MCTS methods, a state of affairs that we plan to redress.The proposed research will develop and evaluate novel extensions of the UCT method to increase its applicability to a broad range of game-related domains including: its use for move planning and decision making in infinite, continuous real-time environments; its application to situations involving uncertainty and incomplete information; and its application to multi-objective and ensemble planning approaches. We will also investigate its use for more general game-related problems including the detection and optimisation or correction of suboptimal game designs and game content, and the automated generation of new high quality games and game content. Further, we will demonstrate how the techniques we develop can be applied to broader non-game domains by demonstrating their application to robotic control and automated music generation, in particular the creatively challenging task of jazz improvisation.The potential impact of UCT and MCTS cannot be overstated. Landmark events that have driven AI research include the introduction of tree search methods which have been the backstay of AI decision making since the inception of this field in the 1950s, and the formalisation of Monte Carlo methods in the 1970s for simulation-based decision making in a broader range of more general and less well-defined problems. UCT/MCTS promises to be the next major breakthrough in AI methods that combines the power of tree search with the generality of simulation-based search.",,
4,4AEDE7E5-A6DA-4DC6-BEA7-F03051790403,UCT for Games and Beyond,"Artificial Intelligence (AI) research and the development of the multi-billion dollar video games industry have gone hand in hand for many years. Video games are by far the most prevalent way that the public encounter AI techniques on a day to day basis, and the desire for better video games has driven AI research in areas such as move/path planning, decision making, non-player character (NPC) behaviour and the automated generation of game content. A recent development of Monte Carlo methods called the Upper Confidence Bounds for Trees (UCT) method promises to have a profound impact on AI for games. Applications of UCT are not limited to games and have potential benefits for almost any domain where simulation and statistical modelling can be used to forecast outcomes, such as planning, decision support, economic modelling, behavioural analysis, and so on.Since it appeared in 2006/7, UCT has revolutionised the demanding problem of move planning for computer Go to produce artificial players able to beat professional players for the first time this year, a feat previously thought infeasible. UCT has also been successfully applied to the less specialised domain of General Game Playing (GGP) to produce the 2008 and 2009 world champion GGP programs. This success in Go, where substantial problem-specific knowledge is used, and in GGP, where it is impossible to use problem-specific knowledge, points to the tantalising possibility of the broad use of UCT between these two extremes. Game AI researchers are now starting to take such a great interest in UCT that we are seeing the birth of a new research field of Monte Carlo Tree Search (MCTS). However, there has been to date no unified effort to fully understand and exploit the UCT algorithm and related MCTS methods, a state of affairs that we plan to redress.The proposed research will develop and evaluate novel extensions of the UCT method to increase its applicability to a broad range of game-related domains including: its use for move planning and decision making in infinite, continuous real-time environments; its application to situations involving uncertainty and incomplete information; and its application to multi-objective and ensemble planning approaches. We will also investigate its use for more general game-related problems including the detection and optimisation or correction of suboptimal game designs and game content, and the automated generation of new high quality games and game content. Further, we will demonstrate how the techniques we develop can be applied to broader non-game domains by demonstrating their application to robotic control and automated music generation, in particular the creatively challenging task of jazz improvisation.The potential impact of UCT and MCTS cannot be overstated. Landmark events that have driven AI research include the introduction of tree search methods which have been the backstay of AI decision making since the inception of this field in the 1950s, and the formalisation of Monte Carlo methods in the 1970s for simulation-based decision making in a broader range of more general and less well-defined problems. UCT/MCTS promises to be the next major breakthrough in AI methods that combines the power of tree search with the generality of simulation-based search.",,"UCT has revolutionised the world of Computer Go, with computer players based on these techniques playing at a level that was inconceivable only a decade ago. We believe that a similar level of impact can be achieved in a far broader range of game and non-game domains to significantly benefit the multi-billion dollar video games industry and, more generally, AI research into search, optimisation and decision making. The vast majority of UCT research to date has been done outside the UK (largely in France, Canada, and the US). This state of affairs should not be allowed to continue; funding this research will make UCT research more accessible to UK industry. The public have already shown a great appetite for innovation in video games, with UK games studios well known internationally for their pioneering work in AI. UCT could provide a huge fillip to a new generation of video game designers. Better AI is seen by most game studios as being critical to the success of future titles, and there are now many well documented cases of smart AI being a major selling point of a game with titles such as F.E.A.R, Halo 3, Left4Dead and Crysis being prime examples. The best way to achieve effective AI is heavily debated and is dependent on many factors such as the complexity of the problem, the tools available, the skills base within a company, etc. However, for many games, good AI is extremely hard to achieve and is often one of the most disappointing aspects of even AAA game titles with budgets in the tens or hundreds of millions of dollars. UCT has the potential to revolutionise video game AI in the same way that it has revolutionised Computer Go and General Game Playing. It will lead to characters that behave in appropriate ways not because they have been explicitly programmed to do so, but because such behaviour emerges as the optimal response to a given situation through statistical simulation. This is a very general approach that can shape behaviour in all kinds of ways, in order to make it smarter, more stupid, more challenging, more supportive, and so on. Our work should provide tools that help deliver an optimised player experience with relatively little programming effort. In addition to the direct use of UCT to enhance the AI of the in-game characters, equally significant is the use of UCT as a game designer's tool to analyse the quality of the game mechanics. The proposed research has the potential for enormous impact in the UK and worldwide and we aim to fully exploit this."
5,D385BC8F-C85D-456E-BD68-CD0F0D1A2EA6,The Neural Marketplace,"Modern computers are more powerful than many ever dared expect. So it is remarkable how much today's computers still can't do. Strangely, some of the hardest tasks for computers are effortless to humans. Problems like vision, natural language comprehension, and walking control will undoubtedly require massive computing power. But the real difficulty is our inability to write down sets of rules that a computer can follow to perform these tasks. The only solution may be to develop computer systems that, like us, learn by example and by trial and error, without needing explicit instructions. The brain contains roughly as many neurons as there are transistors in a modern supercomputer. These cells are computationally more sophisticated than was once believed. But what is most amazing is their ability to organize into large, functionally coherent networks, that constantly learn and adapt to an animal's changing circumstances. This happens with no central point of control, suggesting that something about neurons causes them to automatically assemble into information-processing systems. This fellowship proposal is based on a new hypothesis, derived from neurobiological research, for how this self-organization occurs through competitive processes analogous to those of a market economy. A typical neuron in the cerebral cortex receives about 10,000 inputs, which it integrates to produce a single output, broadcast in turn to about 10,000 targets. Our new hypothesis is for a mechanism by which a neuron receives feedback from its targets, signalling how useful the information it carries is to the rest of the network. Several lines of evidence suggest that in the brain, molecules called neurotrophins can act as carriers of this feedback signal. According to the hypothesis, neurons throughout the brain constantly experiment with new information processing strategies. In most cases, the new information will not be required by the neuron's targets, no feedback will be received, and the neuron will return to its prior state. A few neurons, however, will happen upon information that is useful to the larger network, and will receive feedback causing the recent changes to be retained. In a market economy, interactions like this allow autonomous agents (people and firms) to organize into networks. A firm that makes cars buys parts from suppliers, who buy components from their own suppliers, and so on. At each stage of the supply chain, multiple firms compete to produce the best products, experimenting with new designs that, if successful, will increase market share. The decisions required to build a good car are thus distributed over a large number of agents. No one person has to understand every part of the manufacturing process; instead, decisions made by multiple individual agents cause the system to organize itself. Improvements and adaptations occur by experiments with new approaches at all levels. Scale this picture up, and you have a global economy encompassing billions of individuals. Could similar interactions organize the billions of cells in the brain into a single coherent system? And could they allow us to build scalable learning machines to solve currently intractable problems in computing?The current proposal will answer these questions by constructing a series of increasingly large market-based neural network systems, to solve a series of increasingly challenging tasks from speech recognition and robot control. This research will have impact far beyond these domains, informing the construction of learning systems for applications as diverse as vision and medical diagnosis, as well as to domains such as internet routing that require scalable self-organization of multiple computing devices. Confirming the computational validation of the hypothesis would also provide a step-change in our understanding of how the brain processes information, potentially yielding new approaches to disorders of brain organization.",,"Computer systems that learn from data, without explicit programming, were once just a dream, but are now an everyday reality. Machine learning has seen an incredible number of industrial applications including: internet search; personalization (e.g. collaborative filtering); targeted advertising and sales; financial market analysis and automated trading; credit scoring; automated customer service; voice recognition; machine vision; quality management; robotics; bioinformatics; and homeland security. The brain has long served as a model for machine learning algorithms. Of course, for industrial applications, whether an algorithm functions similarly to the brain is not important - all that matters is that it be powerful, flexible and easy to use. By these criteria, neural networks have to date had partial success. The backprop neural network has been one of the most successful machine learning systems, as it can deal flexibly with many kinds of data, and is simple, intuitive, and easy to understand. However, there are problems with backprop algorithm that severely limit its industrial applicability: the inability to efficiently train recurrent networks and simulated neurons with intrinsic dynamics; and difficulty scaling to large networks. The consequence of these drawbacks is that backprop cannot deal elegantly with dynamic applications (speech recognition and robot control being two prime examples), and that it is not scalable to high-dimensional complex problems. The current proposal will introduce a completely new idea into artificial neural network engineering, with the potential to correct backprop's two major shortcomings, the inability to train recurrent networks that perform dynamic processing and the inability to make scalable systems. This would therefore open a whole new range of application domains for neural networks including: nonlinear control; robotics; image, sound, and movie recognition; automated diagnosis of biomedical signals; speech and natural language processing; security (e.g. automated cctv analysis); econometrics and finance (analysis and prediction of multivariate time series). While the research of the current proposal is aimed at artificial neural networks, the idea of market-based interactions organizing autonomous agents may have applications to organizing a much wider class of scalable multi-agent systems. The idea of social computing : structuring interactions between autonomous computing agents along similar lines as the organization of humans into companies, societies, and economies, is an exciting recent trend in computer science. The current research will investigate how market-based mechanisms can organize networks of simulated neurons; however the principles we will learn from this may have much wider applicability. This promises applications to a wide number of domains including mobile robotics, computer animation, game programming, manufacturing, wireless networking, internet and telecommunications routing, road and air traffic control, power grid management, scheduling, and sensor fusion. This work also has impacts for public health. One in four Britons will experience some kind of mental health disorder in their lifetime, at a cost to the economy of 77.4bn each year. Rational development of treatments for mental illness cannot occur until we understand the way information is processed in the brain. The neural marketplace hypothesis has the potential to revolutionize our understanding of cortical information processing. Experimental work performed in parallel with this research will characterize the role of retroaxonal signals in vivo, and the underlying molecular pathways. Understanding how neurons organize into information-processing networks will provide key insight into the pathology of mental illness. Discovering the underlying molecular pathways would have a revolutionary impact on drug discovery and other therapies for mental illness."
6,ACF9C70E-6799-4BA5-B09D-6E03E25FDF0A,CARDyAL: Cooperative Aerodynamics and Radio-based DYnamic Animal Localisation,"The primary scientific objectives of the CARDyAL project lie (i) in the challenges inherent in creating very lightweight sensing devices capable of accurate localisation; (ii) in the use of those devices in measuring the dynamically changing relative location of a cooperative group of animals in a range of different contexts for which GPS-based localisation techniques are inappropriate; (iii) relating that dynamic behaviour to scientific questions of cooperative fluid dynamics, energetics, and social biology using appropriate physiological data; and (iv) using the measured information about team organisation to inform a particle swarm optimisation model that will be used as a potential way of reducing energy consumption within and outside this application domain. The ability to make such measurements, and thus to undertake science in these areas, is currently very limited and the problem of achieving this is both challenging in research and engineering terms, and likely to be dependent on the environment for which the particular solutions are created. However, the applications for such technologies are numerous, varied and of significant scientific and strategic importance to the biological sciences community and to the public at large.The technology developed in this project is intended to be inherently transferrable: the hardware and software base for tags capable of relative localisation (a) indoors (b) in long-term deployments (c) in high dynamic environments has applications both to other animal models - understanding the dynamics of groups for purposes of conservation, ecology, welfare or epidemiology - and to the monitoring of humans for better facilities management, workplace design, emergency service and military use, amongst other things. The data reduction and analysis methodologies are applicable to a range of situations in which dynamic and social structure is important, and the relationship between true biological data and a derived PSO model is of substantial scientific interest, and of potential applicability to optimisation problems of many sorts, e.g. energy minimisation, in the computer graphics industry in terms of more accurately representing swarm movements, and in swarm robotics of various types, in path planning and control. In terms of the biology, the availability of this technology in a usable form factor allows research to be performed in a way that is not currently possible, and, consequently, allows the potential to ask scientific questions that are not currently capable of being answered.Thus CARDyAL involves research and engineering in the field of lightweight wireless sensing and wireless localisation that would lack specificity without the constraints and demands of a real application domain, and it involves research in the field of animal sciences that it would not be possible to conduct without the sensing devices and their associated algorithms. There are advances to be made in each of the fields that could not be made without the active engagement of the other and this co-dependency is innately both translational (high risk) and high reward since it is founded on a truly symbiotic programme of research in which neither of the constituent research fields dominates.",,
7,71AA7ECE-2B67-437F-8824-DC5449215469,Trusted Autonomous Systems,"Fully autonomous systems are here. In the past 50 years we have quickly moved from controlled systems, where the operator has full control on the actions of the system (e.g., a digger), to supervised systems that follow human instructions (e.g., automated sewing machines), to automatic systems performing a series of sophisticated operations without human control (e.g., today's robotic car assembly lines), to autonomous systems. Autonomous systems (AS) are highly adaptive systems that sense the environment and learn to make decisions on their actions displaying a high degree of pro-activeness in reaching a certain objective. They are autonomous in the sense that they do not need the presence of a human to operate, although they may communicate, cooperate, and negotiate with them or fellow autonomous systems to reach their goals.The objective of this fellowship is to develop the scientific and engineering underpinnings for autonomous systems to become part of our every-day's life. There is a clear benefit for society if repetitive or dangerous tasks are performed by machines. Yet, there is a perceived resistance in the media and the public at large to increasingly sophisticated technology assisting key aspects of our lives. These concerns are justified. Most people have first hand experience of software and automatic devices not performing as they should; why should they be willing to delegate to them crucial aspects of their needs?In a recent influential report published by the Royal Academy of Engineering in August 2009 and widely discussed in the media it is argued that there is a real danger that these technologies will not be put into use unless urgent questions about the legal, ethical, social, and regulatory implications are addressed. For instance, the report highlights the issue of liability in case of collisions between autonomous driverless cars. Who should be held responsible? The passenger? The software? The owner? The maker of the vehicle? Quite clearly society and the government need to engage in a maturedebate on several of these issues. However, the report identifies an even more fundamental point:``Who will be responsible for certification of autonomous systems? [Royal Society, Autonomous Systems, August 2009, page 4]While there are complex regulatory aspects to this question, its underlying scientific implication is that we, as computer scientists and engineers, urgently need to offer society techniques to be able to verify and certify that autonomous systems behave as they are intended to. To achieve this, four research objectives are identified:1) The formulation of logic-based languages for the principled specification of AS, including key properties such as fault-tolerance, diagnosability, resilience, etc.2) The development of efficient model checking techniques, including AS-based abstraction, parametric and parallel model checking, for the verification of AS. 3) The construction and open source release of a state-of-the-art model checker for autonomous systems to be used for use-cases certifications.4) The validation of these techniques in three key areas of immediate and mid-term societal importance: autonomous vehicles, services, and e-health.This fellowship intends to pursue novel techniques in computational logic to answer the technical challenges above. A success in these areas will open the way for the verification of AS, thereby opening the way to their certification for mainstream use in society.",,"Autonomous Systems is a rapidly emerging technology with the potential of revolutionising economies and societies not dissimilarly to what major innovations of the past century, like cars, planes, personal computers, the Internet, etc. As argued in a recent report by the Royal Academy of Engineering the technology cannot be deployed unless certification issues are tackled. This proposal intends to contribute significantly to the scientific and technological basis for certifications to be carried out. Therefore, the proposed research is intended to have a very significant impact on industry and society as a whole in the mid (3-5 years) to long-term. The immediate non-academic beneficiaries of the project are our certification project partners and the key areas they are prominent in, i.e., autonomous vehicles, web-services, and federated health records. These areas will gain advantage from early exposure to principled certification via model checking and may profit from an early adoption of the technology. Should certification standards or recommendations result in any of these areas following work in the project, this will be a very noteworthy development. Even if no complete certification standards arise during the life of the project, given the high-prominence of our partners, it is likely that the results will directly impact the standards that will ultimately arise. The three areas are seen by experts to grow so significantly in the next few years that the project's economic impact may well be of very high significance in each of the three targeted areas. In addition to the well-defined applications above, the project is intended to contribute more generally to the long-term growth of the UK economy. Specifically, given current expertise and early investment, the UK has the opportunity of becoming one of the leading global players in the area of Autonomous Systems. Applications beyond those tackled in the project range from other forms of transport (flight, underwater, rail), to e-government, smart homes for assisted leaving, security systems, etc. These are industries with high technological know-how whose early development would enable the UK to lead world-markets without serious concerns of being undercut by countries with lower manpower costs. Perhaps more importantly than economic considerations, there is a clear benefit for society if AS become part of our everyday life. Many repetitive or dangerous tasks will be delegated to machines and humans will be able to focus on activities of higher added value. In this sense AS will enhance the standards of living of this country and abroad. Certification of systems are recognised to be the necessary precondition for this to happen. Research produced in this project will provide some of the basis for AS certifications to be carried out. Specifically, the formal techniques developed in WP1 and WP2 will be used in training specialised personnel, including students, for the verification of AS. The MCAS model checker released in WP3 will be used in laboratories to verify AS of various nature. MCAS will be released free of charge to ensure very high user take up. MCAS is likely to outlive the project: the open-source release will ensure other parties will be able to contribute to the project and adapt it for their own particular uses. The certification usecases in WP4 will provide initial certifications in key AS areas: ground vehicles, web-services, and federated health records. Each of them has significant economic and societal impact. Furthermore, the usecases studies will provide a frame of reference for other applications to be treated similarly. Detailed impact of these can be found in the accompanying impact plan."
8,97A76777-68F7-4D5F-B855-E23C29CAF7B5,"Evolutionary Algorithms for Dynamic Optimisation Problems: Design, Analysis and Applications","Evolutionary algorithms (EAs) have been applied to solve many stationary problems. However, real-world problems are usually more complex and dynamic, where the objective function, decision variables, and environmental parameters may change over time. In this project, we will investigate novel EA approaches to address dynamic optimisation problems (DOPs), a challenging but very important research area. The proposed research has three main aspects: (1) designing and evaluating new EAs for DOPs in collaboration with researchers from Honda Research Institute Europe, (2) theoretically analysing EAs for DOPs, and (3) adapting developed EA approaches to solve dynamic telecommunication optimisation problems. In this project, we will first construct standardised, both discrete and continuous, dynamic test environments based on the concept of problem difficulty, scalability, cyclicity and noise of environments, and standardised performance measures for evaluating EAs for DOPs. Based on the standardised dynamic test and evaluation environment, we will then design and evaluate novel EAs and their hybridisation, e.g., Estimation of Distribution Algorithms (EDAs), Genetic Algorithms, Swarm Intelligence and Adaptive Evolutionary Algorithms, for DOPs based on our previous research. A guiding idea here is to improve EA's adaptability to different degrees of environmental change in the genotypic space, be it binary or not. Systematically and adaptively combining dualism-like schemes for significant changes, random immigration-like schemes for medium changes, and general mutation or variation schemes for small changes, is expected to greatly improve EA's performance in different dynamic environments. And memory schemes can be used when the environment involves cyclic changes. In order to better understand the fundamental issues, theoretical analysis of EAs for DOPs will be pursued in this project. We will apply drift analysis and martingale theory as the starting point to analyse the computational time complexity of EAs for DOPs and the dynamic behaviour of EAs for DOPs regarding such properties as tracking error, tracking velocity, and reliability of arriving at optima. Based on the above EA design, experimental evaluation, and formal analysis, we will then develop a generic framework of EAs for DOPs by extracting key techniques/properties of efficient EAs for DOPs and studying the relationship between them and the characteristics of DOPs being solved with respect to the environmental dynamics in the genotypic space. Another key aspect of this project is to apply and adapt developed EAs for general DOPs to solve core dynamic telecommunications problems, e.g., dynamic frequency assignment problems and dynamic call routing problems, in the real world. We will closely collaborate with researchers from British Telecommunications (BT) to extract domain-specific knowledge and model dynamic telecommunication problems using proper mathematical and graph representations. The obtained domain knowledge will be integrated into our EAs for increased efficiency and effectiveness. All algorithms and software developed in this project will be made available publicly to benefit as many users as possible, whether they are from academe or industry.",,
9,B78BE7B4-BFC6-4FAF-B305-28E3189A1D9D,Dynamic Adaptation in Heterogeneous Multicore Embedded Processors,"The overall objective of this project is to investigate new and novel methods of automating the design, of both the hardware and software, of embedded systems to enable the timely creation of future generations of high-performance low-power digital appliances. This is a vertically-integrated project, which brings together research in compilers, architectures, signal processing, and an economically-important emerging application area.Embedded processors are an integral part of our everyday lives; from smart phones and flash memory sticks, to wireless communications, automotive computing, bio-medical devices, and many more. Future embedded processors will require significantly higher performance than the processors we have today. However, this must be achieved whilst also increasing their energy efficiency, as such systems are increasingly used in mobile or battery-operated devices.Performance cannot be increased simply by clocking devices at a higher frequency, as this significantly reduces energy efficiency.Previous research has shown that customizing a processor according to its application can provide a significant performance boost whilst simultaneously reducing energy consumption. Similarly, the use of multi-core processors, which can be specialized in heterogeneous ways, offers additional performance in a more energy-efficient way than can be achieved simply by the homogeneous replication of a fixed processor.The first challenge with application-specific processors, which is compounded in heterogeneous multi-core systems, is the vast array of possible designs from which to choose. This increasing complexity of the design space of computer systems, coupled with the drive for lower energy consumption, means that manual approaches to design are no longer feasible. Instead, by automating the process of searching the design space, it becomes possible to find the best designs. However, this approach is computationally intractable, due to the sheer number of designs that must be considered. There is now strong evidence, from our prior work and from others, that machine learning can provide a fast track to design-space exploration in both processor design and compiler design.The second challenge addressed by this project is variability in behaviour. For example, a broadband modem may wish to adapt its behaviour to the environmental conditions affecting signal quality. At the silicon level, factors such as temperature, process variation and operating voltage will affect the performance and energy consumption of the device. Devices that are able to adapt their hardware and software behaviour to meet these changing circumstances will not be constrained by worst-case analysis at design time, but will be able to tune their behaviour dynamically to meet actual real-time constraints. It is widely accepted that variability is a growing concern that requires a new approach. This project examines how dynamic adaptation in software and hardware can solve this problem. This will involve a combination of just-in-time compilation, to create more dynamic software, as well as just-in-time instruction set re-synthesis, to create dynamic processors.A key aspect of this project is the synergy between new design methods and an emerging application; in this case LED-based Visible Light Communication (VLC). The use of LED lighting is growing rapidly, due its low energy consumption. LED light can also be modulated to carry a digital payload at speeds even higher than 100 Mbps. However, this presents a major computational challenge, which we aim to address using the dynamically adaptable customized multi-core processors and compilers outlined above. We aim to extend the use of machine learning from off-line (i.e. performed at design time) to on-line (i.e. performed during system operation). Designs will be fabricated in silicon to demonstrate the impact of our research, and to enable real-time experimentation.",,"This project will have two main areas of impact: (1) in the design of next-generation embedded systems, and (2) in the realization of system-on-chip solutions for free-space optical communications. Our work in system synthesis we will enable the optimization of systems that would previously have been considered impractical. If successful, the project will also provide new design tools that will reduce NRE costs for embedded systems, and open up new application areas. The beneficiaries of this will be the embedded systems industry, from processor IP companies and compiler vendors, to fabless semiconductor companies and system integrators who build electronic devices for use in consumer, automotive, medical, telecommunications and energy industries. By applying dynamic adaptation to embedded systems we hope to solve the fundamental problem of how to cope with on-chip variation at silicon technologies below 65nm. At present this is an unsolved problem, requiring manufacturers to design-in expensive performance margins. Potential beneficiaries will be any future user of a battery-operated device, who will see better performance and longer battery life. The worldwide economic and environmental benefit of a switch to LED lighting is huge, and will drive the update of LED lighting. If this project is able to deliver a system-on-chip solution capable of high bandwidth digital communication through LED-based visible light this would have a far-reaching impact. Such LED light fittings would operate as both sources of low energy lighting and optical wireless access points. This would have a huge impact across a wide range of end-user products spanning the domestic, business, medical, and transportation domains. If the availability of VLC stimulates LED lighting uptake, then a potential future environmental benefit could be a significant reduction in CO2 emissions. This project will also extend and help to sustain the UK skill base in high-performance processor design and nanometre-scale silicon implementation. This project will benefit the UK skills base by training new doctoral students in these highly-specialized skills. The project contains a 10-point plan for maximizing the impact of the research: 1. We shall build demonstrator systems capable of showcasing the theories and algorithms underpinning our work. 2. In the final year of the project we will organise an Innovation Workshop, in order to disseminate our research results to UK and European SMEs. 3. We will engage with potential industrial beneficiaries, to share technologies for research purposes during the project. 4. We will leverage our recent experience in technology licensing to ensure that new technologies emerging from our research are transfered to industry. 5. The formation of a spin-out company will also be considered as a route to industrial exploitation of the VLC demonstrator. 6. We will continue to use online media to communicate to the academic community via http://groups.inf.ed.ac.uk/pasta/ 7. Postgraduate skill sets will be enhanced through training in nanometre-scale chip design. 8. To maximize the academic impact we shall publish our research in the most respected journals and the top conferences in the area. 9. Elements of the demonstrator platform will be offered to academic collaborators to stimulate exchange of ideas. 10. To maximize the academic benefit of our work we shall promote bilateral meetings between our group and others. The timescales for realising these benefits range from 1 to 5 years. The technology demonstrators will act as proof-of-concept, reducing the time needed to mature the ideas before further exploitation to a year or two. The economic benefits of LED-based communications will be realised when uptake grows, which is impossible to predict. However, these devices are driven by Moore's law, suggesting rapid adoption and potential for widespread use within a few years."
10,D3E92159-C752-43AB-89AD-8CD74E806681,Intelligent Agents for Home Energy Management,"Meeting the challenge of cutting UK greenhouse gas emissions by 80% by 2050, and ensuring energy security in the face of dwindling oil and gas reserves, requires a radical change in the way energy (and particularly electricity) is generated, distributed and consumed . Central to delivering this change, is the need to support domestic consumers (who have the least visibility regarding their energy use, but who generate approximately 25% of total UK carbon emissions) in both reducing their demand for energy and improving the efficiency with which they use it. This proposal will do both by applying novel artificial intelligence approaches to the development of intelligent agents that will be transformational in empowering domestic consumers to visualise, understand and manage their energy use.These home energy management agents will collect real-time data from smart gas and electricity meters, and simple low cost temperature and occupancy sensor, and they will learn both the thermal characteristics of the building in which they are deployed and the day-to-day behaviour and energy demands of the home's occupants. In the short term, these agents will provide personalised support to householders by (i) visualising, analysing and comparing energy consumption (e.g. providing itemised energy use information, performing energy audits and comparisons across similar homes), by (ii) autonomously modelling and advising householders of the potential impact of various energy saving practices, and by (iii) tracking, providing feedback and motivating progress toward energy and carbon reduction goals. Such agents will go beyond the simple energy displays of today, and will act as persuasive technologies informed by a cognitive model of behaviour change. In the medium term, they will directly interface with network enabled appliances and will actively manage the delivery of heat and the deferral of electrical loads whilst making efficient use of shared and private variable renewable generation. In doing so, they will provide autonomous and intelligent demand management, whilst satisfying the individual householders' preferences regarding comfort, cost and carbon. Finally, in the long term, these agents will integrate with electric vehicles (EV) and plug-in hybrid vehicles (PHEV), giving the home's occupants visibility and control of their total energy and carbon use, proactively managing electricity storage within these vehicles, and facilitating the delivery of individual carbon budgets and allowances . In essence the project will enable occupants to make appropriately relevant behaviour decisions on their energy consumption and generation, relating the impacts of these decisions on carbon emissions. Beyond the immediate confines of the home, these agents will also have a profound impact at the macro level. They will be developed with a future outlook to facilitate a smart grid in which electricity is bought and sold through short-term dynamically negotiated contracts with local, community-owned and national energy providers in response to real-time pricing and carbon intensity signals.To achieve the goals outlined above, the project brings together an interdisciplinary team comprising world leading experts in the fields of intelligent agents and multi-agent systems (School of Electronics and Computer Science), renewable energy and energy efficiency in the built environment, and human factors in the design of automated control and feedback systems (Sustainable Energy Research Group and Transportation Research Group in the School of Civil Engineering and the Environment) at the University of Southampton. The home energy management agents will be evaluated and demonstrated within two live deployments: one using an existing test-bed of 9 homes in Havant, and one using 25 homes currently undergoing a social house redevelopment programme in Southampton.",,"The technologies developed within this project will help towards the UK's legislated goal of cutting greenhouse gas emissions by 80% by 2050. As such, it will contribute to the global effort to mitigate the worst effects of climate change which challenges the future growth, prosperity, and political stability of all nations. Improving energy efficiency will assist in ensuring UK energy security by reducing the need to import gas from overseas markets, and it represents an opportunity for the UK to be at the forefront of innovation in the area of digital technologies for energy efficiency and smart grids (a market which Siemens estimates to be worth up to 27B over the next five years). Given the innovative digital technologies proposed within this project, which will effectively combine a thermal model of the home, a model of the householders energy use, a principled cognitive model of behaviour change, and sophisticated prediction and optimisation algorithms, our aim would be to better the 20% reductions in energy demand that are seen in well informed and motivated individuals using simple real-time energy displays. If this reduction were delivered across all 25M homes that are to be equipped with smart meters, this could potentially save UK householders 4.6B annually in energy costs. To ensure that these impacts are realised, the project has enlisted the support of an industrial advisory panel to enable early engagement with end users, to guide research toward industrially relevant goals and to identify early exploitation opportunities. This board will be led by a panel of executives (specifically, the chairman, managing director, chief scientist and technology manager) of PRI (a UK-based smart meter manufacturer operating in 50 countries worldwide). Other panel members will be (i) Horstmann Controls (the UK's leading manufacturer of controls for domestic heating systems and an existing partner of PRI) who will advise on the integration of the home energy management agents with current and future home heating systems, (ii) AlertMe (a UK-based manufacturer of home energy monitoring and security systems) who will advise on the immediate commercial potential of the home energy management agents, will work with the project partners to commercialise early research results, and will provide the energy, temperature and occupancy sensing systems for the trial deployments, (iii) Virgin Media (who support the Havant trial homes through the provision of broadband internet) will provide advice on the potential of bundled web, media and energy efficiency packages that will enable or support low-carbon living, and, (iv) the Energy Saving Trust (experts in the public engagement with energy issues) who will advise on future social policy and the dissemination of the research results. The project will hold two public one-day workshops which will encompass additional dissemination to stakeholders and potential end-users. The first workshop will highlight to stakeholders the components of the agents, the data to be gathered and to iron out any ethical issues that may arise through the process, and will provide an opportunity to discuss the implications of smart metering and the use of smart meter data with potential end users. The second workshop will be held 3 months before the end of the project and will summarise the final research outputs to stakeholders and will capture appropriate feedback. Finally, an internship programme, which is intended to enthuse and attract the next generation of researchers, involving students from both the University of Southampton and the Winchester School of Art (specifically, those studying graphic and communication design at post-graduate level), shall engage in demonstration activities and explore innovative and visually engaging means to publicise the specific results of the research project and the need for energy demand reduction in general."
11,C9C1FFA0-68FE-4222-BD43-507817F3ACE6,The Synthesis of Probabilistic Prediction &amp; Mechanistic Modelling within a Computational &amp; Systems Biology Context,"The synergistic advances that can be made by the multidisciplinary interplay between abstracted computational modelling and biological experimental investigation within a system biology context are poised to make major contributions to our understanding of some of the most important biological systems implicated in the genesis of many serious diseases such as cancer. However, due to the unavoidable inherent levels of uncertainty, noise and relative scarcity of biological data it is vital that sound evidential based scientific reasoning be enabled within a systems biology context by formally embedding mechanistic models within a probabilistic inferential framework. The synthesis of mechanistic modelling &amp; probabilistic inference provides outstanding opportunities to make further significant advances in understanding biological systems and processes at multiple levels, by defining system components and inferring how they dynamically interact. There is a major role that statistical machine learning methodology has to play in both computational &amp; systems biology research and a number of important methodological challenges are presented by applications working at this interface.However, one of the most important aspects of successful computational &amp; systems biology research is that it must be conducted in direct collaboration with world-class experimental biologists. An outstanding feature of this Fellowship is that it has set in place six exciting collaborations with internationally leading cancer researchers, proteomics technologists, biochemists and plant biologists who are all fully committed to successfully driving forward a potentially groundbreaking multidisciplinary systems biology research programme as detailed in this proposal. Three important application areas within biological science will shape and direct the research to be undertaken during this Fellowship. The applications are distinct, yet overlap in terms of the modelling &amp; inferential issues which each present and this is important in ensuring a consistent and coherent line of research. They have also been selected for their major importance in the study of cellular mechanisms which are fundamental to cell function, some of which are implicated in certain serious diseases. In addition, the applicant has substantive ongoing collaborations with world-class laboratories engaged in these biological investigations. This ensures the proposed research programme is focused on realistic methodological problems which will have a direct impact on the major scientific questions being asked within each area, as well contributing to the computational and inferential sciences. The first application will develop the inferential tools required by cancer biologists when reasoning about the structures underlying the observed dynamics of the MAPK pathway and these tools will be employed in a large scale study of this pathway in collaboration with the Beatson Institute of Cancer Research. The second application, to be conducted with the Plant Sciences group at the University of Glasgow, will seek to elucidate, in a model-based inferential manner, the remarkable observed phenomenon of organ specificity of the circadian clock in soybean and Arabidopsis, in addition a study of models of transcriptional regulation in the cell-cycle will be conducted. The final application will investigate a number of open issues associated with clinical transcriptomics and proteomics where the identification of possible target genes and proteins is of vital importance to cancer researchers in their studies of, in this case breast and ovarian cancer. This study will be conducted in direct conjunction with the Institute of Cancer Research where an ongoing study of BRCA1&amp;2 mutations implicated in breast and ovarian cancer is underway.",,
12,5D76E2E6-8886-467F-89B7-7CA7E7889637,UCT for Games and Beyond,"Artificial Intelligence (AI) research and the development of the multi-billion dollar video games industry have gone hand in hand for many years. Video games are by far the most prevalent way that the public encounter AI techniques on a day to day basis, and the desire for better video games has driven AI research in areas such as move/path planning, decision making, non-player character (NPC) behaviour and the automated generation of game content. A recent development of Monte Carlo methods called the Upper Confidence Bounds for Trees (UCT) method promises to have a profound impact on AI for games. Applications of UCT are not limited to games and have potential benefits for almost any domain where simulation and statistical modelling can be used to forecast outcomes, such as planning, decision support, economic modelling, behavioural analysis, and so on.Since it appeared in 2006/7, UCT has revolutionised the demanding problem of move planning for computer Go to produce artificial players able to beat professional players for the first time this year, a feat previously thought infeasible. UCT has also been successfully applied to the less specialised domain of General Game Playing (GGP) to produce the 2008 and 2009 world champion GGP programs. This success in Go, where substantial problem-specific knowledge is used, and in GGP, where it is impossible to use problem-specific knowledge, points to the tantalising possibility of the broad use of UCT between these two extremes. Game AI researchers are now starting to take such a great interest in UCT that we are seeing the birth of a new research field of Monte Carlo Tree Search (MCTS). However, there has been to date no unified effort to fully understand and exploit the UCT algorithm and related MCTS methods, a state of affairs that we plan to redress.The proposed research will develop and evaluate novel extensions of the UCT method to increase its applicability to a broad range of game-related domains including: its use for move planning and decision making in infinite, continuous real-time environments; its application to situations involving uncertainty and incomplete information; and its application to multi-objective and ensemble planning approaches. We will also investigate its use for more general game-related problems including the detection and optimisation or correction of suboptimal game designs and game content, and the automated generation of new high quality games and game content. Further, we will demonstrate how the techniques we develop can be applied to broader non-game domains by demonstrating their application to robotic control and automated music generation, in particular the creatively challenging task of jazz improvisation.The potential impact of UCT and MCTS cannot be overstated. Landmark events that have driven AI research include the introduction of tree search methods which have been the backstay of AI decision making since the inception of this field in the 1950s, and the formalisation of Monte Carlo methods in the 1970s for simulation-based decision making in a broader range of more general and less well-defined problems. UCT/MCTS promises to be the next major breakthrough in AI methods that combines the power of tree search with the generality of simulation-based search.",,
13,312B6F44-22E4-498F-989F-F66B1E6C1DBF,A Predictive Modelling based Approach to Portable Parallel Compilation for Heterogeneous Multi-cores,"Modern computers at their heart consist of multiple processingelements. These multi-core processors have the capability ofdelivering high performance with reduced energy consumption, but arehighly challenging to program. As the number of cores is relativelysmall in number at present, operating systems can make good use ofthem. In the near future, however, the number of cores will rise andvary in type and capability. Currently, this means that programmerswill soon have to think in parallel and work out how to partitiondifferent parts of their programs to run on different types ofcores. Each time this program is run on a new platfrom or the currentone is upgraded, this task will have to be repeated. Due to the sheercomplexity of this process, as hardware increases in size andcomplexity, software will not be able to utilise its potential, leadingto software stagnation.This project aims to prevent this software stagnation by investigatingnew techniques to automatically learn how to utilise new multi-coreplatforms. Using ideas and techniques first developed in artificialintelligence, we will develop a system that automatically learns howto adapt software to work on new platforms. It uses statisticalmachine learning to determine what type of cores to use to give thebest performance and also predicts when software is out-of date.If successful it will be of significant benefit to academics workingin the area, UK industry and in the long term applicationsprogrammers.",,"How to effectively utilise multi-cores is the number one priority for academic systems researchers worldwide. It is also one of the main concerns of processor manufacturers and IP vendors. For this reason ARM has provided a letter of support. The demand for ever more cores is likely to fall if no-one can actually achieve any noticable performance improvement. If this project succeeds it will have significant impact on these groups. In addition, application programmers will be able to achieve stable scalable performance and plan their software evolution. This in turn will have significant impact on the economy, given the size of the computing systems market. The following academic groups will benefit from this research - Parallelising Compilers - Runtime Systems - Language Design - Computer Architecture - Machine Learning for Systems Industrial groups - Embedded Systems Providers - Compiler and Tools Vendors - Application Developers"
14,DC0B565B-A9FC-4166-8F90-840E3A733CCF,Biologically inspired transportation: a distributed intelligent conveyor,"A parallel manipulator is a massive array of simple individual actuators with a small power density that collectively transport and position objects with masses considerably higher than the force generated by a single actuator alone. This design is inspired by the biological phenomena of cilia, small hair-like structures on the surface of cells which can either sense local properties such as in the rod photoreceptors for vision or in olfactory neurons for smell, or can move in coordinated wave action to move liquid over their surface, as in the trachea and kidneys. Employing these capabilities in an analogous array of micro-actuators will produce a conveyor of parallel intelligent manipulation able to sense object properties, move them in different directions and effectively sort objects according to their properties. Crucially, the actuator array will be capable of communicating local information about objects to other parts of the array to enable coordinated action.Parallel intelligent manipulation plays an increasingly important role in intelligent robotics, computer science and intelligent manufacturing systems. Significant advantages of the distributed manipulating system are task flexibility (it can be dynamically reprogrammed to implement another task); massive-parallelism (it can process several different objects simultaneously, and different parts of the manipulator can perform separate tasks concurrently); fault tolerance (faults in single actuators do not restrict performance of the system as a whole, so operation of the system can be maintained); autonomy; and the ability to process objects simultaneously and independently.The overarching aim of the project is to build an intelligent autonomous massively parallel manipulator for distributed sensing, recognition, analysis, sorting, transportation and manipulation of light-weight objects. A paradigm of reaction-diffusion computing, i.e. information processing and computation by spreading wave-patterns in non-linear media, will be employed in the control system of the manipulator.Using evolutionary computation and machine learning, we will develop new principles and implementations for non-linear medium based control, and introduce a range of algorithms for distributed sensing (of object properties such as shape), filtration (sorting different objects according to common characteristics), orienting (ensure objects are facing and moving in the correct direction), positioning (moving objects into the correct path of travel on a different part of the manipulator) and shape-determined transportation of the objects.This manipulator system not only has the potential to impact upon the academic community in terms of the advancement of evolutionary algorithms, reaction-diffusion computing, and intelligent robotic systems, but also has ready application domains in industry such as high-tech manufacturing, enabling an advanced network of sensors to control dynamics of mechanical components, automation of assembly of nano-devices, and medical applications such as prostheses and computer controlled implants.",,"The hardware prototype of an intelligent parallel manipulator and a model of the manipulator will be developed as the result of the project. The prototype and its software twin will be available to the industrial community in the UK and abroad for further study, modifications and production of micro-scale analogues. The following domains of science and industry will benefit from the results of the project: developers of distributed smart engineering systems will get mechanical and electronic designs and distributed control procedures for non-standard adaptable fault-tolerant actuators; blueprints for fabrication of autonomous intelligent actuating devices; fully scalable architecture of universal distributed manipulator to be used in smart-structures; MEMS devices are a promising application domain for the new technology of interlayer conductors in MEMS production. Beneficiaries in the industrial sector could not take immediate advantage of this prototype of a smart parallel actuator; however, the design of the parallel actuator will be a 'proof-of-concept' of intelligent distributed control and will be priceless in further engineering implementations of smart conveyor belts, and related applications. Currently the UK Government has as a priority high value manufacturing and sustainable materials and products (www.innovateuk.org). The development of this prototype could lead to implementation on an industrial manufacturing scale enabling UK industry to achieve unparalleled precision in large scale manufacturing, introduce intelligent automated systems capable of participating in not just the movement of assembly parts but the precise manipulation and construction of high-tech equipment even at the nano-scale. In the current economic climate, such a development would place UK manufacturing at the forefront globally and ensure a solid foundation for the future employment and skills of the UK. The developments in control manipulation could be applied in the medical area such as remote surgery, fine manipulation of surgical instruments, or manipulation of surgical implants to ensure precise placement. Indeed, in the public sector, fine sorting of historical and often delicate artefacts could benefit from such robust technology without the sometimes corrosive contact with human hands that can arise form the transfer of skin bacteria, salts and oils to artefacts. Communication and Dissemination Engagement of these potential user markets will be achieved through publication of results in trade magazines such as IET publication Engineering and Technology , engagement at conferences with potential industrial partners, as well as employing the networks currently active across the research team. UWE and BRL specifically have a track record of Public Engagement and Science Communication activities including the EPSRC Stage Award Walking with Robots and Heart Robot , as well as organising events at the Cheltenham Science Festival and others. The project team will ensure that opportunities for engaging with potential users and beneficiaries are identified and fully exploited. A budget of 500 has been included to cover some of these additional costs."
15,E200D524-27CD-4675-B743-4B1DF537E1C6,HUMAN-AGENT COLLECTIVES: FROM FOUNDATIONS TO APPLICATIONS [ORCHID],"With a reported 5 billion mobile subscriptions worldwide, access to communication technologies has reached unprecedented levels and has fundamentally altered the ways in which we experience computational systems. Once delivered through a desktop machine to an office worker, computing has become an interwoven feature of everyday life across the globe in a way that profoundly affects us all. We are now interconnected using mobile devices; we routinely invoke remote services through a global cloud infrastructure and increasingly rely on computational devices in our everyday life. Computational devices monitor our health, entertain us, guide us and keep us safe and secure. However, this explosive growth in these devices and on-line services is only a precursor to an era of ubiquity, where each of us will routinely rely upon a plethora of smart and proactive computers that we carry with us, access at home and at work, and that are embedded into the world around us. As computation increasingly pervades the world around us, it will profoundly change the ways in which we work with computers. Rather than issuing instructions to passive machines, we will increasingly work in partnership with highly inter-connected computational components (aka agents) that are able to act autonomously and intelligently. Specifically, humans and software agents will continually and flexibly establish a range of collaborative relationships with one another, forming human-agent collectives (HACs) to meet their individual and collective goals. This vision of people and computational agents operating at a global scale offers tremendous potential and, if realised correctly, will help us meet the key societal challenges of sustainability, inclusion, and safety that are core to our future. However, these benefits are mirrored by the potential of equally concerning pitfalls as we shift to becoming increasingly dependent on systems that interweave human and computational endeavour.As systems based on human-agent collectives grow in scale, complexity and temporal extent, we will increasingly require a principled science that allows us to reason about the computational and human aspects of these systems if we are to avoid developments that are unsafe, unreliable and lack the appropriate safeguards to ensure societal acceptance.Delivering this science is the core research objective of this Programme. In more detail, it seeks to establish the new science that is needed to understand, build and apply HACs that symbiotically interleave human and computer systems to an unprecedented degree. To this end, it brings together three world-leading academic groups from the Universities of Southampton, Oxford and Nottingham (with multi-disciplinary expertise in the areas of artificial intelligence, agent-based computing, machine learning, decentralised information systems, participatory systems, and ubiquitous computing) with industrial collaborators (initially BAE Systems, PRI Ltd and the Australian Centre for Field Robotics) to collectively establish the foundational scientific underpinnings of these systems and drive these understandings to real-world applications in the critical domains of future energy networks, and disaster response.",,"A key component of ORCHID is the close connection and inter-play between world-leading fundamental research, the demonstration of this research in compelling real-world application scenarios, and the involvement of collaborating partners whose future lies in exploiting such research in these application areas. This provides the ideal environment for producing high impact research across a broad set of areas including: - Advances in knowledge of direct relevance to those involved in understanding the design, construction and use of systems that exploit HACs. These advances will be reflected in new concepts and theories, new interactive techniques and new platforms to support human-agent collaboration (i.e. the new science of HACs). - Novel technologies and demonstrators that showcase the use of HACs in a number of application areas which will show which methods, models and technologies are effective. In particular, the proof of concepts, working prototypes, simulations and studies will provide empirical evidence of the theoretical concepts, the technical feasibility of their construction and the social acceptability of the technologies. - Open source sets of code, toolkits and competitions that encourage a wider community to share and contribute to the research. This will target a broad set of industrial research labs and advanced developers exploring the ways in which increasingly large scale agent arrangements might be exploited. Particularly, relevant in this regard is the Horizon Digital Economy hub with which we will co-promote events and so gain access to their extensive network of partners. This work is of potential interest to a broad set of beneficiaries. In particular, the focus on reducing environmental impact seeks to enhance the quality of life for the everyday citizen and the planet and the work on disaster response seeks to exploit technology to rescue and preserve life. In order to identify where research can be applied, and also for identifying new areas for HAC systems and new collaboration partners, a dedicated Knowledge Transfer Officer will be employed. As part of ORCHID's regular management process, we will reflect upon and update the knowledge transfer strategy by reviewing the key target audiences and the mechanisms used to deliver to them. Our initial set of communities to engage include: - Our collaborating partners, who will benefit from access to the leading researchers in this new area, the ability to shape the ongoing research as it happens, the exchange of researchers through inward and outward secondments, and the ability to demonstrate and evaluate the work in scenarios that closely align with their applications of interest. In recognition of this fact, all of these partners will make a significant investment, which at present, totals over 2M. - Charities concerned with promoting the reduction of carbon emissions and response to disasters, such as the Carbon Trust, Energy Saving Trust, Ushahidi, and the Disasters Emergency Committee, who will benefit from the abilities of HAC technologies to aid in energy reduction and disaster response and the ability to more directly involve people in helping with these issues. - Policy makers and the general public who will benefit from an understanding of the capabilities and issues to emerge from human-agent collectives. For example, the shift to increasingly open access to data, through initiatives such as data.gov.uk, has highlighted innovation in this space and placed issues of access, availability and control of data on the political agenda. The next decade will require a still closer partnership between researchers and law-makers as we establish the key principles of governance in this domain, and we will exploit our links with the James Martin 21st Century School to do so."
16,FF347176-56CA-4707-8E0E-58451B93834C,Biologically inspired transportation: a distributed intelligent conveyor,"A parallel manipulator is a massive array of simple individual actuators with a small power density that collectively transport and position objects with masses considerably higher than the force generated by a single actuator alone. This design is inspired by the biological phenomena of cilia, small hair-like structures on the surface of cells which can either sense local properties such as in the rod photoreceptors for vision or in olfactory neurons for smell, or can move in coordinated wave action to move liquid over their surface, as in the trachea and kidneys. Employing these capabilities in an analogous array of micro-actuators will produce a conveyor of parallel intelligent manipulation able to sense object properties, move them in different directions and effectively sort objects according to their properties. Crucially, the actuator array will be capable of communicating local information about objects to other parts of the array to enable coordinated action.Parallel intelligent manipulation plays an increasingly important role in intelligent robotics, computer science and intelligent manufacturing systems. Significant advantages of the distributed manipulating system are task flexibility (it can be dynamically reprogrammed to implement another task); massive-parallelism (it can process several different objects simultaneously, and different parts of the manipulator can perform separate tasks concurrently); fault tolerance (faults in single actuators do not restrict performance of the system as a whole, so operation of the system can be maintained); autonomy; and the ability to process objects simultaneously and independently.The overarching aim of the project is to build an intelligent autonomous massively parallel manipulator for distributed sensing, recognition, analysis, sorting, transportation and manipulation of light-weight objects. A paradigm of reaction-diffusion computing, i.e. information processing and computation by spreading wave-patterns in non-linear media, will be employed in the control system of the manipulator.Using evolutionary computation and machine learning, we will develop new principles and implementations for non-linear medium based control, and introduce a range of algorithms for distributed sensing (of object properties such as shape), filtration (sorting different objects according to common characteristics), orienting (ensure objects are facing and moving in the correct direction), positioning (moving objects into the correct path of travel on a different part of the manipulator) and shape-determined transportation of the objects.This manipulator system not only has the potential to impact upon the academic community in terms of the advancement of evolutionary algorithms, reaction-diffusion computing, and intelligent robotic systems, but also has ready application domains in industry such as high-tech manufacturing, enabling an advanced network of sensors to control dynamics of mechanical components, automation of assembly of nano-devices, and medical applications such as prostheses and computer controlled implants.",,
17,A2DB7C3A-1880-4FEC-A6EB-55E96BFCC166,LogMap: Logic-based Methods for Ontology Mapping,"In computer science, an ontology is a formal description of some aspect ofthe world in a format that a computer can process. For example, a bio-medical ontologymay contain information such as polyarticular arthritis is a kind of arthritis that affects at least five joints'', juvenile arthritis is a kind of arthritis that affects children up to the age of 13'', and polyarticular juvenile arthritis is the kind of arthritis that is both polyarticular and juvenile''.Ontologies are extensively used in biology and medicine. Aprominent example of a bio-medical ontology is SNOMED CT, which is a core component of the NHS patient recordservice. Other examples include the Foundational Model of Anatomy(FMA) and the National Cancer Institute Thesaurus (NCI).Ontologies such as SNOMED CT, FMA, and NCI are gradually superseding the existing medical classificationsand are becoming core platforms for accessing, gathering, and sharing medical knowledge and data.For example, ontologies can be used to process data (e.g., electronic patient records in the case of a medical application) in a more intelligent way: if JohnSmith's medical record states that he is a 10 years old patient suffering from arthritis, and who has damage in hisknee, ankle, wrist, elbow, and hip joints, then an ontology can be used to conclude that hesuffers from a kind of polyarticular juvenile arthritis.To exchange or migrate data between ontology-based applications,it is crucial to establish correspondences (or mappings) between their ontologies.For example, a mapping between NCI and FMA should establish that the FMA term Cardiac Muscle Tissue'' and the NCI term Myocardium'' are synonyms. Usingthis mapping, a computer program would then be able, for example, to migrate the datastatement Paul Williams has suffered from an infarction affecting the Myocardium'' from an NCI-based application to an FMA-based application.Creating such mappings manually is often unfeasible due to the size and complexity of modern ontologies.Therefore, the problem of automatically generating mappings between ontologies (often referred to as the ontology matching, ontology alignment, or ontology mappingproblem) has been investigated extensively in recent years.Despite the already mature state of the art, bio-medical ontologies still poseserious challenges to existing techniques.Our ultimate goal in this project is to meet these challenges and lay thefoundations for the development of new generation bio-medical informationsystems.Our main research hypothesis is based on the observation that existing techniques for ontology mapping oftendisregard the logic-based semantics of the input ontologies. As a result, they fail to take advantage ofthe available semantics, and of the highly effective reasoning services for modernontology languages. We are proposing to rethink the foundations underlying the current state-of-the art in the field by incorporating logical reasoning in each of the steps of the ontology mapping process. We also intend to go even further and make our techniquespractical and ready to be used in applications.The research is based on our preliminary empirical evidence which suggests the potential benefitsof logic-based reasoning when analysing existing mappings between real-world ontologies.We expect that our results will be directly relevant to the users of ontology-based systems inthe bio-medical domain, where knowledge and data integration is a matter of major concern.",,"We expect that our results will be directly relevant to the users of ontology-based systems, especially in industry and government organisations, where knowledge and data integration is a major concern. These include the NHS, Ordnance Survey, Siemens, IBM, Software AG, Alcatel-Lucent, and Oracle, all of which are already using ontologies. Our results will be especially relevant to the bio-medical community. For example, we intend to develop technologies that could revolutionise the way in which UMLS-Meta and similar data and knowledge integration resources are designed and maintained. Given the importance of UMLS-Meta in applications in bio-informatics (e.g., PubMed), we expect those who use it in their applications will directly benefit from our results. In the long term, beneficiaries could include anyone who uses or depends on an ontology-based information system, from the medical doctor who uses the NHS patient record service to the researcher who uses PubMed to access scientific knowledge. Concerning dissemination and engagement, we will disseminate the results of this research mainly through the following channels: - publications in leading conferences and journals in the fields of artificial intelligence, Semantic Web, and bio-informatics; - contacts with users from the bio-medical community through organisations such as the OBO consortium and the World Wide Web Consortium (W3C) Health Care and Life Sciences Special Interest Group; - participation in relevant coordination and standardisation efforts within groups and organisations such as the (W3C) and the OWL Experiences and Directions Group (OWLED); - distribution of software via the Web; - industry liaisons activities via the Host Institution, which includes talks, industry showcases, periodic newsletters, and an industry liaison website; and - industry liaisons via our proposed collaboration with the European Bio-informatics Institute (EBI). Concerning exploitation, the commercialisation of IP resulting from the project will be managed by Isis Innovation, a subsidiary of the University of Oxford, founded to exploit the outcomes of Oxford's research activities. The Host Institution is very proactive in communicating funding opportunities to its academics for exploiting research output via Isis and therefore extensive departmental support and assistance during a potential exploitation process is to be expected. Possible ways of commercially exploiting our expected results through Isis include: - licensing innovative software; - creating a new start-up company; and - developing new specialised application-software in areas such as bio-medicine. Finally, the PI and named RA have already participated in several research projects, many of which involved close interaction and collaboration with users and industrial partners. The PI has also extensive experience in standardisation efforts as well as in the organisation of workshops and other events which have gathered a significant number of users and practitioners from both academia and industry."
18,CAB8DECB-BF37-4F0E-AE03-2B2BCED11EC9,Word segmentation from noisy data with minimal supervision,"In recent years, the field of natural language processing (NLP) has made great advances in a wide range of areas, such as machine translation, document summarization, and topic identification. However, much of this success is due to systems that are built using large quantities of human-annotated data in a supervised machine learning approach. This means that languages with fewer annotated resources (low-density languages) are left without much useful language technology. An important direction in NLP research is therefore to improve our ability to develop successful systems using as little annotated data as possible. Research on completely unsupervised systems is particularly interesting not only for its potential to broaden the reach of NLP technology, but also because it may shed light on the ways in which human infants manage to learn language with little or no explicit instruction.We propose to focus on the particular problem of word segmentation, and to develop a new type of probabilistic model, the infinite noisy channel model, for solving this problem in settings where little or no annotated data is available. Word segmentation refers to the problem of identifying word boundaries in either text or speech. It arises in NLP systems for many Asian languages, where words are not separated by whitespace, and also for infants learning language, because most spoken words are not separated by pauses. Previous work on unsupervised word segmentation has assumed that every time a particular word occurs, it is realized in exactly the same way. However, this is not the case for infants learning language (since words are subject to phonetic variability and noise in pronunciation), nor is it always true in NLP (if the input text contains errors, such as those produced by an optical character recognition system). Our new model will address this shortcoming by simultaneously performing word segmentation and correction of noise and variability, to recover a sequence of de-noised words from the unsegmented noisy input. We plan to develop two different versions of our model. One of these will be designed to correct for phonetic variability, and will be evaluated as a cognitive model of human language acquisition. With this model, we hope to gain insight into the computational mechanisms that allow infants to successfully extract words from noisy input, and in particular to show that the Bayesian inference techniques used in our model are a plausible explanation of infants' learning behavior. The second version of our model will be designed to correct for errors resulting from optical character recognition, and will be evaluated as a word segmentation and error-correcting NLP application in several different languages. We hope to show that the model reduces the number of character errors in the document while also producing successful segmentations. We expect these improvements to be particularly pronounced in low-density language situations.",,"The research proposed here has the potential to create impact through several different routes: the cognitive modeling component addresses fundamental questions about the nature of language acquisition, the development of new machine learning methodologies will advance the state-of-the-art in unsupervised language processing in general, and the application to OCR post-processing will create improvements in a specific technology. Many of the potential impacts of these advances (especially in the area of language acquisition) have long time horizons and will best be realized by disseminating our work first to other academics, who will be able to develop it further and realize some of the benefits described below. Since our track record in dissemination to academics is detailed elsewhere, we do not discuss it further here. As for more immediate knowledge transfer to non-academic parties (as might be appropriate for some of the NLP/OCR applications described below), Edinburgh has a strong infrastructure in place. This is centered around the 8.25m ProspeKT program, a five-year program of activities in Knowledge Transfer, Entrepreneurism, and Public Outreach within the School of Informatics. ProspeKT provides a dedicated commercialization arm to proactively engage with industry, as well as activities in Informatics to encourage entrepreneurship and company formation. The ProspeKT team will be able to help in identifying and exploiting any commercial opportunities arising from our work. In addition, the University provides services to help with patenting and other protection of intellectual property. Some of the potential beneficiaries of our work include: 1. Language-impaired individuals, their families, and community. Basic research into the mechanisms of language acquisition has the long-term potential to improve diagnosis and treatment of various language disorders. 2. Speakers of low-density languages. These individuals will have better access to language technology, which will improve business and government efficiency, and quality of life for private individuals. 3. Governments and businesses with interests in low-density language areas. Examples include businesses moving into new markets in linguistically diverse areas of the world, and governments desiring better local information for diplomatic and intelligence purposes. 4. Speakers of endangered languages and their communities. Unsupervised language technology has the potential to be useful in the documentation and analysis of endangered languages by field linguists. For example, the OCR correction system described here could be particularly useful for digitizing hand-written fieldnotes that may have been collected before the advent of portable computers. This kind of technological improvement would benefit communities hoping to preserve their linguistic heritage. 5. Digital libraries and their users. Document recognition software has already changed the way in which users access information for libraries, often allowing them to search complete texts from their own homes. However, automatic recognition of older documents with historical and cultural importance is far less accurate than recognition of modern texts. Improvements in this process could significantly decrease the amount of effort required to digitize further documents, thereby increasing their accessibility to both specialized audiences (e.g., libraries and historical societies) and the general public."
19,5EF2232F-55A3-443C-95D9-5518C4968A2F,Generative Kernels and Score Spaces for Classification of Speech,"The aim of this project is to significantly improve the performance of automatic speech recognition systems across a wide-range of environments, speakers and speaking styles. The performance of state-of-the-art speech recognition systems is often acceptable under fairly controlled conditions and where the levels of background noise are low. However for many realistic situations there can be high levels of background noise, for example in-car navigation, or widely ranging channel conditions and speaking styles, such as observed on YouTube-style data. This fragility of speech recognition systems is one of the primary reasons that speech recognition systems are not more widely deployed and used. It limits the possible domains in which speech can be reliably used, and increases the cost of developing applications as systems must be tuned to limit the impact of this fragility. This includes collecting domain specific data and significant tuning of the application itself.The vast majority of research for speech recognition has concentrated on improving the performance of hidden Markov model (HMM) based systems. HMMs are an example of a generative model and are currently used in state-of-the-art speech recognition systems. A wide number of approaches have been developed to improve the performance of these systems under speaker and noise changes. Despite these approaches, systems are not sufficiently robust to allow speech recognition systems to achieve the level of impact that the naturalness of the interface should allow. This project will combine the current generative models developed in the speech community with discriminative classifiers used in both the speech and machine learning communities. An important, novel, aspect of the proposed approach is that the generative models are used to define a score-space that can be used as features by the discriminative classifiers. This approach has a number of advantages. It is possible to use current state-of-the-art adaptation and robustness approaches to compensate the acoustic models for particular speakers and noise conditions. As well as enabling any advances in these approaches to be incorporated into the scheme, it is not necessary to develop approaches that adapt the discriminative classifiers to speakers, style and noise. One of the major problems with speech recognition is that variable length data sequences must be classified. Using generative models also allows the dynamic aspects of speech data to be handled without having to alter the discriminative classifier. The final advantage is the nature of the score-space obtained from the generative model. Generative models such as HMMs have underlying conditional independence assumptions that, whilst enabling them to efficiently represent data sequences, do not accurately represent the dependencies in data sequences such as speech. The score-space associated with a generative model does not have the same conditional independence assumptions as the original generative model. This allows more accurate modelling of the dependencies in the speech data.The combination of generative and discriminative classifiers will be investigated on two very difficult forms of data that current systems perform badly on. The first task is adverse environment recognition of speech. In these situations there are very high levels of background noise which causes severe degradation in system performance. Data of interest for this task will be specified in collaboration with Toshiba Research Europe Ltd. The second task of interest is large vocabulary speech recognition of data from a wide-range of speaking styles and conditions. Google has supplied transcribed data from YouTube to allow evaluation of systems on highly diverse data. The project will yield significant performance gains over current state-of-the-art approaches for both tasks.",,"The growth of business based on speech-enabled technology has been slower than predicted. A major contributing factor to this slow growth is that speech recognition systems are still not sufficiently robust to changing background noise conditions, speaker-styles, and accents. This results in unacceptable performance for too many users. Furthermore the cost of development of these applications is large as data is typically collected for the specific target domain and the application tuned to reduce the impact of the current fragility of speech recognition systems. Any approach that yields significant improvements in robustness (to noise, speaker and domain changes) would therefore be of enormous direct benefit to the speech industry making many new applications of the technology feasible. A range of companies in the UK would benefit in this case from core speech technology providers, such as Autonomy and Toshiba Research Europe Ltd, to application providers, such as Telephonetics, Acuvoice, through to application designers, such as VoxGen and Edify. The companies, such as major airlines, banks and new media, who would like to make further use of speech recognition to reduce operating costs and enable new applications would also benefit. The outcome of this research will be shared in the first instance with providers of core speech recognition technology. The Speech Group at CUED has close collaborations with a number of UK and international speech companies including Toshiba Research Europe Ltd (TREL), Google and IBM. Data from existing collaborations with TREL and Google will be used to benchmark the technology created within this research project. This will allow the companies to easily identify technical advances over their existing technology. In addition to research publications including conference papers and technical reports, software implementations of the research outcomes will be made available. The software will be released as an extension to the existing HTK toolkit via the HTK website. This will enable broader industry to replicate the results on publicly available databases."
20,DB31D309-8EB6-4C86-86D6-226707929957,Petri nets for multiscale Systems Biology,"Computational modelling and analysis of biochemical networks contribute to a better understanding of biological systems in terms of both the ability to explain behaviours and mechanisms as well as being able to predict the behaviour of a system under different conditions. Such approaches are at the core of Systems Biology, which attempts to understand biological systems by modelling the interaction of their components, and is a crucial discipline in the life sciences. Multiscale modeling is the field of solving physical problems which have important features at multiple scales, particularly multiple spatial and(or) temporal scales. In this proposal, we will focus on spatial aspects, with an application to biological systems. 

Petri nets are a natural and established notation for describing reaction networks because they can easily represent reactions and biochemical components, have a formal semantics and are particularly attractive to biologists. The intuitive visualization that is a consequence of the graphical formalism is complemented by a rich set of sophisticated analysis techniques, which are supported by reliable tools.
A drawback of current modelling approaches, including Petri nets, are their limitation to relatively small networks. Biological systems can be represented as networks which themselves typically contain regular (network) structures, and/or repeated occurrences of network patterns. Moreover, this organisation occurs in a hierarchical manner, reflecting the physical and spatial organisation of the organism, from the intracellular to the intercellular level and beyond (tissues, organs etc.).

Although network models can be designed using standard Petri nets, so far there is no support for such structuring, it becomes impractical as the size of the networks to be modelled increases. Besides the purely technical aspect due to the impracticality of handling large flat nets, humans need some hierarchy and organisation in what they are designing in order to be able to conceptualise the modelled object. Thus, models should explicitly reflect the organisation in complex biological systems.

Hierarchical Petri nets reuse the well-established engineering principle of hierarchical decomposition to manage the design of large-scale systems. Sub-networks are hidden as building blocks within macro nodes, and the comprehension of the whole network builds upon the understanding of all building blocks and the interconnection of their interfaces

Coloured Petri nets allow the description of similar network structures in a concise and well-founded way, providing a flexible template mechanism for network designers. In coloured Petri nets, tokens can be distinguished via their colours. This allows for the discrimination of species (molecules, metabolites, proteins, secondary substances, genes, etc.). In addition, colours can be used to distinguish between sub-populations of a species in different locations (cytosol, nucleus and so on). 

Although Hierarchical and Coloured Petri nets have been extensively deployed in modelling technical systems, the use of neither approach has gained popularity so far in Systems Biology. Their combination is potentially extremely powerful, and but has not been explored systematically so far in any area of application.

Specifically, the goal of this project is to develop approaches to support the modelling of large and complex biological systems by the use of a novel integrative combination of hierarchy and colour in Petri nets, which promises to be particularly helpful in investigating spatial aspects of biochemical network behaviour, such as communication at the intra and intercellular levels. Our concrete contribution to the area will be the development of a suitable methodology to underpin the process of engineering robust and useful models for such complex biological systems.",,"This research will initially benefit biomodel engineers, and impact on the process of designing, constructing and analysing models of complex biological systems. The advantage will be both in a sound framework to support reasoning, as well as in the tools that that can be constructed on its basis. It will also benefit experimental bioscientists, impacting on the process of planning and carrying out biological experimentation, because it will provide a means to clearly describe a system that is subject to experimental investigation, as well as providing a sound basis for procedures that explore in-silico the experimental space of in-vitro/in-vivo assays. Thus overall the research will benefit systems biologists, and positively impact the sound development of systems biology, where modeling and laboratory experimentation are connected in a tight loop; it will do this by enabling fruitful interactions between modelers and experimental life-scientists.

At a wider level, the research will benefit a wide range of modelers - not just biosystem modelers, impacting on the science of modeling, because currently colour and hierarchy exist as separate concepts, which can at best be used side-by-side in existing approaches. This applies equally to the object-oriented variations of colouring. Our proposal entails a true integration of both colour and hierarchy giving an increased power in the ability to model multiscale dynamic systems in general where hierarchical structuring is important. Although our research focuses on biological systems, other applications include transport, manufacturing and health.

There will also be an impact on basic computer science, because the integration of colour and hierarchy is a non-trivial task that has yet to be achieved in a sound and robust manner, and this project will contribute a framework to underpin this.

The research will also benefit lecturers due to its impact in the area of teaching, because the outputs from the project can be used to contribute to courses in systems biology. These outputs include the standard set of biological examples, the theory of spatial information, the algorithms for automatic detection of motifs and hierarchy levels, and their automatic assignation of hierarchically structured colour in Petri net systems."
21,83B62E46-4DB5-4C17-851E-38B3B9B33BBE,Advanced Dynamic Energy Pricing and Tariffs (ADEPT),"This project addresses a crucial research question that must be answered in the near term is How complicated can, or should, a dynamic electricity tariff be? , such that it is accepted by the public and offers clear enhancements and incentives for reduction in energy demand? The 'can' and 'should' reflect the fact that any ubiquitous technical system is (primarily) designed and implemented by experts, but has to be accepted and operated by non-experts. This project looks at how the information potentially available from smart meters may be exploited to the advantage of both the distribution network operator and the customer. We are looking for the best overall outcome in terms of energy demand reduction, not the best 'engineering solution'. The driving forces towards the need for dynamic tariffs are strong: increased embedded generation, the introduction of plug-in electric vehicles, decreasing national generating capacity, further additions of medium and large scale variable generators, and the prospect of short-term load-shedding by suspending low priority consumption within commercial and domestic. This project aims to discover understanding of the whole interacting system. This project will take account of the smart metering and infrastructure options outlined in the recent Government consulation and response. Using High-Performance Computing to provide a scalable solution to large-scale data management for smart metering is especially timely as it addresses one of the main issues that was raised in the consultation. If, as a nation, we are to lower our overall energy demand, we will have to shift from fossil fuels to less carbon intensive supplies and optimise our energy consumption across all possible sources. This may mean that electricity demand may increase. At the same time, there is an imminent crisis in generating capacity (by whatever means), so we have to make significantly better use of the energy and the assets which make up the infrastructure. The meter is the interface between the consumer and the network operator, so in principle, a smart meter could manage and provide all of the information which describes the state of the network at that point at that time. Increasing data availability will bring benefits to both users and controllers - with detailed knowledge system behaviour in near-to-real-time at the lowest operational level, network operators have a better opportunity to balance the system load, and concurrently offer consumers much enhanced mechanisms for reducing their own power demand.",,
22,750AB476-668B-4720-B267-5E5C610BDC4A,An Intelligent Integrated Navigation and Autopilot System for Uninhabited Surface Vehicles,"Global positioning systems (GPSs) are very useful navigational aids for both civilian and military vehicular systems. However, owing to the weakness of the radio signals from the satellites they are susceptible to signal loss when operations are being conducted in confined areas such as rivers, mountains and canyons, and are extremely exposed to being deliberately jammed by terrorists or aggressors during times of extreme tension or conflict. Thus given the vulnerability of GPSs to signal loss, it is prudent not to be totally reliant upon them in the design of navigation subsystems for autonomous vehicles (AVs). One solution to this problem is to enhance the subsystem with an algorithm based on simultaneous localization and mapping (SLAM) techniques. SLAM being the process of simultaneously building a feature based map of the operating environment and utilizing it to estimate the location of an AV. To further improve the capability and performance envelope of an AV it is appropriate to combine a SLAM reinforced navigation subsystem with an adaptive control subsystem thereby transforming them into one fully integrated system.This research proposal aims to design and build a new advanced intelligent integrated navigation and autopilot (IINA) system with adaptive capabilities for uninhabited surface vehicles (USVs). The existing intelligent navigation (IN) subsystem will be complemented with a SLAM algorithm which will be newly designed and also capable of interfacing with other types of more traditional navigation system. The new improved IN subsystem will be benchmarked against the existing navigation subsystem and a navigation system enhanced with an inertial measurement unit supplied by Atlantic Inertial Systems (AIS) who are the industrial collaborator for this project. Whilst the intelligent integrated system will be designed and developed for a marine application, the technology evolved will be able to be transferred and used in other types of AV.A common feature with many SLAM algorithms is the reliance upon extended Kalman filters to act as the information data collection mechanism. In this research proposal an interval Kalman filter (IKF) which uses interval calculus in its design will be employed instead and enhanced using artificial intelligence techniques to construct a fuzzy IKF (FIKF). Scene information extraction for SLAM can be from visual or non-visual sources. Visual SLAM has the benefit of selecting and using robust features gained from video imagery of the local scene. A novel aspect of this work will be the design of a feature-matching algorithm (FMA) that will be capable of operating in night-time conditions. Thus, an information data collection mechanism based on a FIKF will be integrated with the FMA to form the overall SLAM enhancement algorithm.Whilst there are a number of methods for introducing adaptability into an autopilot design, in this research project the approach to be taken will be based on a combination of on-line closed loop identification and model predictive control. Thus the methodology described herein represents a new conceptual framework for the design of marine autopilots. It should also be noted that, to date, all uninhabited marine vehicle system identification trials have been performed in the open loop. Upon the successful completion of the design of the adaptive autopilot, it will be merged with the SLAM enhanced IN subsystem to form the IINA system.",,"In summary, the research and technological advancements which will be made in this programme of work will result in the following: (1) Current feature trackers perform well on man-made structures in scenes. The highly repetitive textures that are characteristic of natural scenes require a new method to be researched and developed. We propose to assess existing techniques and compare to two new techniques. The ability to establish landmarks in highly difficult natural scenes including low light conditions will open up new opportunities for SLAM operation. The new feature detector will make a significant contribution to the wider machine vision community. (2) The HD video archive will be developed as a public data set to allow other researchers access to this difficult visual domain. Co-lateral data will be made available as well. This will ensure that concurrent with the video stream GPS and gyro data are made available. It is expected this to foster further co-operative work with related machine vision researchers. (3) An attractive alternative Kalman filter design that possesses excellent qualities in terms of adaptability, accuracy, robustness, reliability and stability. (4) A ubiquitously novel SLAM enhancement algorithm which will be capable of enhancing more conventional navigation systems and allow truly autonomous navigation to take place with a night-time operational capability. (5) A new adaptive marine autopilot that will possess a wide operational envelope and be compatible with a range of navigation systems. (6) A new conceptual paradigm for marine autopilot design. (7) An innovative intelligent integrated navigation and autopilot (IINA) system which is technically and practically superior to its predecessors in terms of accuracy, capability and robustness. The control and robotic academic and industrial communities will benefit greatly from the above research outputs. The approaches taken in the design of the Kalman filter, the feature tracking algorithm and the adaptive autopilot will be capable of being converted into design tools for use in a range of different waterborne and land applications. Also both the SLAM algorithm and the IINA system may be modified for usage in land and indoor mobile robots, and underwater robotic systems. Knowledge of the various research outcomes and their potential will be promulgated in number ways including journals and conference proceedings. Of particular note is the maintenance of a dedicated website that will contain all progress reports, trials data, photographs and videos, published papers and software listings, with links provided to/from the websites of co-researchers and related programmes/research centres within the UK, Europe and the rest of the world. The UP Knowledge Exploitation team in collaboration with AIS will create an exploitation proposal which will include a business plan and potential market opportunities. Product(s) based on the above will be commercially exploited in partnership with AIS possibly as stand alone items or as enhancements to their existing product line. Thus a route to the worldwide financially lucrative military and civilian market places for navigation and control systems hardware will be established which will increase the economic competitiveness of the UK in these particular industrial sectors. Clearly the IINA system has the potential for installation in land and waterborne robotic systems where they are required to operate in dense and cluttered environments without the aid of GPS for both military and civilian applications. Additionally, the IINA system by itself could be employed in the marine sector as an automatic control system for ships and large marine craft."
23,CE978B59-1768-45A7-B751-FD8DFE93BB0A,Evolutionary Approximation Algorithms for Optimisation: Algorithm Design and Complexity Analysis,"In the last two decades, many evolutionary algorithms (EAs), including ant colony optimization, particle swarm optimization and artificial immune systems, have been proposed to tackle NP-hard combinatorial optimization problems. Many papers have been published. Some commercial successes of applying these algorithms in the real world have also been reported. However, the vast majority of such studies rely on computational experiments. Current theoretical studies of EAs are mainly restricted to genetic algorithm and evolutionary strategy, especially for non-population based EAs. Rigorous results about the computational complexity for other types of EAs, e.g. ant colony optimization, artificial immune systems and estimation of distributionalgorithms, have been few. The limited theoretical analysis in recent years has primarily concentrated on the runtime analysis of EAs in finding the exact optimal solution to an optimization problem. Since EAs are not expected to find exact optimal solution to all instances of any NP-hard problem efficiently, the fundamental research challenge here is to study what kind of approximation solutions EAs can find to NP-hard optimization problems, which is the topic of this proposal. Our focus will be on analyzing theoretically what types of problems can be solved approximately and efficiently using what kind of EAs, and why. We are particularly interested in the relationship between problem characteristics and algorithmic features (such as selection, mutation and crossover). As Papadimitriou and Steiglitz pointed out in their 1998 book on Combinatorial Optimization: Algorithms and Complexity: Developing the mathematical methodology for explaining and predicting the performance of these heuristics is one of the most important challenges facing the fields of optimisation and algorithms today. Few theoretical studies exist in anaylsing EAs as approximation algorithms. This project is highly adventurous in trying to tackle the theoretical issue by bringing traditional theoretical computer science and evolutionary computation together. It will study four types of population-based EAs, including genetic algorithms, artificial immune algorithms, ant colony optimization and estimation of distribution algorithms. They are chosen in the proposal because they are all used with success in the real world and because of the need to understand what makes them successful on some problems but not on others and whether they are really different theoretically (or what the fundamental differences are among these algorithms, if any). Two important optimization problems, i.e., scheduling and routing, will be used as case studies in the proposal. These two problems are different but strongly related. Scheduling was the first problem studied for approximation algorithms in 1966 and has wide applications in the real world. Routing is another hard problem with numerous applications in transportation, utility and communication networks, where we have some research experiences. The expected outcomes of the proposed research will deepen our understanding of why, how and when an evolutionary approximation algorithm works significantly.",,"The impact of this research includes several aspects. First, there will be major impact on the research communities in evolutionary computation, operations research and theoretical computer science because the analysis of meta-heuristics, such as evolutionary algorithms, lies in the heart of these fields. Second, this research will have major impact on the principled applications of evolutionary algorithms, including genetic algorithms, ant colony optimisation, artificial immune systems and estimation of distribution algorithms, especially on scheduling and routing problems. Although the research will not produce a recipe book for the application of evolutionary algorithms, it will generate theoretically sound principles and guidelines in the design of appropriate evolutionary algorithms for a given problem. Third, this research will have an indirect impact on industrial applications of evolutionary algorithms, especially in the domains of scheduling and routing. One of the reasons that some industrialists are relunctant to use evolutionary algorithms is the perception that these algorithms are black boxes, i.e., there is not a sound theory explaining why, how and when an evolutionary algorithm works (or does not work). Through our theoretical analysis, we can answer rigorously at least some of these why, how and when questions."
24,4B6D5F9F-AC28-40A7-8E4B-E0F23FA05C59,Evolutionary Approximation Algorithms for Optimisation: Algorithm Design and Complexity Analysis,"In the last two decades, many evolutionary algorithms (EAs), including ant colony optimization, particle swarm optimization and artificial immune systems, have been proposed to tackle NP-hard combinatorial optimization problems. Many papers have been published. Some commercial successes of applying these algorithms in the real world have also been reported. However, the vast majority of such studies rely on computational experiments. Current theoretical studies of EAs are mainly restricted to genetic algorithm and evolutionary strategy, especially for non-population based EAs. Rigorous results about the computational complexity for other types of EAs, e.g. ant colony optimization, artificial immune systems and estimation of distributionalgorithms, have been few. The limited theoretical analysis in recent years has primarily concentrated on the runtime analysis of EAs in finding the exact optimal solution to an optimization problem. Since EAs are not expected to find exact optimal solution to all instances of any NP-hard problem efficiently, the fundamental research challenge here is to study what kind of approximation solutions EAs can find to NP-hard optimization problems, which is the topic of this proposal. Our focus will be on analyzing theoretically what types of problems can be solved approximately and efficiently using what kind of EAs, and why. We are particularly interested in the relationship between problem characteristics and algorithmic features (such as selection, mutation and crossover). As Papadimitriou and Steiglitz pointed out in their 1998 book on Combinatorial Optimization: Algorithms and Complexity: Developing the mathematical methodology for explaining and predicting the performance of these heuristics is one of the most important challenges facing the fields of optimisation and algorithms today. Few theoretical studies exist in anaylsing EAs as approximation algorithms. This project is highly adventurous in trying to tackle the theoretical issue by bringing traditional theoretical computer science and evolutionary computation together. It will study four types of population-based EAs, including genetic algorithms, artificial immune algorithms, ant colony optimization and estimation of distribution algorithms. They are chosen in the proposal because they are all used with success in the real world and because of the need to understand what makes them successful on some problems but not on others and whether they are really different theoretically (or what the fundamental differences are among these algorithms, if any). Two important optimization problems, i.e., scheduling and routing, will be used as case studies in the proposal. These two problems are different but strongly related. Scheduling was the first problem studied for approximation algorithms in 1966 and has wide applications in the real world. Routing is another hard problem with numerous applications in transportation, utility and communication networks, where we have some research experiences. The expected outcomes of the proposed research will deepen our understanding of why, how and when an evolutionary approximation algorithm works significantly.",,
0,C4200591-6BFD-4468-A069-F4608463954F,Generation Challenges 2011: Towards a Surface Realisation Shared Task,"Computers can now perform some writing tasks well (e.g. transcription and spell checking), but we still lack goodcomputational solutions for writing tasks which involve the creation of new text (as opposed to the typing orchecking of existing text). Natural language generation (NLG) is the branch of computer science that aims toaddress this lack, by developing methods and tools for the computational generation of spoken and writtenlanguage. NLG technology has a vast range of potential applications, including increasing the efficiency oftext-production processes (e.g. automated letter and report writing) and making information available in verbal formthat would otherwise be inaccessible (e.g. to the blind) or more time-consuming to process (e.g. converting weatherdata to a textual summary). However, NLG is only just beginning to fulfil this potential. Among the reasons is thefact that NLG did not until recently employ comparative forms of evaluation, as are essential for effectivecomparison of alternative approaches, consolidation and collective scientific progress.The NLG field's evaluation tradition lies in user-oriented and task-based evaluation of complete systems.This tradition is very different from the comparative evaluation paradigms that are predominant in otherareas of Natural Language Processing (NLP) where shared data resources, intrinsic, automatically computedmetrics, and human ratings of quality provide time-efficient and low-cost ways of comparing new systems andtechniques against existing approaches. In contrast, in NLG, until a few years ago, there simply was no comparative evaluation of independently developed alternative approaches. Yet without comparative evaluation there can be noconsolidation or collective progress in a field of research, and individual researchers and groups are left toprogress more or less separately. The GenChal initiative has firmly established comparative evaluation in NLG andproduced data sets and software tools to support it. Past shared tasks have addressed the subfield of referencegeneration and specific applications, and the time is now right to tackle a more ambitious challenge.With the Surface Realisation Task that forms the core of the present proposal, we are aiming for something trulygroundbreaking and of great potential use in practical applications: the development of a new generation of surfacerealisers that can be directly compared and, because they work from common input, can be substituted for eachother. Ultimately, this will mean that data-to-text generation, MT, summarisation and dialogue systems (amongother fields) will directly benefit from the availability of a range of reusable realisation components which systembuilders can test to determine which is best for their purpose, something that has not been possible before.",,"Computers can now perform certain writing tasks well (e.g. transcription and spell checking), but we still lack computational solutions for writing tasks which involve the creation of new text (as opposed to the typing or checking of given text). Natural language generation (NLG) is the branch of computer science that aims to address this lack, by developing methods and tools for the computational generation of spoken and written language. NLG can increase the efficiency of text-production processes (e.g. automated letter and report writing) and make information available in verbal form that would otherwise be inaccessible (e.g. to the blind) or more time-consuming to process (e.g. converting weather data to a textual summary). NLG technology has relevance to the EPSRC's Digital Economy and Assisted Living/Lifelong Health themes, and the number of potential applications is vast. However, NLG has so far not lived up to its great potential. Among the reasons for this is that until recently, NLG lacked comparative evaluation, hence consolidation of research results, and was isolated from the rest of NLP, therefore not directly benefiting from advances there. It was shrinking as a field and lacked the kind of funding and participation that natural language analysis (NLA) fields have attracted. In order to ultimately fulfill its great potential (including commercial applications), NLG needs to achieve substantial technological progress. For this to be possible, NLG needs to (i) increase available reusable and trainable NLG tools and data resources; (ii) establish comparative evaluation as standard in order to consolidate and incrementally improve research results; (iii) to increase the field's critical mass (in terms of numbers of researchers, projects and events); and (iv) bridge to neighbouring disciplines where language is generated (to benefit from technological advances and available resources there). These very considerable impacts are our overarching goals in the Generation Challenges (GenChal) initiative. With the Surface Realisation Task that forms the core of the present proposal, we are aiming for a truly groundbreaking impact that is of great potential use in practical applications: the development of a new generation of surface realisers that can be directly compared and, because they work from common input, can be substituted for each other. Ultimately, this will mean that data-to-text generation, MT, summarisation and dialogue systems (among other fields) will directly benefit from the availability of a range of reusable realisation components which system builders can test to determine which is best for their purpose, something that has not been possible before."
1,BA8CA5D3-7C50-4A3F-B157-E22F9A3D4DFC,Bayesian Synchronous Grammar Induction,"Statistical Machine Translation (SMT) is the technology that allows computers to learn to translate between human languages (English, French, Chinese etc.) by being shown large numbers of example translations. This is the technology that drives popular online translation tools such as those provided by Google and Bing (Microsoft).The last decade of research in SMT has seen rapid progress, as small scale research systems have matured into large commercial products and popular online tools. Unfortunately the success of SMT has not been uniform; current state-of-the-art translation output varies markedly in quality depending on the languages being translated. Those language pairs that are closely related (e.g. English and French) can be translated with a high degree of precision, while for distant pairs (e.g. English and Chinese) the result is far from acceptable. This effect is clearly discernible when comparing the state-of-the-art for two well studied language pairs: Arabic-English and Chinese-English. While the quality of Arabic-English translation could be described as remarkable, translating Chinese into English often results in unreadable output. Clearly SMT has a long way to go before being usable across a large range of languages. It has been tempting to argue that SMT's current limitations can be overcome simply by increasing the amount of data on which the systems are trained. However large scale evaluation campaigns for Chinese-English translation, with ever increasing model sizes, have not yielded the hoped for gains. The failure to adequately translate between languages such as Chinese and English can be attributed to two significant shortcomings of current translation models: 1. an inability to model large changes in word order between input and output languages (referred to as reordering), 2. no reliable mechanism for directly learning phrasal (non-word based) translation units: a significant issue for non-segmenting languages (languages such as Chinese which don't use spaces to separate words) and languages with complex morphology (e.g. German). While a significant amount of research effort is currently being applied to tackling these issues, the proposed solutions are limited by focusing on more expressive models for producing translations rather than addressing the issue of how the translation units are learnt in the first place. In this research proposal I argue that both the fundamental structure and estimation methods of SMT models must change. By recasting the problem of learning translation models as synchronous grammar induction I aim to build models capable of handling complex translation phenomena, bringing us closer to the goal of readily available translation between all the worlds languages. I propose to go beyond current research on inducing statistical translation models by using non-parametric Bayesian methods to directly learn a state-of-the-art synchronous grammar translation model from parallel sentences. This research will have the following research impacts: 1. At present synchronous grammars for translation are learnt from non-hierarchical word alignment models, losing much of the benefit of the grammars ability to represent difficult translation phenomena not captured in the alignments. By simultaneously learning the alignments and the grammar in one model the full power of these hierarchical models will be unlocked. 2. This research will advance the state-of-the-art for learning complex structured models within a non-parametric formulation, an important contribution for both machine translation and many other areas of machine learning.",,"The wide availability of high quality machine translation (MT) holds extraordinary promise for reducing communication barriers in many sectors of the community. In its 2009 Strategy for American Innovation report the Executive Office of the President of the USA recognised this impact by identifying the development of such technologies as a grand challenge of the 21st century, calling for: Automatic, highly accurate and real-time translation between the major languages of the world - greatly lowering the barriers to international commerce and collaboration. The research funded by this proposal will have a significant impact on the state-of-the-art for MT by: * increasing the acceptability of translations produced from current systems by improving the quality of the translation rules which drive the underlying processing * broadening the range of languages for which high quality systems can be built by allowing accurate translation grammars to be extracted for languages with complex morphology and large differences in word order, without reliance on expensive language specific resources. Below we sketch the specific impact of the proposed research across the public and commercial sectors: Commercial: Britain's economic strength is founded on its origins as a trading nation and the development of high quality translation systems hold the potential to greatly strengthen the competitiveness of British companies doing business in a global online comercial environment. Leading international technology companies, such as Google, Microsoft, IBM etc., rapidly absorb the latest advances in MT research to improve their online translation systems, advancing a revolution in the ability of individuals and companies to communicate across language divides on the web. Currently existing commercial translation systems achieve acceptable performance for a limited number of closely related language pairs, typically European languages. The development of the algorithms described in this proposal will broaden these horizons by leading to translation systems particularly suited to translating languages of emerging economic powers such as India and China. In addition, the establishment of a world leading machine translation research group at the University of Oxford will give local companies direct access to state-of-the-art MT research. Public: The British people form a multicultural and multilingual society with a con- tinuing need for high quality translation services. Government departments are large producers of multilingual information in their role of communicating with the public. While much progress has been made in automatically translating between English and European languages, other language pairs of great significance to British society, such as Hindi and Urdu, exhibit linguistic phenomena beyond the capabilities of current systems. The proposed research will address such limitations by designing machine learning algorithms to directly model such phenomena."
2,7E63521A-C394-4EB4-AE30-AE65BD70915B,Providing Access to Life Stories for Adults with Communication and Language Impairment,"Robert is a 50 year old man who was a professional footballer into his early 20s. A stroke at 22 resulted in loss of speech and mobility. He uses a wheelchair and needs daily living assistance. Robert often tries to tell others stories about his football career as it is in telling his story that he hopes people will view him as someone other than a man in a chair . However, because of his severe speech impairment, it takes him time and effort to express even short messages; most people lose patience. Robert worries that when his older sister dies, there will be no one left who knows his life story and his social identity will be lost.Danni, a 30 year old woman with cerebral palsy, has never been able to communicate intelligibly using natural speech. As a child Danni was unable to engage in storytelling and did not develop the skills needed to share stories. Danni has participated in research projects and has demonstrated the ability to learn storytelling skills but remains unable to participate equally in interactive communication. She uses her voice output communication aid to express needs and wants, but sharing stories is frustratingly slow. She wants to explore new possibilities and make new friends, but is severely limited by her existing device. Robert and Danni are two of 365,000 people in the United Kingdom who could benefit more from augmentative and alternative communication. Although they both use voice output communication aids (VOCAs), they find great difficulty in engaging in interactive conversation. Current VOCAs are well suited to supporting the expression of needs and wants (such as I am thirsty), but more complex interactions such as conversational narrative (e.g., Did I tell you about the time I went to Spain?) and social dialogue (e.g., pub chats about football) is not well supported.This project aims to work with adults with severe speech and physical impairment, their support staff, families and friends to harness existing research technology to support them in formulating, editing and telling their own narratives. In particular, we will be using natural language generation to create stories from basic information relating to time, place, people and activity. For example, the nonspeaking person may use a photograph as a prompt to when, where, who and what are in the photo. From this information, it is possible to generate the sentences: Last week I was in London. My sister was there. We went up the London Eye. By choosing to add a positive or negative comment, the system might generate It was great. or I didn't like it. Nonspeaking adults will be involved in providing requirements for the system and will actively participate in designing the system functionality and interface. The system will allow the user to embellish narratives during interactive conversation and to maintain access to narrative over the lifespan of individuals. The end product will be a high level prototype which will be evaluated with users in different environments.",,"Traditional augmentative and alternative communication (AAC) systems do not provide support for conversational narratives despite a growing awareness in the AAC that everyone requires access to personal story telling. Several research projects have applied natural language technology to AAC, but we still do not have a comprehensive system that allows users to create and retrieve lifelong stories. There is also no research to show the effect of longer-term narrative use on self sense of identity, both personal and as part of the wider society. Our approach to working with people with severe speech and physical impairment is unique in that they are actively involved in the research and development of assistive technology. This project has been spearheaded by our expert user group who indicated that they want a means to tell their own story. This active involvement by a hitherto neglected population is exciting as we know from mainstream research that early user involvement increases the successful adoption of technology, a major issue with AAC. We will engage and communicate with the user group at Dundee University and will be working with service users in an adult residential centre in the wild . We will engage with parents, siblings, friends, carers, care managers and therapists by working with expert users in Dundee and participants at the residential centre. We also plan to engage with volunteers who work with our participants and who will want to hear their stories. We plan to create a strand within the How was School Today online community to engage interested parties throughout the project to reduce the chasm between innovation and adoption. In addition, we will run workshops through Communication Matters at the end of our project to present our results to disabled adults, their families, speech and language therapists and other professionals interested in AAC. We have a strong track record working with industry to transfer research into commercial products. Our software will be copyrighted and we will sign non-disclosure agreements with any company with whom we have confidential discussions. Our strategy will be to sell our IPR to an AAC developer who will then create a commercial version of our software. As part of our aim to reduce the time involved in a typical route to market of assistive technology we will set up a commercialisation advisory committee. We are very keen to get our technology out into the real world as we want our ideas to be used to help language-impaired individuals communicate better. We believe the most promising commercialisation path is to make our system into a module which can be incorporated into existing commercial AAC systems. This will require the cooperation of commercial AAC vendors and we are very pleased that one market leader has agreed to support this project and is very interested in incorporating our ideas into their products. We have not signed any formal exploitation agreements with them at this stage, and we will maintain links with other interested AAC companies. Publicity features in newspapers, radio and television will be used to raise the public awareness of the abilities and needs of people who use AAC; building on our success in obtaining media publicity for our previous projects. We also aim to use the STEM program to present work done in the study to schools in the North East of Scotland. Ms Prior is a registered STEM ambassador and be responsible for the dissemination of the work to the general public through this scheme. As part of the STEM program the study would be presented at Sensation Science Centre Dundee to the general public through an interactive exhibit; the work would also be presented at Sensation's Caf Science program where researchers from the local universities are invited to present their work through a more informal and relaxed presentation to adults from Dundee and surrounding areas."
3,8AB17C3E-2DCC-410D-9D05-BD9C828D49CD,"RIDERS: Research In Interactive Drama Environments, Role-Play and Story-telling","The development of interactive graphical environments, mobile platforms, augmented reality and pervasive environments offers new technology resources for the creation of interactive drama, role-play and story-telling whether for entertainment, education, therapy or art. But what is the scientific and theoretical basis for these new media? How is it possible to reconcile the free interaction users expect in such environments with the satisfying narrative structures authors would like to design up front? Researchers have been trying to answer these questions in a range of often disparate disciplines: Computer Science and AI (e.g. planning, HCI, affective computing, digital gaming); Humanities (e.g narratology, digital media studies); Arts (e.g interactive installations, interactive drama); and Psychology (e.g user engagement, auto-biographical memory). Industrially, computer games companies have tried to find pragmatic and implementable solutions too in order to attract a wider range of gamers and move to more complex emotional engagement than the tension and adrenaline of shooting baddies. This network will bring together researchers in the many different disciplines involved so as to breach the current barriers between them, reduce fragmentation and create a synergy between technology development, theoretical analysis and practitioner applications. It will reach out to the creative industries in which new experiences and artefacts are being produced on a significant economic scale and to practitioners in relevant areas such as live and board-based role-play. It thus aligns with the objectives of the EPSRC Digital Economy programme.",,"Story permeates human culture and society. Common stories about the past - history - bind nations, regions and local communities and form a collective memory. Meanwhile newspapers tell the stories of significant events and relay the gossipy stories of celebrities, while at the individual level, personal experience is often encoded as stories of self, forming an autobiographic memory fundamental to self-identity. Drama and story link the individual to wider social groups, forming a powerful mechanism for learning - role-play - and making to possible to engage the deprived and excluded (as for example, theatre in prisons). Authoring and performing can produce empowerment and enhance self-expression and confidence. Interactivity is in turn fundamental to the economic performance of the UK computer games industry. Combining story more successfully with interaction would give this important industry a significant competitive edge, laying the basis for new game genres and increasing their sales base outside of the still largely young-male demographic. Through the development of new genres of Serious Games, new forms of health therapy and health education could be developed, using the affective engagement of story to help change attitudes and behaviour. Educational role-play - used as a means of teaching and training in areas where behaviour and attitudes are more important than simple knowledge - could be extended and enriched by the addition of graphical characters and technology-supported narrative events. New forms of narratively-rich art installations would extend the repertoire of artists and the engagement of the public."
4,A7FC8AEF-6D87-4424-962A-26733228A2A2,Advancing Machine Learning Methodology for New Classes of Prediction Problems,"The last few decades have seen enormous progress in the development of machine learning and pattern recognition algorithms for data classification. This has resulted in considerable advances in a number of applied fields, with some of these algorithms forming the core of ubiquitous deployed technologies. However there exist very many important applications, for example in biomedicine, which are highly non-standard prediction problems, and there is an urgent need to develop appropriate &amp; effective classification techniques for such applications. For example, at NIPS2006 Girolami &amp; Zhong reported state of the art prediction accuracy for a protein fold classification problem which stands at a modest 62%. While this may partly be due to overlaps between classes of fold, it is also clear that some of the fundamental assumptions made by most classification algorithms are not valid in this application. In particular, most algorithms make some assumptions on the structure of the data that are not met in reality: data (both training and test) is independent and identically distributed (i.i.d) from the same distribution, labels are unbiased (i.e. the relative proportions of positive and negative examples are approximately balanced) and the presence of labeling noise both on the input data and on the labels can be largely ignored. Recent advances in Machine Learning, such as kernel based methods and the availability of efficient computational methods for Bayesian inference, hold great promise that classification problems in non-standard situations can be addressed in a principled way. The development of effective classification tools is all the more urgent given the daunting pace at which technological advances are producing novel data sets. This is particularly true in the life sciences, where advances in molecular biology and proteomics are leading to the production of vast amounts of data, necessitating the development of methods for high-throughput automated analysis. Improving classification accuracy may lead to the removal of what is currently the bottleneck in the analysis of this type of data, leading to real impact in furthering biomedical research and in the life quality of millions of people. At present most classifiers used in life sciences applications, especially those deployed as bioinformatics web services, adopt &amp; adapt traditional Machine Learning approaches, quite often in an ad hoc manner, e.g. employing Artificial Neural Networks &amp; Support Vector Machines. However, in reality many of these applications are highly non-standard classification problems in the sense that a number of the fundamental underlying assumptions of pattern classification and decision theory (e.g. identical sampling distributions for 'training' and 'test' data, perfect noiseless labeling in the discrete case, object representations which can be embedded in a common feature space) are violated and this has a direct and potentially highly negative impact on achievable performance. To make much needed &amp; significant progress on a wide range of important applications there is an urgent requirement to systematically address the associated methodological issues within a common framework and this is what motivates the current proposal.",,
5,94A5FF07-14A3-411B-AFB8-7DA20C21D547,Autonomic Software Engineering for online cultural experiences,"This project is about exploiting the predominance of social networking using autonomic software agents to enrich, encourage and enliven engagement with online cultural artefacts such as from a museum or a gallery. With the current problems in the European financial debt, many cultural institutions are planning to shorten the length that visitors can physically enter. In the UK for example we have heard of plans that the British Museum will close earlier and possibly shut down completely for one day a week because of the massive cuts in funding that were presented in the UK Chancellors speech detailing reduction in money for the cultural sector. The basic idea is to have your friends, museums, art galleries and theatres all in your pocket through your handheld device. 
In this project we will harness the power of autonomic agents that work on behalf of human users in an infrastructure that allows for these agents to communicate and negotiate on behalf of their human users to facilitate a collective and social experience of online cultural visits. For example, we could imagine a scenario where 4 students are visiting an art museum with the desire to purchase something (a print or a physical copy of an artefact for example) for a friend. They would wish to be able negotiate about what to see or experience online, what additional information they want to consider, what comments from what previous visitors over any commentary they individually or collectively want to leave for others and, eventually, over what they collectively choose to purchase for their friend. 
We are concerned with the fundamental question of building autonomic agents that can represent their users needs, argue and negotiate on behalf of their users with other software and human agents, maintain models of the other autonomic agents in the system and proactively develop plans and scenarios for their human counterparts. In order for autonomic agents to interact in open systems such as those we are describing we will use the BDI agent architecture (arguably the most important symbolic agent architecture of the last 20 years) on a well-developed infrastructure (called electronic institutions) that facilitates autonomic agent interaction. 
In short we believe BDI architectures represent the stronger and best-developed software engineering device for building autonomic agents, that electronic institutions is the best developed infrastructure for supporting the interaction of autonomous interaction, and that the idea of enabling richer social exploration of cultural artefacts online is a timely and critical case study to address.",,"We aim at having impact in the ICT community by overcoming the scalability problems that electronic institution infrastructures currently existing due to their centralised architecture.
The new p2p distributed architecture will allow for the creation of autonomic communities of agents and components that will enact institutions in a distributed way and that will be resilient to the failure of one of the agents in the institution. It will provide capabilities to self- modify and adapt to changing environments. Moreover, P2P open source software is a democratic weapon as there are almost no barriers to access the technology. To go further in this direction we'll developed a p2p node architecture that can run on mobile devices like iphones and iPads.
Also, the application of a BDI approach to a realistic scenario may have a significant impact on the acceptance of cognitive oriented approaches to the development of autonomic software. On the other hand, it will pioneer a different type of consumption of culture facilitating its social aspects. This is particularly important when cultural content providers (e.g. museums) are suffering severe cuts in their budgets and urgently require new forms of offering their services but at a lower cost. The success of social software solutions may play in our favour by making easier the acceptance of a social solution for cultural consumption. And indirectly, our solution may illustrate the good and positive aspects of social relationships over Internet that are somehow criticised by certain groups of opinion. ACE can provide the infrastructure to make all this possible.

Dissemination. 

The results of the project will be disseminated by presentations, publications, participation in workshops, and a Web page. We will target the following audience: scientists, Industry and the general public. Moreover, the project will seek for potential venues of exploitation of the results. 

Presentations. 

The ACE researchers will participate in international conferences and workshops in the field of multiagent systems, Web services, mobile computing and artificial intelligence. An important aspect of our presentations will be to meet and to discuss the results with other researchers from the same or related fields. While most presentations will be held in research oriented workshops and conferences, we will also present our results on more application-oriented workshops and conferences attended by industry. 

Publications. 

Conference papers and journal articles will be generated during the entire project, according to the progress of work. The members of the consortium have regularly published, and intend to continue doing so, in the best scientific journals and the proceedings of major international conferences in the area. We also anticipate the possibility of submitting papers that result from the execution of the case study to events that are more oriented to the application area studied.

Presentation on the Web. 

General information as well as all non-confidential results, i.e., papers, presentations, course material, and software, will be published on a Web server to allow easy access for researchers, interested companies, institutions, and the general public. The homepage will be set up within one month after the project started and will be continuously updated. 

Exploitation

Will be carried out involving on the one hand teaching and research activities within the academic community offering new research opportunities for Ph.D. students. On the other hand the case study will be a first step to foster technology transfer (research results, as well as expertise and know-how) from academy to the industrial community. The consortium expects to exploit the project results by helping create a community and by running an open-source software development project."
6,574C6E7B-BEB5-4EBE-A4AD-E74DCE0B3910,An integrated model of syntactic and semantic prediction in human language processing,"When humans process language, they do so incrementally: they compute the meaning of a sentence on a word-by-word basis, rather than waiting until they reach the end of the sentence. As a consequence, readers and listeners have to constantly update their interpretations as new input becomes available. Experimental evidence shows that they also make predictions about upcoming input: for example, when hearing a verbs such as eat , the listener predicts that an object such as soup is likely to follow. The prediction process has two components: syntactic prediction, i.e., the structure of the upcoming input is anticipated (after eat , an object is likely, but a subject isn't), and semantic prediction, i.e., the meaning of the upcoming input is anticipated (after eat , a noun referring to edible things is likely, but one referring to abstract things isn't).Previous research has developed computational models of either syntactic or semantic prediction in human sentence processing. But there are currently no models that capture both processes in a single framework, despite clear experimental evidence that humans rely on both types of information when generating predictions. The aim of this project is to develop a model of human sentence processing that integrates syntactic and semantic prediction; such a model will not only make it possible to investigate an important theoretical question in psycholinguistics, but it also has important potential applications in natural language processing.Our model will bring together two key approaches in sentence processing. On the syntactic side, we will develop an incremental, probabilistic parser that generates syntactic predictions. This parser will be based on an extension of the Tree-adjoining Grammar (TAG) formalism, which in previous work has been shown to capture prediction data. The parser will be combined with a distributional model of semantics, which is the standard way of modeling word meaning in cognitive science; we will extend this model to capture sentential meaning, thus making it amenable to integration with a parser. Three distinct ways of achieving such an integration will be pursued, each corresponding to a theoretical position in psycholinguistics: the autonomous processing view, which holds that syntax and semantics operate independently, the syntax-first view, which holds that semantic processing has access to syntax, but not vice versa, and the interactive processing view, according to which the two components freely exchange information.By implementing these three approaches, and evaluating the resulting predictions against data from eye-tracking and priming experiments, we will be able to shed light on a key question in psycholinguistics, viz., how syntactic and semantic processing interact.Apart from this theoretical contribution, the project also has a practical aim: a computational model of human sentence processing can be used to determine which parts of a text are hard to understand. This information can be used to provide feedback to human writers, score essays, or correct the output of automatic language generation systems. In order to assess the potential for such applications, we will focus on one particular problem, viz., text simplification. We will develop a system that takes input text and makes it easier to read, e.g., for language-impaired readers or for language learners. Our integrated model of syntax and semantics will be used to pinpoint the difficult parts of a text, which will then be replaced by simplified passages using a technique called integer linear programming, which has previously been used successfully for text rewriting. The resulting simplified texts will be evaluated for their intelligibility in studies with human readers.",,"Benefits for users of simplified texts The proposed project will further our understanding of human language processing, through the development of a model that predicts word-by-word processing effort for text. The project will also develop a system for text simplification that utilizes this model of processing effort. The aim of this system is to detect passages in a text that are hard to process and automatically replace them with simplified text. The beneficiaries of high-quality text simplification are people with language impairments like aphasia, who often encounter problems in understanding written text, second language learners (by aiding the construction of texts that are of the desired linguistic complexity), and users that face other cognitive demands at the same time (e.g., while driving). The quality of life of these users would be enhanced by the proposed text simplification system. This impact could be realized as soon as a commercial version of the text simplification system is available. Benefits for language-impaired individuals An accurate model of human processing effort has the potential to improve the diagnosis of language impairments (e.g., aphasia), as it makes it possible to compare model simulations with the actual processing behavior (e.g., reading times) of impaired individuals. It is also conceivable that the model is useful for the diagnosis of learning disabilities (e.g., autism) that affect linguistic abilities. The primary beneficiaries of such an application would be the language-impaired individuals themselves, as well as health care professionals involved in the diagnosis and treatment of such individuals. Ultimately, this would result in a positive impact on the quality of life and health in the UK, as well as contributing to the effectiveness of public services. The time scale for this impact to be realized is 5-10 years. Benefits for the language technology industry The size of the language technology industry in the EU was 8.4 billion euros in 2008, with a projected growth rate of 10% annually [1]. The text simplification system to be developed by this project can be commercialized and will therefore benefit this industry, contributing to global economic performance, and specifically the economic competitiveness of the UK. Commercialization can begin as soon as the project is finished. Apart from text simplification, the model of human processing difficulty envisaged in this project will be useful for a range of other language technology applications. This includes tutoring systems that provide feedback to human writers, pinpointing passages that are difficult to read and need editing. Another possible application is essay scoring, which is already widely automated for proficiency tests such as TOEFL. In a machine translation system, a model of processing difficulty could be used to identify badly translated passages, thus improving translation system output (or automating system evaluation). For these benefits to the language technology industry to be realized, significant further development will be required, with a realistic time scale of 5-10 years. [1] http://ec.europa.eu/dgs/translation/publications/studies/size_of_language_industry_en.pdf"
7,2D1EEB67-B7AE-4180-B23B-9E4F70A3253F,Foundational Structures for Compositional Meaning,"Words are the building blocks of sentences, yet meaning of a sentence goes well beyond meanings of the words therein. Indeed, while we do have dictionaries for words, we don't seem to need them to infer the meaning of a sentence from meanings of its constituents. Discovering the process of meaning assignment in natural languages is one of the most foundational issues in linguistics and computer science, whose findings will increase our understanding of cognition and intelligence and may assist in applications to automating language-related tasks, such as document search as done by Google.

To date, the compositional logical and the distributional probabilistic models have provided two complementary partial solutions to the problem of meaning assigning in natural languages. The logical approach is based on classic ideas from mathematical logic, mainly Frege's principle that meaning of a sentence can be derived from the relations of the words in it. The distributional model is more recent, it can be related to Wittgenstein's philosophy of `meaning as use', whereby meanings of words can be determined from their context. The logical models have been the champions on the theory side, whereas in practice their probabilistic rivals have provided the best predictions. This two-sortedness of defining properties of meaning: `logical form' versus `contextual use', has left the question of `what is the foundational structure of meaning?' even more open a question than before. This project has ambitious and far-reaching goals; it aims to bring together these two complementary concepts to tackle the question. And it aims to do so by bridging the fields of linguistics, computer science, logic, probability theory, category theory, and even physics. Its scope is foundational, multi and inter disciplinary, with an eye towards applications. 

Meaning assignment is a dynamic interactive process involving grammar and logic as well as meanings of words. Both of the two existing approaches to language miss a crucial aspect of this process: the logical model ignores meanings of words, the distributional model ignores the grammar and logic. We aim to model the entire dynamic process alongside the following three strands of integration, foundations, and applications. 

(I) In integration we develop a process of meaning assignment that acts with the compositional forms of the logical model on the contextual word-meaning entities of the distributional model. 

(II) In foundations, we go beyond classical logical principles of compositionality and context-based models of meaning to develop more fundamental processes of meaning assignments based on novel information-flow techniques, mainly from physics, but also from other linguistic approaches and other models of word meaning, such as ontological domains and conceptual spaces. 

(III) In applications, we evaluate our theories against naturally occurring data and apply the results to practical issues based on meaning inference and similarity, e.g. in search. To be able to work with logical connectives in Google, one needs to re-enter them by hand in the `advanced search' tab, by manually decomposing the logical structure of the sentence and moreover providing the extra context for their different meanings. This is fundamentally non-compositional and goes against the spirit of automated search. It is exactly here that the lack of compositional methods in meaning assignment causes practical problems and where our compositional methods become of use. Hence, we aim to put forward our results to tackle such problems, e.g. to be able to use our sentence similarity models for paraphrasing, question-answering, and retrieving documents that have the same meaning and/or are about the same subject. Our proposed partnership with Google, ensures access to real life data and helps implementation and applicability of our methods in small and large scales.",,"On the knowledge side, the proposed research will cause significant scientific advances across different disciplines of logic, linguistics, mathematics, physics, and computer science. This is by modeling the process of cognition and natural language generation and developing new mathematics, logic, and high level diagrammatic tools.

The project has 3 partners, from Computer Science in Cambridge, Cognitive Sciences and Artificial Intelligence in Utrecht, and industry in Google. These extend the geographical boundaries of the impacts of the project from UK to Europe and the US, but also from academia to industry. I also have on-going collaborations with experts from these various disciplines in venues including UK, Italy, France, USA, and Canada. 

On the economy and social side, internet with its huge pool of services and data has become an inseparable part of our daily lives. The theoretical results of this project will be put forward to improve the quality of services on the internet. At the moment documents are identifies as bags of their words. If the relationships between the words is also taken into account, language processing tasks will hugely benefit, for instance tasks such as information retrieval from text and document search. As a result, new techniques for applications such as question answering and textual entailment will be developed, better answers to online questions will be provided, and more comprehensible summaries of news and articles will be constructed automatically. The partnership with Google and Cambridge is exactly towards following and realizing this pathway. 

From the other side of the spectrum, the results will help us understand the nature of intelligence and language understanding. This has conceptual importance of its own, will improve the quality of human life in due time, by facilitating mutual understanding in society and across societies of different languages. 

Finally, the proposed theoretical setting is based on using simple diagrammatic techniques to depict mathematical structure and logic. The simplicity and accessibility of these methods provides the public with a chance to understand advanced academic developments, a chance which will have an impact on educating the society. We have had open sessions to introduce Computer Science research to high school students and especially to girls in Oxford. The diagrammatic methods and their application areas caused much discussion with and within the students. 

On the academic side, apart from publishing articles and attending already-established workshops and conferences, I have asked for funding to organize two workshops. This is to fill the interdisciplinary gap and bring together researchers from the different disciplines of the project, so that we can discuss and disseminate ideas and results and help start a multi-subject community across these different fields. I will also organize the interdisciplinary seminar series of the logic group of computing lab at oxford. These are open to all academic fields and also the public. Other impacts are through training and teaching. I have asked funding for a doctorate student and plan to continue lecturing my field of expertise based on the project. I have already been invited to give advanced lecture series about the subject in Utrecht and in a Masters course in Cognitive Science in University of Latvia."
8,F94D0050-E08F-44AC-BC13-3015DFD5FE16,Computational Creativity Theory,"Computational Creativity is the study of how to build software which takes on some of the creative responsibility in arts and science projects. We are at a stage where software can generate pictures, melodies, jokes and poems, can invent new words and discover new and interesting mathematical theorems, and regularly helps scientists to make important discoveries. This kind software can be used autonomously, or in collaboration with creative people. It is also used in cognitive modelling projects, to shed light on aspects of human and animal creativity. In the last decade, Computational Creativity has come of age, as evidenced by special issues of publications such as the Minds and Machines journal and the AI magazine, and the first International Joint Conference on Computational Creativity, which replaced 10 years of successful workshops at major AI conferences. 

The proposed Leadership Fellow, Simon Colton, is a recognised expert in Computational Creativity, and has been working in the field since 1996. He is unique in having been involved in successful applications of creative software to four different domains, namely mathematical invention, video game design, graphic design and the visual arts. His mathematical theory formation software, HR, has produced theorems and concepts published in the mathematical literature; his visual art software, The Painting Fool, has produced pictures that have been exhibited and attracted much public attention; and research being done in the Computational Creativity group that he leads at Imperial College is helping video games companies to design the next generation of adaptive, personalised games.

A number of authors, such as Boden, Wiggins and Ritchie, have introduced formalisms which help us to be more precise about the creativity of software. However, there is no agreed upon theory which can describe the behaviour of software with sufficient acuity, coverage and formality that enables accurate comparison of implementations. In short, we have no generic way of saying that software B is more creative than software A. This has held back our field, because with no concrete and formal measures of the creativity of the software we build, it has been hard to put forward falsifiable scientific hypotheses that one approach is more creative than another, hence it has been difficult to progress, and to show progress. 

With this Fellowship, we propose to change this situation, by developing Computational Creativity Theory (CCT). This will comprise a series of models, each of which contains some conceptual definitions and some calculations involving those definitions which can be used to compare and contrast the creativity of software. The foundational models will make more precise the notion of a creative act and the impact they can have, and the more acute models will cover aspects of creative behaviour including intentionality, interpretation, imagination, appreciation and affect. To model computer creativity sufficiently well, we generalise past the merely generative and past usual AI notions of value, into new areas where software is expected to invent its own aesthetic and utilitarian measures, and frame its creations by describing its motivations, intentions, methods and innovations and by putting its work into historical and cultural contexts.

The proposed programme of research has the development of CCT at its heart. This is informed by a series of practical projects involving applications to creative language, music, visual arts, mathematics and games, and covering modes of creativity including realtime generation, assistive technologies and creative collaborations. By building and disseminating CCT, we will help to bring Computational Creativity research into a new era, where formal notions of creativity underpin software systems which really enrich our cultural lives.",,"Creativity is a hugely emotive, loaded and often confusing and contradictory term. While the creative industries of a country are as important to its economy as its manufacturing output, we are only beginning to understand the value of innovative practice in the workplace. Scientists, educationalists, engineers, politicians, leaders of industry, philosophers and artists all study creativity for different reasons, whether it is as an intellectual challenge, to increase productivity or to drive through policy changes. The popular conception of human creativity is confused and often steeped in mythology and romantic notions based on ill-defined terms such as imagination and inspiration. Given all these factors, our study of the controversial notion that software can be creative has the potential to seriously impact business, education and culture. In particular, our proposal to impose a concrete formalism able to describe creative behaviour in such a way that numerical comparisons can be used to measure creativity, is likely to be disruptive, and to have a wide impact.

In September 2010, the British Council in Madrid organised a public event to mark its 70th anniversary. The proposed Fellow, Simon Colton, was asked to speak at the event, alongside Ramon Lopez de Mantaras, a high profile Spanish AI researcher with similar interests in creativity studies. The public interest in the notion of software being creative is sufficiently high that Spain's two most popular newspapers, El Pais and El Mundo both covered the event, and ran articles in their weekend editions which covered Colton's work on The Painting Fool project (which aims to build a software system which is one day taken seriously as a creative artist in its own right). The combined print circulation of the newspapers is around 700,000 and they are the most popular Spanish language newspapers on the internet. Hence we hope that the issues of Computational Creativity were firmly planted in the minds of tens or hundreds of thousands of people. Again, due to our covering notions of creativity, our work has similarly been discussed in the New Scientist, More4 TV news and the Metro Newspaper. We will continue to react to press enquiries by being as forthcoming as possible about our work and the creativity issues it raises. Moreover, we will pro-actively write articles for the popular and specialist press, and regularly print and disseminate summary information about our work to governmental, educational and industrial organisations, in addition to releasing iPhone/Android/iPad applications for general consumption.

Software which can act in creative ways is clearly of value to the creative industries and the wider Digital Economy. We have already collaborated on funded projects with the Emote, Introversion, Lionhead, NestorGames and Rebellion games companies, in addition to Universal Music and Sony. Two of the project partners are from industry, and we will work closely with them to embed our research ideas in their working practice. As an example of the kind of industrial impact we expect from the Fellowship, in summer 2010, the proposed Fellow was asked to sit on a steering group looking at the future of digital tools for the creative industries, organised by the Creative Industries Knowledge Transfer Network (CIKTN). This led to the publication of a beacon project report, which was widely circulated to creative industry firms, educational establishments and governmental bodies. On the inside cover was a quote from the proposed Fellow: &quot;We are approaching an exciting time when computers will be powerful enough, and software sophisticated enough, for computers to be our creative partners&quot;. Again, we hope that this led to Computational Creativity issues being raised in creative industry firms and wider organisations, and working with the CIKTN to meet new industry partners and disseminate our work widely form important pathways for impact that we have planned."
9,9AA20574-DD8C-4CE0-81F7-0C4E1B78FBEE,"Fast, Locally Adaptive Inference for Machine Learning in Graphical Models","Graphical models are a powerful tool in machine learning with successful applications in diverse areas such as medical diagnosis, natural language processing, robotics, speech recognition and analysis of genetic data. Despite this success, modern data sets place new demands on the graphical modelling framework, because the models can be enormous, but exact inference in graphical models is intractable. Despite the extensive literature on approximate inference, there is still a huge gap between the largest data sets that we wish to analyse and the largest graphical models that we can handle.

In order to meet the challenges of these new applications, this project concerns new approximate inference algorithms for the large-scale graphical models that arise in practical applications of machine learning. Very few existing inference algorithms can handle extremely large models with continuous variables, and important classes of inference algorithms, such as Monte Carlo techniques, have not been scaled to such models at all. Computationally efficient inference would significantly expand the range of applications to which the graphical modelling framework can be applied.",,"The work in this proposal has the potential for substantial economic benefits, because better inference algorithms will enable the graphical modelling framework to be applied with less effort to a broader variety of problems. Graphical modelling provides a powerful, principled reasoning framework that has proven successful for problems in which there is noisy data that only indirectly indicates the true variables of interest. Clearly, this describes a broad variety of applications, both in academia and industry, and for this reason the range of successful applications of graphical modelling has been impressive---including mobile robotics, computer vision, social network analysis, managing online advertisements, and automatically extracting information from product reviews. Despite this success, we believe that graphical modelling has even broader potential for explosive success throughout industry, but this potential cannot be realised without fundamental research into scalable inference algorithms.

The problem is that graphical models are more difficult to apply than they should be. An external user canot really be expected to apply general graphical models without either collaborating with an expert in graphical models or becoming an expert themselves. Although the graphical modelling approach has high practical usefulness, unfortunately the expertise needed to apply it is also high. New, scalable inference inference algorithms have the potential to significant reduce the burden of applying the graphical modelling approach, enabling new industrial applications."
10,A3BECA92-EB74-4CB9-ABA9-E54F12199780,Maximising Efficiency of Resource Usage Under Uncertainty in AI Planning,"In recent years planning technology has a enjoyed significant increase in real-world application, with industry and general science research benefiting from the great depth theoretical work done in this area over many decades. The core problem of deciding which activities to carry out and when occurs in a vast range of domains; the research area of planning is concerned with developing generic problem solving technology that automates the task of performing this core reasoning. Planning technology has been employed in a wide range of domains including controlling printing presses, power management for the UK National, train scheduling on the Hungarian Railway Network, scheduling aircraft landing in airports, and autonomous robotic control, both in space and in the oceans. Experience in these areas has given rise to two key observations. First, the existing theoretical work done in AI Planning has been extremely valuable, allowing planning technology to begin to solve real world problems. Planning is a fundamental component of intelligent autonomous behaviour and as such planning technology has real potential for application in many different areas, both now and in the future. The second is that whilst one can observe that planners can now begin to be applied to these problems, there is still a great need for improvement of the underlying technology, in terms of expressivity and performance, in order to be able to create greater autonomy by allowing reasoning about an uncertain world.At the heart of this lies deeply theoretical computer science research: a planner is a generic problem solving system, consisting of search algorithms and heuristics. Of particular interest is reasoning about time and resources, something key to many areas of computer science, from compilers and programming languages to web services and optimisation. In order to tackle application problems well, reasoning effectively about these is essential. Of specific interest here is uncertainty in time taken and resources consumed. This occurs in many application domains, and in each of these a similar approach is taken: conservatism about time and resource availability in order to guarantee success. This, however, comes at a cost. By way of example, when planning for autonomous Martian exploration, the models used by both the ESA and NASA are pessimistic, underestimating the amount of power the rover will receive from the sun, and overestimating the amount of energy and time each activity will take. The result is that the equipment is highly under-utilised, with fewer science targets being achieved than could have been with better on-board reasoning. Given the expense of placing rovers on Mars and the limited equipment lifespan, this is a great cost to mankind's exploration of space. A similar problem occurs when deploying renewable energy generation: wind farms are assumed to provide 10% of their maximum output, even thought the reality is almost always greater than this. This conservative assumption, there to ensure power is always provided, causes great environmental and economic cost, as extra production capacity must be available through other sources regardless of whether it is required.The major benefit of planning is in generating a generic problem solving technology. Developing several bespoke solvers would take many years, and incur great financial cost. By developing efficient planning systems, a single domain-independent problem-solving core is built, capable of solving many problems without the cost of developing a bespoke solver for each. The core of this research is addressing challenges in solving the general planning problem that will allow future application of planning, extending the range of problems to which this generic technology can be applied.",,"Here we consider the wider impact of this research on society and industry; this is subject to varying time-scales, with some areas expected to benefit within the lifespan of the project, and other benefits to become apparent in the longer term. Planning is generic problem solving technology, with the potential for impact on many different application areas. The UK has established itself as a world leader in planning technology on the international scene, and we are already exporting this technology to the US and Europe. This project will help the UK to maintain its leading role in applications of planning, by developing the technology to allow further pioneering deployment in a range of different areas. The fellowship provides the opportunity for a young researcher, who has shown great promise, to stay in the UK to pursue this world leading research, thus preventing brain drain and maintaining expertise within the UK. Supporting a young researcher will clearly have a great impact on career development, nurturing an independent research career. Environmental protection is a leading priority across the world at the current time, with governments, including that of the UK, aiming to meet emissions targets. This is the key reason I have chosen to focus on wind farm planning, an application in the area of smart grids. When planning power generation in a setting with renewable and non-renewable energy sources, many sources of power are available, from wind-farms, to hydroelectricity, to nuclear power, to coal/gas power stations. The challenge is to automatically adapt to meet customer demand under uncertain weather conditions whilst making the best use of renewable, low emission power sources. Research in this area can directly reduce the UK's use of non-renewable and environmentally damaging generation sources, helping the government to meet targets, and improve global environmental protection. A further potential application is in micro-generation --- effectively, small-scale power grids amongst networks of homes --- and smart electricity meters, running high energy appliances during times of surplus production. The work done in the project will be directly applicable to these problems. These applications in power engineering have clear potential to directly benefit UK industry. The power supply industry is one of the largest in the UK, with the relevant companies, e.g. National Grid PLC, standing to benefit, both by achieving environmental targets, and through cost reduction by efficient use of resources. This in turn benefits the UK, establishing a cheaper energy supply, helping all UK businesses, as well as establishing the UK as a technological leader in energy production and distribution. Optimisation planning under uncertainty is core to many businesses (for example logistics firms, oil supply companies, airports): making the best use of finite resources. In the longer term, small businesses will be able to benefit from planning research: these companies also need to optimise resource usage, but writing a bespoke problem solving system is expensive; using an off-the-shelf problem-independent technology allows this to be done at at much lower cost. Research being done into intuitive modelling technologies for planning, and development of planning technology, will make this possible in the future. Autonomy is important to allow tasks to be carried out that either pose too great a risk to human life, or simply that the costs of the manpower required to perform such tasks would be prohibitive. Examples of such tasks are nuclear decommissioning, search and rescue following disasters, space and oceanographic exploration and autonomous assistance to help our ageing population to remain independent for longer. These are long-term beneficiaries that will be reached through the continued efforts of the planning community, as well as improving technology."
11,FB7AEE2C-F765-4E3F-B8D7-32E4CB8F5029,Automated Modelling and Reformulation in Planning,"Although AI Planning and Constraint Programming share many techniques and approaches, an important difference lies in the approach to modelling. In CP and also in Operations Research, modellers spend considerable time and effort evaluating alternative models and selecting representations of a problem that will make it most amenable to solution by existing technology. In Planning, researchers typically spend little time considering alternative models and are content to work with the first model they construct, working instead on improving the planning technology to try to tackle the problem, whatever its form. The reason for the strategy of planning researchers is that the intention is to avoid the need for expert planning knowledge in order to exploit a planner. However, the price for this strategy is that there is very little accumulated research expertise in the problem of modelling and no systematic comparison of the performance of planners using alternative models of the same problem. Although avoiding the need for expert planning knowledge in order to use a planner is an important goal, there is clearly a lost opportunity to identify ways in which models might be structured to be most amenable to solution. We propose to combine these strategies by exploring the automatic reformulation of planning problems in order to better exploit the existing planning technology by restructuring models to expose the information that can make a planner make more intelligent choices.",,
12,0E345206-9C5B-4F26-A8CB-EAD1E94BF4AE,Robust Incremental Semantic Resources for Dialogue,"When humans process language, they do so incrementally, understanding and producing sentences on a word-by-word basis. In conversation, we easily switch roles between speaker and hearer mid-sentence, taking turns speaking and listening to show attention, clarify information or add detail when needed, interactively contributing to a shared, emerging picture of what we mean. If we want human-computer dialogue systems to be natural, efficient and easy to use, they must behave as incrementally as humans do: understanding and reacting interactively on a word-by-word basis rather than insisting on fully-formed sentences. We would prefer a system which behaves as in (1) below to the more familiar but annoying (2), or even the more patient but less interactive (3):

(1)
Usr: I'd like er [pause] . . .
Sys: Yes?
Usr: a ticket to Paris from, hang on . . . 
Sys: Paris, France? 
Usr: right, from London please. 
Sys: OK, checking for Paris to London.

(2)
Usr: I'd like er [pause] . . .
Sys: I'm sorry, I don't understand. Please state your destination.

(3)
Usr: I'd like er [pause] . . .
Usr: a ticket to Paris from, hang on . . . 
Usr: from London please. 
Sys: OK. Do you mean Paris, France?

Previous research has developed computational models of dialogue which can behave incrementally, allowing the kind of interaction shown in (1); but they currently rely on hand-written rules or statistical models to relate words to actions and concepts. These lack the ability to express the complex meanings that human language is so good at conveying, and are time-consuming to create for any new system, domain or task. Instead, they need incremental models which deal with semantics, updating some representation of meaning as each word is heard or spoken, and which can be automatically learned from data; but general methods for doing this are currently lacking. This project will bridge this gap, providing a linguistically-based, learnable framework for incremental semantic interpretation and generation, which can be used to improve and extend existing dialogue systems.

The project will start from recent work in theoretical linguistics and dialogue modelling which has produced the incremental semantic processing framework Dynamic Syntax (Kempson et al., 2001). This shows promise in modelling complex incremental dialogue, but is currently under-developed from a practical point of view, needing time-consuming expert hand-crafting, and missing a link between action planning and language generation. This project will address these issues. First, we will develop methods for automatically learning Dynamic Syntax grammars from data, allowing other researchers to easily produce and use their own versions in their own systems. Second, we will develop its methods for generating language so that it can be integrated with the way dialogue systems plan their actions on the fly. These new capabilities will be implemented computationally and evaluated on real data. Together, they will then be used to build a demonstration dialogue system which can behave incrementally, and will be packaged into a publicly available toolkit for researchers to develop their own incremental, semantic dialogue systems.",,"Benefits for users of human-computer dialogue systems:

Language-based or speech-enabled human-computer interfaces are common, and are becoming more common as mobile phone use and technology improves, and as computer games become more sophisticated. Users of these interfaces now include general internet users (via language-based interfaces to remote databases and networked applications); mobile phone/device users (via speech-based interfaces to local and remote devices); computer games players (via dialogue-based interaction with virtual characters); visually impaired individuals (via screen-free dialogue interaction). The quality of life of these users would be enhanced by the increased naturalness and interactivity of the incremental dialogue systems that will be enabled by this research. Realising this impact will involve transfer of the tools and toolkit resulting from the project to other academic and commercial research groups, with subsequent commercial development requiring timescales of 2-5 years - see below.

Benefits for the language technology industry:

Dialogue systems are used by many commercial applications (telephone call-handling, customer response, ticket-booking etc.), and the size of the language technology industry in the EU was 8.4 billion euros in 2008, with a projected growth rate of 10% annually [1]. The resources and toolkit to be developed by this project can lead to commercialisable dialogue system applications, and will therefore benefit this industry in future, contributing to global economic performance, and specifically the economic competitiveness of the UK. Commercialisation will require significant further development, but the compatibility of the resources with existing approaches to dialogue system implementation mean timescales could be short (e.g. 2-5 years, as witnessed by similar developments of e.g the CHAT or LetsGo! systems).

Benefits for school and university students:

The tools and toolkit produced will provide resources for use in university coursework within QMUL and beyond (resources will be released publicly under an open-source license), and for use in school teaching on relevant technology and language-related courses. We will facilitate realisation of this impact by publishing a schools-targetted article in the cs4fn magazine and website, by presenting at events such as the Big Bang and National Science and Engineering Week, and by developing a web-based demonstration of the technology. 

Benefits for academic researchers:

Many dialogue researchers are currently engaged in research into incrementality in dialogue modelling and dialogue system implementation, but this is a young and emerging sub-field. The project results, and resulting toolkit and resources, will directly provide resources for continued development of such systems, providing general incremental semantic processing capabilities for interpretation and generation and easy deployment via learning from data and integration with general agent planning methods. Through continued research into incremental systems, we expect commercialisation options to begin over the next few years, as was the case with non-incremental dialogue system research in the preceding decades.

[1] http://www.langtech.co.uk/us/knowledge-center/doc_download/10-study-on-the-size-of-the-language-industry-in-the-eu.html"
13,4AA4C3DC-44B5-458C-9139-9B8982BCAEC6,Towards a New Generation of Matrix Learning Methods in Machine Learning,"Machine Learning (ML) has been a very active field in computer science over the past two decades. The interplay between ML, statistics and numerical optimisation is becoming increasingly fruitful. In particular, the understanding of optimisation within ML is a process that has just begun and has recently received most of the attention. This project will consider challenging optimisation issues in the context of matrix learning in ML. Most of the research in this area has been based on &quot;recycling&quot; knowledge and general-purpose softwares acquired from numerical optimisation research. In other words, their special structures related to specific ML tasks are largely ignored which hampers their applications to large-scale datasets. At the same time, the modern technologies are creating a huge number of high-dimensional and large-scale datasets, which is particularly true in industry, e-commerce, life science and computer vision. We believe that it is now the time to build on the success of the existing approaches to develop a new generation of matrix learning methods for high-dimensional and large-scale data analysis. The main theme in this proposal is to develop a completely new eigenvalue optimisation framework for various matrix learning problems in ML by exploring their special convex structures. This new framework will not only provide new insights into matrix learning problems in ML but also, most importantly, the beautiful mathematics underlying eigenvalue optimisation will greatly facilitate the design of efficient algorithms. More powerful ML methodologies than generic SDP solvers and exiting first-order methods will arise from the innovative interaction between ML and eigenvalue optimization. Therefore, the proposed research theme represents a significant shift in emphasis.

Specifically, this proposal aims to develop a new line of matrix learning methods in ML by exploring their special structures in convex optimisation which includes developing new models, designing efficient optimisation algorithms and rigorously establishing their convergence characteristics. Extensive empirical studies will be carried out to illustrate the potential of the new methods developed and refine the state-of-the-art results. In particular, we will implement the algorithms by means of user-friendly software tools, and apply them on two large and challenging application problems: face identification (e.g. Labeled Faces in the Wild dataset from Yahoo News) and collaborative filtering (prediction of users' preferences to products).",,"In the short term, the project will have an immediate impact on different groups of researchers (machine learning, statistics, numerical optimisation) named as academic beneficiaries and development of the PDRA. In particular, the collaboration with Prof. Charles Micchelli will create a new path of impact across a wide range of academic communities and lead to additional dissemination paths outside of the UK. 

As the collection of massive data sets expands in all areas of industry and administration, machine learning methodologies are increasingly making their way into industrial applications. For instance, they are extremely useful in information security and crime prevention (e.g. in fraud detection, spam email detection, people identification) and in the retail sector (e.g. prediction of users' preference data), to name but a few. The availability of powerful computers and the development of new models and algorithms in this proposal will definitely accelerate this process. Thus, while the research to be undertaken under the remit of this proposal will have an immediate impact on the research community, it will also make a less immediately measurable but even more important impact on the way our economy and society evolve. 

To increase the likelihood of the impacts, serious efforts will be made to engage and reach the different target audiences. In particular, we will publish papers in prestigious academic journals, give presentations at leading conferences which attract many attendees from industry, hold consultation meetings with potential industrial users, maintain an up-to-date website of the project and organize workshops as part of computing outreach activities for secondary school students."
14,D24FA4AE-3CE8-4837-9C5D-A376444D0B2E,Computer-Human Interactive Performance Symposium (CHIPS),"Popular music (e.g. folk, rock, music theatre) plays a central role in the lives of millions of people. Musicians of all standards from amateur to professional produce music that is heard on radios and televisions, and performed in concert halls and theatres. Teenagers are motivated to learn instruments and play in bands to emulate their professional idols, serious amateurs play and sing together at open-mike nights, charity concerts, and in churches, and professionals perform in clubs, theatres, and multimedia shows like Cirque du Soleil and the Blue Man Group. To learn, rehearse, and perform popular music often requires a musician to be part of an ensemble yet forming such a group can be challenging, particularly for amateur musicians. Even in established communities such as churches, the demands of everyday life mean that musicians cannot always attend rehearsals or play regularly together. In professional ensembles, illness can cause the absence of key musicians in rehearsal or performance. Computer music technology offers the potential to substitute for musicians in these situations, yet reliable, robust, and simple systems that can be quickly set up, and that play musically and creatively do not yet exist. To focus broader attention on this significant and potentially high-impact problem, the CHIPS project will form a network of interest around the computer-human performance of popular music. The aim is to understand and shape the future research agenda by learning from experiences of technological adoption in relevant contexts, understanding the technological state of the art in relation to popular music performance, and imagining future performance practices incorporating computer &quot;musicians&quot;. The focus of the project will be a symposium, supported before and after by web-enabled collaborative discussion, and with the longer-term aim of establishing a network of interest to subsequently organise a self-sustaining series of symposia or working sessions at relevant major international conferences in the field.",,"The primary non-academic impact of the CHIPS project will be on practicing musicians engaged in popular music performance in various settings e.g. theatres and churches. The nature of the impact will be to raise awareness of the current technological state of the art and potential of interactive popular music performance systems, and to encourage performers to consider and become more aware of the issues surrounding the introduction of technology in their performance practice. Primarily, impact will be achieved through those academic researchers who are also performers (a relatively common situation) but as the network of interest evolves, it is expected that more performers will be drawn into the ongoing discussions."
15,C535633D-D74E-4982-BFEF-6859CD52C2F4,Towards More Autonomy for Unmanned Vehicles: Situational Awareness and Decision Making under Uncertainty,"It is anticipated that unmanned vehicles will be widely used within military and civilian operations and have a profound influence in our daily life in near future. Before fully realising the potential that unmanned vehicles bring, it is reasonably expected that to make unmanned vehicles accepted by users, the public and regulatory authorities, they shall achieve a similar level of safety as human operated systems. Among many others, a fundamental requirement for an unmanned vehicle is the capability to respond to internal and external changes in a safe, timely and appropriate manner. Therefore, situational awareness and decision making are two of the most important enabling technologies for safe operation of unmanned vehicles. To a large extent, they determine the level of autonomy and intelligence of an unmanned vehicle. Compared with a human driver or pilot residing in the vehicle, a major safety concern is the inevitable reduction in situational awareness of the unmanned vehicle operator remotely located in a control station. 

Unmanned vehicles operate in a dynamic, unpredictable environment with incomplete (or inaccurate) sensory information, which creates many challenges in situational awareness and decision making. Probabilistic and bounded approaches are widely used to represent uncertainty with a known distribution or with a known upper and lower bounds respectively. Situational awareness includes the perception of the objects in the environment within a volume of time and space, the comprehension of their meaning and the projection of their status in the near future. For example, in projection of the near status of moving objects of interest, any initial uncertainty associated with perception and comprehension will expand exponentially with the increase of the projection time span. However, it is possible to significantly reduce the uncertainty by utilising the information in the world model such as the operation environment, the Rules of the Road (or of the Air) and the properties of an identified object. For probabilistic uncertainty, this makes the Gaussian distribution assumption invalid, which is fundamental for most of the current statistical approaches such as Kalman filtering. Under the Gaussian distribution assumption, the estimated state about a moving object can be presented by its mean with a variance, and a symmetric uncertain region can be defined with the mean located at the centre (under a specified confidence level such as 99%). The introduction of knowledge (e.g. constraints due to the roadway layout) makes this not true anymore. To address the challenge of non-Gaussian distributions imposed by making use of information from the world model, a rigorous Bayesian learning framework will be developed for pooling all the knowledge from the world model and measurement data to provide a better estimate of the environment, and to propagate the uncertain regions with projection time. Reachability analysis will be developed for bounded uncertainty for worst case analysis, where the uncertainty will be reduced using constraints from the world model. Hazard analysis will be carried out to identify any potential risk. The key idea is to take a proactive approach to prevent any emergent situation through improving situational awareness reasoning and decision making. The estimates and associated uncertain region provided by the situational awareness will be fed to novel decision making and planning tools. The research activities will be strongly supported and verified by experimental tests on small scale ground and aerial vehicles. This project aims to significantly improve the level of safety of unmanned vehicle operation and to bridge the gap between the development and deployment of unmanned vehicles in real world applications, which is a strategically important area for new business growth.",,"Although unmanned vehicles have the potential to provide huge benefit to the economy, end users and the society, they do impose unprecedented challenges as they are in an uncharted area. Among other concerns such as legal issues and ethics, safety is a paramount consideration for a wide application of unmanned vehicles. In Afghanistan, on 13th Sept., 2009, the American Air Force was forced to shoot down one of its own MQ-9 Reaper aircraft that did not go into failsafe mode after the service lost remote control of the aircraft, and license for patrolling along the Mexico border was suspended by the Federation Aviation Authority (FAA) for a short time due to safety issues.

This Autonomous and Intelligent Systems Programme is to respond to the imperative needs of fundamental research in this emerging business area, backed by an industrial consortium consisting of companies that share the same vision but may have different business interests. Although there is a wide spectrum of unmanned vehicles, each with different operational environments/needs, all the unmanned vehicles face the same challenge, i.e. operating in a dynamic and unpredictable environment. This project aims to tackle the fundamental issues faced by the industrials by improving situational awareness and decision making in a dynamic and uncertain environment so to improve the safety in operating unmanned vehicles. Therefore, all the UK industrials with business interests in unmanned vehicles, in particular the partners, will directly benefit from this project.

The outcomes of this proposed project will assist regulatory authorities to formulate their policies for the operation of unmanned vehicles, helping them to understand the behaviour of unmanned vehicles and the risks and safety issues caused by increasing the level of autonomy. The situational awareness and hazard analysis functions developed in this project will help end users and unmanned vehicles operators to determine proper levels of autonomy in response to the change of real operation scenarios. The public will benefit from a better understanding about the true risk involved in using unmanned vehicles, and the reduced risk due to better onboard situational awareness and decision making functions (e.g. unmanned vehicles will less likely become a hazard to the public). In the long term, the situational awareness and decision making techniques developed in this proposal will help to develop other autonomous systems such as domestic support, health care or other service robots. This would improve quality of life and health care by providing domestic support for society and providing health care for disabled and elderly people living at home.

The academic community will be able to take the results generated by this project to identify new research directions. This project identifies and addresses a number of new academic challenges, for example, non-Gaussian distributions arising from situational awareness in autonomous and intelligent systems, and comprehension, projection and hazard analysis for unmanned vehicles. This project takes a true multi-disciplinary effort in tackling these challenges, which will promote interdisciplinary collaboration and cross-fertilise in a number of areas, e.g. mathematics/statistics, autonomous/intelligent systems, transport, computational intelligence and safety engineering. It supports and contributes to the wider academic community of unmanned vehicles in the UK and worldwide, where further academic research in the relevant areas will ultimately lead a pathway towards economic/societal impact as highlighted above. The public and other interested parties (non-academic) will be informed about the results of this project via the project web site, newsletters and media reports through Lougborough University's Public Relations Office."
16,3FA81073-3651-4BAE-8424-0FA522045005,Autonomous Behaviour and Learning in an Uncertain World,"The key challenges facing research, development and deployment of autonomous systems require principled solutions in order for scalable systems to become viable. This proposal intertwines probabilistic (Bayesian) inference, model-predictive control, distributed information networks, human-in-the-loop and multi-agent systems to an unprecedented degree. The project focuses on the principled handling of uncertainty for distributed modelling in complex environments which are highly dynamic, communication poor, observation costly and time-sensitive. We aim to develop robust, stable, computationally practical and principled approaches which naturally accommodate these real-world challenges.

Our proposed framework will enable significant progress to be made in a large number of areas essential to intelligent autonomous systems, including 1) the assessment of reliability and fusion of disparate sources of data, 2) allow active data selection based on Bayesian sequential decision making under realistic time, information &amp; computation constraints, 3) allow the advancement of Bayesian reinforcement algorithms in complex systems, and 4) extend Model predictive control (MPC) to probabilistic settings using Gaussian process non-parametric models.

At the systems level, these developments will permit the design of overarching methods for 1) controlled autonomous systems which interact and collaborate, 2) integration of sensing, inference, decision making and learning in acting systems and 3) design methods for validation and verification of systems to enhance robustness and safety.

The ability to meet these objectives depends on a multitude of recent technical developments. These include, 1) development of practical non-parametric algorithms for on-line learning and adaptation 2) approximate inference for Bayesian sequential decision making under constraints, 3) the development of sparse data selection and sparse representation methods for practical handling of large data sets with complex decentralised systems and 4) the implementation of and deployment on powerful modern parallel architectures such as GPUs.

We aim to build on our expertise in Bayesian machine learning, multi-agent systems and control theory and by drawing together closely related developments in these complementary fields we will be able to make substantial improvements to the way artificial agents are able to learn and act, combine and select data sources intelligently, and integrate in robust ways into complex environments with multiple agents and humans in the loop.",,
17,B475B841-1188-44E8-A3DB-0FBDF52F7748,MACHINE LEARNING COALGEBRAIC AUTOMATED PROOFS,"Some steps in formal reasoning may be statistical or inductive in nature.
Many attempts to formalise or exploit this inductive or statistical nature of formal reasoning are related to methods of Neuro-Symbolic Integration, Inductive Logic and Relational Statistical Learning.
The proposal is focused on one statistical/inductive aspect of automated theorem proving -- proof-pattern recognition. 

Higher-order interactive theorem provers (e.g. HOL or Coq) have been successfully developed into 
sophisticated environments for mechanised proofs. 
Whether these provers are applied to big industrial tasks in software verification, or to formalisation
of mathematical theories, a programmer may have to tackle thousands of lemmas and theorems of variable sizes and complexities.
A proof in such languages is constructed by combining a finite number of tactics. Some proofs may yield the same pattern of tactics, 
and can be fully automated, and others may require a user's intervention.
In this case, manually found proof for one problematic lemma may serve as a template for several other lemmas needing a manual proof.
At present this kind of proof-pattern recognition and recycling is done by hand, and the ML-CAP project will look into methods to automate this. 

Another issue is that unsuccessful attempts of proofs --- in the trial-and-error phase of proof-search, are normally discarded once the proof is found.
Conveniently, analysis of both positive and negative examples is inherent in statistical machine learning. And ML-CAP is going to exploit this.

However, applying statistical machine-learning methods to analyse data coming from proof theory is a challenging task for several reasons. 
Formulae written in formal language have a precise, rather than a statistical nature. 
For example, list(nil) may be a well-formed term, while list(nol) - not; although they may have similar patterns 
recognisable by machine learning methods.

Another problem that arises when merging formal logic and statistical machine-learning algorithms is related to their computational complexity.
Many essential logic algorithms are P-complete and inherently sequential (e.g., first-order unification), while neural networks and other similar devices 
are based on linear algebra and perform parallel computations.

As a solution to the outlined problems, the coalgebraic approach to automated proofs 
may provide the right technique of abstraction allowing to analyse proof-patterns using machine learning methods. 
Firstly, coalgebraic computations lend themselves to concurrency, 
and this may be the key to obtaining adequate representation
of the outlined problems.
Secondly, they are based on the idea of repeating patterns of potentially infinite computations, rather than outputs of finite computations. 
These patterns may be detected by methods of statistical pattern recognition. 

ML-CAP is based upon a novel method of using statistical machine learning in analysis of formal proofs.
In summary, it provides algorithms for extracting those features from automated proofs that allow to detect proof patterns 
 using statistical machine learning tools, such as neural networks.
As a result, neural networks can be trained to distinguish well-formed proofs from ill-formed; distinguish whether a proof belongs to a given family of proofs, 
and even make accurate predictions concerning potential success of a proof-in-progress. All three tasks have serious applications in automated reasoning. 
The project will aim to generalise this method and develop it into a sound general technique for automated proofs. It will result in new methods useful 
for a range of researchers in different areas, such as AI, Formal Methods, Coalgebra and Cognitive Science.",,"Impact Summary.

1. Knowledge (science and technology). 
The proposed research is a pilot attempt to merge two very different groups of methods - 
statistical machine learning and formal methods - using a third group of coalgebraic methods as a method of abstraction and suitable representation. 
The ultimate goal for the resulting technique is to advance real-life applications of ITPs,
in the aspects where decidable algorithms and methods are not possible. 
The project is distinctly interdisciplinary and has an original research idea and methodology underlying it. 
My discussions of the proposed idea and method with
various groups of researchers at AI4FM'11 (AI and Formal Methods community), STP'11 (Theorem Proving community), 
and ITP'11 [f] (Inductive Logic and Machine Learning communities) have confirmed my belief that 
my approach is indeed new and ground-breaking, (see also letters of support).

 
I see the EPSRC First Grant scheme as an excellent opportunity to invest efforts into proving the concept,
developing more evidence and accumulating data supporting the proposed ML-CAP method, with a view to a bigger project if the pilot studies are successful. 
Another important consequence of the proposed work will be 
building confidence among various communities of researchers in both AI and Formal Methods in the potential of 
statistical machine-learning methods as useful tools in handling 
undecidable aspects of automated reasoning. 

2. Economy and Society.
The project contributes to a long-term goal of
achieving the Grand Challenge in Computing - Dependable systems evolution; 
as explained in detail in the Background section.

The proposed method will help to automate those 
steps in formal proofs that cannot be automated using the state-of-the-art technology available today.
The new technology will directly affect programmers working on software and hardware verification.
In the longer term, the new method will make the process of software verification by means of ITPs lighter, faster, and hence cheaper,
and thus will have serious economic impact.
Also, ML-CAP will work towards creation of new technologies, and therefore may trigger creation of new companies.

As Formal methods improve dependability and security of software, development of these technologies will ultimately have an effect on the Society in terms of the security and quality of life.
As A. Ireland states in the attached letter of support,
``The pervasive nature of software means that software dependability plays a crucial role within the
world economy, as well as the security - from maintaining national security through to protecting
personal data.''


3. People. 
In terms of new skill development, the project will affect the newly established group ``Theory of Computing and AI'' at Dundee,
and e.g. my current Honours, MSc and PhD students, as it will create a more vibrant research environment for them,
as well as reinforce our links with researchers from the partner universities in the UK and abroad."
18,AEC030F3-12B3-4BB3-BADB-3CF04A6F0D26,Machine Learning and Adaptation of Domain Models to Support Real-Time Planning in Autonomous Systems,"Simulating low-level cognitive behaviour, such as reaction to stimuli, has been a major focus of research and development in the autonomous systems (AS) community for many years. Automated assessment of sensor data, and reactive selection of actions in the form of condition-action pairs, is well developed in robotic and control application areas. In contrast, a characteristic of more intelligent behaviour is the ability to reason with self-knowledge: an AS knows about the actions it can perform, the resources it has, the goals it has to achieve, the current state and environment it finds itself in; and it has the ability to reason with all this knowledge in order to synthesise, and carry out, plans to achieve its desired goals. So for example an unmanned vehicle on the Mars surface might be requested to collect a rock sample at some position, or a spacecraft might be required to take a photograph of some star constellation. These tasks require an AS to generate or be given detailed plans to achieve them. Enabling applications involving AS to have the general ability to generate reliable plans in this manner is a great challenge, because of the difficulty of creating plans fast enough in real-time situations, and the problems in representing and keeping up to date the AS's domain knowledge.

Recently researchers who are working on automatically creating plans (automated planning) have made many breakthroughs, so that now such automated planners are capable of reasoning very efficiently and accurately with detailed representations of knowledge. This has resulted in automated planning software being used within a wide range of applications including fire fighting, elevator control, emergency landing, aircraft repair scheduling, workflow generation, narrative generation, and battery load balancing. In Space applications, scientists at NASA have been developing systems with such technology for the control of autonomous vehicles, and have deployed systems which can plan activities for spacecraft, schedule observation movements for the Hubble Telescope, and control underwater vehicles.

While the development of automated planning has been encouraging, a major problem remains in all these applications, which limits their adaptability, and makes them difficult to maintain and validate: much of the AS's high level self-knowledge, that is knowledge of such things as actions, resources, goals, objects, states and environment, has to be programmed or encoded into the system before its operation (this encoded knowledge is often called a `domain model'). Experience has shown that this encoding involves a great deal of expert time and effort. It also means that if the AS's capabilities change, for example if the preconditions or effects of an action change, then new knowledge describing this must be re-entered into the system by human experts. 

This research project seeks to lead the way to overcoming this challenge by enabling the AS to learn and adapt its own domain model. It seeks to discover methods for an AS to acquire knowledge initially, and to maintain and evolve that knowledge via feedback sensing after executing actions. While methods for empowering AS to learn basic reactions, or learn how to classify data, are well established, methods for getting an AS to learn and adapt knowledge of structures such as actions, is not so well developed. The proposers will use their recent research results in this area and research and develop prototype AS which can learn and adapt their domain models. They will demonstrate and evaluate their research using virtual worlds which model real applications.",,
19,E1752D73-FF45-42DB-A98A-00218DE16C15,Non-Parametric Models of Phrase-based Machine Translation,"Machine Translation is the automatic process of translating human language text in one language into another language using a computer. Statistical Machine Translation (SMT) is a data-driven approach to machine translation which uses machine learning techniques to learn how to translate directly from large collections of sentences and their translations. SMT has seen a surge in popularity in the last decade, and has now matured into an invaluable means for data access and communication, as evidenced by the many successful commercial SMT systems, e.g., Google Translate and Microsoft Translator. These technologies are beginning to have a substantial impact on individuals, businesses and governments by enabling communication with foreign language speakers and enabling access to the growing amounts of foreign language data. Consequently automatic translation is rapidly becoming a key technology across all levels of the community, and improvements in its quality have the potential to further increase its impact. Although current systems can produce good translations for some language pairs, such as French-English, their performance is markedly worse for many others, e.g., Chinese-English and Basque-Spanish. There are two key reasons for this: language similarity and data availability. Predominant SMT approaches do not model the structure of the language, instead assuming that translation can be performed largely at the level of single words or small groups of words. For this reason they are unable to describe large changes in word order which are commonly required for translating between dissimilar languages. For example, in Japanese sentences typically follow a subject-object-verb order, while in English sentences are subject-verb-object. In order to translate between Japanese and English the positions of the verb and object phrase must be reversed. Another reason for SMT under-performing is data availability. Current techniques require very large collections of sentences and their translations in order to learn a good translation model (needing hundreds of thousands or millions of sentence pairs), but performance suffers when considerably less data is available. For official languages of the leading countries in the West and Asia this type of data is often plentiful, however for the remaining majority of the world's languages and dialects translation data is exceedingly rare. This problem is exacerbated for languages with large vocabularies, for instance Finnish in which each word can convey a wide range of syntactic and semantic information including the gender, syntactic case, tense, number and aspect. This project will tackle both of these issues, focusing primarily on the second issue of generalising from small training sets; The novelty of our approach will help to make inroads into the first issue of dealing with word-order differences between structurally dissimilar languages. The project will develop a translation model which reframes phrase-based translation in a novel way by using much simpler translation units than phrase-based models, primarily single words and their translations, while also modelling correlations between the translations used in a sentence. This will allow the model to describe implicitly arbitrarily large translation units and thus the approach is more general that current phrase-based translation. Additionally, our approach confers a number of further benefits. Most notably, it will make better use of training data by compiling denser and more reliable statistics, and thus will generalise more accurately from small training sets. In addition our approach will support a richer set of translation fragments than current phrase-based models, including gapping phrase-pairs which can describe syntactic divergences between structurally dissimilar languages. These benefits should lead to improvements in translation accuracy across a wide range of language pairs.",,"Automatic machine translation is becoming an invaluable tool for reducing the barriers to communication across all levels of the community - government, business and individuals. Research into addressing the flaws in the current technology will lead to improvements in translation quality, thus magnifying its usefulness to the community. This proposal has the potential to impact the research knowledge base in a number of ways. Firstly, it will support a richer set of translation rules, which should provide particularly useful for translating between dissimilar language pairs, e.g., Chinese and English and Basque and Spanish. Secondly, it aims to better model small data samples without recourse to linguistic resources, such that the model may be easily ported to different language pairs. Therefore our approach has the potential to improve translation performance for all manner of language pairs. We anticipate that our proposal will have long term effects on the modelling techniques used in the research community and, in due course, industry translation software, thereby indirectly affecting the wider community. Improvements to automatic translation will directly impact the global economy. Communication is a critical requirement for international commerce, and the cost and inconvenience of human interpretors and translators represents a significant barrier to commercial operations. Automatic translation has the potential to reduce this barrier and thereby improve the international competitiveness of the UK and many other countries. Automatic translation is also of paramount interest to the European Union, which currently has 23 official languages and more than 60 regional or minority languages and is continuing to expand. The translation burden for internal communication within the EU government is already astoundingly large; this burden is also shared by industry in Europe. Translation has long been a critical tool for government, in particular the military and foreign relations divisions. Machine translation began as a way to improve intelligence operations during the Cold War, and has continuing to be influenced by the military agenda. Consequently much of the research into translation focuses on languages with strategic importance or links to terrorism. Automatic translation has also proved invaluable for humanitarian relief operations, as evidenced in the wake of the Haitian earthquake in January 2010 where Microsoft and Google quickly improvised support for translating to and from Haitian Creole in only a few days. Automatic translation also holds considerable promise to help bridge the language gap for immigrant and migrant communities, of which there are many in the UK. In such communities the level of English is often poor, which leads to these groups having poor social cohesion with the rest of society. The cost of human translation of official documents into immigrant languages is already very high in many counties, and looks set to increase further with the recent wave of migration from Eastern Europe. Automatic translation could become an invaluable tool to assist government in integrating immigrants into British society, providing more accessible translation services and thus increased ease of communication."
20,99070BE9-E048-4547-AB95-8E92BA6A1BCC,Ant Colony Optimisation for the Discovery of Gene-Gene Interactions in Genome-Wide Association Studies,"Genome-wide association studies investigate the small changes in DNA among individuals in a population that lead to variations in traits such as height and the propensity to suffer from diseases. Recent advances in genetic technology allow researchers to measure these small differences in DNA in a population (known as single-nucleotide polymorphisms or SNPs) and have already discovered SNPs that are associated with diseases including the widely publicised 'FTO' gene which has been shown to be highly associated with type 2 diabetes. However, single SNPs do not account for all of the variation that is suspected to be inherited and researchers are now beginning to investigate the potential for interactions between multiple SNPs to explain this variation. The number of possible pairs and triplets in the genome though is vast and so a full enumeration search is not possible, meaning that intelligent techniques are required to process the large space of potential interactions. A method that has shown considerable promise in this area is ant colony optimisation (ACO), a nature-inspired search technique based on the way that insects find the shortest path from a nest to a food source in the wild. This search algorithm has two unique properties that make it ideal for this task. The first is that local heuristics can be used to influence the search to find specific gene-gene interactions such as epistasis and the second is that the algorithm creates a pheromone matrix that provides a detailed map of the importance of variables (SNPs) found during the search. This project will investigate the use of ACO to search the space of SNP interactions and their association with a number of diseases including type 2 diabetes and Crohn's disease and also the potential for them to explain human traits such as height. The discovery of these interactions will advance our knowledge of how disease is inherited and could pave the way for highly personalised and pre-emptive treatment based on an individual's genetic makeup.",,"The chief, longer term (3-10 years) societal impact from this research will be achieved through the identification of gene-gene interactions in genome-wide association studies of human populations and the resulting potential to better stratify individuals suffering from a disease. The ability to accurately determine the disease-risk posed to an individual increases the potential for early and personalised treatment plans. Early or pre-emptive treatment has the potential to delay the onset of symptoms of a disease, reduce symptom severity and complications, and to reduce costs associated with treating the disease across a population of individuals Taking the example of type 2 diabetes, there are an estimated 2.5 million people suffering from this disease currently in the UK and this is expected to rise to 4 million by 2026 (Diabetes UK, 2010). If even a small proportion of these individuals can benefit from diagnosis of disease risk and subsequent early treatment then this would in turn lead to improved health and quality of life for thousands of people and a significant reduction in the burden on the NHS. 

The chief, longer term (3-10 years) economic impact will be in the potential to reduce costs of treatments of diseases and their complications through early treatment, an approach that has begun to be used in the USA for type 2 diabetes (Herman et al. 2005). In the UK, the current costs for treating type 2 diabetes are around 10% of the entire budget for the NHS (around &pound;9bn) and this figure is expected to rise significantly (Bagust et al. 2002), so clearly any significant reduction in treatment costs for this disease would have profound economic benefits for the country. 

The chief impact on people will be the training of a post-doctoral researcher in this new area of research. The researcher is expected to come to the project with a significant existing skillset, but the precise nature of the algorithms and technologies used in this work means that they will need to become familiar with the particular skills required, rapidly furthering their expertise in this growing field. The proposal will also foster the collaboration between Dr Keedwell and Prof. Frayling, with the attendant increase in knowledge of each field that comes from working together on a project of this novel and interdisciplinary nature.

References
- Bagust, P. K. Hopkinson, L. Maslove and C. J. Currie (2002) &quot;The projected health care burden of Type 2 diabetes in the UK from 2000 to 2060&quot; Diabetic Medicine 19; Supp/4, pp1-5
- Herman et al. (2005) &quot;The Cost-Effectiveness of Lifestyle Modification or Metformin in Preventing Type 2 Diabetes in Adults with Impaired Glucose Tolerance&quot; Ann. Intern. Med. 2005 March 1; 142(5) pp 323-332."
21,B989C673-5A8F-4CE9-902E-F0B98D8B348E,Machine Learning and Adaptation of Domain Models to Support Real-Time Planning in Autonomous Systems,"Simulating low-level cognitive behaviour, such as reaction to stimuli, has been a major focus of research and development in the autonomous systems (AS) community for many years. Automated assessment of sensor data, and reactive selection of actions in the form of condition-action pairs, is well developed in robotic and control application areas. In contrast, a characteristic of more intelligent behaviour is the ability to reason with self-knowledge: an AS knows about the actions it can perform, the resources it has, the goals it has to achieve, the current state and environment it finds itself in; and it has the ability to reason with all this knowledge in order to synthesise, and carry out, plans to achieve its desired goals. So for example an unmanned vehicle on the Mars surface might be requested to collect a rock sample at some position, or a spacecraft might be required to take a photograph of some star constellation. These tasks require an AS to generate or be given detailed plans to achieve them. Enabling applications involving AS to have the general ability to generate reliable plans in this manner is a great challenge, because of the difficulty of creating plans fast enough in real-time situations, and the problems in representing and keeping up to date the AS's domain knowledge.

Recently researchers who are working on automatically creating plans (automated planning) have made many breakthroughs, so that now such automated planners are capable of reasoning very efficiently and accurately with detailed representations of knowledge. This has resulted in automated planning software being used within a wide range of applications including fire fighting, elevator control, emergency landing, aircraft repair scheduling, workflow generation, narrative generation, and battery load balancing. In Space applications, scientists at NASA have been developing systems with such technology for the control of autonomous vehicles, and have deployed systems which can plan activities for spacecraft, schedule observation movements for the Hubble Telescope, and control underwater vehicles.

While the development of automated planning has been encouraging, a major problem remains in all these applications, which limits their adaptability, and makes them difficult to maintain and validate: much of the AS's high level self-knowledge, that is knowledge of such things as actions, resources, goals, objects, states and environment, has to be programmed or encoded into the system before its operation (this encoded knowledge is often called a `domain model'). Experience has shown that this encoding involves a great deal of expert time and effort. It also means that if the AS's capabilities change, for example if the preconditions or effects of an action change, then new knowledge describing this must be re-entered into the system by human experts. 

This research project seeks to lead the way to overcoming this challenge by enabling the AS to learn and adapt its own domain model. It seeks to discover methods for an AS to acquire knowledge initially, and to maintain and evolve that knowledge via feedback sensing after executing actions. While methods for empowering AS to learn basic reactions, or learn how to classify data, are well established, methods for getting an AS to learn and adapt knowledge of structures such as actions, is not so well developed. The proposers will use their recent research results in this area and research and develop prototype AS which can learn and adapt their domain models. They will demonstrate and evaluate their research using virtual worlds which model real applications.",,"Automated planning and scheduling (APS) is considered a necessary component of systems that are to exhibit intelligent behaviour.

Impact to Knowledge

A domain model (DM) represents reusable knowledge that is required as input by APS systems. DMs are difficult to create, requiring a domain expert and a planning expert. The tools and methods developed in this project will enable the development of formal DMs more rapidly, on a larger scale, and with fewer conceptual flaws. We will disseminate research results at relevant academic conferences, building on Prof McCluskey's excellent links with the knowledge engineering community. The AI planning community will benefit from this research as the increased availability of more diverse DMs will drive new planning research.

Impact to the Economy

The tools and methods developed in this project will make AI planning technology more accessible to other disciplines, enabling software engineers to include sophisticated planners into their systems, such as autonomous vehicles. For example, in multi-vehicle cooperative autonomy the learned DMs provide capability descriptions that can be communicated between the agents; a long range Mars rover will be able to learn new domain knowledge in an unknown environment; to investigate and repair defective infrastructure the DMs will enhance flexibility; and for teleoperation and teleautonomy the results of this project will facilitate autonomous, goal-directed behaviour. Another target area will be autonomic systems for road network support where Prof McCluskey is leading an EU COST action. More generally, we will create a community of DM developers that will enable the UK to gain a competitive advantage in developing autonomous intelligent systems using APS technology.

Impact to Society

The most direct impact to the people living in the UK and in other countries will be achieved through the engagement with and promotion of results to the ISCRAM (Information Systems for Crisis Response and Management), where we will build on Dr Wickler's excellent links to this community. The aim here is to use autonomous intelligent systems to create a safer, more sustainable environment and a more effective disaster response when necessary. More specifically, the results of this project will be in several of the research areas listed in the call, which we will relevant to the ISCRAM community: model-building and learning will be directly addressed as we will learn DMs; planning will be addressed as DMs are a required input for a planning system; situational awareness and information abstraction will be addressed as part of the learning, where sensor data is abstracted to knowledge used by a planner; and verification and validation will be directly addressed as part of the domain analysis.

Impact to People

The impact of the project on people will be twofold. On the one hand, people working as developers of autonomous systems will be able to use automated planning technology as part of their tool suite. On the other hand, people that currently have to operate in dangerous environments, e.g. in a post-disaster scenario, will benefit from more flexible autonomous systems, significantly decreasing the risk for emergency responders.

Summary

The results of this project will contribute to knowledge in several academic disciplines, including knowledge engineering, AI planning, and autonomous systems. Several applications of the results of this project, including all the scenarios described in the call, will lead to an economic benefit to the partners. The links with the ISCRAM community should ultimately lead to a safer environment, specifically, more safety for people working in emergency response."
22,F6317A3F-AD3D-44A8-A1B4-936908B49389,"Crime, Policing and Citizenship (CPC) - Space-Time Interactions of Dynamic Networks","Crime continues to cast a shadow over citizen well-being in big cities today, while also imposing huge economic and social costs. Prevention, early detection and strategic mitigation are all critical to effective policy intervention, especially in domains where coordinated responses are required. Every day, about 10,000 incidents are reported by citizens, recorded and geo-referenced in the London Metropolitan Police Service Computer Aided Dispatch (CAD) database. Today, impending funding cuts bring new pressures for central accountability and improved efficiency, while community empowerment initiatives bring new opportunities and challenges to policing. Timely understanding of how criminality emerges and how crime patterns evolve is crucial to anticipating crime, dealing with it when it occurs and developing public confidence in the police service. It is widely understood that policing, crime and public trust all have strong spatial and temporal dimensions. An integrated approach to space-time analysis is needed in order to analyse crime patterns, police activity patterns and community characteristics, so as to understand and predict the when, where and what of how criminal activities emerge and are sustained.

This research will consolidate achievements in integrated spatio-temporal data mining and emergent network complexity to uncover patterning in crime, policing and citizen perceptions at a range of spatial and temporal scales. Each dataset of police movement, crime (and disorder) reported in the CAD, and citizens making '999' calls constitutes a spatio-temporal network (STN), which has its own characteristic patterning and behaviour in space-time, and which interacts with the other STNs. The (geotagged) deployment of police manpower in space and time, the spatio-temporal patterning of crime and disorder, and the perceptions of members of the public are likely to be interlinked to differing extents. The first of these purportedly both anticipates and responds to the second, while the third is a lagged response to the first two, giving reason to anticipate that all three networks should be tightly coupled. The project will first analyse spatio-temporal patterns of individual STNs, then associate the patterns among these STNs via integrated spatio-temporal data mining developed using innovative statistical regression and machine learning.

This research will utilise a range of disciplines (crime, geography, geoinformatics, and computer science) to help engineer effective practical solutions to crime problems. It proposes a new method for exploring crime patterns and integrating information on crime and police activity. It systematically addresses a structured programme of analytical issues in spatio-temporal data mining, which are becoming core to Geographical Information Sciences. It will advance the theory, methodology and application of research into network complexity by evaluating the forms and interactions of the networks that characterise crime and other socio-economic phenomena. This will make it possible to not only understand activity networks but also to use them for prediction and decision making. 

This addresses the aims of RCUK's Global Uncertainties Programme in crime, terrorism, and ideologies and beliefs. It will extend our appreciation of the subtle interplay of different forms of complex systems, in ways that will contextualise tactical and strategic responses to terrorism and organised crime. It will enable intelligent policing of London Met Police by granting unforeseen levels of prediction. The best practice of individual Metropolitan boroughs can be extended to others in the UK. The methodology developed here will be transferrable to other international cities using similar incident report systems. This will directly benefit people who live, work and visit London and those cities to make them feel safe.",,"1. Government: London Metropolitan Police Service and policing policy makers 
The industrial partner, London Metropolitan Police Service (MPS, with about 32,000 officers and 4,500 Support Officers), will directly benefit from the research as it addresses their operational and policy issues - providing 'the right service at the right price'. The intelligent allocation of police resource will make best use of their resources under the current budgetary requirement of achieving savings of 3% per annum (or &pound;7.5 million). 

It will help the individual Borough police forces to consider their activities as active or reactive, and to evaluate this balance in relation to neighbouring London Boroughs. It will help Borough police forces to think in terms of coordinated responses to outbreaks of different types of crime, based upon new understanding of spatio-temporal dynamics. It will contribute to best policing practice by providing detailed geographic analysis of crime patterns and public perceptions of crime. This in turn will stimulate wider debate by refocusing discussion of performance metrics away from clear-up rates alone to assessing how proactive policing can reduce citizen incident report numbers and encourage a more broadly based confidence in the police. In future this approach will allow links to be made on re-offending so that crime prevention can be targeted more sustainably and economically.

The research findings will be transferable to other potential end users elsewhere in the UK, particularly metropolitan areas such as Manchester and Liverpool. The findings will also be transferable to other international cities (such as Beijing, New York, Paris) that use similar incident reporting systems to improve their policing. The UCL Jill Dando Institute of Crime Science (the first university department in the world devoted specifically to reducing crime) will be well equipped to use these findings to encourage best practice through its continuing professional development activities, and the National Crime Mapping Conference and International Crime Science Conference. 

2. People: Citizens and local communities 
Benefits for the police go hand in hand with improved citizen well-being for those who live and work within London (over 7.4 million), who wish to feel safe and secure. Improvements in timeliness, visibility and accountability of policing will improve confidence and engagement with the police service. It will also contribute towards making London's visitors (3.2 million in 2010, and the many more international visitors are expected for the 2012 Olympics) feel safer. The experience acquired in London will also be transferrable to other international cities. There will be direct benefits to the communities who join the participatory mapping exercises. Borough police will be able to adjust their resource according to the results of the local mapping to make neighbourhoods safer. 

3. Social enterprises: Mapping for Change 
Mapping for Change (MfC) is a social enterprise, jointly owned by UCL and the charity London 21 Sustainability Network. It specialises in community mapping, citizen science and participatory GIS. The organisation has so far worked in the area of environmental and social issues with communities across the UK. The proposed project will allow the organisation to develop its skills in engaging communities in mapping perceptions of crime and policing: a human subject that has not yet been addressed by MfC. This will also allow MfC to develop methods that can be used with police forces across the country to improve public understanding and engaging of police activities. 

4. Software Industry
The innovative machine learning algorithms to be developed within the project, such as space-time ANN (STANN),support vector machines (STSVM) and space-time weighted regression (STWR) may be integrated with industry software for GIS (ESRI) and Data Mining (Matlab), or open source software (R)."
23,B2CFFDE5-C772-4881-9765-5F6EA8DC73D7,Autonomous behaviour and learning in an uncertain world,"The key challenges facing research, development and deployment of autonomous systems require principled solutions in order for scalable systems to become viable. This proposal intertwines probabilistic (Bayesian) inference, model-predictive control, distributed information networks, human-in-the-loop and multi-agent systems to an unprecedented degree. The project focuses on the principled handling of uncertainty for distributed modelling in complex environments which are highly dynamic, communication poor, observation costly and time-sensitive. We aim to develop robust, stable, computationally practical and principled approaches which naturally accommodate these real-world challenges.

Our proposed framework will enable significant progress to be made in a large number of areas essential to intelligent autonomous systems, including 1) the assessment of reliability and fusion of disparate sources of data, 2) allow active data selection based on Bayesian sequential decision making under realistic time, information &amp; computation constraints, 3) allow the advancement of Bayesian reinforcement algorithms in complex systems, and 4) extend Model predictive control (MPC) to probabilistic settings using Gaussian process non-parametric models.

At the systems level, these developments will permit the design of overarching methods for 1) controlled autonomous systems which interact and collaborate, 2) integration of sensing, inference, decision making and learning in acting systems and 3) design methods for validation and verification of systems to enhance robustness and safety.

The ability to meet these objectives depends on a multitude of recent technical developments. These include, 1) development of practical non-parametric algorithms for on-line learning and adaptation 2) approximate inference for Bayesian sequential decision making under constraints, 3) the development of sparse data selection and sparse representation methods for practical handling of large data sets with complex decentralised systems and 4) the implementation of and deployment on powerful modern parallel architectures such as GPUs.

We aim to build on our expertise in Bayesian machine learning, multi-agent systems and control theory and by drawing together closely related developments in these complementary fields we will be able to make substantial improvements to the way artificial agents are able to learn and act, combine and select data sources intelligently, and integrate in robust ways into complex environments with multiple agents and humans in the loop.",,"Autonomous intelligent systems are expected to have an increasing effect on our lives. Initially they will be used within highly technical systems, such as robots for handling radioactive materials in power stations, for performing military reconnaissance without risking soldiers' lives, for helping surgeons, or for excavating building sites. Later they are likely to become used in situations which involve interaction with the general public, for example to provide assistance to the elderly or disabled, or perhaps to take over the driving of cars.

In order to allow any of these things to be done successfully and safely, research is needed in some underpinning engineering and science. The biggest challenge is that of dealing with the huge amount of uncertainty that will be faced by autonomous intelligent systems - uncertainty that humans can often deal with pretty well, but that we can still handle to only a limited degree in our mathematics and engineering. Therefore the scientific core of our project rests on the use of probability theory and in particular on the use of Bayes' theorem, and a whole family of methods that result from it, for calculating and keeping track of uncertainty in very complex situations. This is necessary if autonomous agents are to act intelligently in non-trivial situations.

Many people have already done plenty of work on 'Bayesian methods', so what is new in this project? The novelty is the use of these methods as a 'glue' to tie together the many and various technologies that are needed to make autonomous intelligent systems work. Some of these (such as flying an aeroplane) are well known and can achieve great precision; others (such as distinguishing a pedestrian in the road from a shadow, or an underground cable from a rock) are still at a relatively early stage of evolution. A key capability required of autonomous intelligent systems is that of planning, and re-planning in the face of incoming evidence and measurements, how to perform or complete a task. This requires some understanding of behaviours, both of one's own system (eg 'how does this aeroplane work?') and of one's environment (eg 'where will that storm go next?'). The methods currently used to represent and analyse these different aspects differ a lot in their mathematical forms. Our ambition is to develop methods, with a Bayesian basis, which will allow them all to be used with each other. 

Our project will investigate in detail how to integrate such technologies in a way that enables them all to work together. In addition to discovering how to do this, we will also train a number of researchers in the required techniques, spread the word to other academic researchers, and share our results with a number of companies who are trying to develop autonomous intelligent systems."
24,3135F0D9-7DCB-40BC-959D-AE76522181DC,Formal Representation and Proof for Cooperative Games: A Foundation for Complex Social Behaviour,"This research applies methods and tools from mathematical knowledge
management and theorem proving to theoretical economics, by working
with a class of cooperative games called pillage games. Pillage
games, introduced by Jordan in 2006, provide a formal way of
thinking about the ability of powerful coalitions to take resources
from less powerful ones. While their name suggests primitive,
violent interactions, pillage games are more applicable to advanced
democracies, in which coalitions seek to form governments to alter
the distribution of society's resources in their favour. If, for
some allocation of society's resources, the coalition preferring
another allocation is stronger than that preferring the status quo,
the other allocation `dominates' the status quo.

The most conceptually intriguing, and the most computationally
intractable solution concept for cooperative games is the `stable
set'. A stable set, has two features: no allocation in the set
dominates another; each allocation outside the set is dominated by an
allocation in the set. For pillage games with three agents under a few
additional conditions, we have determined when stable sets exist, that
they are unique and contain no more than 15 allocations, and how to
determine them for a given power function.

In this research, we first formally represent the mathematical
knowledge developed in Jordan's and our work using sTeX, a
mathematical knowledge management tool. This allows, e.g., automatic
identification of how various results depend on each other.

We then use two modern automated theorem provers (ATPs), Isabelle and
Theorema, to formally prove these results. Theorem proving is a hard
task and if not provided with domain specific knowledge ATPs have to
search through big search spaces in order to find proofs. To increase
their reasoning power, we shall seek to identify recurring patterns in
proofs, and extract proof tactics, reducing the interactions necessary
to prove the theorems interactively. As important results in pillage
games can be summarised in pseudo-algorithms, containing both
computational and non-computational steps, we shall study such
pseudo-algorithms, seeking to push them towards the much more
efficient computational steps. Finally, we shall use the identified
proof tactics to help the ATPs prove new results in order evaluate
their true value.

The research seeks to make a number of contributions. For theorem
proving, pillage games form a new set of challenge problems. As the
study of pillage games is new, and the canon of applicable knowledge
small, this gives an unprecedented opportunity to encode most of
it. The research will expand the tractable problem domain for ATPs;
and - by identifying successful tactics - increase both the efficiency
with which ATPs search for proofs, and - ideally - their ability to
establish new results.

For economics, this is the first major application of formal knowledge
management and theorem proving techniques. The few previous
applications of ATP to economics have formalised isolated results
without engaging economists and have thus largely gone unnoticed by
the discipline. As cooperative games are a known hard class of
economic problems, and pillage games known to be tractable, this
research therefore presents a strong `proof of concept' for the use of
ATP within economics. Cooperative game theory is formally similar
to graph theory, the techniques and insights developed may be
applicable to matching problems, network economics, operations
research, and combinatorial optimisation more generally.
Additionally, the researchers will introduce ATP techniques to the
leading PhD summer school in computational economics, and are working
in collaboration with economic theorists with strong computational
backgrounds. Thus, the research seeks to form a focal point for formal
knowledge management and theorem proving efforts in economics.",,"The project seeks impact primarily by engaging with the academic
computer science and economics communities. This will be done in
departmental seminars, workshops (e.g., UK Automated Reasoning
Workshop), tutorials (e.g., the ESSLLI summer school) and
conferences (e.g., CICM, and conferences by economics societies
such as the Society for Computational Economics). We have been
invited to present the work to the 2012 Initiative for
Computational Economics in Chicago, economics' principle training
ground in computational methods for PhD students.

Finished work will be submitted to internationally recognised
journals. Within computer science, these are journals such as the
Journal of Automated Reasoning, Artificial Intelligence, the
Journal of Applied Logic. Within economics, outlets include leading
theory journals such as the International Journal of Game Theory,
Games and Economic Behavior, Computational Economics and, for a
survey article, the Journal of Economic Literature.

Efforts will be made to communicate results to the public more
broadly, ideally through a popular science journal.

The project will also establish a website with three goals. First,
to disseminate work and results, including research reports,
encodings of the problems for various theorem provers, all code and
algorithms written. Second, the website will seek to facilitate the
growth of an ATP sub-community within economics, encouraging the
use and understanding of ATP and a search for fruitful application
domains. Initially, this will be organised around a wiki on key
theorems in economics. It will also host a discussion forum for the
discussion of approaches, problems, and solutions in formalised
economics. Finally, the website will be a key resource for formally
representing and proving knowledge related to theoretical
economics, whether by hosting material itself, or linking to it
elsewhere.

Research in mechanised theorem proving has applications beyond
academia. We shall initially explore applications formally close to
either our work or to previous work. As our work is formally similar
to matching problems, they are a natural application domain. Matching
problems include that of matching students to schools, organ donors to
recipients, and interns to hospitals.

Existing applications of mechanised theorem proving to economics have
focused on results in social choice. One of these, the
Gibbard-Satterthwaite theorem, is a central result in mechanism
design, which includes auction theory as a subfield. Auctions are now
a major method for allocating resources, whether via Google's online
ads, oil fields development, contract auctions, or procurement
auctions. Further, good design can make large differences to the
revenues generated: the top two European spectrum license auctions in
2000 earned over 30 times what the bottom one earned. Thus, mechanised
theorem proving could aid in the design of and participation in
auctions.

Another important way to achieve impact is by our internationally
leading project partners. They will not only advise us but also
actively support us in our aspiration and in the dissemination of
the results. We have an industrial advisor who will not only advise
us on the research side of the project but will also give valuable
contributions on how to apply the work outside academia. Our other
project partners are senior figures in computer science and
economics. They have strong expertise in all relevant aspects of
the project. We have leading figures who are opinion formers in
their respective fields of expertise: development of mathematical
knowledge representation formalisms, theorem proving systems,
maintaining a large collection of challenge problems on the
computer science side of the project; pillage games, rationality,
computational economics, developing software for solving
non-cooperative games on the economics side."
0,C0D75E5C-139A-4CAE-8752-BAD16097812E,RAnDMS (Real time Analysis of Digital Media Streams),"RAnDMS will study, implement and evaluate Real-time Data and Visual Analytic techniques to enable intelligence agencies, the MoD, the police and emergency responders to monitor and make sense of local, regional and global events using web-scale data from social and traditional media streams. The intelligence gathering task will be defined as identifying, correlating, integrating and presenting data and information, in order to understand situations as they arise. Current technology does not provide efficient and effective solutions, as it mainly focuses on detecting trends in the use of keywords and tags. While this is able to spot overall patterns in the data, it just enables the retrieval of relevant documents, without any correlation and integration of the contained information. Moreover, information concerning local situations and events, which may only be discussed within a handful of documents, is ignored.
Within RAnDMS data analytics will focus on enabling the capture of information from media streams; illuminating situations at all levels, from global to local. This information will support decision making for the intelligence community, which is expected to increase their ability to monitor events and situations relevant to homeland security and to peace-keeping efforts. The scientific challenge is that data and information in these streams are: (i) high in volume, and constantly increasing, (ii) often duplicated, incomplete, imprecise and incorrect; (iii) written in informal style (i.e. short, unedited and conversational); and (iv) generally concerning the short-term zeitgeist. These characteristics make analysis very hard, especially when considering that major requirements of the intelligence community are that (i) documents must be processed in real-time and (ii) the relevant information may be in the long-tail of the distribution, i.e. it may be mentioned very infrequently. 
We will provide highly efficient and effective technologies able to associate each document with its context. A documents context is provided by four dimensions: (who) the author of the document, (when) the time it was sent, (where) the location referred to in the document and (what) other documents with similar content. This information is either provided by the media stream or extracted from the document's content using efficient statistical text-mining techniques. By interpreting documents in terms of these four dimensions we enable: (i) the detection of events, i.e. documents and their content (what) are clustered around a time and place; (ii) the profiling of authors from the content (what and where) of the documents they have created; and (iii) determine information that is missing or ambiguous in document, using information present in the documents within their context.
Visual analytics will facilitate the exploration of the information by providing multiple views; enabling focused investigation and trend visualisations across the four dimensions. We will devise methods to (i) suggest the right level of detail (granularity) for the user focus in rapidly changing environments; (ii) alert users to any significant development outside of their current viewpoint; and (iii) enable users to understand how the current state of affairs came into being by browsing along the all information along the time dimension. Methods will en able to see through the irrelevant banter (noise) that often surround events in social media and go directly to the relevant information that can be hidden in the long tail of the distribution. 
RAnDMS will be tested on the task of supporting intelligence operators during relevant events happening during 2012/13. We will publish the research and its findings in international journal and conferences. Subject to MoD agreement, we will also create public research resources by generating one publicly available task (inclusive of corpora, resources, etc.) to enable comparison of research results by other researchers.",,"The project will have an impact on industry, the government and society. 

The main target of dissemination and exploitation will be companies working in the fields of intelligence/anti-terrorism, such as Ultra Electronics (one of the major providers of the UK government) both in the UK and abroad. These companies tend to have a traditional focus and are not currently working on solutions involving the use of the social Web to help make sense of events. This is despite the large interest created at an international level by events such as the Arab Spring and Hurricane Katrina and the large interest in the intelligence and defence sector, as demonstrated by some recent NATO reports, especially of US origin. 

We will build on technology and know-how developed as part of the WeKnowIt and TRIDS projects, moving towards a level of complexity that currently is not possible to reach with that technology, as some fundamental research problems must be addressed. We will develop the science and transfer some intermediate results on top of the baseline technology. Moreover, we will create demonstrators of capabilities that are made possible by the research but that will need more investments (e.g. via TSB or MoD funded projects) to create actual applications. 

It is expected that the technologies will be industrialised and applied in subsequent projects through collaboration with companies. Exemplars of datasets will be made available for benchmarking and evolving technology in this field and for future experiments. We have a long and successful experience in collaborating with industry and we expect this to continue with this project.

As for the government, the science and technology developed in the project will provide a feasibility study in the use of social Web monitoring for intelligence. The technology developed in the project will enable a deep insight into crises and emergencies nationally and abroad. Recent events, both local (e.g. the Aug 2011 UK riots), but especially abroad (e.g. during the Arab Spring) have shown that Western governments struggled to make sense of the events as they were missing direct information from the ground. The technology proposed in this project will enable harvesting of information directly from the ground, via the monitoring of social media and news feeds. We plan to test the developed technology on corpora collected during emergencies and crises which develop during 2012. This will show the way to how future emergencies and crises could be managed, hence creating an interest in the government on the exploitation of the results. DSTL will be key to this plan: We plan to spend several weeks at DSTL to derive requirements, to co-create the science and the technology and finally to test the results in real world scenarios with real users. The time frame for exploitation of results will be 1-5 years after the project end, when products based on the developed technology will be available. 

Finally citizens and society will benefit from the results of the project in an indirect way through improved security nationally and internationally. More directly, the ability to gain intelligence through the use and analysis of social media may be reused by the ordinary citizen in, for example, education and to encourage civic engagement."
1,6A47FF2B-EC16-4814-90A2-2D46AB7DE8F3,Rogue Virtual Machine Identification in DaISy Clouds,"Our work in this proposal focuses primarily on the safe and secure cloud computing challenge of the EPSRC DaISy call. In addition, it also addresses closely issues relevant to the extracting meaningful information challenge, specifically extracting meaning from large-scale monitoring information collected in a cloud computing environment, and the ensuring confidence in collaborative working challenge by developing meta-data and methods that enable users to monitor how their digital assets are being used in shared environments. 

The proposal builds on the investigators' expertise in building secure cloud computing systems (especially the IC-Cloud system), developing large-scale data analysis systems and developing algorithms and methods for the security analysis of program behaviour to develop develop and evaluate novel methods to detect subtle attacks by adversaries who have already gained access to a VM within a secure cloud system.

Our general methodology for this 1-year project is based on 1) Building on the existing IC-Cloud platform as a test-bed our research. The use of the in-house infrastructure based on the popular XEN Hypervisor enables us to rapidly develop and evaluate monitoring tools, to develop and test different attack scenarios and collect the log data from the real applications. 2) Building on our in-house repertoire of data mining and analysis tools developed over the years for classification, clustering and association analysis and on our recent expertise developed in real-time data mining methods and Bayesian analysis frameworks for modeling and analysis anomalies in large scale sensor data for environmental and security applications. 3) Close collaboration with partners and collaborators of the Institute and Security Science and Technology to receive ongoing feedback on our methodology, results and methods as we develop them.",,Improved security and trust in Cloud computing
2,D3D929E2-50FA-4155-9331-42620B0883CC,Game Theoretic Privacy-Preserving Collaborative Data Mining,"This proposal focuses on the problem setting where coalition parties, each owning a large set of data, desire to discover new knowledge when they collaborate to jointly process all the datasets; while ensuring that each individual dataset is not revealed to the other parties. 

Solutions to this problem are key enablers for ensuring smooth cooperation among parties who do not necessarily trust each other fully. Example situations that reflect this need include coalition forces on military or peace-keeping missions, nations joining forces to detect and prevent terrorism activities, while not willing to reveal their actual intelligence data on national security, and organisations collaboratively analysing consumer behaviour while keeping their customers' profiles private. 

The proposed research addresses challenges within the three themes of the DaISy Call, notably those aimed at secure and privacy-preserving collaborative extraction of meaning from data intensive systems comprising different parts of data owned by adaptively changing coalition partners.

We aim to develop novel techniques with guarantees of privacy that will perform data mining even when data are in encrypted form, including the specific tasks of clustering, dependency modelling, classification, regression, and association. The behaviour of such coalition parties involved in performing joint data mining will be analysed using a game theoretic framework, and various innovative collaborative data mining techniques will be developed using this framework while ensuring that privacy is preserved, even when some coalition members may collude. Our techniques will also be designed to be adaptive to and efficient when coalition membership changes.",,"The proposed project's results will provide solutions to the challenging problem of how to perform data mining jointly on different datasets and involving coalition parties each of which has its own interests and thus may behave selfishly, deviating from normal behaviour and colluding with a subset of other parties.

The main potential beneficiaries are the defence industry and the UK government, as well as the data mining and cloud services industry, and society at large. 
It is expected that the impacts described herein will reach the stakeholder groups within 5 to 10 years after the proposed project completion; essentially because major technical results will be published and codes put into the public domain, and the techniques we develop can be implemented in software, and thus do not require additional hardware functionalities to be built into existing deployments of data-intensive systems.

The immediate beneficiary is the Ministry of Defence (MoD) in the military sector and the defence prime contractors. Indeed, our developed game theoretic collaborative data mining techniques can be deployed for strategic and tactical military operations involving different military units of varying levels of trust and even between the UK MoD and military forces of other nations when they are engaged in a coalition for international peace-keeping missions. In such a setting, the MoD can contribute its share of security sensitive military data to perform joint data mining operations with other coalition nations without risking leakage of these data to coalition members. Game theoretic collaborative data mining can unlock the potential for cooperation even among distrusting parties who may share a common goal achievable through joint collaborative data mining operations, but who are unwilling to fully disclose their own datasets to other parties as these are viewed as competitors or may leave the coalition in the future. 

For similar reasons, law enforcement and national security organisations have a stake in the results developed from our research. The reason is that the developed techniques will facilitate collaborative extraction of clusters, trends, detection of undesired behaviour and profiling of individuals and events relevant to the interests of national and public safety such as terrorist and criminal activities, while keeping each party's datasets private.

Industries should position themselves for addressing the challenges posed by the increasingly widespread deployment of data outsourcing, clouds and the need for dynamic collaborative operations. In the longer term, our techniques can be applied within the UK government and commercial sectors. More precisely, large sets of data in these sectors are increasingly being hosted in outsourced locations and within the cloud infrastructure. With our techniques, these can be mined even when data are concealed or encrypted independently by different owning parties. Such a feature is ideal if any party is to ever rest assured in the idea of its private data being outsourced.

The fact that data mining can still be performed despite data outsourcing and concealment is expected to lead to the potential market success of industries which offer the services of data outsourcing and cloud computing as this means customers are more willing to subscribe to these services since data can be concealed while being hosted without sacrificing the utility for data mining purposes.

In summary, the above described impacts will offer new business opportunities for wealth creation and maintain the UK's scientific and technological competitiveness in collaborative data mining, secure data outsourcing and cloud services."
3,E2417148-51EA-4AD4-B352-DC5F1059DD22,Reconfigurable Autonomy,"As computational and engineering applications become more sophisticated, the need for autonomous systems that can act intelligently without direct human intervention increases. Yet the autonomous control at the heart of many such systems is often ad-hoc and opaque. Since the cost of failure in critical systems is high, a more reliable, understandable and consistent approach is needed. Thus, in this project we aim to provide a rational agent architecture that controls autonomous decision-making, is re-usable and generic, and can be configured for many different autonomous platforms. In partnership with the industrial collaborators we aim to show how such &quot;reconfigurable autonomy&quot; can be achieved in relevant applications.",,"Regardless of any industrial sponsorship for this Call, there is a clear industrial need for this technology. Industries involved in developing robotics, autonomous vehicles, or remote exploration systems will be interested in our developments. Yet the generic architecture potentially extends beyond these, more obvious, areas to any autonomous system that must intelligently deal with a stochastic, continuous environment. This then can cover very many software, embedded, pervasive, or autonomic systems. So, to industry, the proposed approach (if demonstrated successfully) can help to reduce future autonomy software and system development costs (including speeding up the development process). 

The development of a generic, yet reconfigurable, core for autonomous systems will bring many potential benefits to academic and industrial researchers:

 * improved reliability;
 * cost-effective re-use;
 * applicability of solutions to wider areas;
 * improved clarity and user interaction; and, potentially,
 * reduced vulnerabilities.

Clearly, we will collaborate with academics and the industrial sponsors of this Call in order to explore some of these advantages. Through our wider network of industrial collaborators, we will also target more general exploitation within the broad area of autonomous systems."
4,1EFC2506-8881-4A70-90D9-B70B5BB13942,EMOTIVE - Extracting the Meaning Of Terse Information in a geo-Visualisation of Emotion,"The ability for ordinary people to express and exchange their opinions and feelings has increased beyond all expectations in the past ten years of internet expansion and availability. To the military and national security agencies this has provided both opportunities and challenges. Opportunities have emerged in the sense of readily available awareness of discontent and oppositional movements and initiatives. Recent urban disturbances have illustrated the key role played by social networks in the fast-moving events of Summer 2011. The challenges have escalated due to the sheer number of sources of social interaction and public communication media. This research addresses some of these issues in a bold initiative to combine well established and considered science with the increasingly familiar tools of Web 2.0.

Four of the most popular sources of the public exchange of ideas (email, social networks, such as Facebook, microblogs, such as Twitter and comments to newspaper editorials and high-profile stories) will be selectively monitored. Sensitive words and phrases which may be of concern to the military and national security agencies will be extracted by extending a Natural Language Processing technique already developed for email by the Principal Investigator. The team will develop an ontology (a rule-based linguistic database) in which the extracted words and phrases will be semantically filtered and restricted to a manageable set of agreed terms. An example of how the ontology will work can be illustrated by suggesting the number of ways the word 'looting' might be expressed in, for example, established vocabulary (raiding, pillaging, ransacking, etc.) as well as in urban and regional street language and text speak ( doin' over, scamming, etc.). The ontology will be trained to recognise the words and phrases, make semantic links between them and deliver one or more accepted descriptors to the analysts. 

EMOTIVE will monitor the traffic of sensitive words and phrases filtered through the ontology when applied to specific incidents, individuals and groups. Increased activity will be indicated by frequency of occurrence or severity, which will be presented through a concept cloud which uses the size of words as a metaphor for frequency and hence importance.

Further to this, a second ontology will be created in which words and phrases that express emotion will be harnessed and this ontology will process the emotionally charged words and phrases extracted from the four sources described above in a similar way to the first

The output of both ontologies will be linked, so that the monitoring analyst will be presented with a colour-coded indication of the strength of emotion attached to the language-based terms.
The final feature in Emotive will be a geo interface to point to the location of the emotionally charged traffic. This interface will be refreshed every 60 seconds with the effect of helping to identify sensitive hot spots of communication and activities. Outputs from the system consisting of effectively presented new knowledge will enable defence and national security agencies both to predict and monitor selected events as they develop and will assist in the formulation of policy.

It can be argued that the general public will be direct beneficiaries of this research in that the defence and national security agencies who act as guardians of public safety and order will be further equipped by this tool to identify, evaluate and ultimately safeguard the public from potentially harmful events.

Defence and national agencies will already be experienced at monitoring these data sources but this tool adds an extra filter of analysis, it will work in almost real time, will amalgamate data from several sources if desired and will provide harmonised output.",,"This research will contribute primarily to defence/national security. It offers a means to identify and analyse communication traffic in the context of rapidly moving events. It is now the norm for critical events to be planned, discussed and even conducted via social networking sites. Monitoring staff will be enabled to identify increasing communications on a topic not only by frequency measures but also by interpreting the strength of feeling via the emotional charge in the exchanges, thus adding a powerful and more holistic aspect to the analysis.

Outputs from the system consisting of effectively presented new knowledge will enable defence and national security agencies both to predict and monitor selected events as they develop and will assist in the formulation of policy.

These agencies are already be experienced at monitoring such data sources but this tool adds an extra filter of analysis, it will work in almost real time, will amalgamate data from several sources if desired and provides harmonised output.

The research has the potential for wider impact subject to the agreement of the primary funders. The demonstrator developed could have an important impact in the area of law and order and policing. Applications of the research can be envisaged with rapidly unfolding events such as the Olympic Games, Occupy London or the recent civil disturbances, where the scrutiny of social networks was crucial. The PI's strong research partnerships with the police can contribute to further collaborative initiatives.

Local authorities and government departments could use the tool to observe, collect and act upon customer comments on their websites and other web presences. Both positive and negative indicators will be readily available from the concept cloud.

State funded cultural organisations similarly rely on data to evaluate community engagement through the participation of users of their web and social network presence. Although the real time feature of the tool would not be essential in this context, there is the potential to tailor response to user reaction linked to high profile exhibitions, e.g., the need to increase the number of late opening based on demand.
Similar impact can be envisioned in the commercial sector where there is a vital need to understand, collect and interpret customer reaction to products and services. Most small, medium and global companies have extensive social network presence for marketing and customer engagement.

It can be argued that the general public will be direct beneficiaries of this research in that the defence and national security agencies who act as guardians of public safety and order will be further equipped by this tool to identify, evaluate and ultimately safeguard the public from potentially harmful events."
5,4FC476EC-7C21-4ECD-8415-CF50330DF43B,Real-Time Detection of Violence and Extremism from Social Media,"The explosive use of social media tools in recent years has turned them into a double-edged sword. On one hand, social media is viewed as a positive factor in Middle East revolutions. On the other hand, violence events such as the UK riots occurred in August this year appeared to be driven by the use of social media. The proposed project represents a timely development of intelligent systems for addressing the emerging defense and security issues arising from social media.

The proposed research describes a new approach to detecting trends of violent radicalization and extremism from social media. In particular, it proposes a Bayesian modeling approach which detects violence contents from social media without the use of any labelled data. Words indicating violence, anger, hate, racism, etc. are naturally incorporated as prior knowledge into the model learning process. Efficient online parameter updating and parallel data processing procedures will be investigated. This proposal falls into the area of &quot;Extracting meaningful information&quot; listed in the original call for proposals. It particularly aims to tackle the technical challenge of real-time processing of large-scale social media data for early detection of violent extremism from text. The results of the research are potentially very important to society as they aim to enable the deployment of the forces of law to prevent violent events.",,"This research will have a great impact on enhancing the national security capability by enabling real-time detection of potential threats such as violence and extremism from social media. Online social networking and micro-blogging service have enabled users to share and diffuse information to a tremendous number of audiences within very short time. While such a new form of service has proven to have major social-economic benefits, it is also at the same time under the risk of being used for violent and criminal activities. 

This project therefore aims to develop efficient computational tools for detecting violent radicalization and extremism from social media, which will ultimately help improving the national security capability with the online monitoring function offered by the system. Specifically, the tools seek to detect and extract topics relating to violent and criminal activity from large-scale social media data in real-time, and constantly track any events that are identified suspicious. Owing to the fast-evolving nature of social media, such a system will be potentially very important for the forces of law to respond to and deal with the potential security risks timely which will consequently help improve public security.

Although targeting the defense and security sector, the developed tools and algorithms of the project can be applied to other public and private sectors, and be easily transferred into commercial use. For public sectors, it is possible to use the tools to analyse citizen's opinions on hot issues such as welfare reforms, Eurozone crisis, new policies introduced by the government, etc. In the ongoing &quot;Occupy London&quot; movement, our tools can be used to monitor what people are tweeting about and their sentiment and debates towards their most concerned issues. 

For private sectors, the tools can be used to detect and monitor the trends of topics being discussed about products or services, allowing business organizations to gauge insights into the potential changes in the market opinions and hence to adapt their business strategies accordingly. As the social media data published by individuals have strong correlations with their behaviors, deploying the tools for individual data analysis can help to address social issues such as preventing adolescent suicide by recognising the suicidal thoughts expressed in social media. 

We plan to disseminate our research findings in several ways. First, information about the scientific work will be published in a regularly updated project website. A dedicated online forum will be provided for engaging with interested parties including the public as well as potential researchers and collaborators (academic and non academic). Research results will also be disseminated at public engagement events such as the networking events with potential business partners. Such events help in building connections with industry and mapping the developed technologies with industrial R&amp;D needs."
6,8B02F2AB-F5C6-4BF6-AC7B-74255FA58317,Interpreting and integrating mismatched data on the fly,"This project is concerned with the problem of interpreting information and requests which are received during communication: for example, during an emergency response event. If the data sources of the sender of the information/request and the data source of the receiver are not fully compatible (which is highly likely even within an organisation and almost certain between different organisations) then the received information or request will not be understood by the receiver. Our matching system is designed to explore the receiver's data source to find one or more terms which appear to be similar to the received term, in order to allow the receiver to interpret this term with respect to its own data. The implications of this match (for example, that a more general term is being used, that certain attributes of a term are being ignored or introduced, and so on) are explained to a user representing the receiver organisation, who can then approve the proposed match, and interaction will proceed as if the two different terms were equivalent.

The matching system is local to every participant that wishes to make use of it and so it is only able to access the data of the organisation which is using it. This therefore avoids privacy and security issues by ensuring that only data that has been approved for sharing in the current context can be accessed by others.

The aims of the project are:
- to investigate the kinds of mismatches that have been encountered in real emergency response scenarios and within real data sources, which will be provided by our collaborators (in Dstl, the Met Office, Scottish Resilience and Strathclyde FireRscue).
- to implement a system which will apply techniques based on existing matching techniques to these data sources, to enable appropriate interpretation of unknown data, and to provide feedback to users so that they can approve these matches (or in situations where the consequences of errors is low, these can proceed automatically without user approval).
- to evaluate the system against real data, with feedback from expert users as to how useful and appropriate the suggested matches are.

This project will be exploratory and is designed to evaluate the potential of the approach. There are several factors which should be addressed before the system is ready to be used in the field that are outside the scope of this project, specifically:
 - Carry out further analysis of data, and of mismatches that have actually occurred in emergency response situations. Performing this will be a key feature of this project, but this can only be an initial study;
 - Design and implement new matching techniques to address any possible mismatches which are not addressed by those currently implemented. Depending on the complexity of the data, it is not anticipated that it will be possible to cover every possible mismatch within this project;
 - Provide a carefully designed web interface for user interaction with the system. This would require specialist design advice and will not be attempted during the project; instead, a simple interface will be provided.

The key output of this project will be an understanding of the potential of this approach, based on the evaluation of the implementation, and a roadmap for how to develop this prototype work into a system which can be used in the field.",,"The ability to share information quickly and efficiently is essential for real time communication and collaboration. This need is especially obvious in situations such as emergency response, where time is often critical and where many different organisation with data that is likely to be incompatible need to communicate to provide an effective response. The system we are proposing to develop will provide a way to enable this communication, by automatically matching unknown terms (from someone else's data source) to terms they appear to approximate (within one's own data source), and provide effective feedback to users that allows them to understand the impact of using
this match. The system will be able to interpret data which is mismatched both semantically (by which we mean that different words which mean more or less the same thing are used) and structurally (where similar terms have different attributes) and will therefore be usable when complex information is being shared between participants who may or may not have interacted previously.

Because this matching is done locally - i.e., every participant can have the matching system installed on their own devices - there is no problem with security. A participant will receive information and/or a request to provide information, and, using the matching system, they will be able to assimilate this information or provide an appropriate response even if the information or request is phrased in terms which are not compatible with their data. There is no requirement to allow others to access their data.

There are several key areas of impact for this project. 

- A set of matching techniques and a prototype tool. 
Beneficiaries: both key responders who interact with one another frequently (including our collaborators: Dstl, the Met Office, Scottish Resilience and Strathclyde Fire &amp; Rescue) and anyone who may become involved in a disaster response. Additionally, it could easily be adapted to those interested in automated interaction in different situations. 
The prototype we create will enable sharing of data between separate groups and individuals. The tool will be open source and available to all. It will have been developed through interaction with end-users so that it will be suitable for their needs. 

- Enabling wider participation. 
Beneficiaries: anyone potentially involved in the response, such as non-local emergency responders, other public and private services, local and national businesses and the general public.
It will be straightforward for all such responders to share information and for this information to be quickly integrated into existing plans. The need to facilitate such interactions was strongly emphasised in the Pitt report.

- Improved resilience in aftermath of disaster. 
Beneficiaries: anyone potentially affected, socially or economically, by a major natural disaster such as flooding.
An increased ability to respond to the requirements of the situation will result in damage prevention and limitation. 

- Continued development of a community that spans academia, industry and government, where those that deal with different aspects of these problems can interact and share ideas.
Beneficiaries: academics in these fields; on-the-ground emergency-responders, interested participants in industry and government.
Academic techniques will become available to these participants and they will be educated about the possibilities being developed. Equally, it provides academics with a clearer understanding of the requirements of on-the-ground responders, and in industry and government in general, and provides an excellent test-bed for these academic techniques, so large-scale, real-world evaluation is possible (often very difficult for techniques such as ontology matching)."
7,5AC7C926-8437-4DA0-B591-B2C659B90E33,"Testing, Verifying, and Generating Software Patches Using Dynamic Symbolic Execution","A large fraction of the costs of developing and maintaining software is associated with detecting and fixing software errors. As a result, the last decade has seen a sustained research effort directed toward designing and developing techniques for automatically detecting software errors, with some of these techniques making their way into commercial and open-source tools. However, detecting an error is only the first step toward fixing it. In fact, many known errors remain unpatched due to the high cost required to diagnose and repair them, combined with the fear that patches are more likely to introduce failures compared to other types of code changes.

The goal of this research project is to address both of these problems, by devising novel techniques based on dynamic symbolic execution for:
(1) automatically testing and verifying the correctness of software patches, and
(2) (semi-)automatically generating candidate patches for software bugs.

The strength of dynamic symbolic execution lies in its ability to precisely model the behaviour of program paths using mathematical constraints. However, the cost associated with this level of precision is poor scalability. The number of paths in a program is usually exponential in the number of branches, which makes it difficult to scale the analysis to very large programs. However, by focusing the analysis on the incremental changes introduced by program patches, we hope to significantly reduce the cost of symbolic execution and significantly increase its applicability in practice. Furthermore, the ability to check software patches opens up the possibility of performing patch generation in an automatic or semi-automatic fashion. In particular, starting from the mathematical constraints gathered from a buggy execution path -- and with the potential addition of a manually-written patch template -- we plan to design techniques for generating a set of candidate patches resembling the ones that would be generated manually by developers.",,"Key Non-Academic Beneficiaries: The primary non-academic beneficiaries of the proposed research are software companies and the open-source software community. It is well known that software bugs have a large negative impact on the global economy. For example, a report published by the US National Institute of Standards and Technology (NIST) in 2002 has found that software bugs cost annually the US economy an estimated $60 billion dollars or 0.6 percent of the gross domestic product [G. Tassey, The economic impacts of inadequate infrastructure for software testing, NIST 2002], and a similar impact is to be expected in other developed countries like the UK. A large fraction of this cost is associated with writing patches---either to fix existing bugs or to add new features---which is one of the critical software development and maintenance activities. This research project plans to investigate mechanisms for improving the quality of patches and for (partially) automating their development, which on the long term might bring significant economic benefits.

In addition to the commercial and open-source software development communities---who will directly benefit from improving the software patching process---the resulting increase in software quality will positively affect software users across a wide variety of industrial sectors. In particular, the NIST study cited above mentions the financial services and the automotive and aerospace industry as major sectors affected by software errors.

Mechanisms for Exploitation and Application: Our goal is to disseminate the results of this research from an early stage through a variety of mechanisms: (1) by applying it to open-source and commercial code to demonstrate the effectiveness of our approach; (2) by open-sourcing our initial prototype to be used directly by software developers on their code; and (3) by communicating the results of our research to the non-academic user community.

On the short-term, we will apply our technique to widely-used open-source software, and report any errors found to developers. Furthermore, we will send automatically generated patches to developers, use the feedback they provide response to these patches will be used to improve our techniques and associated prototype.

After the initial evaluation on open-source software, we plan to apply our approach to commercial code. We have already discussed our technique with Maxeler Technologies, who are interested in applying our initial prototype to their software (please see the attached letter of support). We also intend to seek further contacts with the software industry, in order to better assess the applicability of our technique, and in particular its integration into the development process.

One of the key ways in which we plan to disseminate our results to the user community is by releasing our initial prototype under an open-source license. This would allow developers from both the commercial and open-source communities to directly apply our technique to their code. Our previous experience releasing our tool KLEE (http://klee.llvm.org) as open-source software should prove beneficial in making sure that our prototype reaches a wide audience. (Since being open-sourced in June 2009, KLEE has been used and extended by a variety of users, with some of these extensions being contributed back to the main branch.)

Communicating the results of our research to a non-academic audience will play an important role in our dissemination strategy. In conjunction with open-sourcing our prototype system, we plan to set up a dedicated website for the project to introduce our research to a general audience. In a similar spirit, we plan to integrate our research into the PI's teaching activities, through group and individual projects offered to undergraduate and Master's students at Imperial College."
8,901987B1-21D8-4D34-BA61-F62C621FBC97,FISH: Fast Semantic Nearest Neighbour Search,"This proposal addresses Theme 1 of the Data Intensive Systems (DaISy) Call &quot;Extracting meaningful information: Deriving meaning from large, heterogeneous, incomplete, contradictory, noisy and dispersed data sets with many different forms and formats (e.g. text, image and sound files)&quot;. The project will research and develop novel fast semantic nearest neighbour search (FISH) data structure, algorithm and software utilities for rapidly discovering semantically similar neighbours of complex data objects and automatically assigning semantic class labels to them. It will use large scale, high dimensional, heterogeneous, incomplete, and noisy internet image labelling datasets as Case Study for technology development and evaluation. The FISH solutions and utilities can be directly applied to automatically label security surveillance videos and defence reconnaissance imageries to extract semantic meanings to facilitate security and defence intelligence gathering and analysis.",,"Defence reconnaissance systems such as the RAF's RAPTOR contain imaging sensors (visible and infrared) which can acquire huge volumes of high resolution imageries of ground targets. Rapidly and accurately interpreting the images, identifying and recognizing ground objects and assessing the battle-field situations will be very important. Post-battle analysis of these imageries will be also valuable for future operations. Manually analysing these large volumes of high resolution imageries can be both difficult and labour intensive. Automatic image analysis and interpretation will play a very important role in making good use of defence reconnaissance data such as those acquired by the RAF's RAPTOR. The research results of this project can be useful for these purposes. 

The FISH algorithms and solutions can be applied to automatic labelling of the images and videos returned by RAPTOR. There are at least two scenarios where FISH can be useful. It can be implemented onboard the aircraft interpreting and labelling the images in real-time thus helping the pilots assessing the battle-field situations and react to them more rapidly. It can also be implemented in ground stations for post battle analysis and for archiving - by automatically labelling the images of different situations can facilitate search and retrieval of specific and relevant situations. 

There are an estimated 1.85 million surveillance video cameras in the UK constantly collecting billions of image footages which could contain vital security information for antiterrorism, for preventing crime and for protecting human lives. For such huge volumes of data, it will be impossible to interpret them manually. Automatic tools will be valuable and FISH can help develop such tools. 

The FISH algorithms and solutions can be used to automatically label security surveillance videos, identify dangerous and emergency situations quickly thus helping security authorities to gather security intelligence and react to them fast. Such techniques can be employed by the police and other security authorities for real-time situation monitoring or for post-event analysis - semantically labelled video frames can greatly facilitate the retrieval of specific and relevant contents. 

For defence, the FISH technology can help extract meanings from imageries acquired by reconnaissance systems such as RAF's RAPTOR thus facilitating defence intelligence gathering. For security, the FISH technology can help extract meanings from security videos thus helping gathering security information which may be valuable for antiterrorism, preventing crime and saving lives. 

In summary, the research of the FISH project can contribute state of the art technology for improving defence and security intelligence gathering and analysis which will lead to improvement in the UK's defence and security capability. 

Beyond defence and security, the research of the FISH project will have wider applications in academic research, and in applications fields ranging from biomedical to multimedia and the internet. 

In biomedical research, e.g., disease classification in medical images and general biomedical data classification and analysis such as matching DNA sequences involve finding similar data objects, the FISH technology can have a direct applications in these areas."
9,F55B234F-00C6-4238-A0C1-6CBDB7802431,DAASE: Dynamic Adaptive Automated Software Engineering,"Current software development processes are expensive, laborious and error prone. They achieve adaptivity at only a glacial pace, largely through enormous human effort, forcing highly skilled engineers to waste significant time adapting many tedious implementation details. Often, the resulting software is equally inflexible, forcing users to also rely on their innate human adaptivity to find &quot;workarounds&quot;. As the letters of support from the DAASE industrial partners demonstrate, this creates a pressing need for greater automation and adaptivity.


Suppose we automate large parts of the development process using computational search. Requirements engineering, project planning and testing now become unified into a single automated activity. As requirements change, the project plans and associated tests are adapted to best suit the changes. Now suppose we further embed this adaptivity within the software product itself. Smaller changes to the operating environment can now be handled automatically. Feedback from the operating environment to the development process will also speed adaption of both the software product and process to much larger changes that cannot be handled by such in-situ adaptation.


This is the new approach to software engineering DAASE seeks to create. It places computational search at the heart of the processes and products it creates and embeds adaptivity into both. DAASE will also create an array of new processes, methods, techniques and tools for a new kind of software engineering, radically transforming the theory and practice of software engineering. DAASE will develop a hyper-heuristic approach to adaptive automation. A hyper-heuristic is a methodology for selecting or generating heuristics. Most heuristic methods in the literature operate on a search space of potential solutions to a particular problem. However, a hyper-heuristic operates on a search space of heuristics. 

We do not underestimate the challenges this research agenda poses. However, we believe we have the team, partners and programme plan that will achieve the ambitious aim. DAASE integrates two teams of researchers from the Operational Research and Search Based Software Engineering communities. Both groups of researchers are widely regarded as world leading, having pioneered the fields of Hyper-Heuristics and Search Based Software Engineering (SBSE); the two key fields that DAASE brings together.",,"Training: The experience that our (many) early career researchers will obtain will be highly valuable and transferable: optimisation is a generic concern, widely applicable to many engineering problems (not merely those found in software engineering). DAASE will facilitate a wide-ranging and comprehensive programme of research training that will encompass exposure to different research communities and a wide range of different industrial environments. We plan internal research training, industrial secondments, international outreach visits and cross-site collaboration to maximise DAASE's potential to offer our team the best possible opportunities to develop their career, skills and expertise.

General Public: We plan to reach the public through direct engagement activities, print and broadcast media and by creating smart phone and social media apps that inform and explain. This activity will be headed up by Drs. Sue Black and Peter Bentley, both of whom have outstanding track records of media engagement and advocacy of science and engineering. The section on `Advocacy for Engineering and the Physical Sciences' in the case for support explains this in more detail. 

SMEs: DAASE has the potential to directly impact hundreds of SMEs. This is why UCL has committed to appoint (and fund) the full-time DAASE Business Development Manager to ensure that the project collaborates with an order of magnitude more SMEs than could otherwise be possible.

DAASE will benefit from significant and sustained support and interaction with a large set of industrial partners, including, 
IBM and Microsoft (representing the software industry), BT, Ericsson and Motorola (representing the telecoms sector) and ABB, Berner&amp;Mattner and Honda (representing the manufacturing sector). We also set out more detailed plans in the &quot;Pathways to Impact&quot; document, explaining how we shall use these DAASE partners to experiments with our ideas, evaluate new techniques, pose industrial challenges and to provide support and consultancy concerning the industrial application of DAASE techniques. Through this interaction with leaders in the field DAASE will seek to impact the immediate industrial partners. The Business Development Manager will ensure that DAASE also reaches out to and impacts a far wider pool of SMEs."
10,C63452D0-CE20-4836-9932-8CC055719EA7,SOCIAM: The Theory and Practice of Social Machines,"SOCIAM - Social Machines - will research into pioneering methods of supporting purposeful human interaction on the World Wide Web, of the kind exemplified by phenomena such as Wikipedia and Galaxy Zoo. These collaborations are empowering, as communities identify and solve their own problems, harnessing their commitment, local knowledge and embedded skills, without having to rely on remote experts or governments.

Such interaction is characterised by a new kind of emergent, collective problem solving, in which we see (i) problems solved by very large scale human participation via the Web, (ii) access to, or the ability to generate, large amounts of relevant data using open data standards, (iii) confidence in the quality of the data and (iv) intuitive interfaces.

&quot;Machines&quot; used to be programmed by programmers and used by users. The Web, and the massive participation in it, has dissolved this boundary: we now see configurations of people interacting with content and each other, typified by social web sites. Rather than dividing between the human and machine parts of the collaboration (as computer science has traditionally done), we should draw a line around them and treat each such assembly as a machine in its own right comprising digital and human components - a Social Machine. This crucial transition in thinking acknowledges the reality of today's sociotechnical systems. This view is of an ecosystem not of humans and computers but of co-evolving Social Machines.

The ambition of SOCIAM is to enable us to build social machines that solve the routine tasks of daily life as well as the emergencies. Its aim is to develop the theory and practice so that we can create the next generation of decentralised, data intensive, social machines. Understanding the attributes of the current generation of successful social machines will help us build the next.

The research undertakes four necessary tasks. First, we need to discover how social computing can emerge given that society has to undertake much of the burden of identifying problems, designing solutions and dealing with the complexity of the problem solving. Online scaleable algorithms need to be put to the service of the users. This leads us to the second task, providing seamless access to a Web of Data including user generated data. Third, we need to understand how to make social machines accountable and to build the trust essential to their operation. Fourth, we need to design the interactions between all elements of social machines: between machine and human, between humans mediated by machines, and between machines, humans and the data they use and generate. SOCIAM's work will be empirically grounded by a Social Machines Observatory to track, monitor and classify existing social machines and new ones as they evolve, and act as an early warning facility for disruptive new social machines.

These lines of interlinked research will initially be tested and evaluated in the context of real-world applications in health, transport, policing and the drive towards open data cities (where all public data across an urban area is linked together) in collaboration with SOCIAM's partners. Putting research ideas into the field to encounter unvarnished reality provides a check as to their utility and durability. For example the Open City application will seek to harness citywide participation in shared problems (e.g. with health, transport and policing) exploiting common open data resources. 

SOCIAM will undertake a breadth of integrated research, engaging with real application contexts, including the use of our observatory for longitudinal studies, to provide cutting edge theory and practice for social computation and social machines. It will support fundamental research; the creation of a multidisciplinary team; collaboration with industry and government in realization of the research; promote growth and innovation - most importantly - impact in changing the direction of ICT.",,"The proposed programme will have beneficial impact on a wide range of stakeholders. Via technology transfer, companies will gain access to new technologues, and also gain the understanding that will allow them to develop new products for communities organising themselves in social machines. Those companies that partner us or support our research will of course have the ability to feed ideas into the research, and frame the problems we are trying to solve; we consider it essential that fundamental research feeds into, and back from, real-world applications.

Smaller-scale entrepreneurs will have new outlets for innovation, and new opportunities to develop radical business models. The public sector and third sector will have available new tools and methods for achieving policy ends. Communities using social machines will also benefit, of course, by the ability to identify and define their own problems, and develop their own solutions. These benefits, in social cohesion and cooperation, will often outlive the immediate issue which drove the development of the social machine.

We should not forget the benefits to the wider academic community of the proposed research. Of course, the development of a community of multi-disciplinary researchers in social machines will benefit the computer science field, but via the observatory and the strong social relevance of the research, we would expect a wide academic community in science and social science benefiting from the deepening of expertise in this area, and the large quantity of data. The 5-year programme would allow a strong multi-disciplinary cohort of researchers to emerge, able to influence a range of fields, spreading expertise in these relatively novel methods of social collaboration. Dissemination will also take place via our programme of Town Meetings, sandpits, hackathons, disruptive skills workshops, etc. Groups associated with the consortium, such as the Web Science Trust, will be able to ensure that SOCIAM's work is widely disseminated and one of our Partners is the world's largest Technical PR Agency.

The impacts will be both economic and non-economic. The economic impacts will be the benefits that come from innovation and cooperation, and from bottom-up solutions to problems. These will include both lowering costs of social problems (e.g. via community policing lowering the costs of crime), and creating opportunities for innovation and commercial exploitation of innovation (as for example with the development of new services based on creative uses of available data). Some of these benefits will fall to entrepreneurs, while others will spill over into the wider community.

Furthermore, the research will enable value to be extracted from the ever-growing quantities of data we see. The social return on investment in data acquisition, particularly public open data, will be dramatically improved as more tools and methods are created for using the data to drive services.

There will be several non-economic impacts too. In policy terms, the impacts will be high, particularly as local solutions for problems - inherently more efficient than centralised problem-solving which cannot always take account of local conditions - will emerge from collaboration in social machines in small communities. Communities will become empowered and self-reliant. The result will be a suite of tools and methods which can be put to work in social contexts by a range of actors - government, to achieve policy goals, groups of people, to achieve social goals, or entrepreneurs, to achieve commercial goals. Indeed, one would expect a social machine to encompass all of these at different times."
11,4CE8E89E-7106-4B47-AD16-DDB5F8EF2124,Language Processing for Literature Based Discovery in Medicine,"The amount of published material in biomedicine has been growing exponentially in recent years, particularly in very productive areas, such as genomics. The knowledge it contains is now so vast and fragmented that it is no longer possible for any individual or research group to keep up with the advances relevant to their area. The research literature is also fragmented and researchers naturally concentrate their attention on their own area of expertise, meaning they may not identify research that is relevant to their own if it does not appear within the literature of their scientific discipline. However, medical research is becoming increasingly interdisciplinary with progress being made by combining outputs from various fields.

Hidden knowledge occurs when a connection can be inferred by combining information from multiple documents, but that connection has not been noticed. Literature Based Discovery (LBD) provides tools that analyse the research literature to identify hidden knowledge automatically. Connections it has been used to identify include treatments for diseases (e.g. that fish oil can be used to treat Raynaud's syndrome) and cases of diseases (e.g. that migraines can be related to magnesium deficiency). Despite these successes, the knowledge that has been discovered has been limited by the relatively simple techniques used to analyse the research literature. 

This project will develop new approaches to LBD by applying recent advances in the automatic processing of biomedical literature. This analysis will provide a LBD system with more detailed and accurate information about this literature than has previously been possible. In particular, the project will make use of two language processing technologies, Information Extraction and Word Sense Disambiguation, which can now be applied to the biomedical literature on a large scale. Information Extraction will be used to identify connections between items mentioned in documents and will provide more accurate analysis than the simple techniques used by previous LBD systems. Word Sense Disambiguation will be used to avoid the problems caused by polysemy and synonymy (the suggestion of spurious connections and connections being missed) which can adversely effect LBD performance.

The project will implement a LBD system and test it on two domains: oncology and neuroscience. The effectiveness of the system will be judged by researchers working in these areas with interests in melanoma and Parkinson's disease.",,"This project has the potential to significantly improve the quality of life, health and well being of a significant portion of society through the development of novel treatments and therapies for important diseases. In particular the project will investigate the causes of and treatments for cancer and Parkinson's Disease, both of which affect significant potions of society. Cancer has been estimated to cause around 13% of human deaths worldwide while Parkinson's Disease affects around 4% of those over 80 years of age. 

There is also an economic benefit to providing treatments of these diseases; the annual cost of Parkinson's Disease to the UK has been estimated to be between 449 million and 3.3 pounds annually [1].

The project will also improve the capability of groups carrying out medical research including the NHS, university departments and independent research institutes. The systems developed will allow these groups to access medical literature more effectively and to identify the hidden knowledge it contains. More generally the techniques could benefit any organisation that carries out automatic analysis of large bodies of text. Examples of such organisation include intelligence agencies, internet search engines, marketing companies and the police.

The project will enhance the UK's position as a leader in the language technology industry. This has been estimated to be worth 8.4 billion Euros and growing at a rate of 10% per year [2]. It has also been estimated that the growth rate for companies that focused on particular industries, such as the life sciences, exceeds the industry average [3]. The technologies developed in this project will improve the automatic processing of documents in the life sciences. 

[1] L. Findley (2007) &quot;The Economic Impact of Parkinson's Disease&quot; Parkinsonism and Related Disorders 5(6):525-535
[2] http://www.euractiv.com/en/culture/eu-language-industry-worth-84bn-euros/article-187814
[3] S. Grimes (2002) &quot;Text Technologies in the Mainstream&quot; report at Text Analytics Summit 2008"
12,74DB2164-EE85-4DBB-B745-E70D0CCE523C,Reconfigurable Autonomy,"As computational and engineering applications become more sophisticated, the need for autonomous systems that can act intelligently without direct human intervention increases. Yet the autonomous control at the heart of many such systems is often ad-hoc and opaque. Since the cost of failure in critical systems is high, a more reliable, understandable and consistent approach is needed. Thus, in this project we aim to provide a rational agent architecture that controls autonomous decision-making, is re-usable and generic, and can be configured for many different autonomous platforms. In partnership with the industrial collaborators we aim to show how such &quot;reconfigurable autonomy&quot; can be achieved in relevant applications.",,
13,F075B38D-58D2-47FF-BC69-E275F6DB64B6,"DISTRIBUTED SENSING, CONTROL AND DECISION MAKING IN MULTIAGENT AUTONOMOUS SYSTEMS","Autonomous intelligent systems will find important applications in our future society. Initial applications will be in the following areas: surveillance, intelligence gathering and operational control in the areas of disaster mitigation (earthquake, nuclear catastrophe, military combat, oil-spills at sea, transport infrastructure breakdown, analysis and assistance with terrorist attacks), space exploration at remote locations (at Trojan asteroids, on Mars and in orbit observations around planets, deep underwater explorations and robotics for offshore oil exploration disasters) followed by large scale applications such as agricultural, search and rescue, manufacturing, and autonomous household robots. These autonomous system will require quick, appropriate, and at the same time informative-to-partners, actions by teams of robots. They can also be computing network based intelligent agents with sensing and control capabilities. It will be a societal requirement that these (semi-)autonomously operating systems to inform their human supervisors about the reasoning behind their actions and their future plans in concise notes for their safety and acceptability by society.

Network based software agents have been in use by our society for some time. Our society is going through information exchange revolution that is developing towards networked intelligent devices. Many of these infrastructure systems are based on well defined discrete inputs and outputs either from human operators or from low dimensional sensor measurements. Little progress has however been made in robot intelligence of autonomy where high complexity, changing environment is to be sensed, reasoned about and acted upon quickly. Partial results have been reported in DARPA, Robocup projects that do not provide comprehensive systematic approach or are not fully publicly available. Progress has only been made in heavily infrastructured environments of robots. 

We do not yet have the methodology for a set of autonomous vehicles or agent systems to operate reliably and (semi-)autonomously in complex infrastructure-free environments to solve problems efficiently with minimal human supervision. The reason is that current intelligent agent technology does not provide solutions. Sensor networks with simple computational nodes, that were developed for low power and computational resources do not provide solutions. They miss the ability of high complexity conceptual abstractions onboard a single agent. The computations of these type of agents cannot be substituted by data fusion of low complexity agents due to typical real-time and communication bottlenecks. Methods of multi-agent decentralized decision theory have been developed and very successfully used prior to this project but have not been properly exploited for multiple complex agents.

This project intends to develop a new methodology for autonomous cooperating multi-agent systems that is to boost the technological capabilities of our partner companies and the robotics industry in general. The project will provide the missing capabilities of abstractions concerning world modeling, situational awareness, learning and information management onboard a single agent. These capabilities will enable efficient realtime decision making within multi-agent cooperation and decentralized decision making in poorly structured or infrastructure free environments. These methods will connect digital computing power with human conceptual structures to enable robots to model the world with layers of high and low level concepts as humans do.",,"The high level of autonomy levels achieved (up to 4b on the extended PACT scale and levels 7-9 on the AFRL scale by Clough) will include decentralised decision making and control by a team of autonomous intelligent agents, including coalition formation for specific goals. These results will have a major impact on industrial automation in general.

The demonstrations will demonstrate to the public the high relevance of this research to society and efficient spending of research funds in terms of the multi-agent vehicle systems. The relevance of this research for networked intelligent autonomous agents with sensing and actuator capabilities for fast response in infrastructure breakdown mitigation in rail transport, nuclear and complex manufacturing systems, will also be demonstrated. 

The academic and industrial novelty of the ability of the agents to read system English (sEnglish) technical documents will raise international interest as our agents will read about new physical and mental skills, such as movement controls, movement planning, procedures of payload tasks, situational awareness techniques, response rules and behaviour constraint handling.

Our methods of building up layered abstractions of agent operations, for their physical self-control, communications and team working capabilities, will be academically significant and will show the way for future software engineering in the area of autonomous systems. 

The above impact will be achieved via a series of 

- Journal papers
- Conference presentations
- Workshops for our industrial partners
- Media publicity where appropriate"
14,5526F9F3-2A6A-4691-9F10-9FFA27896610,Reconfigurable Autonomy,"As computational and engineering applications become more sophisticated, the need for autonomous systems that can act intelligently without direct human intervention increases. Yet the autonomous control at the heart of many such systems is often ad-hoc and opaque. Since the cost of failure in critical systems is high, a more reliable, understandable and consistent approach is needed. Thus, in this project we aim to provide a rational agent architecture that controls autonomous decision-making, is re-usable and generic, and can be configured for many different autonomous platforms. In partnership with the industrial collaborators we aim to show how such &quot;reconfigurable autonomy&quot; can be achieved in relevant applications.",,
15,9D63B22C-D417-4CB1-9C82-5BD6D71AAE15,Sustained Autonomy through Plan-based Control and World Modelling with Uncertainty,"Sustained autonomous behaviour requires a system that is robust to uncertainty, both at the low level of interactions between actuators and sensors and its environment, but also at the intermediate level of sensory perception and interpretation, action dispatch and execution monitoring and, at the highest level of planning, action selection, plan modification and world modelling. In this project we bring together a team of experts with complementary and linked skills and experience, from robotics and sensor data processing, from planning and from reasoning under uncertainty. Our goal is to combine these areas in order to build and demonstrate a robust approach to sustained autonomy, coupling plan-based control to the construction of world models under constraints on resources and under uncertainty. 

We plan to demonstrate the approaches in an underwater environment, using Autonomous Underwater Vehicles (AUVs), performing inspection and investigation missions. These missions share many features with space exploration, the use of autonomous Unmanned Aerial Vehicles (UAVs) for track-and-target missions and investigation of terrestrial hazardous sites, such as nuclear waste storage sites. In all of these case, communication between a human supervisor and the autonomous system is often tightly constrained. This is particularly true of deep space missions (for example, Mars missions face transmission delays of about 15 minutes, but windows might consist of a just two 30 minute slots in 24 hours). However, in aerial observation missions involving multiple assets the need for rapid responses in the control of fast moving vehicles reacting to agile targets also leads to bandwidth constraints for a single human controller attempting to manage and coordinate the mission. Hazardous sites, particularly those subject to radiation emissions, often contain communication black-spots where vehicles must operate without human intervention over extended periods. The underwater setting also imposes limits on communication due to the physical difficulties in transmitting control signals over significant distances. 

Many of these missions involve multiple assets, often mounting different capabilities. Space missions might combine orbital observing assets, ground-based landers or rovers, possibly aerial vehicles (in some settings) and even astronauts, each offering different subsets of capabilities. Aerial observation might combine slower but more agile vehicles with others that are fast but less manoeuverable, while mounted imaging systems might exploit different wavelengths (visible, infrared, radar) and vehicles might offer other capabilities. We intend to explore the use of multiple assets, including the coordination of AUVs mounting different sensors and actuators. 

Uncertainty offers different challenges according to environment. Many space environments have relatively predictable dynamics (although Martian winds are one example of a highly dynamic and uncertain factor), but aerial observation missions operate in highly dynamic and unpredictable environments: both atmospheric conditions and target behaviours can be a source of dynamic uncertainty. The underwater environment is also highly dynamic: phenomena such as currents will act as useful proxies for similar dynamic sources of uncertainty in other execution environments. Other sources of uncertainty arise from the inherent limitations of sensors and actuators and our ability to process and interpret the data that can be recovered from these devices. One of the biggest challenges in achieving robust autonomy is in recognising that uncertainty about the state of the world and the state of execution of a plan is inevitable, but the form of that uncertainty is itself an unknown.

By combining techniques in modelling and reasoning about uncertainty, plan modification and sensor data perception and interpretation, we propose to build a robust approach to autonomous systems control.",,
16,B5322839-522D-4334-BF2C-611ED09503B4,Sustained Autonomy through Coupled Plan-based Control and World Modelling with Uncertainty,"Sustained autonomous behaviour requires a system that is robust to uncertainty, both at the low level of interactions between actuators and sensors and its environment, but also at the intermediate level of sensory perception and interpretation, action dispatch and execution monitoring and, at the highest level of planning, action selection, plan modification and world modelling. In this project we bring together a team of experts with complementary and linked skills and experience, from robotics and sensor data processing, from planning and from reasoning under uncertainty. Our goal is to combine these areas in order to build and demonstrate a robust approach to sustained autonomy, coupling plan-based control to the construction of world models under constraints on resources and under uncertainty. 

We plan to demonstrate the approaches in an underwater environment, using Autonomous Underwater Vehicles (AUVs), performing inspection and investigation missions. These missions share many features with space exploration, the use of autonomous Unmanned Aerial Vehicles (UAVs) for track-and-target missions and investigation of terrestrial hazardous sites, such as nuclear waste storage sites. In all of these case, communication between a human supervisor and the autonomous system is often tightly constrained. This is particularly true of deep space missions (for example, Mars missions face transmission delays of about 15 minutes, but windows might consist of a just two 30 minute slots in 24 hours). However, in aerial observation missions involving multiple assets the need for rapid responses in the control of fast moving vehicles reacting to agile targets also leads to bandwidth constraints for a single human controller attempting to manage and coordinate the mission. Hazardous sites, particularly those subject to radiation emissions, often contain communication black-spots where vehicles must operate without human intervention over extended periods. The underwater setting also imposes limits on communication due to the physical difficulties in transmitting control signals over significant distances. 

Many of these missions involve multiple assets, often mounting different capabilities. Space missions might combine orbital observing assets, ground-based landers or rovers, possibly aerial vehicles (in some settings) and even astronauts, each offering different subsets of capabilities. Aerial observation might combine slower but more agile vehicles with others that are fast but less manoeuverable, while mounted imaging systems might exploit different wavelengths (visible, infrared, radar) and vehicles might offer other capabilities. We intend to explore the use of multiple assets, including the coordination of AUVs mounting different sensors and actuators. 

Uncertainty offers different challenges according to environment. Many space environments have relatively predictable dynamics (although Martian winds are one example of a highly dynamic and uncertain factor), but aerial observation missions operate in highly dynamic and unpredictable environments: both atmospheric conditions and target behaviours can be a source of dynamic uncertainty. The underwater environment is also highly dynamic: phenomena such as currents will act as useful proxies for similar dynamic sources of uncertainty in other execution environments. Other sources of uncertainty arise from the inherent limitations of sensors and actuators and our ability to process and interpret the data that can be recovered from these devices. One of the biggest challenges in achieving robust autonomy is in recognising that uncertainty about the state of the world and the state of execution of a plan is inevitable, but the form of that uncertainty is itself an unknown.

By combining techniques in modelling and reasoning about uncertainty, plan modification and sensor data perception and interpretation, we propose to build a robust approach to autonomous systems control.",,"Autonomous functioning of complex systems is becoming an increasingly widespread problem of interest, from the management of specialised missions (space, tracking, inspection, military, rescue and so on) to management of infrastructure (energy, transport and water), the objective is to improve efficiency, responsiveness and reliability without increasing costs. The consequence is a huge increase in interest in technologies to support autonomous decision making and intelligent control. The industrial sponsors for this Call for Proposals represent an important subset of companies in the forefront of the drive for this technology. It is clear that the competitiveness of the UK will depend on leading in the development of these technologies for intelligent automation, robotics and autonomous control.

Our first pathway to impact is therefore clear: we will work closely with the sponsors of this call to develop solutions to intelligent control under uncertainty that are close to deployment. The structure of our workplan is designed to facilitate this, with a 6 month lead in during which we will ensure the closest fit between our reseach directions and the partners' goals and a 6 month conclusion during which we will aim to transfer the ideas from the project into the partner organisations. This will be an excellent and organised opportunity for exploitation and will provide a route to embedding the technologies within the labs of the sponsors.

Beyond this, all partners have links with a variety of companies and organisations that will be interested in the further development of these technologies. Fox and Long have connections with the power industry through project partners in Electrical Engineering at Strathclyde, including Scottish Power, where autonomous smart grid operations are seen as an essential future technology. Fox is adjunct researcher at Monterey Bay Aquarium Research Institute, offering opportunities for pushing this technology into the oceanographic community. Dearden has close associations with the Autosub group at Southampton, offering similar opportunities and Lane is a member of the Marine Alliance for Science and Technology in Scotland (MASTS) as well as enjoying a wide range of links into the marine technologies community, so there will be a wide range of possible avenues open to partners for passing this technology into relevant areas of application in underwater operations. Long has good connections with Steve Chien, Chief Scientist at JPL, working on autonomy for space, including AEGIS and EO-1. The work being proposed in this project will be of interest to Chien and his group and Long has regular contact with Chien, including through formal meetings such as the International Workshop on Planning and Scheduling in Space and the IJCAI Workshop on AI in Space series by which the work can be disseminated.

The impact we expect that this project will have is in defining and demonstrating new ways to manage uncertainty thoughout the architecture of an autonomous system, from the sensor data interpretation, through world modelling, to the planning and plan modification in response to the effects of uncertainty. We also envisage that the work on V&amp;V for autonomous behaviour in an uncertain environment could have broad impact, particularly amongst existing users of technologies for autonomy, such as JPL and NASA. Current approaches to V&amp;V depend on extensive testing and software V&amp;V techniques such as model checking. New ideas in this area are urgently needed and any successes would have significant impact."
17,EBBADF03-C620-4C38-A43A-0A01F1D4C7CA,Sustained Autonomy through Coupled Plan-based Control and World Modelling with Uncertainty,"Sustained autonomous behaviour requires a system that is robust to uncertainty, both at the low level of interactions between actuators and sensors and its environment, but also at the intermediate level of sensory perception and interpretation, action dispatch and execution monitoring and, at the highest level of planning, action selection, plan modification and world modelling. In this project we bring together a team of experts with complementary and linked skills and experience, from robotics and sensor data processing, from planning and from reasoning under uncertainty. Our goal is to combine these areas in order to build and demonstrate a robust approach to sustained autonomy, coupling plan-based control to the construction of world models under constraints on resources and under uncertainty. 

We plan to demonstrate the approaches in an underwater environment, using Autonomous Underwater Vehicles (AUVs), performing inspection and investigation missions. These missions share many features with space exploration, the use of autonomous Unmanned Aerial Vehicles (UAVs) for track-and-target missions and investigation of terrestrial hazardous sites, such as nuclear waste storage sites. In all of these case, communication between a human supervisor and the autonomous system is often tightly constrained. This is particularly true of deep space missions (for example, Mars missions face transmission delays of about 15 minutes, but windows might consist of a just two 30 minute slots in 24 hours). However, in aerial observation missions involving multiple assets the need for rapid responses in the control of fast moving vehicles reacting to agile targets also leads to bandwidth constraints for a single human controller attempting to manage and coordinate the mission. Hazardous sites, particularly those subject to radiation emissions, often contain communication black-spots where vehicles must operate without human intervention over extended periods. The underwater setting also imposes limits on communication due to the physical difficulties in transmitting control signals over significant distances. 

Many of these missions involve multiple assets, often mounting different capabilities. Space missions might combine orbital observing assets, ground-based landers or rovers, possibly aerial vehicles (in some settings) and even astronauts, each offering different subsets of capabilities. Aerial observation might combine slower but more agile vehicles with others that are fast but less manoeuverable, while mounted imaging systems might exploit different wavelengths (visible, infrared, radar) and vehicles might offer other capabilities. We intend to explore the use of multiple assets, including the coordination of AUVs mounting different sensors and actuators. 

Uncertainty offers different challenges according to environment. Many space environments have relatively predictable dynamics (although Martian winds are one example of a highly dynamic and uncertain factor), but aerial observation missions operate in highly dynamic and unpredictable environments: both atmospheric conditions and target behaviours can be a source of dynamic uncertainty. The underwater environment is also highly dynamic: phenomena such as currents will act as useful proxies for similar dynamic sources of uncertainty in other execution environments. Other sources of uncertainty arise from the inherent limitations of sensors and actuators and our ability to process and interpret the data that can be recovered from these devices. One of the biggest challenges in achieving robust autonomy is in recognising that uncertainty about the state of the world and the state of execution of a plan is inevitable, but the form of that uncertainty is itself an unknown.

By combining techniques in modelling and reasoning about uncertainty, plan modification and sensor data perception and interpretation, we propose to build a robust approach to autonomous systems control.",,
18,C47A2E1E-FFC7-403E-84BA-2EC4E2D19769,Brain-Computer Interface for Monitoring and Inducing Affective States,"Brain Computer Interfaces (BCI) allow for modification of subject's environment or control of external devices by the power of thought alone. They achieve this by analysis of small electrical potentials generated by the subject's brain while its owner is thinking. Emotions involve specific mental states hence also involve particular patterns of brains electrical activity. The proposed research will build innovative intelligent BCI systems that can monitor our emotions, and modify them automatically and adaptively via controlled computer music generation system. Creation of such systems would advance our understanding of fundamental relationships between the subjective emotions, corresponding brain states and characteristics of music that can induce very vivid and powerful emotions in humans. Such systems can be used for treatment of emotional/mood disorders such as depression so are of direct benefit to society and NHS. In addition, they are of interest for healthy subjects as means of relaxation or perhaps by enhancement of gaming experience. Thus the proposed project could also lead to interesting developments in the entertainment industries such as the gaming industry.",,
19,FF455BE0-9E83-4862-BAE2-FD8688CE44C2,Brain-Computer Interface for Monitoring and Inducing Affective States,"Brain Computer Interfaces (BCI) allow for modification of subject's environment or control of external devices by the power of thought alone. They achieve this by analysis of small electrical potentials generated by the subject's brain while its owner is thinking. Emotions involve specific mental states hence also involve particular patterns of brains electrical activity. The proposed research will build innovative intelligent BCI systems that can monitor our emotions, and modify them automatically and adaptively via controlled computer music generation system. Creation of such systems would advance our understanding of fundamental relationships between the subjective emotions, corresponding brain states and characteristics of music that can induce very vivid and powerful emotions in humans. Such systems can be used for treatment of emotional/mood disorders such as depression so are of direct benefit to society and NHS. In addition, they are of interest for healthy subjects as means of relaxation or perhaps by enhancement of gaming experience. Thus the proposed project could also lead to interesting developments in the entertainment industries such as the gaming industry.",,"This research will contribute towards the enhancement of the quality of life, health in the society and the economic competitiveness of the entertainment industry in the UK, including the gaming industries and music. Its interdisciplinary nature will also benefit several academic stakeholders as outlined in the Academic Beneficiaries section.
BCI technology to monitor and induce affective states will impact on the health sector by providing a valuable tool in preventive and therapeutic programmes for health problems such as stress, depression, anxiety and other conditions related to affective states. Stress is an endemic problem in our society and economy. According to a study by the mental health charity Mind (http://www.mind.org.uk/), every year UK businesses lose &pound;26 billion and 70 million working days because of stress. Excessive stress leads to anxiety and more severe cases of depression. Stress may cause suppression of the immune system, insomnia, fatigue, and affect adversely productivity. Persistent cases may lead to social withdrawal and family dysfunction. The project impact will be enhanced by liaising with the health sector in order to raise their awareness on this projects goals and prepare the ground for taking our systems out of the lab into the real world of special needs as soon as we have a working prototype.
Activities that will enhance impact on creative industry include organisation of the Workshop during Peninsula Arts Contemporary Music Festival in Plymouth which will enable us to spread awareness about the technology developed in the music industry.
The project will positively impact public understanding of science via press exposure campaign, use of internet media (dedicated WWW page, youtube etc) for dissemination of the information about the project as well as by using the exploitation channels available at both universities.
Lastly, in additional to the usual channels of academic dissemination the impact on academia will be enhanced by the organisation of the International Workshop at Reading as well as by providing a an excellent training for highly qualified interdisciplinary research staff."
20,609713AF-CD64-4583-AA13-557DAEFB63E2,Supportive Automated Feedback for Short Essay Answers (SAFeSEA),"The aim of this research is to provide an effective automated interactive feedback system that yields an acceptable level of support for university students writing essays in a distance or e-learning context. Our tool will be based on an existing system supporting the online writing and assessment of essays, OpenComment (Whitelock &amp; Watt, 2008), but this requires several problems to be solved, both in natural language processing, and in educational theory and practice. This proposal is therefore a truly interdisciplinary one.

The natural language processing problems are how to `understand' a student essay well enough to provide accurate and individually targeted feedback, and how to generate that feedback automatically. We intend to use techniques from document classification (measures of semantic similarity), information extraction, document summarisation, and text generation to address these problems. The tools we build need to be flexible enough to support different types and models of feedback.

The educational problem is how to develop and evaluate effective models of feedback. This requires research into the selection of the content, mode of presentation and delivery of the feedback. The primary aspects of feedback which will be investigated are: summarisation; recognition of positive achievements; location of errors of commission or omission; misconceptions or problems connected with causal relationships; tactical and strategic hints both relating to the specific issues being addressed in the essay and issues connected with `metacognition' (i.e. self awareness of the learning process), and self-regulation of the learning process. For feedback to be effective, it should assist students not only to manage their current essay-writing task, but it should also lead to further development of their essay-writing skills, the skills associated with efficient self-regulation, and their motivation to complete their course. The methods used to obtain data on effectiveness will be student surveys, interviews with selected participants, analysis of the various versions of the essays submitted for feedback, and the quality of the final essay.",,"Summary of Impact

The research directly impacts on the technology enhanced learning community, the feedback community, the e-assessment community and the e-learning community - as well as the Society for Research into Higher Education. Evidence that state of the art analysis of tests is sufficiently advanced to provide a firm basis for the generation of feedback will be of interest to these communities. The text processing community should find the hybrid approach to textual analysis of short essays to be worth studying further. 

There is the potential for firms such as QuestionMark to find a commercial value to this work.

Less directly, Higher Education Institutes in the UK (and around the world) will benefit either by adopting and further developing the technologies developed within this project or by utilising the research results to inform their own feedback procedures for work carried out online.

Policy makers such as QCA and SQA and other bodies involved in managing Higher Education in the UK and elsewhere will benefit from seeing a proof of concept.

Last, but not least, most students in higher education can benefit from the research - and also, eventually, students in secondary schools. 

The work also has the potential to improve levels of student attainment and enhance their abilities to provide improved feedback in their working lives. This could lead to improvements in the capability of the UK to generate high quality added value products and processes. 

The research staff working on the project will be encouraged to develop their own careers through continuous monitoring and support from the senior researchers, through opportunities to develop their own skills for devising research plans, writing research documents and presenting the work in academic and other settings."
21,57BC1864-25B6-4B9A-8469-49FD8E68599A,UCT for Games and Beyond,"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.",,
22,A1C6CCEC-7D88-48AD-B1A4-E8C2306FD179,Automated Plan-Based Policy-Learning for Surveillance Problems,"Surveillance problems give rise to many challenges including the management of uncertainty in an unpredictable environment, the management of restricted resources and the communication of commitments and requests between multiple heterogeneous agent ``observers''. At the heart of surveillance problems lies the need to plan complex sequences of behaviour that achieve surveillance goals. These goals are typically expressed in terms of gathering as much information as possible given constraints, and communicating findings to a human operator. Planning is combinatorially hard, and planning problems involving metric resources, continuous time and concurrency, as would be required in the solution of non-trivial surveillance problems, are time-consuming to solve. This complexity is greatly exacerbated if uncertainty is captured explicitly within the planning domain models. Although online planning, and plan repair in the case of failure, are feasible in stable situations, they take too long in situations that are changing rapidly. Online planning also requires significant on-board computational resources, which are often not available in surveillance vehicles. Planning under uncertainty cannot therefore be done online in situations typical of many surveillance problems, where computational resources are limited and rapid responses are frequently required. On the other hand, forward planning is certainly required in order to avoid the observers behaving in a purely reactive (and therefore easily distracted) manner. 

Since online planning, and planning under uncertainty, are both unrealistic for large-scale, fast-moving surveillance problems, we propose an alternative approach based on plan-based policy-learning. We assume that time and resources are available offline to train effective policies. Our approach is based on Monte Carlo sampling: we sample many instances of the stochastic problem, each instance being a challenging temporal and metric planning problem. We then solve each instance using a high-performing planner, and then apply a classifier to learn a policy as a mapping from states to actions, using the set of solutions as input. We have already demonstrated the effectiveness of this approach in two single-agent cases: management of the loading of multiple batteries, and the control of an autonomous underwater vehicle following the edge of a patch (distinguished by high chlorophyll or high temperature readings) in the coastal waters of the Monterey Bay. We know from our work in both cases that the resulting policies can be very high-performing in terms of robustness to the high degree of uncertainty that often occurs in the physical execution environment. We are now proposing to scale up the approach we took in the batteries and patch-following cases, to the multi-agent coordination problem, addressing the challenges that arise when many agents are coordinating in solving a surveillance problem that requires the integration of multiple policies.",,"In the last few years there has been increasing interest, amongst companies concerned with robotics, in the automation of intelligent reasoning. We have contributed to the development of autonomous decision-making in collaboration with Scottish Power and National Grid, SciSys Ltd and, most recently, BAE Systems. In recent months there has been an explosion of interest in the role that planning can play in these developments, and we have been approached by Thales Ltd, USAF, and Mako Surgical Corp., all concerned with automating different aspects of the control of their advanced robotic systems. The names of the companies behind the Call to which this proposal is addressed, demonstrate that industry is ready for the benefits of automated plan-based reasoning. 

The impact of the proposed work will be achieved through close collaboration with the industrial sponsors named in the Call. In order to ensure the impact of our approach across the scenarios they outlined, we will work closely with the companies, working with them to derive accurate models of their problem domains and appropriate sampling techniques for sampling the space of problem instances in each case. As outlined in the case for support, we will produce a number of deliverables (domain models, planning software, learned policies and simulation results) that will inform discussion and enable refinement of our approaches towards more closely tailored solutions. We will then work with the companies to field the technology we develop, first at the level of demonstrations and then as components in deployed systems. 

Our solutions will have wider application than to surveillance scenarios, as the features of Intelligence-Gathering, Tracking and Hazard Investigation arise in many application domains including search and rescue, domestic robotic support, healthcare, computer-aided learning, future power systems, future transport systems and many others. The class of surveillance problems we have defined raises many new research challenges, in modelling, planning and machine learning, in which we are experts. We expect that, in addressing the specific needs of the scenarios described in the Call, we will produce technology that moves the AI Community forward in terms of fielding fast, light-weight and adaptive autonomous deliberative reasoning."
23,9E35B835-970F-4141-9BF2-6165A796393F,Supportive Automated Feedback for Short Essay Answers (SAFeSEA),"The aim of this research is to provide an effective automated interactive feedback system that yields an acceptable level of support for university students writing essays in a distance or e-learning context. Our tool will be based on an existing system supporting the online writing and assessment of essays, OpenComment (Whitelock &amp; Watt, 2008), but this requires several problems to be solved, both in natural language processing, and in educational theory and practice. This proposal is therefore a truly interdisciplinary one.

The natural language processing problems are how to `understand' a student essay well enough to provide accurate and individually targeted feedback, and how to generate that feedback automatically. We intend to use techniques from document classification (measures of semantic similarity), information extraction, document summarisation, and text generation to address these problems. The tools we build need to be flexible enough to support different types and models of feedback.

The educational problem is how to develop and evaluate effective models of feedback. This requires research into the selection of the content, mode of presentation and delivery of the feedback. The primary aspects of feedback which will be investigated are: summarisation; recognition of positive achievements; location of errors of commission or omission; misconceptions or problems connected with causal relationships; tactical and strategic hints both relating to the specific issues being addressed in the essay and issues connected with `metacognition' (i.e. self awareness of the learning process), and self-regulation of the learning process. For feedback to be effective, it should assist students not only to manage their current essay-writing task, but it should also lead to further development of their essay-writing skills, the skills associated with efficient self-regulation, and their motivation to complete their course. The methods used to obtain data on effectiveness will be student surveys, interviews with selected participants, analysis of the various versions of the essays submitted for feedback, and the quality of the final essay.",,
24,B5E8BE52-CFB9-4BDA-9937-D121741E349E,The Memory Network,"From Plato's wax tablet and Freud's exploration of the 'mystic writing pad' to contemporary metaphors of digital archives, our understanding of memory has often been shaped by technological models. But in recent years, this understanding has been significantly enhanced through exciting scientific breakthroughs. Writers, critics and academics working in the arts and humanities are now turning to new ways of thinking within biosciences, psychology, mathematics, and computer science to reconsider individual and collective memory in literary narratives. At the same time, many scientists acknowledge the benefits of engaging with creative ideas and the ethical and hermeneutic perspectives offered through fiction and humanities methodologies. 
The Memory Network (MN) brings together researchers, authors and organisations to provoke and fuel original ways of thinking about memory. Many important contemporary writers are already in a vibrant dialogue with science and offer crucial starting points for the MN's activity: for example, Ian McEwan's and Will Self's research into neuroscience for Saturday (2005) and The Book of Dave (2006); Margaret Atwood's and Kazuo Ishiguro's thinking about the impact of 'the posthuman' upon human memory; Terry Pratchett's quest to understand the effect on his writing of progressive Alzheimer's; and Hanif Kureishi's interest in understanding how matter and mind are related.
 One of the Network's ambitions is to grow and nurture a community of thinkers interested in cross-disciplinary approaches to memory studies, open to communicating with each other and with the general public. It will also stimulate new creative work and we are particularly keen to actively involve living writers in this community and its various debates. We will encourage a dialogue to develop between the academic community, creative writers and artists, and the general public. Interaction will be encouraged in a number of ways. Firstly, and following an initial symposium called 'The Future of Memory' (funded by the Wellcome Trust), the MN will run a series of free lectures, memory experiments, panel discussions and interactive workshops throughout the first year of the scheme. These events will tackle topics including science and psychogeography, memory, food and phenomenology, memory and new media, memory and prediction, human and artificial intelligence memory, and writing, aging and forgetting. A website called the 'Memory Repository' will support discussion and correspondence between Core Members and the general public in response to these events. Finally, the MN will publish two collaborative works: a 'manifesto' called The Future of Memory: Notes for the New Millennium and an edited collection of essays provisionally titled Memory, Literature and Science.
The Memory Network has literary studies at its core, and most of its initial members are based in English departments with an established or emerging research interest in memory and narrative. In addition, key researchers from other disciplines will join as active members or in an advisory capacity. At present the Network is primarily based in contributors from the UK and North America, but an additional aim for its final phase is to critically situate these interdisciplinary investigations in wider contexts with a European and global emphasis, culminating in a major international conference.",,"Professionals and Practitioners
Writers and artists will benefit from the MN through being directly involved in the project and through using the various outputs as a source of research and inspiration. Collaboration with colleagues working in different fields (other artists and critics, but especially scientists) should generate a wealth of new ideas that will lead to innovative artistic and cultural activity. Previous collaborative work undertaken by creative writers such as Will Self, A. S. Byatt and Ian McEwan is to be taken as a model here: Self has used his research into work by the Wellcome Trust (Maguire et al.) as the basis for his novel The Book of Dave (2006); Byatt uses her extensive knowledge of memory as the basis of composition of her novels; and McEwan has shadowed a neurosurgeon for his novel Saturday (2005). It is this kind of cross-disciplinary activity which the MN stimulates. 

Wider Public
The MN's various activities will have direct benefits for the general public, bringing a diverse set of high-profile intellectuals into the public sphere, increasing public awareness of the work of, and between, different disciplines in memory studies, and encouraging individuals to engage and interact with cutting-edge research. For example, the inaugural lecture and panel combines the attractions of a hugely popular and critically acclaimed author (A. S. Byatt), an innovative neuroscience researcher (Hugo Spiers), and two world-class literary scholars (Patricia Waugh and Claire Colebrook) who seek to explore the relationship between the arts and sciences. The event with Hugo Spiers and Will Self has already attracted the attention of Channel 4, and we expect Self to be writing about this experience in The Guardian. More intimately, the 'Food, Phenomenology and Memory' event invites members of the public to take part in a food tasting experiment and undergo a live feedback assessment. 'Rewired: Memory, New Media and the Datastate' has the potential to reach a younger audience by featuring young adult author M. T. Anderson.

The Memory Repository website is open for general discussion following on from public events and members of the public can also sign up to a quarterly newsletter detailing the academic network's progress. The whole series of events and complementary online discussion has the potential to bring science and the arts to new audiences in an accessible manner.

Public Sector
The programme of activities will also benefit the various non-profit organisations, institutions and charitable trusts with which the MN is forging relationships. For example, the Museum of London, who are hosting the Science and Psycho-geography event with Hugo Spiers and Will Self, will not only profit from the publicity of this evening but will be put into contact with the core members of the Memory Network for future advisory work. Similar consultancy opportunities will hopefully arise with the Greenwich Observatory, the British Film Institute, the British Library, and the Wellcome Trust. The Alzheimer's Society, the Society of Authors, and the University of the Third Age will also be involved in public events and will benefit in similar ways.

Commercial Private Sector and Media
Many of the public events will generate media interest, not only disseminating ideas to the general public but also stimulating quality journalism. We are expecting that the various contributors will be writing in broadsheets and mainstream opinion magazines such as Prospect and The New Statesman. Venues such as The Blueprint Caf&eacute; will benefit from the short and long term exposure the project generates and may be able to use academic discussion of the event in developing its own corporate identity. Palgrave Macmillan will publish The Future of Memory: Notes for the New Millennium, and the MIT Press have been approached to publish the edited collection, Memory, Literature and Science."
0,8A8016E2-AC7E-480E-AE8D-23B4C03C7C26,Scrutable Autonomous Systems,"The aim of the project is to make so-called distributed autonomous systems &quot;scrutable&quot; by humans. Autonomous systems can perform tasks without continuous human guidance. This project considers autonomous systems consisting of many components (called agents) which have to make joint plans on how to act together. It is important that humans can understand why the system behaves in particular ways, for example, why the agents have decided upon a certain plan. To address this, the project will develop and evaluate computational techniques for gathering information about the planning process, including how the various agents interacted, and presenting this information to humans in an understandable way. 

To allow information to be gathered easily, the agents will use so-called argumentation techniques: agents engage in dialogues whereby they justify their decision-making as they agree on joint plans, and each decision is based on a collection of arguments in favour and/or against it. This approach has the additional benefit to potentially produce plans faster, because it offers agents an insight into each others' motives. However, this approach will gather much information, and the information will be complex. To allow humans to inspect this information easily, the project will use a combination of Natural Language and diagrams to present it in an understandable way; simplifying, summarising and aggregating information as needed. 

For example, our system might explain:

&quot;To achieve goal G, a plan comprising the sequence of actions A, B, C has been agreed upon. Agents P1, P2 and P3 were involved in the preparation of the plan. P1 argued that A had to be done first, because of its standard operating procedures; P2 and P3 agreed to this. P1 suggested that A should be followed by action D, because it could do A and D with low costs, however P2 disagreed, pointing out that action D did not address safety concerns, and suggested that A should be followed by B. No one objected to this. Finally, P1 suggested that B should be followed by C, and that it could perform the action with the help of P3.&quot;
 
If the user requests further details about the safety concerns, she would be presented with the following:
 
&quot;Current guidelines establish that actions B and D both achieve the same goal, but B does so addressing safety concerns X, Y and Z; D does not address any safety concerns&quot;

The design of a scrutable argumentation-based planning system faces substantial technological challenges. One challenge is to endow distributed planning with the ability to judiciously record its process, whilst creating useful plans quickly. The other is to present the record of the distributed planning process optimally to users. Optimality, in this context, means a combination of clarity and information-richness. We will carry out extensive experiments to find out what aspects of a plan (and the motivation behind it) to emphasise, and where details are better left unsaid. Business partners will help us by taking part in these experiments, and by engaging in discussions about the types of insight users demand.

The project will seek to find solutions that apply across a large range of applications, varying from ones that involve highly complex software agents and robots to ones that involve a plurality of simple, sensor-based agents. We hypothesise that our argumentation-based approach is suitable for modelling disparate scenarios, regardless of their complexity, and that Information Presentation techniques, coupled with state-of-the-art requirements gathering and user-based evaluation, has much to offer across this range.",,"Businesses using autonomous systems

Autonomous systems are being used in many UK and multi-national businesses, in particular in the areas of Aerospace, Aviation, Defense and Energy Generation and Supply. As recognized in this funding call, it is of vital importance for the successful deployment of autonomous systems that humans are kept &quot;in the loop&quot; with timely and relevant information and can humans can interact with the autonomous systems effectively. The importance of transparency and user involvement in autonomous systems was also highlighted in a recent US government report which stated that human-performance problems with autonomous systems are exacerbated by &quot;inadequate visibility into how automation is working, inadequate feedback about the automation's activities, lack of facilities for operators to communicate with automation, inadequate explanation of its reasoning processes&quot; (O'Hara &amp; Higgins, 2010, p53). To achieve effective scrutability, it is mandatory to know what information presentations would be useful for different business users, including the required levels of aggregation and summarisation. Dissemination of the results to the wider business community is also vital for the exploitation of the research results.


General public

Autonomous systems have a great potential to be used in businesses. However, the general public is likely to be worried by the potential impact of of time- and safety-critical autonomous systems if they malfunction. It is therefore important to raise public understanding of how such systems work, and how they can be made safer by allowing humans to inspect how the system behaves. Autonomous systems are also a very exciting computer science research area, and for the future of the UK economy it is important that young people are enthused about careers related to computer science. 


Early-career researchers

It is of strategic importance to the UK to attract and train early-career researchers in its core areas of expertise, such as distributed autonomous systems (DAS) and information presentation especially natural language generation (NLG). The early-career researchers who will work on this grant (3 research fellows and 1 research assistant) will benefit from learning how to apply their research across the traditional DAS and NLG boundaries, and from learning how to perform research that involves end-users throughout the project. They will also benefit from learning how to engage with business communities and the general public. In addition, we will train other early-career researchers in the topics of this research."
1,CAE30259-7C8D-4F4E-BBF0-57C939F149D8,WhatIf: Answering &quot;What if...&quot; questions for Ontology Authoring,"We have a richness of data about numerous aspects of our activities, yet these 
data are only any use when we know what they are, agree upon what they are and 
how they relate to each other. Semantic descriptions of data, the means by 
which we can achieve these aims, are widely used to help exploit data in 
industry, academia and at home. One way of providing such meaning or semantics 
for data is through &quot;ontologies&quot;, yet these ontologies can be hard to build, 
especially for the very people that are expert in the fields whose knowledge
is being captured but who are not experienced in the specialised &quot;modelling&quot; field. 

In the &quot;what if...?&quot; project we look at
the problems of creating ontologies using the Web 
Ontology Language (OWL). With OWL logical forms, computers can 
deduce knowledge that is only 
implied within the statements made by the modeller. So any statement made 
by a modeller can have a dramatic effect on what is implied. 
These implications can be both &quot;good&quot; and &quot;bad&quot; in terms of the aims of the
modeller. Consequently, a 
modeller is always asking themself &quot;what if...?&quot; questions as they model a field 
of interest. Such a question might be &quot;what happens if I say that a planet
must be orbiting a star?&quot; or &quot;what happens if I add in this date/time 
ontology?&quot;. 

The aim of the &quot;what if...?&quot; project is to
build a dialogue system allowing a person building an ontology
to ask such questions and get
meaningful answers. This requires getting the computer to determine what
the consequences of a change in the ontology would be and getting it to
present these consequences in a meaningful way. To do a good job,
the system will have to understand something about what the person is trying
to do and what sorts of results will be most interesting to them. For this,
we need to understand more about 
how ontologists model a domain and interact with tools; be able to model the 
dialogues between a human and the authoring system; achieve responsive 
auttomated reasoning that can provide the dialogue system with the information 
it nees to create that dialogue.",,"Making consensus knowledge about a domain available to its community of users plays an increasingly important 
part of both economic activity, scientific research and everyday life. Ontologies are best created by the people that 
know the field, rather than so-called knowledge engineers. However, current ontology authoring environments impose a
high entrance barrier in terms of technical understanding. In particular, it is very hard for an author to envisage
the possible consequences of adding or changing an ontology axiom. At present, &quot;what if...?&quot; questions cannot be 
answered by the tools used to aid modelling and such help is necessary if scientists and system developers in all areas 
of commerce and academia are to be able to more effectively model their ontologies and hence more effectively exploit their data. 

The &quot;what if...?&quot; project will increase understanding of the ontology authoring process, the use of natural dialogue
models and responsive ontology reasoning. This will directly affect academic disciplines such as Computer Science and
Computational Linguistics. However, the tools that it produces will also have indirect impact on other fields and
organisations that hold data, by enabling the easier construction of higher-quality ontologies. As our letters of support
(and the people willing to serve on our advisory board) show, we can expect impact of this kind in a range of areas of
government, industry and research."
2,E5CD5BCB-A2ED-4772-9228-44FE604B3EAA,Transparent Rational Decisions by Argumentation (TRaDAr),"Argumentation provides a powerful mechanism for dealing with incomplete, possibly inconsistent information and for the resolution of conflicts and differences of opinion amongst different parties. Further, it is useful for justifying outcomes. Thus, argumentation can support several aspects of decision-making, either by individual entities performing critical thinking (needing to evaluate pros and cons of conflicting decisions) or by multiple entities dialectically engaged to come to mutually agreeable decisions (needing to assess the validity of information the entities become aware of and resolve conflicts), especially when decisions need to be transparently justified (e.g. in medicine).

Because of its potential to support decision-making when transparently justifying decisions is essential, the use of argumentation has been considered in a number of settings, including medicine, law, e-procurement, e-business and design rationale in engineering. Potential users of existing argumentation-based decision-making methods are empowered by transparent methods, afforded by argumentation, but lack either means of formal evaluation sanctioning decisions as (individually or collectively) rational or a computational framework for supporting automation. The combination of these three features (transparency, rationality and computational tools for automation) is essential for argumentation-based decision-making to have a fruitful impact on applications. Indeed, for example, a medical practitioner would not find a &quot;black-box&quot; recommended decision useful, but he/she would also not trust a fully transparent, dialectically justified decision unless he/she were sure that this is the best one (rational). In addition, the plethora of information doctors need to take into account nowadays to make decisions requires automated support.

TRaDAr aims at providing methods and prototype systems for various kinds of argumentation-based (individual and collaborative) decision-making that generate automatically transparent, rational decisions, while developing case studies in smart electricity and e-health to inform and validate methods and systems. In this context, TRaDAr's technical objectives are:

(O1) to provide novel argumentation-based formulations of decision problems for individual and collaborative decision-making;

(O2) to study formal properties of the formulations at (O1), sanctioning the rationality of decisions;

(O3) to provide real-world case studies in smart electricity and e-health for (individual and collaborative) decision-making, using the formulations at (O1) and demonstrating the importance of the properties at (O2) as well as the transparent nature of argumentation-based decision-making;

(O4) to define provably correct algorithms for the formulations at (O1), supporting rational and transparent (individual and collaborative) decision-making;

(O5) to implement prototype systems incorporating the computational methods at (O4), and use these systems to demonstrate the methodology at (O1-O2) for the case studies at (O3).

The project intends to develop novel techniques within an existing framework of computational argumentation, termed assumption-based argumentation, towards the achievements of these objectives, and adapting notions and techniques from classical (quantitative) decision theory and mechanism design in economics. 

The envisaged TRaDAr's methodology and systems will contribute to a sustainable society supported by the digital economy, and in particular they will support people in making informed choices. The project will focus on demonstrating the proposed techniques in specific case studies (smart electricity and e-health for breast cancer) in two chosen application areas (digital economy and e-health), but its outcomes could be far-reaching into other case studies (e.g. in other areas of medicine) as well as other sectors (e.g. in engineering, for supporting decisions on design choices).",,"The need for automated computational support for transparent, justifiable decisions that are at the same time rational is widespread in several domains, notably medicine. This is recognised, for instance, within EPSRC digital economy theme, envisaging tools that ` will support people in making informed choices'. TRaDAr aims at developing argumentation-based decision-making methods for supporting the computation of transparent and rational decisions. These methods could be deployed to provide systems and apps of widespread use, thus potentially impacting society in a significant way.
 
Overall, the project will benefit, in addition to the artificial intelligence academic community (as discussed in the Case for Support and the section Academic Beneficiaries), at least the following: (1) developers of applications necessitating (semi-)autonomous but transparent and auditable decision-making, either by a single entity or by several collaborating entities, including e-health and smart electricity, but also other applications such as design rationale in engineering, trust computing and waste water management; (2) developers of e-health and smart electricity applications (e.g., for the latter, in the form of mobile phone apps or smart meter devices incorporating intelligence for decision support); (3) researchers in psychology and the social sciences who wish to explore computational theories and applications of argumentation and decision-making.

We shall support pathway to impact by disseminating results in four forms: (i) top-quality academic publications (see the Case for Support and Academic Beneficiaries); (ii) open-source software tools for argumentation-based decision-making, available from SourceForge; (iii) publications and soft-
ware tools from suitable web pages; (iv) demonstrating videos, e.g. on utube.

The involvement of key (academic, medical and industrial) players (AIT, UCLH and Fujistu, respectively) will further support dissemination of ideas and tools to practitioners and prompt feedback to guarantee appropriate functionalities and usability for deployment. Organisation of and participation in suitable outreaching events (such as events organised by the Technology Strategy Board) will pave the way towards impact.

Our software tools will be made available for academic research and teaching. Their use for other purposes will require further agreements to protect and benefit from intellectual property. In this context, Imperial College London will provide ideal expertise, via its technology transfer programme, which endeavours to protect intellectual property, and Imperial Innovations Limited, helping to commercially exploit academic research. We believe that the possibility of commercial exploitation of our systems is very high, in both e-health (where decision support of the form we envisage is very much in demand in virtually every medical domain) and smart electricity (as we will demonstrate the added value of smart meters in a novel manner that could benefit all citizens)."
3,1A5A35EA-D54D-4AA0-A214-3FECFD966A37,Disambiguation of Verbs by Collocation,"In this project, we propose using statistical approaches to the analysis of corpus data in order to discover Typical Usage Patterns (TUPs) and hence create a resource for the Disambiguation of Verbs by Collocation (DVC). This project goes beyond the current state-of-the-art represented by word sense disambiguation based on machine-readable dictionaries; typical valency-based approaches, which rarely pay attention to collocations; WordNet, which does not analyse lexical syntagmatics or collocates; and semantic role labelling, which tags mainly thematic roles (e.g. Agent, Patient, Location), rather than semantic types (e.g. Human, Firearm, Route). In our project we propose to associate meanings with normal usage patterns, rather than words in isolation, and to integrate lexical collocations with valency, providing an empirically well-founded resource for use in mapping meaning onto word use in free text. DVC will show the comparative frequency of each pattern of each verb, enabling programs to develop statistically based probabilistic reasoning about meanings, rather than trying to evaluate all possibilities equally.

The internal structure of lexical arguments of verbs will be analysed using computational linguistics techniques, so that for example the relationship between &quot;repair the roof&quot; and &quot;repair the damage&quot; is recognized. Even though the nouns &quot;roof&quot; and &quot;damage&quot; have different semantic types, they activate the same meaning of &quot;repair&quot;. Once this has been done, the structural relationship is applied to other verbs, e.g. &quot;treat a patient&quot; and &quot;treat his injuries&quot;.

In a pilot project at Masaryk University, Brno, CZ (http://nlp.fi.muni.cz/projects/cpa/), involving analysis of 700 verbs, Prof. Hanks, the co-investigator of the project, showed that, while words may be highly ambiguous, patterns are rarely ambiguous and, furthermore, most uses of most verbs can be assigned unambiguously to a pattern. The existing verbs will be used to train a statistical method the output from which will be verified lexicographically. As the number of annotated verbs increases, the training procedure will be repeated and so improve the accuracy and speed of the annotation. At each step, the researchers employed by the project will analyse the computer output and correct errors. The objective of the DVC project is to analyse 3000 common English verbs and annotate at least 250 corpus lines for each verb. An in-depth data analysis of 100 verbs will be carried out. The resource will be made publicly available at the end of the project.

The DVC project is based on and will contribute to the Theory of Norms and Exploitations (TNE) of Prof. Hanks. TNE says, in essence, that a language consists of two interlinked systems of rules governing word use: a set of rules for the normal uses of words and a second-order set of rules governing the ways in which normal patterns are exploited. Exploitations are deliberately unusual utterances. They play a large role in linguistic change (word-meaning change). as today's exploitation may become tomorrow's norm. 

The value of DVC will be proven by textual entailment and paraphrasing, in this way demonstrating its potential usefulness in a large number of fields of computational linguistics which benefit from these two applications.

The project will be disseminated using a wide variety of means. A fully user-friendly publicly available website will contain news about the progress of the project and will provide links to project research papers. It will also host interactive demos that will enable visitors to see the patterns collected and test the technologies developed in the project. Papers will be submitted to international conference and peer-reviewed journals. Evaluation conferences such as SEMEVAL and RTE will be used to assess the methods developed in this project in a standard environment. An important outcome of the project will be a monograph (theory, methodology, empirical findings).",,"The immediate beneficiaries will be computational linguists working both in academic and commercial environments, who will be able to use DVC as a resource in improving applications such as machine text interpretation, question answering, information retrieval, machine translation, and idiomatic text generation. DVC will provide an inventory of natural, normal and largely unambiguous phraseology (accompanied by meanings for each phrase), with a mechanism for interpreting unusual and imaginative phraseology by &quot;best-match&quot; preferential procedures. For these researchers, it will be a reference resource with a much-needed focus on phraseology and methods for disambiguation, areas of information that are neglected in currently available dictionaries. 

It will also provide a resource that can contribute to making a reality of Berners Lee's original dream of the semantic web, in which machines will be able to process the information contained in the natural language of raw text (in contrast to the adding and processing of tags which is the current focus of &quot;semantic web&quot; research).

E-book reading devices such as the Amazon Kindle enable a dictionary to interact with any text that a user is reading. It is technically straightforward to set up a link connecting any word in any electronic text with the relevant entry in an appropriate electronic dictionary. However, for such an application to be fully effective, it is also necessary to enable the software to select, not only the most relevant word, but also the most relevant sense of that word. For this application, currently available research on word-sense relevance is not satisfactory. Research into normal phraseology such as that entailed by the DVC project can be shown to be an essential prerequisite, enabling programmers to match the contexts of word uses in a text with the best-match pattern(s) in a pattern dictionary. By embedding technology like the one developed in the DVC project into e-book readers, it will be possible to enhance the reading experience.
 
At a theoretical level, DVC results will support developments in empirically well founded theories of language and meaning. It will provide supporting evidence for use by the many researchers who are now beginning to re-evaluate speculative linguistic theories received from a previous generation of linguists. It will do this because it will be based on painstaking analysis of how people actually use words to make meanings in everyday language use, rather than on speculations about boundary-case possibilities. A new generation of linguists is focusing increasingly on the lexicon and aspects such as lexical preferences rather than on deterministic syntax. DVC is a lexical project par excellence, aiming to show how words combine (syntagmatics and collocations) to make meanings.

Researchers into metaphor and figurative language will welcome the many corpus-derived examples of anomalous uses of words, which will be a by-product of DVC's corpus analysis. 

DVC will also be of benefit to English language teachers, course-book writers, and lexicographers, showing how words can be grouped into sets and how they combine syntagmatically to make meanings. The Pattern Dictionary of English Verbs, which will be an eventual outcome of DVC research, will aim among other things to identify the normal patterns in which each verb is used. Feedback from the language-teaching community has already indicated that language teachers who are aware of the complexity of corpus data regard such a dictionary as a much-needed resource."
4,FC557A85-E746-476C-830F-CAB531279413,SEQuence-Analysis Based Hyperheuristics (SEQAH) for Real-World Optimisation Problems,"Selective hyperheuristics are a set of optimisation techniques that effectively optimise the search algorithm during an optimisation run by selecting combinations of lower level heuristic operations (e.g. mutation, crossover &amp; replication). They operate at the level above metaheuristics (e.g. evolutionary algorithms) and are thus able to react to changes in the search space by modifying the heuristics that are applied to the search problem. Traditional selective hyperheuristics consider single heuristics and heuristic pair performance when determining the heuristic to select next. This project will develop new methods known as a sequence analysis based hyperheuristics (SEQAH) and will investigate the use of sequence analysis techniques, taken from other computational domains such as bioinformatics and natural-language processing, to determine heuristic selection. SEQAH methods will record the search process as a sequence of pairs of heuristic application and performance, and will process this information to inform the selection of the next heuristic to apply in the optimisation. This will allow the technique to automatically select the best heuristics to apply for a given problem - effectively tuning the algorithm to new optimisation problem types, regardless of the underlying application area. By selecting from a set of heuristics, the SEQAH techniques can combine ordinary heuristic operations (e.g. mutation and crossover) with more problem-specific heuristics such as human-designed 'rules-of-thumb' into one coherent algorithm that is able to generate near-optimal solutions in less computational time.

The developed techniques will be tested on problems from the literature and a suite of real-world problems in water distribution optimisation including the design, rehabilitation and operation of large-scale water systems. The optimisation of these systems has the potential to offer improved services in terms of reliability and water quality and to reduce the future cost and environmental impact of providing clean, safe drinking water to homes across the country. The SEQAH technique also has the potential to extend beyond the water industry and should be applicable to any number of optimisation problems in many application areas due to its ability to adapt to new problem spaces online.",,"Water Distribution Impact
The initial impact from this project will be wide-ranging with beneficiaries including our partners, Mouchel Ltd., their clients (large UK water companies) and ultimately the consumers that require the provision of clean, safe drinking water with minimal financial and environmental cost. The effective optimisation of the operation and design of water distribution systems in the UK is having and will have a profound effect on the socio-economic wellbeing of the UK. With an ageing infrastructure, increasing demand and regulation and the pressure exerted by climate change UK water companies are required to optimise their operations as effectively as possible. This will yield reduced bills for customers, will go some way to preserving an increasingly scarce resource and promises to reduce the environmental impact (including carbon emissions) of water supply.

Wider Impact
The wider impact of the research will be achieved through the expansion of application fields to which the hyperheuristics will be applied. Many companies that currently do not employ optimisation as they do not have the expertise to tune the algorithms will be able to apply this self-tuning technique which can incorporate problem-specific expertise in the form of heuristics. The exploration of the relationship between problem type and optimisation algorithm will provide key information as to the extent to which algorithms must be modified and tuned from one problem type to another. The results from the investigation in water distribution problems will demonstrate the variation in heuristics and their sequencing required to generate good results for a problem type in reasonable computational time. However, towards the end of the project, we fully intend to develop and test the techniques on a wider number of application areas and industry sectors using the contacts from our industrial partner.

Background
Optimisation is a key component in delivering efficiency and cost savings to businesses in many sectors and evolutionary optimisation has been shown to provide excellent results within reasonable timeframes in a wide variety of sectors. The investigators for this project have been involved in high-impact research involving industry for a number of years. Dr Keedwell has developed evolutionary algorithms for the optimisation of city-scale water distribution systems [1] and work is ongoing under the auspices of a CASE studentship to improve discolouration risk modelling in UK water companies [2]. Prof. Savic has had significant involvement with industry in the UK and further afield through a large number of EPSRC, UKWIR &amp; FRMRC funded projects including EP/H015736/1 (start May 2010) which names some 14 industrial partners and includes both Prof. Savic (PI) and Dr Keedwell (Co-I) as investigators. This background demonstrates that the investigators have a history of working with business and achieving impact through the use of innovative computing technology and their application to problems in the water industry.

[1] Randall-Smith, M, Rogers C, Keedwell E, Diduch R, Kapelan Z, (2006) &quot;Optimized Design of the City of Ottawa Water Network: a Genetic Algorithm Case Study&quot; 8th Annual Water Distribution Systems Analysis Symposium pp1-20
[2] McClymont, K., Keedwell,E. Savic, D. Randall-Smith, M. (2011): &quot;A Hyper-heuristic Approach to Water Distribution Network Design&quot;, in the Proceedings of Computing and Control in the Water Industry 2011, University of Exeter, UK"
5,8E93BE78-0E07-4E83-8C63-CCB21B4BB70A,A Unified Model of Compositional and Distributional Semantics: Theory and Applications,"The notion of meaning is central to many areas of Computer Science, Artificial Intelligence (AI), Linguistics, Philosophy, and Cognitive Science. A formal, mathematical account of the meaning of natural language utterances is crucial to AI, since an understanding of natural language (i.e. languages such as English, German, Chinese etc) 
is at the heart of much intelligent behaviour. More specifically, Natural Language Processing (NLP) --- the branch of AI concerned with the computer processing, analysis and generation of text --- requires a model of meaning for many of its tasks and applications.

There have been two main approaches to modelling the meaning of language in NLP, in order that a computer can gain some &quot;understanding&quot; of the text. The first, the so-called compositional approach, is based on classical ideas from Philosophy and Mathematical Logic. Using a well-known principle from the 19th century logician
Frege --- that the meaning of a phrase can be determined from the meanings of its parts and how those parts are combined --- logicians have developed formal accounts of how the meaning of a sentence can be determined from the relations of words in a sentence. This idea culminated famously in Linguistics in the work of Richard Montague in the 1970s. The compositional approach addresses a fundamental problem in Linguistics -- how it is that humans are able to generate an unlimited number of sentences using a limited vocabulary. We would like computers to have a similar capacity also.

The second, more recent, approach to modelling meaning in NLP focuses on the meanings of the words themselves. This is the so-called distributional approach to modelling word meanings and is based on the ideas of the &quot;structural&quot; linguists such as Firth from the 1950s. This idea is also sometimes related to Wittenstein's philosophy of &quot;meaning as use&quot;. The idea is that the meanings of words can be determined by considering the contexts in which words appear in text. For example,
if we take a large amount of text and see which words appear close to the word &quot;dog&quot;, and do a similar thing for the word &quot;cat&quot;, we will see that the contexts of dog and cat tend to share many words in common (such as walk, run, furry, pet, and so on). Whereas if we see which words appear in the context of the word &quot;television&quot;, for example, we will find less overlap with the contexts for &quot;dog&quot;. Mathematically we represent the contexts in a vector space, so that word meanings occupy positions in a geometrical space. We would expect to find that &quot;dog&quot; and &quot;cat&quot; are much closer in the space than &quot;dog&quot; and &quot;television&quot;, indicating that &quot;dog&quot; and &quot;cat&quot; are closer in meaning than &quot;dog&quot; and &quot;television&quot;.

The two approaches to meaning can be roughly characterized as follows: the compositional approach is concerned with how meanings combine, but has little to say about the individual meanings of words; the distributional approach is concerned with word meanings, but has little to say about how those meanings combine. Our ambitious proposal is to exploit the strengths of the two approaches, by developing a unified model of distributional and compositional semantics. Our proposal has a central theoretical component, drawing on models of semantics from Theoretical Computer Science and Mathematical Logic. This central component which will inform, be driven by, and evaluated on tasks and applications in NLP and Information Retrieval, and also data drawn from empirical studies in Cognitive Science (the
computational study of the mind). Hence we aim to make the following fundamental contributions:

1. advance the theoretical study of meaning in Linguistics, Computer Science and Artificial Intelligence;

2. develop new meaning-sensitive approaches to NLP applications which can be robustly applied to naturally occurring text.",,"The proposal is an ambitious scientific endeavour aiming to solve a fundamental problem which cuts across Computer Science, Linguistics, Philosophy, Artificial Intelligence and Cognitive Science. Hence providing a solution to the problem will make a substantial contribution to the Academic knowledge base in the UK (and
beyond). Beyond the Academic knowledge base, the techniques that we aim to provide for representing meaning in a computer will increase the knowledge base available for companies operating in the area of Semantic Computing and Language Technology, e.g. in the area of web search.

Language technology will become an increasingly important technology in the 21st century, allowing people to communicate naturally with computers through text, and increasingly speech, interfaces. In order for this natural communication to take place, natural language processing needs to provide sophisticated meaning representations which allow the computer to &quot;understand&quot; natural language, which is the focus of the proposal. Hence the proposed research has the potential to impact positively on a massive proportion of the population, in the short-to-medium term through better search engines, and in the longer term through more natural language interfaces to computers. ''Computers&quot; here should be interpreted in a wider sense than desktop computers, to include the sorts of pervasive computing devices that will soon be commonplace, such as speech-enabled devices controlling various aspects of the home and car, for example.

The pathway to impact for the commercial collaborator, Metrica, is clear, in that the proposed research can impact directly on their sentiment analysis technology. Sentiment analysis is a rapidly growing area of language technology which aims to automatically determine the sentiment being expressed by consumers or the public about a particular product, or political party, for example. Given the massive increase in online content, and the increasing desire for people to make their views known via social networking sites, this technology is only going to become more important.

The main way in which the people skills pipeline will be influenced by the research is through the training of the researchers associated with the project. The training will involve working with 5 of the leading Computer Science and Informatics departments in the UK, and some internationally renowned researchers. The skills gained will be a broad set of research skills cutting across Computer Science, Linguistics, Philosophy, Artificial Intelligence and Cognitive Science. Such skilled researchers will be in increasing demand by Academia and Industry as the importance of language technology continues to grow."
6,4A6355F0-4E39-4ED2-8ACC-4000661B9F71,Sub-acute ruminal acidosis: a multidisciplinary approach to understand and prevent a multifactorial disease,"Ruminant animals, including cattle, sheep and goats, rely on microbial activity in their digestive tract to digest grass and other forages that they consume. A balanced, stable digestion (fermentation) is essential for good growth or milk production. Most livestock producers require productivity higher than that which can be sustained by forage feeding alone, and include some grain in the diet to increase production rates. Gut microbes produce acids more rapidly from the starch in grain than the cellulose in forages, leading to lower pH values prevailing in grain-fed animals. This has adverse effects on the microbes, which require near-neutral pH to perform optimally. This sub-acute ruminal acidosis (SARA) is a major economic and health issue in ruminant livestock production. Animals suffering SARA are less productive, and they suffer from necrosis of the rumen wall, liver abscesses and laminitis. SARA is often difficult for the farmer to detect - it is 'sub-acute' and can only be detected easily at slaughter. SARA is an under-researched condition, such that only a small number of papers have addressed the dietary and microbiological causes of SARA and its underlying pathology, particularly concerning the role of the large intestine. 
This project aims to understand why SARA is prevalent on some farms but not others, an observation that is common knowledge but not well documented. Farm management conditions and nutrition will be monitored in these farms, and the animals will be followed to slaughter, when the extent of pathological damage will be assessed. Samples of ruminal digesta and wall tissue will be taken for analysis and tissue necrosis, abscesses and laminitis will be scored. SARA also affects some animals but not others within a herd. Remote motion-sensing technology will be used to externally monitor movements, such as rumination activity, that may alert livestock producers to problematic animals. Post mortem analysis will also be carried out on these animals.
The root cause of SARA lies in altered gut microbiology. Digesta samples will be taken forward to describe the microbes that are present in the rumen and intestine in susceptible and non-susceptible animals, with the idea that some microbial species may be particularly important in causing the disease while others may be protective. Candidate 'probiotic' bacteria isolated from non-susceptible animals will be investigated with a view to developing them as feed additives. The role of soluble lipopolysaccharide (LPS) in the inflammation will be investigated. LPS is released when bacteria lyse - it is known as 'endotoxin' in human medicine. Materials that may bind soluble LPS to prevent inflammation will also be investigated as potential feed additives.
The overall aims are to explain the underlying mechanism of pathogenesis of SARA, to investigate if microbiome analysis can predict the severity of SARA, and to develop simple, non-invasive methods for monitoring animal behaviour relating to SARA and preventing the condition. Three academic partners, three complementary companies, Quality Meat Scotland and DairyCo are involved in the project. The industrial partners will ensure that relevance to the livestock industry is maintained throughout the project and that the pathway to impact will be short and rapid.","Sub-acute ruminal acidosis (SARA) affects some cattle herds, but not others, and individual animals within herds, but not others. The root cause of the dysfunction is the loss of control of ruminal and possibly intestinal pH. SARA is generally caused by grain feeding, but the basis of the variability requires better definition and the precise cause of inflammation and pathology remains uncertain. This project will monitor six beef and two dairy herds for nutrition and management and relate the information to post-mortem scores of ruminal wall lesions, liver abscesses and laminitis. The potential value of remote motion sensors in alerting farmers to problematic animals will be explored. Ruminal digesta and ruminal epithelial tissue samples taken post mortem will be analyzed by state-of-the-art microbiomic analysis based on 16S rRNA gene sequences, in order to identify significant differences in microbial communities in different animal types. Different Escherichia coli biotypes will be monitored. Bacteria will be isolated from least affected animals for their ability to inhibit the growth or adhesion properties of E. coli, on the basis that these animals may already possess bacteria that protect them from SARA. Candidate strains will be evaluated for their potential usefulness as probiotics to prevent or treat SARA. The hypothesis that soluble lipopolysaccharide (LPS) lies at the heart of the inflammation which results in pathology will be tested by measuring the quantities and nature of cell-free LPS in different animals. Possible feed additives that adsorb or break down LPS will be evaluated in digesta samples from badly affected animals. Demonstration animal trials will be carried out in the third year of the project to test the potential nutritional, management and feed-additive solutions identified by the project's results.","A wide international spectrum of academic researchers will benefit from the project, because SARA is a widespread problem internationally. Researchers interested in ruminant health and welfare and those whose main interest is ruminant gut microbiology will be the primary beneficiaries. They will be able to translate the results to their own production conditions where SARA is endemic. The results will be of wider interest, however, because although SARA is a disorder that afflicts ruminant species, researchers interested more generally in inflammation that originates in the gastrointestinal tract of other species, including man, will benefit. Related fields include equine health and poultry production: researchers in these areas will build upon the novelty of this project to determine if the aetiology of their target diseases, in terms of microbiology, inflammatory factors and pathology compare. New research and research proposals for these different animal species could result. Medical researchers may benefit too: the concept of soluble LPS has not yet been explored in human disease, for example. One might speculate that soluble LPS may have an important, as yet uninvestigated, role in inflammatory bowel disease (IBD) in man. Our project may spark useful medical research in this area. Microbiome profiling will provide information about how diet and the individual animal affect numbers and species of methanogenic archaea in the rumen, thereby aiding research into greenhouse gas emissions from animal agriculture. Ruminal microbial ecology in UK ruminants is under-studied in the modern era: the project will help correct that deficiency.
Veterinary practitioners and public sector advisers from colleges of agriculture and levy bodies will benefit by being better equipped to advise farmers. The project will describe management and nutritional factors associated with variations in the incidence of SARA, information that farmers' advisers can use on-farm. Vets will be able to make better diagnosis of SARA and they will be able to suggest solutions from those feed additives coming out of the SARA project. The probiotic(s) will be useful to help prevent pathological problems in susceptible farms and animals. The adsorbants may be useful in prevention but also to treat the condition. The market potential for a successful feed additive is many millions of pounds. The advisors may consider the remote monitoring system a useful mechanism to monitor the success or otherwise of strategies to ameliorate SARA. The academic partner expert in remote broadband technology has a spin-out SME that will benefit if the technology is found to be generally useful in an on-farm situation. The companies selling these additives and devices will benefit commercially. Initially, those will be from within the consortium. However, as the results are disseminated, more companies in the UK and worldwide will take up the technologies.
Farmers will benefit economically. The beef industry nationwide has a value of &pound;2.22bn at 2009 values. It follows that even a 5% overall loss of productivity caused by SARA may cost livestock producers &pound;111M p.a. Dairying has a similar incidence of therefore the loss to farmers (and the UK economy) could easily exceed &pound;200M p.a. The economic impact will be felt across the globe in countries that suffer from SARA in their ruminants, including the US (26% of mid-lactation cows suffer SARA), the Netherlands and Germany (18%). Livestock production in the UK will gain against its international competitors and, by retaining the IP in the UK, the economic competitiveness of the United Kingdom will be enhanced. Societal benefit will stem from improved economics and sustainability of food production and a healthier national herd. The livestock will benefit by being less subject to the distress of SARA, thus enhancing animal welfare."
7,EC57FD55-DD19-4DC2-BC9C-2E6320D987B6,A Unified Model of Compositional and Distributional Semantics: Theory and Applications,"The notion of meaning is central to many areas of Computer Science, Artificial Intelligence (AI), Linguistics, Philosophy, and Cognitive Science. A formal, mathematical account of the meaning of natural language utterances is crucial to AI, since an understanding of natural language (i.e. languages such as English, German, Chinese etc) 
is at the heart of much intelligent behaviour. More specifically, Natural Language Processing (NLP) --- the branch of AI concerned with the computer processing, analysis and generation of text --- requires a model of meaning for many of its tasks and applications.

There have been two main approaches to modelling the meaning of language in NLP, in order that a computer can gain some &quot;understanding&quot; of the text. The first, the so-called compositional approach, is based on classical ideas from Philosophy and Mathematical Logic. Using a well-known principle from the 19th century logician
Frege --- that the meaning of a phrase can be determined from the meanings of its parts and how those parts are combined --- logicians have developed formal accounts of how the meaning of a sentence can be determined from the relations of words in a sentence. This idea culminated famously in Linguistics in the work of Richard Montague in the 1970s. The compositional approach addresses a fundamental problem in Linguistics -- how it is that humans are able to generate an unlimited number of sentences using a limited vocabulary. We would like computers to have a similar capacity also.

The second, more recent, approach to modelling meaning in NLP focuses on the meanings of the words themselves. This is the so-called distributional approach to modelling word meanings and is based on the ideas of the &quot;structural&quot; linguists such as Firth from the 1950s. This idea is also sometimes related to Wittenstein's philosophy of &quot;meaning as use&quot;. The idea is that the meanings of words can be determined by considering the contexts in which words appear in text. For example,
if we take a large amount of text and see which words appear close to the word &quot;dog&quot;, and do a similar thing for the word &quot;cat&quot;, we will see that the contexts of dog and cat tend to share many words in common (such as walk, run, furry, pet, and so on). Whereas if we see which words appear in the context of the word &quot;television&quot;, for example, we will find less overlap with the contexts for &quot;dog&quot;. Mathematically we represent the contexts in a vector space, so that word meanings occupy positions in a geometrical space. We would expect to find that &quot;dog&quot; and &quot;cat&quot; are much closer in the space than &quot;dog&quot; and &quot;television&quot;, indicating that &quot;dog&quot; and &quot;cat&quot; are closer in meaning than &quot;dog&quot; and &quot;television&quot;.

The two approaches to meaning can be roughly characterized as follows: the compositional approach is concerned with how meanings combine, but has little to say about the individual meanings of words; the distributional approach is concerned with word meanings, but has little to say about how those meanings combine. Our ambitious proposal is to exploit the strengths of the two approaches, by developing a unified model of distributional and compositional semantics. Our proposal has a central theoretical component, drawing on models of semantics from Theoretical Computer Science and Mathematical Logic. This central component which will inform, be driven by, and evaluated on tasks and applications in NLP and Information Retrieval, and also data drawn from empirical studies in Cognitive Science (the
computational study of the mind). Hence we aim to make the following fundamental contributions:

1. advance the theoretical study of meaning in Linguistics, Computer Science and Artificial Intelligence;

2. develop new meaning-sensitive approaches to NLP applications which can be robustly applied to naturally occurring text.",,
8,5B83CB21-C592-497B-B339-2BD036779B87,Learning Highly Structured Sparse Latent Variable Models,"Technological advances have brought the ability of collecting and analysing patterns in high-dimensional databases. One particular type of analysis concerns problems where the recorded variables indirectly measure hidden factors that explain away observed associations. For instance, the recent National NHS Staff Survey of 2009, taken by over one hundred thousand staff members, contained several questions on job satisfaction. It is only natural that the patterns of observed answers are the result of some common hidden factors that remain unrecorded. In particular, such answers could arguably be grouped by factors such as perceptions of the quality of work practice, support of colleagues and so on, that are only indirectly measured.

In practice, when making sense out of a high-dimensional data source, it is useful to reduce the observations to a small number of common factors. Since records are affected by sources of variability that are unrelated to the actual factors (think of someone having a bad day, or even typing wrong information by mistake), removing such artifacts is also part of the statistical problem. A model that estimates such transformations is said to perform &quot;dimensionality reduction&quot; and &quot;smoothing&quot;.

There are a variety of methods to accomplish such tasks. At one end of the spectrum, there are models that assume the data match some very simple patterns such as bell curves and pre-determined factors. Others are very powerful, allowing for flexible patterns and even an infinite number of factors that are inferred from data under some very mild assumptions. The proposed work tries to bridge these extremes: the shortcomings of the very flexible models are subtle but important. In particular, they can be very sensitive to changes in the data - meaning some very different conclusions about the hidden factors might be achieved if a slightly different set of observations is provided. Moreover there are computational concerns: calculating the desired estimates usually requires an iterative process, a process that needs some initial guess about these estimates. So, even for a fixed dataset, results can vary considerably if such an initial guess is not carefully chosen. Our motivation is that if one does have these concerns, one might as well take the trouble of incorporating domain knowledge about the domain. The upshot: we do not aim to be general, and instead target applications where some reasonable domain knowledge exists. In particular, we focus on problems where the hidden targets of interest are pre-specified, but infinitely many others might exist. While we map our data to a fixed space of hidden variables, we provide an approach that is robust to the presence of an unbounded number of other, implicit, common factors. The proposed models are adaptive: they account for possible extra variability between the given hidden factors that would be missed by the simpler models. At the same time, they are designed to be less sensitive to initial conditions while being less sensitive to small changes in the datasets.",,"Institutions and research groups with high-dimensional data often found themselves with records that are either noisy measurements of some unobserved factors of interest, or heavily confounded by hidden common causes. In this case, estimating a small-dimensional representation of the data is useful for data summarization and visualization; as a smoothing device to estimate relationships between unobservable factors; and as an alternative representation for imputing missing values and providing features that can be used for prediction tasks and clustering.

In particular, applications that fit well the assumptions of our proposal are processes where large surveys are collected - be it in the government (NHS, Home Office and other departments), industry (marketing and employee surveys) and health services (questionnaires applied to patients and staff). Another source of applications come from natural scientists that have theories on how the data were generated (say, postulating hidden functional modules of the cell that cause gene expression levels) and want to understand the consequences of their assumptions. Moreover, in financial domains, a substantial number of variables correspond to entities such as assets (relatively easy to categorize as belonging to particular sectors of the economy) and market indicators (designed to measure theoretical market factors). 

Essentially, we plan to change how data analysis practice is done in domains where variables follow a natural and relatively simple partition, but one that is arguably not perfect and can be improved by allowing for residual associations due to infinitely many other factors. In many cases, this natural partition is often implied by the way data collection was designed. For instance, a company interested in market segmentation may find more useful (say, in a predictive sense or by providing insights that translate more directly to policy making) to generate latent embeddings of its customers according to a pre-determined set of factors used in the very design of the questionnaire - as opposed to (say) having an potentially unbounded number of factors being generated by a non-parametric model. However, since the analyst cannot predict the infinitely other implicit factors that explain the associations in the data, the model has to be robust to these other possible factors. The resulting pre-determined latent variables retain their interpretability, but now can fit the data better.

It is certainly the case that several applications do not fit this format, and there is definitely no shortage of very important domains where non-parametric models for latent structure should be the approach of the choice. In any case, it is only sensible that we provide a choice: often, sophisticated statistical methods for dimensionality reduction are ignored by the practitioner because they promise a level of automation and flexibility that simply is not there. The burden of providing information to the model is simply postponed to the stage of trying to make sense of the resulting factors. Where applicable, the philosophy of this proposal is to fit the data well while making the statistical model respect the goals of the analysis, instead of the opposite. 

Quantifying the hidden factors that explain observed associations is a hard task, one of the ultimate goals of science and data analysis in general. The proposal tackles the problem using advanced computational tools while listening to practical needs that are not explicitly exploited in the state of the art. The resulting work aims at showing that a new level of results can be achieved if the science behind such applications is exploited in their full potential."
9,ECDD3BB5-BC87-435A-8B21-374DC7B83E20,The Neural Marketplace,"Modern computers are more powerful than many ever dared expect. So it is remarkable how much today's computers still can't do. Strangely, some of the hardest tasks for computers are effortless to humans. Problems like vision, natural language comprehension, and walking control will undoubtedly require massive computing power. But the real difficulty is our inability to write down sets of rules that a computer can follow to perform these tasks. The only solution may be to develop computer systems that, like us, learn by example and by trial and error, without needing explicit instructions. The brain contains roughly as many neurons as there are transistors in a modern supercomputer. These cells are computationally more sophisticated than was once believed. But what is most amazing is their ability to organize into large, functionally coherent networks, that constantly learn and adapt to an animal's changing circumstances. This happens with no central point of control, suggesting that something about neurons causes them to automatically assemble into information-processing systems. This fellowship proposal is based on a new hypothesis, derived from neurobiological research, for how this self-organization occurs through competitive processes analogous to those of a market economy. A typical neuron in the cerebral cortex receives about 10,000 inputs, which it integrates to produce a single output, broadcast in turn to about 10,000 targets. Our new hypothesis is for a mechanism by which a neuron receives feedback from its targets, signalling how useful the information it carries is to the rest of the network. Several lines of evidence suggest that in the brain, molecules called neurotrophins can act as carriers of this feedback signal. According to the hypothesis, neurons throughout the brain constantly experiment with new information processing strategies. In most cases, the new information will not be required by the neuron's targets, no feedback will be received, and the neuron will return to its prior state. A few neurons, however, will happen upon information that is useful to the larger network, and will receive feedback causing the recent changes to be retained. In a market economy, interactions like this allow autonomous agents (people and firms) to organize into networks. A firm that makes cars buys parts from suppliers, who buy components from their own suppliers, and so on. At each stage of the supply chain, multiple firms compete to produce the best products, experimenting with new designs that, if successful, will increase market share. The decisions required to build a good car are thus distributed over a large number of agents. No one person has to understand every part of the manufacturing process; instead, decisions made by multiple individual agents cause the system to organize itself. Improvements and adaptations occur by experiments with new approaches at all levels. Scale this picture up, and you have a global economy encompassing billions of individuals. Could similar interactions organize the billions of cells in the brain into a single coherent system? And could they allow us to build scalable learning machines to solve currently intractable problems in computing?The current proposal will answer these questions by constructing a series of increasingly large market-based neural network systems, to solve a series of increasingly challenging tasks from speech recognition and robot control. This research will have impact far beyond these domains, informing the construction of learning systems for applications as diverse as vision and medical diagnosis, as well as to domains such as internet routing that require scalable self-organization of multiple computing devices. Confirming the computational validation of the hypothesis would also provide a step-change in our understanding of how the brain processes information, potentially yielding new approaches to disorders of brain organization.",,"Computer systems that learn from data, without explicit programming, were once just a dream, but are now an everyday reality. Machine learning has seen an incredible number of industrial applications including: internet search; personalization (e.g. collaborative filtering); targeted advertising and sales; financial market analysis and automated trading; credit scoring; automated customer service; voice recognition; machine vision; quality management; robotics; bioinformatics; and homeland security. The brain has long served as a model for machine learning algorithms. Of course, for industrial applications, whether an algorithm functions similarly to the brain is not important - all that matters is that it be powerful, flexible and easy to use. By these criteria, neural networks have to date had partial success. The backprop neural network has been one of the most successful machine learning systems, as it can deal flexibly with many kinds of data, and is simple, intuitive, and easy to understand. However, there are problems with backprop algorithm that severely limit its industrial applicability: the inability to efficiently train recurrent networks and simulated neurons with intrinsic dynamics; and difficulty scaling to large networks. The consequence of these drawbacks is that backprop cannot deal elegantly with dynamic applications (speech recognition and robot control being two prime examples), and that it is not scalable to high-dimensional complex problems. The current proposal will introduce a completely new idea into artificial neural network engineering, with the potential to correct backprop's two major shortcomings, the inability to train recurrent networks that perform dynamic processing and the inability to make scalable systems. This would therefore open a whole new range of application domains for neural networks including: nonlinear control; robotics; image, sound, and movie recognition; automated diagnosis of biomedical signals; speech and natural language processing; security (e.g. automated cctv analysis); econometrics and finance (analysis and prediction of multivariate time series). While the research of the current proposal is aimed at artificial neural networks, the idea of market-based interactions organizing autonomous agents may have applications to organizing a much wider class of scalable multi-agent systems. The idea of social computing : structuring interactions between autonomous computing agents along similar lines as the organization of humans into companies, societies, and economies, is an exciting recent trend in computer science. The current research will investigate how market-based mechanisms can organize networks of simulated neurons; however the principles we will learn from this may have much wider applicability. This promises applications to a wide number of domains including mobile robotics, computer animation, game programming, manufacturing, wireless networking, internet and telecommunications routing, road and air traffic control, power grid management, scheduling, and sensor fusion. This work also has impacts for public health. One in four Britons will experience some kind of mental health disorder in their lifetime, at a cost to the economy of 77.4bn each year. Rational development of treatments for mental illness cannot occur until we understand the way information is processed in the brain. The neural marketplace hypothesis has the potential to revolutionize our understanding of cortical information processing. Experimental work performed in parallel with this research will characterize the role of retroaxonal signals in vivo, and the underlying molecular pathways. Understanding how neurons organize into information-processing networks will provide key insight into the pathology of mental illness. Discovering the underlying molecular pathways would have a revolutionary impact on drug discovery and other therapies for mental illness."
10,D3C287D8-B051-4DFE-86BD-484968AABBD2,A Unified Model of Compositional and Distributional Semantics: Theory and Applications,"The notion of meaning is central to many areas of Computer Science, Artificial Intelligence (AI), Linguistics, Philosophy, and Cognitive Science. A formal, mathematical account of the meaning of natural language utterances is crucial to AI, since an understanding of natural language (i.e. languages such as English, German, Chinese etc) 
is at the heart of much intelligent behaviour. More specifically, Natural Language Processing (NLP) --- the branch of AI concerned with the computer processing, analysis and generation of text --- requires a model of meaning for many of its tasks and applications.

There have been two main approaches to modelling the meaning of language in NLP, in order that a computer can gain some &quot;understanding&quot; of the text. The first, the so-called compositional approach, is based on classical ideas from Philosophy and Mathematical Logic. Using a well-known principle from the 19th century logician
Frege --- that the meaning of a phrase can be determined from the meanings of its parts and how those parts are combined --- logicians have developed formal accounts of how the meaning of a sentence can be determined from the relations of words in a sentence. This idea culminated famously in Linguistics in the work of Richard Montague in the 1970s. The compositional approach addresses a fundamental problem in Linguistics -- how it is that humans are able to generate an unlimited number of sentences using a limited vocabulary. We would like computers to have a similar capacity also.

The second, more recent, approach to modelling meaning in NLP focuses on the meanings of the words themselves. This is the so-called distributional approach to modelling word meanings and is based on the ideas of the &quot;structural&quot; linguists such as Firth from the 1950s. This idea is also sometimes related to Wittenstein's philosophy of &quot;meaning as use&quot;. The idea is that the meanings of words can be determined by considering the contexts in which words appear in text. For example,
if we take a large amount of text and see which words appear close to the word &quot;dog&quot;, and do a similar thing for the word &quot;cat&quot;, we will see that the contexts of dog and cat tend to share many words in common (such as walk, run, furry, pet, and so on). Whereas if we see which words appear in the context of the word &quot;television&quot;, for example, we will find less overlap with the contexts for &quot;dog&quot;. Mathematically we represent the contexts in a vector space, so that word meanings occupy positions in a geometrical space. We would expect to find that &quot;dog&quot; and &quot;cat&quot; are much closer in the space than &quot;dog&quot; and &quot;television&quot;, indicating that &quot;dog&quot; and &quot;cat&quot; are closer in meaning than &quot;dog&quot; and &quot;television&quot;.

The two approaches to meaning can be roughly characterized as follows: the compositional approach is concerned with how meanings combine, but has little to say about the individual meanings of words; the distributional approach is concerned with word meanings, but has little to say about how those meanings combine. Our ambitious proposal is to exploit the strengths of the two approaches, by developing a unified model of distributional and compositional semantics. Our proposal has a central theoretical component, drawing on models of semantics from Theoretical Computer Science and Mathematical Logic. This central component which will inform, be driven by, and evaluated on tasks and applications in NLP and Information Retrieval, and also data drawn from empirical studies in Cognitive Science (the
computational study of the mind). Hence we aim to make the following fundamental contributions:

1. advance the theoretical study of meaning in Linguistics, Computer Science and Artificial Intelligence;

2. develop new meaning-sensitive approaches to NLP applications which can be robustly applied to naturally occurring text.",,
11,6E350B12-553C-44BC-B3BB-E12666A8A275,Providing Autonomous Capabilities for Evolving SCADA (PACES),"SCADA (Supervisory Control And Data Acquisition) has proved to be a powerful and successful technology. Across the world SCADA systems are deeply integrated into the large scale infrastructures used in power generation, power transmission, radioactive waste processing, manufacturing, refining, water treatment, space exploration and various military applications. Traditionally SCADA implementations adopt a centralised architecture, either across one single geographical site, or across multiple sites using proprietary communications protocols. In spite of this trend to distributed solutions the supervisory control function continues to be performed at a centralised location, typically supervised by trained personnel using compliant HMI (Human-Machine Interface) tools. This centralised architecture presents overwhelming problems for system designers needing to integrate ever more diverse components and to scale to larger, more complex deployments. Moreover, this increasing complexity can realistically only be achieved through the adoption of autonomous or intelligent system components that can replace the supervisory function provided by humans. 

More recent trends show SCADA systems incorporating widely available COTS (Commercial Off-The-Shelf) software to deliver functionality and adopting recognised communications standards such as TCP/IP to facilitate integration and remote administration. The use of COTS increases available functionality and robustness but introduces new vulnerabilities. Attackers can exploit their knowledge of such widely available components and attacks can be 'designed' in ways previously not possible with the earlier proprietary systems. Closely linked to security is the need for fault tolerance. Here too we must develop intelligent SCADA systems that can self-monitor and detect anomalous behaviour (resulting from malicious attack or component fault) and invoke response that protects the goals of the whole system.

The next generation of SCADA systems must develop a set of autonomous and intelligent capabilities to address a number of pressing requirements. Problems presented by increasing process complexity, advances in sensor technologies, the increasing demand for integration with other enterprise solutions, increasingly inadequate security protection and a higher required standard of fault tolerance must all be solved. To provide solutions to these problems the proposed research focuses on the development of a novel Multi-Agent System (MAS) architecture. This architecture is integrated with an advanced event reasoning framework that can fully exploit sensor data and domain knowledge, including treatment of inherent uncertainties, incompleteness and inconsistency to autonomously infer system state and crucially to inform human and autonomous decision makers in the system. 

Increased autonomy presents new challenges of system security. The next generation of autonomous SCADA must detect, diagnose and respond in real-time to security breaches and anomalous behaviours. The proposed research exploits new Deep Packet Inspection capabilities and network traffic analysis to develop a unique 'cyber-sensor', providing visibility of overall system health and integrity to human operators and autonomous components. Brought together, these novel research outputs will equip the next generation of autonomous SCADA systems with the capabilities to respond in real-time to evolving situations, self-awareness of changes and abnormal behaviours, fault and noise tolerance, and real-time decision support.",,"Beyond the immediate academic beneficiaries it is anticipated that the following groups will also benefit from the research undertaken:

(1) Corporate and Governmental Bodies: Bodies tasked with predicting future trends in large-scale autonomous and intelligent control systems, such as next generation SCADA will gain insight into emerging capabilities, opportunities for exploitation, security planning and pathways to integration. We believe all six industrial partners currently linked with this call (BAE Systems, NNL, Sellafield Ltd, Cisco, SciSys and Network Rail) will have opportunity to benefit from the findings of this proposal.

(2) UK PLC: The increasing scale and complexity of industrialisation in various parts of the globe and the pressures for increased efficiency in such processes presents an opportunity for the UK economy to place itself at the centre of a vast and expanding sector facilitating such large scale infrastructures offering autonomous and intelligent implementations.

(3) World Citizens: The pervasiveness of supervisory control systems such as SCADA mean that efficiency improvements in these systems have far reaching effects. Efficiencies in service delivery and security inevitably enable improved, cheaper production and delivery of goods and services to global citizens. These efficiencies could manifest themselves in many areas such as reliable treatment and distribution of drinking water, reliable fault-tolerant treatment of nuclear waste, efficient power distribution to electrified railways facilitating cheaper rail travel, increased confidence in the security of critical national infrastructure installations.

The proposers envisage that the proposed research will deliver technology classified at TRL4 (Technology Readiness Level 4) on completion of the programme in 36 months from the start date. The technology will be demonstrable via full scale simulations. Models will be ready for test or evaluation in field scenarios."
12,F1D1AE3A-334D-4B03-B9F1-A754CF5776E7,A Unified Model of Compositional and Distributional Semantics: Theory and Applications,"The notion of meaning is central to many areas of Computer Science, Artificial Intelligence (AI), Linguistics, Philosophy, and Cognitive Science. A formal, mathematical account of the meaning of natural language utterances is crucial to AI, since an understanding of natural language (i.e. languages such as English, German, Chinese etc) 
is at the heart of much intelligent behaviour. More specifically, Natural Language Processing (NLP) --- the branch of AI concerned with the computer processing, analysis and generation of text --- requires a model of meaning for many of its tasks and applications.

There have been two main approaches to modelling the meaning of language in NLP, in order that a computer can gain some &quot;understanding&quot; of the text. The first, the so-called compositional approach, is based on classical ideas from Philosophy and Mathematical Logic. Using a well-known principle from the 19th century logician
Frege --- that the meaning of a phrase can be determined from the meanings of its parts and how those parts are combined --- logicians have developed formal accounts of how the meaning of a sentence can be determined from the relations of words in a sentence. This idea culminated famously in Linguistics in the work of Richard Montague in the 1970s. The compositional approach addresses a fundamental problem in Linguistics -- how it is that humans are able to generate an unlimited number of sentences using a limited vocabulary. We would like computers to have a similar capacity also.

The second, more recent, approach to modelling meaning in NLP focuses on the meanings of the words themselves. This is the so-called distributional approach to modelling word meanings and is based on the ideas of the &quot;structural&quot; linguists such as Firth from the 1950s. This idea is also sometimes related to Wittenstein's philosophy of &quot;meaning as use&quot;. The idea is that the meanings of words can be determined by considering the contexts in which words appear in text. For example,
if we take a large amount of text and see which words appear close to the word &quot;dog&quot;, and do a similar thing for the word &quot;cat&quot;, we will see that the contexts of dog and cat tend to share many words in common (such as walk, run, furry, pet, and so on). Whereas if we see which words appear in the context of the word &quot;television&quot;, for example, we will find less overlap with the contexts for &quot;dog&quot;. Mathematically we represent the contexts in a vector space, so that word meanings occupy positions in a geometrical space. We would expect to find that &quot;dog&quot; and &quot;cat&quot; are much closer in the space than &quot;dog&quot; and &quot;television&quot;, indicating that &quot;dog&quot; and &quot;cat&quot; are closer in meaning than &quot;dog&quot; and &quot;television&quot;.

The two approaches to meaning can be roughly characterized as follows: the compositional approach is concerned with how meanings combine, but has little to say about the individual meanings of words; the distributional approach is concerned with word meanings, but has little to say about how those meanings combine. Our ambitious proposal is to exploit the strengths of the two approaches, by developing a unified model of distributional and compositional semantics. Our proposal has a central theoretical component, drawing on models of semantics from Theoretical Computer Science and Mathematical Logic. This central component which will inform, be driven by, and evaluated on tasks and applications in NLP and Information Retrieval, and also data drawn from empirical studies in Cognitive Science (the
computational study of the mind). Hence we aim to make the following fundamental contributions:

1. advance the theoretical study of meaning in Linguistics, Computer Science and Artificial Intelligence;

2. develop new meaning-sensitive approaches to NLP applications which can be robustly applied to naturally occurring text.",,
13,CD3616FD-37AA-4956-8148-FE670972916B,Modelling the human heart: an integrated experimental and computational study,"The proposed project will provide new experimental data from human hearts that will be used to make a realistic computer model of the human heart. 

Heart disease is the main cause of mortality in the developed world. This disease is characterised by a reduced ability of the heart to pump blood, due to changes to the mechanical and electrical properties of heart cells. However, despite extensive experimental studies, the complicated sequence of events leading from altered function at the cellular level to life-threatening pump failure remains poorly understood. 

This situation has motivated rapid advances in the development of computer models of the heart that now provide new and powerful quantitative tools for understanding the triggers and progression of heart disease. These models have delivered an important means for capturing the complex function of the heart by establishing a consistent, quantitative and multi-level framework for integrating measurements and understanding. From this work, important insights into the inter-relationships between cell contraction, heart shape and muscle structure have already been revealed.

However, while the scientific importance and significant clinical potential in this approach is widely acknowledged, the promise of these computer models to increase our understanding of human heart function remains largely unfulfilled. This is because the vast majority of cardiac mathematical models are currently developed and validated using data collected from measurements in animal, rather than human, experiments. Furthermore these experiments are often performed under conditions that are very different from the environment of either a normal or diseased heart in the body. In particular, individual cells are held at constant length and are studied in the cold, whereas in the intact heart the cell length changes during the heartbeat and the cells are at body temperature; these differences alter profoundly the amount of force muscle cells can produce.

This situation means that there are inherent limitations to using animal-based model frameworks and experimental data for understanding human heart function and for answering clinical questions. Thus an important challenge to address is the development of a model for the human heart that can be applied directly in clinical contexts. 

Recently we have developed the capacity to collect unique data on isolated cells from human hearts. Importantly these measurements can be performed at body temperature and the cell length can be changed to mimic the full range of conditions the cells experience as the heart beats. This information enables, for the first time, the ability to construct a model that will be able to directly capture human heart function. To achieve this goal, mathematical equations representing these human heart cells will be developed and combined using high-performance computers to construct a computational model - a 'virtual' heart. The heart's pumping capacity will then be computed under different conditions and linked back to the human heart experiments. 

Using this virtual heart we will be able to isolate the important mechanisms that govern how the human heart responds to meet the wide range of requirements placed on it. Specific examples include understanding the cellular changes that enable the heart to pump larger volumes of blood, such as in exercise, or produce more force in conditions of high blood pressure. 

Finally, by publishing the experimental data and model, and by making all of our computer code freely available, we will enable other heart modellers to use our model to perform their own human-based simulations. Through this work this study will provide a new way to investigate and understand human heart function and ultimately the progression of heart disease, together with ways to improve its diagnosis and prevention.","Rapid advances in computing now provide the potential for the development of powerful quantitative models for understanding the working of the human heart in health and disease. Finite-element based models that accurately represent heart structure and microstructure have incorporated cellular models of electrical activation and force generation to produce cardiac contraction. However, all previous heart models are severely limited in their applicability to the human heart because they used myofibrillar contraction data that were obtained from experiments using animal myofibrils at sub-physiological temperatures (the latter due to preparation degradation at higher temperatures); the contractile properties of human cardiac myofibrils at 37oC are currently unknown.

We have devised a novel technique for studying the contraction of human (and animal) cardiac myofibrils at 37oC. In a comprehensive study, we will establish the calcium-, sarcomere length-, and velocity-dependence of force production during calcium-activated contractions of human cardiac myofibrils. This rich, physiologically-relevant dataset will be shared with the academic community. In addition, we will use this dataset to populate a mathematical model of human myofibrillar contractile dynamics, the predictions of which will be tested with further experimentation, so that we can refine the model until it provides excellent simulations of cross-bridge dynamics under all conditions relevant to heart function. This myofibrillar model will then be integrated into a model of the contracting cardiac myocyte which will, in turn, form the core element in a realistic whole-heart computational model. The heart model will be validated against real data obtained from patient imaging studies. The coupling of the unique human data acquired during this project with our heart model will be used to explore which parameters of myofibrillar function are key determinants of the pump function of the working human heart.","The output of this project will be a novel human heart contraction model and a unique analysis of the coupling mechanisms by which individual cardiac cells regulate their contractile response to produce integrated contraction of the whole heart. This work will provide the ability to quantitatively characterise the highly complex regulation of contraction in the human heart that is currently beyond analysis by clinical observation and intuition alone. For these reasons the project output has potential to provide significant impact for cardiac healthcare workers, the patient cohorts they treat and the cardiac imaging industry.

Specifically the work of this proposal will enhance the capacity of our simulation software to predict human physiological function This addition will in turn enable the application and implementation of our models within the clinical environment and the ability to engage medical imaging companies seeking to integrate simulation software within their current hardware products. Two specific examples of translational pathways to impact are outlined in further detail in the sections below.

Improved patient selection for Cardiac Resynchronisation Therapy (CRT): Despite the increasing prevalence of heart failure, it continues to have a terrible prognosis with 50% mortality in the first 3 years after diagnosis, worse than most malignancies. Randomized, controlled clinical trials have shown that some patients benefit from CRT, in which a pacemaker is embedded into the heart wall. However, there are still major issues associated with patient selection, since up to a third of the patients treated do not show any response to this very expensive therapy. Our preliminary work has shown the fundamental cellular excitation-contraction coupling mechanisms are likely to underlie the clinical response. However, these results are sensitive to the exact parameterization. If this result is confirmed via detailed model development with consistent human-derived cell data, this project will have a significant impact on the treatment and selection of CRT patients. 

Coronary Artery Disease (CAD) analysis of images: Despite its significance, the determination of optimal clinical diagnosis and treatment strategies for CAD patients remains problematic. Exacerbated by the high risk of the disease, and the difficulty in excluding it, the clinical problem is demonstrated tangibly by the large number of patients who currently undergo invasive angiography unnecessarily and achieve negative results. Central to both diagnosis and treatment of CAD is the relationship between perfusion and contraction. In particular, understanding if a region of tissue is mechanically viable and thus a candidate for reperfusion is extremely valuable. The application of a human contraction model using the strain- and motion-derived imaging information will provide the ability to estimate tension and thus contractility of tissue. From this information a much-improved prediction of the resulting benefits from revascularisation can be determined, which, in turn, will provide the ability to select patients more accurately. 

Finally, in addition to dissemination of this research in high-quality journals and scientific meetings, the investigators will continue their work engaging with media to further the impact of the project with the general public. Recent examples of this engagement include appearance on the BBC Horizon documentary &quot;How to Mend a Broken Heart&quot; and winner of the BHF-sponsored Reflections on Research award."
14,106018E1-64BF-4702-895B-12D326C3E905,FuzzyPhoto,"Study of photography as cultural history is relatively new and under-exploited discipline. It is important because the early history of photography coincides with significant global scientific, industrial, artistic, social, political and economic changes that inform understanding of the spread of scientific ideas, the relationship between science and art, the interplay between new technologies, popular culture and commerce, and the creation of personal and national identities. Access to photohistorical resources is essential for future cross-disciplinary research but these resources are often ephemeral, fragile, widely dispersed, poorly documented and difficult to access, although of enormous scope. Poor and inconsistent levels of documentation make it difficult to assess the significance of material beyond the relatively small nucleus of already well-known and heavily researched artists and scientists. However, image collections are increasingly being published online and search engines are becoming increasingly powerful, creating a timely opportunity to match photographs with other textual sources that can enrich our understanding without travel to numerous archives. 
De Montfort University has created an extensive corpus of digital resources for researchers of 19th century photography comprising photographic exhibition catalogues and collections of letters. This includes two databases of the earliest known photographic exhibition catalogues: Photographs Exhibited in Britain 1839-1865 (PEIB) http://peib.dmu.ac.uk and Exhibitions of the Royal Photographic Society 1870-1915 (ERPS) http://erps.dmu.ac.uk. 
These combined resources comprise the single most comprehensive record of British photographic exhibitions at this time. But these early exhibition catalogues were often devoid of pictures. A further problem is that amongst the visual arts, photography is unique - multiple versions of the same image can be produced and exhibited simultaneously at diverse locations. Photographs were commonly exhibited/published more than once, at different times, with different titles and even by different people, thus associating a specific exhibition catalogue reference with a specific image published elsewhere can be a complex and involved process. 
This project will develop and test computer based &quot;finding aids&quot; that will be able to recommend potential matches between historical exhibition catalogue entries and images of photographs in online collections even where there is not a precise match. Incomplete data sets and imprecise information are common problems in arts and humanities research so the results of this research will be widely applicable across a wide range of subjects, allowing researchers to save considerable time and travel in the early stages of their research when identifying material most likely to be of interest to their studies and suggesting possible connections that would not otherwise be easily recognised using conventional research methods. The project outcomes will enable museums, libraries and archives to enhance the value and utility of their collections and of their online services through increased information, improved accuracy and functionality. Within the UK alone over 10,000 galleries, museums and archives could potentially benefit from this research. Within the private sector, beneficiaries will include commercial dealers and auction houses concerned with attribution and value. More accurate identification of artefacts such as photographs can help buyers and sellers and even help to prevent inadvertent export of nationally important treasures. The general public will benefit from improved accuracy and detail of information about objects in museums, libraries and archives, and lay communities of interest such as those carrying out genealogical or local history research will benefit in particular from increased access and awareness of information about historical photographs and related objects.",,"In addition to academic researchers, teachers and students, direct beneficiaries of this research will include galleries, libraries, archives and museums with significant holdings of artefacts likely to have been previously exhibited or for which separate text records exist elsewhere. Collection holders will be able to enhance the value and utility of their collections and of their online services through increased information, improved accuracy and functionality. This is likely to include not only photographs but paintings, prints, sculpture, jewellery, furniture, fashion, costume, toys, textiles, scientific equipment, industrial products etc. In each of these contexts similar questions about attribution, manufacture, historical sequencing and development are likely to be pertinent and amenable to research using the proposed approach.There are about 2,500 museums in the UK. Of these 54 are designated National Museums. In addition to six UK National Libraries there are 979 Academic Libraries and 4,517 Public Libraries (2008-09 statistics) Out of a total estimate of 2,389, there are 122 nationally recognised archives, 654 local and 328 university, as well as 1,224 special and 61 business archives. Thus in total within the UK alone over 10,000 institutions could potentially benefit from this research.
Within the private sector beneficiaries will include commercial dealers and auction houses concerned with attribution and value. More accurate identification of artifacts such as photographs can help buyers and sellers and may even help to prevent inadvertent export of nationally important treasures. 
The general public will benefit generally from improved accuracy and detail of information about objects in galleries, museums, libraries and archives and lay communities of interest such as those carrying out genealogical or local history research will benefit in particular from increased information about historical photographs and other objects available online. While the value of such benefits is hard to quantify, recent rapid growth in such activities is testament to their importance in a society characterized by high levels of mobility, multi ethnicity, breakdown of traditional family units and ties and social, economic and political isolation of disadvantaged minority groups.
Many of these benefits are potentially realizable on successful completion of this research because the results can be implemented by a significant number of leading heritage institutions who are partners in this proposal and &quot;finder/recommender&quot; tools produced by the project will be freely available as open source to the developer community. The inclusion of US and European partners will ensure that impact will extend beyond the the UK. 
Academic staff working on the project will gain new skills in data analysis and development of computational finding aids. More importantly there will be significant transfer of these learning outcomes to the partner organizations in the heritage sector."
15,66C99D4D-F8B1-4549-982D-61E81A8D2553,RefNet: An interdisciplinary network focussing on reference,"RefNet proposes an EPSRC research network advancing the interdisciplinary study of reference by linking communities that have so far tended to work in near isolation from each other. 

Reference is the process of making sure that a user/receiver can identify an entity - for example a person, thing, place, or event. Reference can be accomplished through language, but also through for example pointing, gaze, highlighting, and combinatons of the above. Reference can be considered the &quot;anchor&quot; of communication, and as such it is crucial for practical applications, from robotics and gaming to embodied agents, satellite navigation, and multimodal interfaces. 

RefNet will foster collaboration between researchers in Computing Science and researchers in Psycholinguistics, and between researchers and practitioners in industry and the public sector. Through the study of reference, RefNet will build a wider interdisciplinary skills base for research on communication that will be of crucial importance at a time when computing science is becoming closely interlinked with other disciplines.",,"The main purpose of RefNet is to change the way in which research communities interact, and the way in which they are able to influence, and be influenced by, practical applications.

Reference is a key mechanism in communication, &quot;anchoring&quot; words to things, people, events, times, places, etc. Understanding reference is therefore an important aspect of human knowledge. RefNet will contribute significantly to this understanding, though its skill building activities (e.g. the Summer School, bursaries, lab visits), and through improved interactions between Computational Linguists and Psycholinguists. In many cases, understanding will take the form of precise quantitative models.

Reference plays a crucial role in many practical applications of computing science, such as multimodal interfaces, Geographic Information Systems, spoken dialogue, gaming, Satellite Navibation, and robotics. RefNet will help the design of these applications by offering the sophistication of computational psycholinguistics. Confidence in the potential for RefNet to succeed in this regard can be gathered from the support letters (attached) from AT&amp;T, Nokia, and Data2Text. 

Reference also plays an important role in daily life, including situations where errors can be fatal, such as emergency medicine, where protocols for spatial reference (e.g., to explain where on a mountain the victim of a mountaineering accident lies) are still error prone. A better understanding of reference and direction giving could improve such protocols. If this were to happen, lives would be saved, for example because quick and accurate direction finding is often crucial (e.g. in rural areas)."
16,C7C30806-E56E-42BC-BB3F-6A4C49D85120,Network in Internet and Mobile Malicious Software (NIMBUS),"The transformation of communication and computing technologies in terms of accessibility, ubiquity, mobility and coverage, has enabled new opportunities for personalised on-demand communication (e.g. Facebook, Twitter). This is in addition to new market places for e-commerce and e-businesses, personalised platforms for e-governments and a vast range of new user-centric applications and services. The number of mobile apps (iPhone, Android) taking advantage of the cloud infrastructure has risen beyond several hundred thousand, reshaping the way we communicate and socialise.
 
This shift in communication technology and services has also led to the emergence of unforeseen types of security and privacy threats with social, economic and political incentives, resulting in major research challenges in terms of the protection and security of information assets in storage and transmission. 
Therefore, Digital Security is vital in ensuring the UK is a safe place to do business, can act as a source of competitive advantage for foreign direct investment and provide a platform for SMEs and large corporations alike to develop products that use or supply this security market.

Recent years have seen a massive growth in malware, fuelled by the evolution of the Internet and the migration from malware written by hobbyists to professionally devised malware developed by rogue corporations and organized criminals, primarily targeted for financial or political gain. In 2010, Symantec identified more than 240 million new malicious programs; albeit that many of these are variants of existing malware. Another report, suggests that the actual malware family count is between 1,000 and 3,000.

The detection of malware is a major and ongoing problem. The battle against malware has escalated over the past decade as malware has evolved from simple programs that had little ability to evade detection, the main objective of which was to cause havoc, to more complex programs that target profit and deploy sophisticated evasion techniques.

The focus of the NIMBUS network of researchers is to act as a catalyst to develop a balanced programme of both blue skies research and near term applied research that will assist in the fight against cyber crime in the UK. Malware related cyber threats are global in nature, hence it is essential that an international approach is taken to address these issues. Only a global network of centers of excellence is expected to provide the essential breadth and depth of know-how and the necessary critical mass of specialist competencies for resolving major Cyber Security challenges. NIMBUS will act as the UK's interface to international engagements with research networks in Europe, US and Asia.",,"Collaboration
CSIT was established at Queen's University Belfast with the main objective of promoting industrial partnership and the commercial exploitation of University research. CSIT has an active Industrial Advisory Board with wide participation from UK and international industrial experts and partners. Manned by senior representatives from the security, systems integration, IT and networking industries complemented by staff from UK government and security agencies, the board provides an active pathway for the commercial exploitation of research. Malware research is of particular interest to the following board members: BAE Systems, Advanced Technology Centre; Thales UK, Research &amp; Technology Centre; Home Office Science &amp; Technology Branch representatives, IBM security products division, DSTL and GCHQ. We have promoted and demonstrated our work to all these organisations. These existing and well developed relationships offer an excellent basis for establishment of a network of research excellence which engages directly with many of the key UK stakeholders. In addition to the CSIT industrial advisory board we have well developed contacts in the UK retail banking &quot;virtual task force&quot; and the Metropolitan Police eCrime Unit.

Exploitation
Queen's University Belfast also has an excellent track record of developing spin-out companies to commercialise research, including several within CSIT. In 2009 the University was awarded the prestigious UK &quot;Times Higher Education Entrepreneurial University of the Year&quot; award in recognition of the sustained economic contribution made by spin-out companies from Queens. The institute has recently offered all staff and research students the opportunity to attend the ECIT Entrepreneurship Programme. The programme will consist of a series of workshops delivered at ECIT over 5 consecutive days and completion of the programme leads to the QUB Certificate in Entrepreneurship Studies. 

Capacity and Involvement
CSIT has a dedicated commercial team, responsible for initiating industrial contacts and developing commercial opportunities such as intellectual property licensing, targeted research contracts and knowledge transfer partnerships. The CSIT commercialisation team raises awareness of our security research by actively participating in (e.g. taking stands at) industry trade shows such as the RSA Europe Conference, and the InfoSecurity Europe Conference held in London. They develop marketing collateral and handle press relations. The commercial team also have the services of a technical marketing executive with extensive experience of Web site management and social media campaigns. CSIT will use these proven, well established structures and mechanisms in order to promote the work of the NIMBUS malware research network, generate interest among (and receive feedback from) stakeholders, disseminate knowledge and exploit the intellectual property and know-how generated by the work."
17,5154D51C-8B68-4B0D-811D-64CDF691126C,REFNET: An interdisciplinary network focussing on reference,"RefNet proposes an EPSRC research network advancing the interdisciplinary study of reference by linking communities that have so far tended to work in near isolation from each other. 

Reference is the process of making sure that a user/receiver can identify an entity - for example a person, thing, place, or event. Reference can be accomplished through language, but also through for example pointing, gaze, highlighting, and combinatons of the above. Reference can be considered the &quot;anchor&quot; of communication, and as such it is crucial for practical applications, from robotics and gaming to embodied agents, satellite navigation, and multimodal interfaces. 

RefNet will foster collaboration between researchers in Computing Science and researchers in Psycholinguistics, and between researchers and practitioners in industry and the public sector. Through the study of reference, RefNet will build a wider interdisciplinary skills base for research on communication that will be of crucial importance at a time when computing science is becoming closely interlinked with other disciplines.",,
18,5F63532D-88D4-4466-BED6-C694EE909FCC,Real World Optimisation with Life-Long Learning,"Many practical problems arising in industrial domains concerned with operating sustainably, meeting demand and minimising costs cannot be solved exactly. Meta-heuristic optimisation techniques have been widely developed in academia to solve such problems with much success reported in the literature. However, there remains a worrying void between scientific research into optimisation techniques and those problems faced by end-users and addressed by commercial optimisation software vendors. From a commercial perspective, the problems addressed by academia are too simplistic compared to those faced in the real-world, failing to embrace many real-world constraints. From the scientific perspective, researchers have also identified a &quot;lack of advanced metaheuristic techniques in commercial software'' which has been attributed in part to the academic community failing to demonstrate that their solutions are applicable to the needs of the commercial world, and in part to academics failing to impart their message the industrial community.

Meta-heuristic approaches can be costly to develop as they generally require human expertise to integrate specialist knowledge into an algorithm, and expertise in heuristic methods to design and tune algorithms. Recent research has therefore focused on automated algorithm design and configuration which produce tuned solvers that perform well on either individual problems or across suites of problems. One branch of this field is hyper-heuristics, which operates on a space of low-level heuristics, searching for combinations of heuristics which exploit the strength and compensate for the weaknesses of individual known heuristics. The resulting algorithms are cheap to implement, require less human expertise, have robust performance within a problem class, and are portable across problem domains. These features compensate for some reduction in solution quality compared to tailor-made approaches, while still ensuring solutions of acceptable quality. However, most automated design approaches fail to incorporate or recognise a crucial human competence; human beings continuously learn from experience - by generalising observations and feedback, they are able to update their internal problem-solving models in order to continuously improve them, and adapt to changing circumstances. The failure of computational solvers to exploit previous knowledge both wastes useful knowledge and potentially hinders the discovery of good solutions. Furthermore, if the characteristics of instances of problems in the domain change over time, solvers may need to be completely re-tuned or in the worst case redesigned periodically.

This proposal addresses these dual concerns raised above. We propose a novel lifelong-learning hyper-heuristic system which addresses current deficiencies inherent in current systems: it will exhibit short-term learning, producing fast and effective solutions to individual problems and at the same time, long-term learning processes will enable the system to autonomously adapt to new problem characteristics over time. It therefore exploits existing knowledge whilst simultaneously adapting to new information. Secondly, by working closely with two collaborators, a commercial routing software vendor and a forestry expert, our research will be directly informed by real-world problems, accounting for real constraints and performance criteria, thereby producing economic impact. Future advances in optimisation techniques will be facilitated by the development of a problem generator and a number of problem suites which reflect real-world priorities and constraints, derived from actual problem data provided through our collaborators and defined in conjunction with metrics which reflect not only economic drivers but also address environmental impact and the reduction of carbon emissions. This information database will be widely disseminated to provide an extensive platform for future research.",,"The proposed research has economic impact and societal impact.

From an economic perspective, there is direct economic benefit to be derived from optimising the daily activities of companies, in the domains addressed of routing, forestry management and packing but also more widely in related domains. End-users of the research - companies who need to optimise their activities - will benefit from algorithms which increase efficiency, drive-down costs and improve sustainability (in economic terms and relating to environmental impact). Crucially, the proposed methods are cheap to implement and adapt autonomously to dynamic environments, thereby reducing the need for specialist expertise to implement or ongoing maintenance costs. Users will have confidence in the developed algorithms due to the use of low-level heuristics accepted by users as opposed to black-box methods, and the rigorous performance evaluation we will conduct which places significant emphasis on calculating the worst-case performance bounds and reliability in order to address user concerns. Secondly, vendors of optimisation software will benefit from access to state-of-the-art optimisation techniques which can be integrated with confidence within existing systems, thereby having direct economic benefit. The close collaboration with commercial partners proposed in this research takes important steps towards aligning research within academia with real-world priorities, which will directly result in economic benefit in the future.

The research also has societal impact. The emphasis on sustainability as a performance criteria when optimising solutions to problems by considering carbon emissions and the environmental impact of solutions (particularly in the work in forestry management addressed) will have future benefits to society as a whole in moving towards a greener, more sustainable way of life."
19,703CA69C-6E5B-4F22-B163-F82B11148F55,Stories at the Dentist.,"A major challenge facing the health and wellbeing of people with intellectual disabilities is the level of anxiety experienced by both the disabled patient and the dentist. When a patient with intellectual disabilities is anxious, they may become defensive and exhibit challenging behaviour when the dentist attempts to treat them. As a result of this, the procedure may have to either be abandoned or the patient sedated. The need for sedation is a common problem in dentistry as patients with intellectual disabilities often require a general anaesthetic for simple dental treatment. This carries increased risks, a longer recovery time and increases the cost of the procedure to the NHS.

One reason for patients' anxiety is the difficulty in communication between patient and dentist. In particular, it can be difficult for dentists to provide patients with intellectual disabilities with information about the treatment they require in a way that they can understand. An inability to understand what is about to happen or to express feelings makes a visit to the dentist frightening and stressful. In addition, it is considered good practice to obtain informed consent or assent from all patients. However, clinicians are often unsure if a patient with intellectual disabilities has understood explanations of procedures. It is difficult for people with intellectual disabilities to understand how abstract information relates to them with the result that, compared to the general population, patients with intellectual disabilities find it significantly more difficult to make healthcare decisions. 

This study aims to develop a computer based communication system to support people with intellectual disabilities to understand dental procedures with the aim of reducing anxiety for both patients and clinicians, and to enable patients to be more involved in the decision making process. The system will run on a tablet device, e.g. an iPad. The system will automatically generate a story about a dental procedure which is personalised to the patient. This will allow the dentist to explain the procedure to the patient using a graphical interface. The patient will be able to use the system to ask questions about the procedure and express their feelings.

We know that improved communication reduces the anxiety in both the patient and the dentist. This research will investigate whether giving more information to patients with intellectual disabilities improves the outcomes for: (i) the patient; (ii) the practitioner; and (iii) the health service by reducing the time and the resources required to support patients with intellectual disabilities. While this has the potential to produce benefits across the health service, this study will focus on dental health.",,"People with ID are more likely to experience anxiety at the dentist. The resulting problems of challenging behaviour and the need for sedation require increased resources. We will develop new technology that supports better communication between people with Intellectual Disabilities (ID) and dentists. This system will have the potential of reducing both the resources needed to treat people with ID and, more importantly, the risks associated with the need to resort to general anaesthesia when patients are anxious or exhibit challenging behaviour. We will disseminate the results of this research by targeting: 

People with Intellectual Disabilities (ID), their family and support staff are the main beneficiaries of our work. Our approach to working with people with ID is unique in that they are actively involved in the research. This active involvement by a hard-to-reach population is exciting as we know from mainstream research that early user involvement increases the successful adoption of technology, a major issue with Assistive Technology. Some participants may be involved in dissemination activities in collaboration with the NHS, e.g. as speakers at workshops or as spokespeople at public dissemination events. The study will have a direct impact on family and support staff, providing strategies to reduce anxiety of those for whom they care. These strategies may be generalisable to other situations in their lives. We will create an online community. We will engage interested parties to reduce the chasm between innovation and adoption of new technology. In addition, we will run workshops in collaboration with disability charities to present our results to people with ID, their families and support staff. This will raise awareness of the importance of using accessible interactive information with people with ID to support their involvement in medical (and other) decision making.

Medical Practitioners: This work is a result of collaborative teaching on special needs training for newly qualified (foundation level) dentists in Scotland. Together, we have identified the potential that communication technology has to support better interaction between professionals and patients. Local dental practices will act as test beds to develop a working prototype. The research outcomes will contribute to good practice for special needs dentistry, with practical support and guidance on how to include people with ID in decisions making, in particular health care. This will feed into the national training programme for foundation level dentists, providing a unique dissemination route for uptake in Scotland and further afield. We will also strengthen our links with general medical training and will seek contribute to the training on ID through our links with the local learning disability nurses.

General Public: Publicity features in newspapers, radio and television will be used to raise the public awareness of the abilities and needs of people with ID and the Assistive Technology they use. We will use the STEM program to present work done in the study to schools in the North East of Scotland. The study will be presented during the Dundee Science Festival to the general public through an interactive exhibit. The work would also be presented at the Dundee Sensation's Caf&eacute; Science program where researchers from the local unviersities are invited to present their work.

Industry: We have a strong track record of working with industry to transfer research into commercial products. We have an industrial partner who will support us in any future commercialisation. By developing this software as an application for a tablet computer we have the additional option to sell the software as a commercial application on an online tablet market place (such as the Apple App Store or Android Marketplace). We already have a non-disclosure agreement in place with our industry partner who will have first refusal on licensing this software through the NHS."
20,99AA1843-7691-4049-AF7C-FEC3E0147AEE,Real-Time Detection of Violence and Extremism from Social Media,"The explosive use of social media tools in recent years has turned them into a double-edged sword. On one hand, social media is viewed as a positive factor in Middle East revolutions. On the other hand, violence events such as the UK riots occurred in August this year appeared to be driven by the use of social media. The proposed project represents a timely development of intelligent systems for addressing the emerging defense and security issues arising from social media.

The proposed research describes a new approach to detecting trends of violent radicalization and extremism from social media. In particular, it proposes a Bayesian modeling approach which detects violence contents from social media without the use of any labelled data. Words indicating violence, anger, hate, racism, etc. are naturally incorporated as prior knowledge into the model learning process. Efficient online parameter updating and parallel data processing procedures will be investigated. This proposal falls into the area of &quot;Extracting meaningful information&quot; listed in the original call for proposals. It particularly aims to tackle the technical challenge of real-time processing of large-scale social media data for early detection of violent extremism from text. The results of the research are potentially very important to society as they aim to enable the deployment of the forces of law to prevent violent events.",,"This research will have a great impact on enhancing the national security capability by enabling real-time detection of potential threats such as violence and extremism from social media. Online social networking and micro-blogging service have enabled users to share and diffuse information to a tremendous number of audiences within very short time. While such a new form of service has proven to have major social-economic benefits, it is also at the same time under the risk of being used for violent and criminal activities. 

This project therefore aims to develop efficient computational tools for detecting violent radicalization and extremism from social media, which will ultimately help improving the national security capability with the online monitoring function offered by the system. Specifically, the tools seek to detect and extract topics relating to violent and criminal activity from large-scale social media data in real-time, and constantly track any events that are identified suspicious. Owing to the fast-evolving nature of social media, such a system will be potentially very important for the forces of law to respond to and deal with the potential security risks timely which will consequently help improve public security.

Although targeting the defense and security sector, the developed tools and algorithms of the project can be applied to other public and private sectors, and be easily transferred into commercial use. For public sectors, it is possible to use the tools to analyse citizen's opinions on hot issues such as welfare reforms, Eurozone crisis, new policies introduced by the government, etc. In the ongoing &quot;Occupy London&quot; movement, our tools can be used to monitor what people are tweeting about and their sentiment and debates towards their most concerned issues. 

For private sectors, the tools can be used to detect and monitor the trends of topics being discussed about products or services, allowing business organizations to gauge insights into the potential changes in the market opinions and hence to adapt their business strategies accordingly. As the social media data published by individuals have strong correlations with their behaviors, deploying the tools for individual data analysis can help to address social issues such as preventing adolescent suicide by recognising the suicidal thoughts expressed in social media. 

We plan to disseminate our research findings in several ways. First, information about the scientific work will be published in a regularly updated project website. A dedicated online forum will be provided for engaging with interested parties including the public as well as potential researchers and collaborators (academic and non academic). Research results will also be disseminated at public engagement events such as the networking events with potential business partners. Such events help in building connections with industry and mapping the developed technologies with industrial R&amp;D needs."
21,F7ED485E-237E-4DD3-8E59-45DFB7D5FAC6,Predicting the Volume of Distribution of Drugs and Toxicants with Data Mining Methods,"Paracelsus, a physician in the early 16th century, is credited with the phrase: &quot;All things are poison, and nothing is without poison; only the dose permits something not to be poisonous&quot; (http://en.wikipedia.org/wiki/Paracelsus). Despite significant advances in pharmacology in the last decades, at present it is still very difficult to find good answers to the questions of how much, how often and for how long a drug should be given to a patient, in order to maximize its therapeutic effect and minimize its adverse effects. These problems are the central concern of the related areas of pharmacokinetics and pharmacodynamics. Pharmacokinetics is concerned with how a drug is processed by the body, i.e., the relationship between drug input parameters (e.g. amount of drug in a dose and dose frequency) and the concentration of the drug in the body with time. In contrast, pharmacodynamics is concerned with how a drug affects the body, i.e., the relationship between drug concentration and the therapeutic and adverse effects of the drug with time.

This project focuses on an important pharmacokinetics problem: how to estimate the volume of distribution of a drug, which represents the volume into which a drug is distributed once it has entered systemically into the body. Estimating a drug's volume of distribution is important because it predicts the drug's plasma concentration for a given amount of drug in the body and it influences the drug's half-life, which in turn is very important to determine the correct dosage regimen that clinicians should prescribe to patients.

This project aims at developing new computational data mining methods to predict the volume of distribution of drugs. The data mining context for this project is the regression task, where the system is given a set of instances representing a set of objects, where each instance consists of a target (response) attribute (or dependent variable) and a set of predictor attributes (features or independent variables) describing an object. Then the system discovers a regression model that predicts the value of the target attribute for an instance based on the values of its predictor attributes. In this project, the objects to be classified will be chemical compounds or medical drugs, the target attribute to be predicted will be a drug's volume of distribution and the predictor attributes will refer to several types of molecular and physicochemical properties of drugs. The data mining methods to be developed in the project will be compared against traditional data analysis methods used for predicting a drug's volume of distribution.",,"This is a discipline hopping proposal, where the investigator will hop into an entirely new discipline for him, namely pharmaceutical sciences. It is also a relatively short project, involving the equivalent of only one full-time researcher for one year (more precisely, only the investigator working on a part-time (50%) basis for two years). Hence, the pathways to impact naturally have to be interpreted in this context.
Concerning academic impact, in the relatively short term (next two years), the research will benefit mainly researchers and practitioners in the area of pharmacoinformatics, which consists of using computational methods for solving pharmaceutical sciences problems. The benefit for those users will be a novel computational method (developed in this project) for predicting the volume of distribution of a drug, with the applications extendable to other fields of computer-based drug discovery and design problems. The research will also benefit, to a lesser extent, researchers and practitioners in the computer science area of evolutionary algorithms. The benefit for those users will be a novel evolutionary algorithm for attribute selection in regression. In this project that algorithm will be applied only to pharmaceutical sciences data, but, like other algorithms of this type, it will be generic enough to be used in other application domains. 
The socio-economic impact refers mainly to engagement with pharmaceutical scientists in industry, but since the applicant will be hopping into a new discipline, engagement with academic pharmaceutical scientists is also an important part of the pathway to impacts. Hence, as part of this project, the applicant will establish contact with pharmaceutical scientists working in both academia and the pharmaceutical industry, in order to do follow up research involving collaboration with those types of partner. In terms of medium-term and long-term socio-economic impact, the main purpose of the project is to provide the applicant with enough knowledge and skills in the pharmaceutical sciences to allow him to carry out research in that area in collaboration with pharmaceutical scientists working in industry. Once the project is completed, the applicant will be in a good position to do follow up research involving collaboration with the pharmaceutical industry. That research will have a potentially significant socio-economic impact, since it will be aimed at providing the pharmaceutical industry with a more cost-effective approach to drug design, which in turn would tend to provide better healthcare for the general population (in the UK and abroad), given the strategic importance of the pharmaceutical industry in the improvement of patients' health. A more detailed discussion of how the applicant will engage with pharmaceutical scientists in academia and industry, in order to achieve the desired impact, is provided in the document &quot;Pathways to Impact&quot;."
22,FC2AB483-BAF1-4068-98A1-D34558490149,"ENGAGE : Interactive Machine Learning Accelerating Progress in Science, An Emerging Theme of ICT Research","Our vision is to establish and lead a new theme in ICT research based on Interactive Machine Learning (IML). Our expansion of IML will give scientists and non-ICT specialists unprecedented access to cutting-edge Machine Learning algorithms by providing a human-computer interface by which they can directly interact with large scale data and computing resources in an intuitive visual environment. In addition, the outcome of this particular project will have a direct transformative impact on the sciences by making it possible for non-programming individuals (scientists), to create systems that semi-automatically detect objects and events in vast quantities of A) audio and B) visual data. By working together across two parallel, highly interconnected streams of ICT research, we will develop the foundations of statistical methodology, algorithms and systems for IML. As an exemplar, this project partners with world leading scientists grappling with the challenge of analysing enormous quantities of heterogeneous data being generated in Biodiversity Science.",,"This research project ultimately will have a broad impact across a range of disciplines and contribute to the strategic development of the EPSRC portfolio in ICT. By the nature of research that reaches across a number of disciplines within ICT and beyond to other sciences, there is a range of beneficiaries as detailed below.

Machine Learning
The ML academic and industrial community will benefit from the investigation of the proposed methods, and systems for IML, which have far wider impact than the application areas being targeted by this research. While Stream.A. of the proposal will develop, analyse and apply new ML methodologies specifically within the IML framework, there is considerable hope that this will lead to further substantial cross-fertilisation of ideas within the ML research community pertaining to user interaction and the formal quantification of information and uncertainty inherent in the synthesis of user and system as a whole. 

Computer Vision
Only a few individual groups in the Computer Vision community have put effort into building interactive systems. Dissemination of our findings and prototypes from this project will help focus the CV community on challenges beyond the typical objectives of &quot;just&quot; making algorithms run real-time: the labeler(s) providing the training data must be modeled just like other variables. Most importantly, to maintain our field's track record of transferring technology beyond academia, we must plan ahead to models and algorithms that perform online learning as specialized users become more sophisticated and demanding. 

Communications and Engagement

The project will directly impact ICT and scientific communities via workshops, publications, public software releases, and the training of highly qualified personnel. Further details about academic value are explained in the main proposal, so the focus here is on impact beyond the ICT academic sphere.

As a major new area of ICT is being established, it is important that a community is built to foster the interface between the various disciplines that this research in IML will have impact on, to share early results, stimulate enquiry and adoption of the research results, as well as to encourage a wider community to engage in this area of research. To engage researchers and users of IML systems, we will organize two qualitatively different styles of workshops. The first set of workshops will be carried out in the top venues for ML (NIPS), CV (ICCV), and Human Computer Interaction (CHI). These workshops will perform the crucial task of bridging the separate research communities to create the strong inter-community bonds necessary for long-term research in IML. 

The second style of workshop will be a hands-on workshop to introduce our IML tools to non-programming scientists who can apply them in their own work. These workshops will be modelled after similar, highly successful endeavours supporting open-source software by the Blender Foundation, a non-profit corporation that has created a number of computer animated short films."
23,76F18D3A-AFFC-46BA-BC89-CF75F50B7344,SHARE-IT: School-Home Research Environment through Intelligent Technologies,"Autism Spectrum Conditions (ASCs) are neurodevelopmental conditions that affect an increasing number of individuals globally, and 1 child in 100 in the UK. Children with ASCs have marked difficulties in social interaction and communication skills and in performing tasks that require initiation of and responding to social actions, such as imitation, turn-taking and collaborative (joint) actions. Many children with ASCs go on to experience a life-time of unemployment and often severe mental health difficulties. There is no cure for ASCs. However, early intervention and consistent support that is also sustained over time and contexts is paramount to improving the child's ability to cope with social situations and, to enhancing their and their caregivers' quality of life, and outlook. Provision of consistent and sustainable support for children in and outside of school is advocated by the autism best practice community and by many schools with specialist provisions for ASC pupils. Modern interventions emphasise consistent support across contexts, and for teachers and parents to share the management of goals for each child through co-creation of learning experiences. Increasingly, teachers and parents look to technology as an effective complimentary intervention that, thanks to growing affordability and efficacy of mobile and cloud computing may provide the basic infrastructure for a continuous and sustainable link between the support given to children at home and at school.

Recently there has been a massive growth in TEIs for Autism, but as with previous teaching-learning innovations, design and research have evolved in a sequential manner, with little direct influence on practice and limited research having been conducted in real-world classrooms. Maturing mobile and cloud computing make an open system for intervention creation and delivery in school and at home viable. Given the calls from the autism best practice community for such an open platform, they make it also necessary, relevant and timely. Artificial Intelligence techniques such as machine learning and gaze-tracking, which are increasingly embedded in everyday technologies, can transform such open platforms from places where ideas and experiences are exchanged, to places that can be co-created dynamically. 

The objective of SHARE-IT is to systematically investigate how different personal and mobile devices can be used individually and together to create a scalable intelligent learning environment for children with Autism Spectrum Conditions (ASCs). The overarching aim is to facilitate continuity of support for children across school, home and other relevant contexts. SHARE-IT has two research questions: (Q1): Can the efficacy of autism interventions be optimized by using technology to interface across school, home and the child's existing therapeutic regime?; (Q2): How can such technological infrastructure be sustained over time through a combination of (i) continuous, voluntary input from teachers and parents and (ii) the application of Artificial Intelligence (AI) techniques to the real-time tracking and logging of children's behaviours in naturalistic environments?

The potential impact of SHARE-IT is significant as its findings, along with the research process itself, can affect many people in many contexts: teachers in how they deliver support to children at school; parents in how they help their children at home in a way that is consistent with school intervention and in how they cope with the significant demands of caring for a child with an ASC; children in being offered a tailored and on-demand support. Undertaking research in the wild under the EPSRC is crucial to achieving SHARE-IT's ambitions because this mechanism caters for both the engineering effort required and the crucial involvement of stakeholders in the process of creating technology that is relevant and useful to them.",,"SHARE-IT's aim is to make a difference to the lives of children on the autism spectrum and their parents, by exploring how cutting-edge technologies can be deployed best to improve communication between school and home, and to facilitate co-creation and co-ownership of learning experiences by parents, children and teachers. Technology, when used in the right way, can inspire and motivate children who can be otherwise difficult to engage. It can be used also to actively involve parents in their children's development and learning and provide alternative ways for teachers to tailor the support to the individual children's needs. The potential impact of SHARE-IT is significant as its findings, along with the research process itself, can affect many people in many contexts: teachers in how they deliver support to children at school; parents in how they help their children at home in a way that is consistent with school intervention and in how they cope with the significant demands of caring for a child with an ASC; children in being offered a tailored and on-demand support. 

Our work with teachers will contribute to their professional development and we will work with them to find best ways to operationalize the SHARE-IT infrastructure. Involvement of parents at the same time as teachers and children is crucial to understanding the dynamics of school-home communication and the respective roles of the different stakeholders in this communication. Such understanding is necessary for researchers to tailor the technology aimed to support such school-home links, and by teachers and parents to co-ordinate the support they give to the children. The involvement of parents on SHARE-IT brings also another dimension - that of parent education, literacy and empowerment in relation to their children's futures. Specifically, successful involvement of parents from lower income and education backgrounds, as planned in SHARE-IT, can have profoundly positive impact on their and their children's aspirations and success, but will require additional and specially tailored training to enable the parents to take part in the research. 

SHARE-IT's industrial partnership is an opportunity to influence the future development of and market for personal and mobile technologies with gaze tracking and Artificial Intelligence that addresses real-life needs of a growing population with ASCs. Our partner has an unrivaled insight into the future commercial potential of home eye tracking systems and will advise SHARE-IT on all aspects of commercialization throughout the project. 

SHARE-IT will support collaboration in which academics and non- academics, including our industrial partner, will have the time and scope to build connections as they work together, thereby contributing to instrumental, conceptual and capacity building impact. Instrumental, because we want to influence practice of using technology in creative and exploratory ways to encourage social communication and social skills for children through co-ordinated, informed and equal collaboration between teachers and parents; Conceptual, because we aim to identify the role of school-home communication through advanced intelligent technologies and how such technologies support continuity of experience between different contexts; Capacity building, because through the process of the work, we will be involved in increasing children's and parents' technological skills and learning as well as developing the technical and pedagogical skills of teachers. We will also facilitate knowledge exchange between the users and the industry, to enable industrial partners to develop an understanding of the real needs and potential of the technologies developed by them. As researchers, we will also develop our own skills and expertise in intelligent technology-enhanced interventions design and use in multiple contexts and in addressing how to engage in successful knowledge exchange."
24,C43F34D3-16F1-430B-9E1F-483BBADCD8FA,Evolutionary Computation for Dynamic Optimisation in Network Environments,"The research on optimisation problems in network environments has a long history but it generally fails to capture real-world scenarios as it usually assumes that both the network environments (such as network topologies, node processing capabilities, interference, etc) and the optimisation problems (such as the user requirements) are known in advance and remain unchanged in the problem-solving procedure. However, most real-world network optimisation problems (NOPs) are highly dynamic, where the network topologies, availability of resources, interference factors, user requirements, etc., are unpredictable, change with time, and/or are unknown a priori. This poses many difficulties for decision makers, generating significant optimisation challenges. This research aims to investigate Dynamic NOPs (DNOPs) in various network environments. The dynamics in both network environments and problems will be studied in depth. DNOPs occur across a wide range of application areas, such as communication networks, transport network, social networks, and financial networks. Our theoretical study in this project will seek fundamental insight that is applicable to multiple application areas, while our applied research will focus on railway networks and telecommunications networks.

Evolutionary Computation (EC) encompasses many research areas, which applies ideas from nature (especially from biology) to solve optimisation and search problems. EC has been successfully applied to many real world scenarios, especially for difficult and challenging problems and those problems that are difficult to define precisely. This project aims to investigate EC methods for solving DNOPs. We aim to gain insight and further our understanding of how different EC methods can be applied to DNOPs via empirical and theoretical studies. It is important to carry out this research at both theoretical and empirical levels, as one can feed into the other. We will work with industrial partners (e.g., Rail Safety and Standards Board, and Network Rail) who will validate our research and participate in our project. We can utilise their skills and expertise in producing the underlying theoretical models, which can then be validated on real-world data supplied by them. This project has great potentials to fundamentally change the way in which DNOPs are treated, both from a real-world point of view and from the point of view of advancing our theoretical understanding. We plan to develop a prototype system, in collaboration with our industrial partners, for our industrial partners.

In order to test and evaluate our newly developed algorithms for DNOPs, we will develop a set of common DNOP models that capture the real-world complexities, and develop advanced EC methods to solve these DNOP models. This will benefit wider research communities due to the ubiquity of DNOPs in so many different fields from communication networks to transport networks to social networks to financial networks. The research results of this project will also be of significant benefit to many industries that involve DNOPs and will provide significant savings both from a cost point of view as well as from an environmental perspective.",,"This project will provide significant economic and social benefits in the generation and analysis of evolutionary computation (EC) methods for dynamic network optimisation problems (DNOPs). The main beneficiaries, how they will benefit from this research, and relevant activities are summarized as follows.

1. The academic community: DNOPs have not been well understood in research. The DNOP models and advanced EC methods that we will develop in this project will be very useful for researchers in DNOPs and EC to pursue their research and hence benefit them directly. This project will also open new frontiers for computer scientists and mathematicians to analyse EC methods for DNOPs. We will publish in highly rated journals and conferences, put project materials online, and interact with the research communities (through conferences, seminars and visits) to disseminate our research results to the academic community.

2. Benefits to the UK: The potential economic benefits to the UK are significant, including more effective and efficient dynamic optimisation software and decision making processes for many industries in communication, transport, social, and financial networks, etc. For example, the DNOP models and EC methods developed will help to manage the traffic in UK railway networks more efficiently, which will lead to increased capacity and customer satisfaction and reduced costs and carbon emissions. Utilising the EC methods to be developed in this project will also enable UK researchers and practitioners to improve their processes to maximise the usage of resources (staff, computers, vehicles etc). More effective DNOP algorithms will increase the competitiveness of UK network industries internationally.

3. Industry partners: The close collaboration with industrial partners via PDRA secondments, industry-focussed workshops, regular project meetings and additional email/phone communications, ensures the development of effective and efficient algorithms for dealing with real-world DNOPs, and thus benefit the industry directly. The prototype software that we will develop for railway networks will enable Network Rail to test and evaluate our system themselves in a realistic environment. Even a small improvement in Network Rail's operations by using our system could mean millions of pounds of real savings. The success with one industrial partner, such Network Rail, can set a good example and inspire other companies to also exploit the knowledge and technologies developed from this project.

4. Education and skill training: PDRAs on this project will gain significant expertise in EC and its innovative applications to DNOPs. They will be armed with skills and knowledge in both theoretical analysis and applied research. If they move into industry after the project, they will have significant impact on industrial research and development because their theoretical insight gained from the project will help them to understand the real world problems better and hence provide more better solutions. If they move into academia, their experience in working with industrial partners and real world problems will help them in formulating new research problems that model the real world better and in focusing their research effort on the most pressing needs of the industry. Furthermore, the knowledge gained in this project can also be incorporated into relevant BSc and MSc modules at Brunel and Birmingham. Both universities have several modules that are relevant to EC and optimisation. 

5. The general public: This project is likely to produce interest amongst the media and general public since railway transport touches the life of millions in the UK. Our carefully designed public engagement activities will use railway networks as an example to illustrate the importance of scientific research and the potential benefits of scientific research to our economy and society in general."
0,ACDCBDD3-60B5-44B8-B920-033B63906FC3,Evolutionary Computation for Dynamic Optimisation in Network Environments,"The research on optimisation problems in network environments has a long history but it generally fails to capture real-world scenarios as it usually assumes that both the network environments (such as network topologies, node processing capabilities, interference, etc) and the optimisation problems (such as the user requirements) are known in advance and remain unchanged in the problem-solving procedure. However, most real-world network optimisation problems (NOPs) are highly dynamic, where the network topologies, availability of resources, interference factors, user requirements, etc., are unpredictable, change with time, and/or are unknown a priori. This poses many difficulties for decision makers, generating significant optimisation challenges. This research aims to investigate Dynamic NOPs (DNOPs) in various network environments. The dynamics in both network environments and problems will be studied in depth. DNOPs occur across a wide range of application areas, such as communication networks, transport network, social networks, and financial networks. Our theoretical study in this project will seek fundamental insight that is applicable to multiple application areas, while our applied research will focus on railway networks and telecommunications networks.

Evolutionary Computation (EC) encompasses many research areas, which applies ideas from nature (especially from biology) to solve optimisation and search problems. EC has been successfully applied to many real world scenarios, especially for difficult and challenging problems and those problems that are difficult to define precisely. This project aims to investigate EC methods for solving DNOPs. We aim to gain insight and further our understanding of how different EC methods can be applied to DNOPs via empirical and theoretical studies. It is important to carry out this research at both theoretical and empirical levels, as one can feed into the other. We will work with industrial partners (e.g., Rail Safety and Standards Board, and Network Rail) who will validate our research and participate in our project. We can utilise their skills and expertise in producing the underlying theoretical models, which can then be validated on real-world data supplied by them. This project has great potentials to fundamentally change the way in which DNOPs are treated, both from a real-world point of view and from the point of view of advancing our theoretical understanding. We plan to develop a prototype system, in collaboration with our industrial partners, for our industrial partners.

In order to test and evaluate our newly developed algorithms for DNOPs, we will develop a set of common DNOP models that capture the real-world complexities, and develop advanced EC methods to solve these DNOP models. This will benefit wider research communities due to the ubiquity of DNOPs in so many different fields from communication networks to transport networks to social networks to financial networks. The research results of this project will also be of significant benefit to many industries that involve DNOPs and will provide significant savings both from a cost point of view as well as from an environmental perspective.",,
1,B22052CD-8E9F-48E8-8FC0-0A5251840A79,Lexico-syntactic text simplification for improving information access,"Text simplification is the process of reducing the grammatical and lexical complexity of a text, while
retaining its information content and meaning. The main goal of simplification is to make information
more accessible to the large numbers of people with reduced literacy. The National Literacy Trust
(www.literacytrust.org.uk) estimates that one in six adults in the UK have poor literacy skills. There is
therefore a need to make information available in simple English, as advocated by organisations such as
the Plain English Campaign (www.plainenglish.co.uk). This need for text simplification is likely to
become more acute for a variety of reasons; for instance, a growing aging population with language difficulties
arising from neurodegeneration and other causes, children accessing information on the internet or lay
readers trying to access technical writing online (perhaps, to research an illness or treatment). 

One of the most popular information sources online is Wikipedia (www.wikipedia.org), a free-content
encyclopedia written collaboratively by internet volunteers. The Simple English Wikipedia
(simple.wikipedia.org) initiative to make information more accessible contains over 60,000 articles in
Simplified English. However, these are only a fraction of the 3.3 million articles in the main English
Wikipedia and further, the simplified articles tend to be very short (often just the first paragraph). Our
goals in this proposal are twofold. From a theoretical perspective we want to gain an understanding of
the text revisions humans perform to simplify text, and learn rules for simplification from corpora. From
an applied perspective, we want to implement a system for automatic text simplification that can perform
the wide range of revisions that humans perform. We will make this system available to the Simple
English Wikipedia community as a tool to expand the content available in simplified form.",,"We identified the expansion of content available in Simple English Wikipedia (SEW) as an objective. To achieve this we will engage with the SEW community in the last four months of the project and encourage them to edit and revise content that has been automatically created through simplification of existing English Wikipedia articles. We will seek structured user satisfaction feedback and free-text comments about the quality of the texts and level of simplification.

We are also in contact with teachers involved with deaf education. Current UK policy is to integrate deaf children in mainstream schools. This means that individual teachers with deaf students in their class have to prepare simplified material for these students who can suffer from a range of linguistic deficits stemming from lack of early exposure to language. Text simplification is a demanding task, like translation, and school teachers typically do not have any training to do this. We are in contact with educationists involved in deaf education, from a teaching as well as a policy perspective. We will explore the possibility of running our final free recall based evaluation using deaf students (if this is not possible, we will use other participants with language difficulties, such as second language learners). 

Society's increasing dependence on online information creates both a challenge (to make information accessible) and an opportunity (to tailor language to the requirements of users). This issue is likely to increase in importance due to a variety of reasons - a growing aging population with language difficulties arising from neurodegeneration and other causes, children accessing information on the internet, lay readers trying to access technical writing online (perhaps, to research an illness or treatment), etc. The National Literacy Trust (www.literacytrust.org.uk) estimates that one in six adults in the UK have poor literacy skills and many organisations, such as the Plain English Campaign (www.plainenglish.co.uk), advocate the need to make information available in simple English. Thus, this research has the potential to positively impact on a large segment of the population.

Given the volume of evidence from cognitive science, psychology and literacy studies supporting the efficacy of text simplification, there has been surprisingly little research on automating the process. This is in stark contrast to other regeneration applications such as summarisation and translation that are well established. There has however been a recent spurt of interest in approaches to text simplification and we believe that this project is timely in that it will spur further research in this field."
2,ECFFF4E5-BCF8-4AAA-9879-F6A43A949725,Analysis of Facial Behaviour for Security in 4D (4D-FAB),"The overall aim of the project is the development of automated tools for automatic spatio-temporal analysis and understanding of human subtle facial behaviour from 4D facial information (i.e. 3D high-quality video recordings of facial behaviour). Two exemplar applications related to security issues will be specifically addressed in this proposal: (a) person verification (i.e. using facial behaviour as a biometric trait), and (b) deception indication. 

The importance of non-obtrusive person verification and deception indication is undisputable - every day, thousands of people go through airport security checkpoints, border crossing checkpoints, and other security screening points. Automated, unobtrusive monitoring and assessing of deceptive behaviour will form a valuable tool for end users, such as police, justice and prison services. This is in particular important as currently only informal interpretations for detecting deceptive behaviour are used. In addition, the development of alternative methods for person verification that are not based on physical traits only but on behavioural, easily observable traits like facial expressions, would be of great value for the development of multimodal biometric system. Such multi-modal biometric systems will be of great interested to government agencies such as the Home Office or the UK Border agency.

For automatic deception indication we propose to develop methodologies for detecting 4D micro-expressions and their dynamics being typical of deceptive behaviour as reported by research in psychology. For automatic person identification we propose to increase the robustness of static face- image-based verification systems by including facial dynamics as an additional biometric trait. The underlying motivation is that the dynamic 4D facial behaviour is very difficult to imitate and , hence, it has natural resilience against spoof attacks. 

The project focuses on 3D video recordings rather than on 2D video recordings of facial behaviour due to two main reasons: (1) increased robustness to changes in head-pose, and (2) ability to spot subtle changes in the depth of facial surface such as jaw clench and tremor appearance on the cheeks, which are typical of deceptive behaviour and cannot be spotted in 2D images. The research on 3D facial dynamics is now made possible by the tremendous advance of sensors and devices for the acquisition of 3D face video recordings.

The core of the project will deal with both the development of 4D-FAB research platform containing tools for human subtle facial behaviour analysis in 4D videos and the development of annotated data repository consisting of two parts: (1) annotated 4D recordings of deceptive and truthful behavior, and (2) annotated 4D recordings of subjects uttering a sentence, deliberately displaying certain facial actions and expressions, and spontaneously displaying certain facial actions and expressions. The work plan is oriented around this central goal of developing 4D-FAB technology and is carried out in 3 work packages described in the proposal.

A team of 3 Research Associates (RAs), led by the PIs, and having the background in computer vision and machine learning, will develop 4D-FAB technology. The team will be closely assisted by 6 members of the Advisory Board:
Prof. Burgoon, University of Arizona, advising on psychology of deception and credibility
Prof. Cohn, Pittsburgh University / Carnegie Mellon University, advising on face perception and facial behaviometrics
Prof. Nunamaker, Director of BORDERS, US Nat'l Center for Border Security and Immigration, advising on making 4D-FAB useful for end users in security domain
Dr Hampson, Head of Science &amp; Technology, OSCT, Home Office, advising on making 4D-FAB useful for end users
Dr Cohen, Director of United Technologies Research Centre Ireland, advising on making 4D-FAB useful for end users 
Dr Urquhart, CEO of Dimensional Imaging, advising on 4D recording setup design",,"The overall aim of the project is the development of automated tools for automatic spatio-temporal analysis and understanding of human facial behaviour from 4D facial information (i.e. 3D high-quality video recordings of facial behaviour). Two exemplar applications related to security issues will be specifically addressed in this proposal: (a) person verification (i.e. facial behaviour as a form of behaviometrics), and (b) deception indication. 

The importance of non-obtrusive person verification and deception indication is undisputable - every day, thousands of people go through access control points and security screening checkpoints. Automated, unobtrusive person verification and assessing of deceptive behaviour will form a valuable tool for end users, such as police, justice and prison services. Such systems will be of great interest to government agencies such as the Home Office and Border Agency (both being project partners in this proposal).

While the proposal focuses on applications in the area of security, the technology developed will have numerous applications beyond this. Human behaviour understanding plays a critical role underlying the development and design of ICT systems in a human-centred manner, built for humans based on human behaviour models. Engineering ICT systems with the capability to sense and understand unstructured human user's behaviour is a challenge that goes beyond today's systems engineering paradigm, which can free computer users from the classic keyboard and mouse and enable technologies like ambient intelligence and ubiquitous computing. Other potential benefits from efforts to automate the analysis of facial expressions are varied and numerous and span fields as diverse as:
(1) cognitive sciences - automated tools would speed up tremendously current research processes as they would replace the current lengthy and tedious manual analysis of the studied behaviour.
(2) medicine - remote monitoring of conditions like pain and depression, remote assessment of drug effectiveness, computer-based remote treatment of facial paralysis, etc., would be possible, leading to more advanced personal wellness technologies than those available today. 
(3) transportation - automatic assessment of the driver's stress level, detection of micro sleeps, and spotting driver's puzzlement, would be facilitated, enabling a next generation in-vehicle assisting technology. 
(4) education - automatic assessment of student's interest level, puzzlement, and enjoyment would become possible, facilitating development of truly intelligent tutoring systems.

We believe that the technology developed in this project has very high potential for commercialization. In particular, the developed image acquisition technology will be of substantial interest beyond the area of facial expression analysis, e.g. in healthcare and creative industries. The PIs have already extensive experience in close collaborations with industry, in particular in the healthcare domain, as well as in setting up spin-off companies. We will use our previous experiences to work in collaboration with industry to exploit opportunities for commercialisation of the developed technology. To ensure the potential for commercial exploitation we will protect the developed IP where appropriate (e.g. via patents, if and when appropriate, before dissemination to the community). 

To ensure appropriate involvement of end users in the proposed research we assembled an advisory team of potential users and industrial collaborators interested in the technology developed in this project. To ensure engagement with the wider community we also work in close collaboration with the Institute for Security Science and Technology at Imperial College London, that focuses on innovation in homeland and national security.

We will also disseminate our research to a wider audience through activities such as participation at the Royal Society Summer Exhibition, Meetings, etc."
3,5BF002D7-D38D-4A9D-8161-133803023A1D,AUTONOM: Integrated through-life support for high-value systems,"Summary - This proposal extends research in novel sensing, e-maintenance systems, and decision-making strategies. It is supported by the EPSRC National Centre for Innovative Manufacturing in Through-life Engineering Services. Maintenance of widely-dispersed assets is expensive because it involves widespread inspection, checking and measurement. The integration of sensor-based information in geographically dispersed and less structured environments poses challenges in technology and cost justification. Academic challenges include improvement of embedded sensing, reliable estimation of monitoring parameters, a unified approach to the mathematics and data structures, and a rigorous approach to cost estimation and benefit analysis. The industrial drivers include standardisation, automation, connectivity, and reduction of unit cost. 
We will build on e-maintenance principles in automated, intelligent maintenance in the context of industrial application: transitions from sensor level to management decision-making. Within the integration architecture we will investigate some research challenges in depth:
- Requirements for interchange of maintenance data between fixed and mobile actors, e.g. track-to-train data exchange in rail or aerospace applications.
- Automation of monitoring on mobile platforms, and interface with other actors and fixed systems.
- New cost modelling tools for complex distributed health monitoring, and especially the value to the business. 
The novelty lies in the implementation of effective information flows and analysis techniques to support optimisation, and uncertainty based resource scheduling, capturing business &quot;pull&quot; which struggles with lag in the data, and insufficient tools to turn it into information for decision making. 
Aims and Objectives: The overall aim is to enable improved integration between architectural levels in data-rich environments in automated, intelligent maintenance, responding to business pull and demonstrating value.
Objectives:
- Identification of the industrial pull for plant and fixed asset management information for maintenance decisions, in the context of technologically rich data sources; i.e. designing the condition based maintenance from the top down.
- Transition between architectural levels, from data to information to decision, embedding automatic functionality in appropriate hardware such as wireless platforms and RFID. Transfer between fixed and mobile actors.
- Improved data fusion in condition monitoring problems in large data sets.
- A demonstrated methodology for mining and optimally reconfiguring maintenance data in the form of a business process.
- An estimation tool for cost, value and benefits, with validated application.
In a programme directed by industrial &quot;pull&quot;, four work packages will cover 
- Integration, with milestones &quot;Think tank&quot; event report; Definition of data/information and information/decision transitions, illustrated by industrial cases; Strategy document for design; Final report.
- Data fusion and mobile platforms, with milestones: Strategy for fusion of multiple unsynchronised sensor data; Case studies of fused multiple unsynchronised sensor data; Strategy for top-down monitoring system design; Reporting/message passing architecture - proof of concept.
- Planning and Scheduling Based on Intelligent Reconfigurable Business Processes, with milestones: Identification of a representative set of business processes, in cooperation with the partners; The multi-objective optimisation framework; Validation on a partner's example.
- Cost Analysis, with milestones: Identification of best practice cost modelling methods, and cost benefit analysis and value analysis; Development of cost and value models; Framework for estimating costs and benefits of integrated maintenance to the business; Framework for estimating through life costs and benefits of distributed heterogeneous high integrity assets, and Validation of tool set.",,"This research sets out to further automate the transitions between the architectural levels in automated, intelligent maintenance. The research will examine the characteristics of the levels and their transitions, in the context of industrial application, from sensor level to management decision-making. It will put additional effort into quantifying the value of the maintenance technology proposition.

The first beneficiary group to consider is our sponsoring companies. Economically, the cost of maintenance systems in high integrity equipment is dwarfed by the losses relating to loss of production/availability; safety implications; and damage to high capital plant, through spares/repairs/replacement. Moreover the competitiveness of global organisations is increasingly determined by availability, avoiding failure, and the ability rapidly to restore service after failure. The work will ensure that these factors are properly estimated, to allow &quot;pull through&quot; of appropriate and optimised solutions. The impact will be in the growth and reduction of losses from improved availability. 

In large industrial endeavours the wider impact is through the supply chain. Many businesses, including a wide range of SMEs, support our sponsors, and it is those who will manufacture and support the systems which we propose, over the longer term. Large businesses concentrate on their core business, buying in products and services from wide range of companies - in this case the sensing hardware, IT solutions, expertise, software, and routine monitoring services will form the future business of many small companies. The impact will be in new opportunities for growth in products and services, in a customer base much wider than the sponsors alone.

The societal impact of such work is more difficult to quantify directly. Companies' competitiveness will offer better prospects of continued employment. The ability to share data and expertise remotely and through time zones could benefit those who work part-time and from home, a group who are disproportionately female in the UK. But the online and asynchronous nature of work in monitoring and diagnostics could also offer the opportunity to penetrate markets in other time zones - and by extension, overseas suppliers could penetrate the UK market unless we foster local provision. The impact will be in employment opportunities and security in a wide group of companies, partnerships and sole traders."
4,E04E24C5-AE50-4B57-AACE-EB15717F5968,A Unified Model of Compositional and Distributional Semantics: Theory and Applications,"The notion of meaning is central to many areas of Computer Science, Artificial Intelligence (AI), Linguistics, Philosophy, and Cognitive Science. A formal, mathematical account of the meaning of natural language utterances is crucial to AI, since an understanding of natural language (i.e. languages such as English, German, Chinese etc) 
is at the heart of much intelligent behaviour. More specifically, Natural Language Processing (NLP) --- the branch of AI concerned with the computer processing, analysis and generation of text --- requires a model of meaning for many of its tasks and applications.

There have been two main approaches to modelling the meaning of language in NLP, in order that a computer can gain some &quot;understanding&quot; of the text. The first, the so-called compositional approach, is based on classical ideas from Philosophy and Mathematical Logic. Using a well-known principle from the 19th century logician
Frege --- that the meaning of a phrase can be determined from the meanings of its parts and how those parts are combined --- logicians have developed formal accounts of how the meaning of a sentence can be determined from the relations of words in a sentence. This idea culminated famously in Linguistics in the work of Richard Montague in the 1970s. The compositional approach addresses a fundamental problem in Linguistics -- how it is that humans are able to generate an unlimited number of sentences using a limited vocabulary. We would like computers to have a similar capacity also.

The second, more recent, approach to modelling meaning in NLP focuses on the meanings of the words themselves. This is the so-called distributional approach to modelling word meanings and is based on the ideas of the &quot;structural&quot; linguists such as Firth from the 1950s. This idea is also sometimes related to Wittenstein's philosophy of &quot;meaning as use&quot;. The idea is that the meanings of words can be determined by considering the contexts in which words appear in text. For example,
if we take a large amount of text and see which words appear close to the word &quot;dog&quot;, and do a similar thing for the word &quot;cat&quot;, we will see that the contexts of dog and cat tend to share many words in common (such as walk, run, furry, pet, and so on). Whereas if we see which words appear in the context of the word &quot;television&quot;, for example, we will find less overlap with the contexts for &quot;dog&quot;. Mathematically we represent the contexts in a vector space, so that word meanings occupy positions in a geometrical space. We would expect to find that &quot;dog&quot; and &quot;cat&quot; are much closer in the space than &quot;dog&quot; and &quot;television&quot;, indicating that &quot;dog&quot; and &quot;cat&quot; are closer in meaning than &quot;dog&quot; and &quot;television&quot;.

The two approaches to meaning can be roughly characterized as follows: the compositional approach is concerned with how meanings combine, but has little to say about the individual meanings of words; the distributional approach is concerned with word meanings, but has little to say about how those meanings combine. Our ambitious proposal is to exploit the strengths of the two approaches, by developing a unified model of distributional and compositional semantics. Our proposal has a central theoretical component, drawing on models of semantics from Theoretical Computer Science and Mathematical Logic. This central component which will inform, be driven by, and evaluated on tasks and applications in NLP and Information Retrieval, and also data drawn from empirical studies in Cognitive Science (the
computational study of the mind). Hence we aim to make the following fundamental contributions:

1. advance the theoretical study of meaning in Linguistics, Computer Science and Artificial Intelligence;

2. develop new meaning-sensitive approaches to NLP applications which can be robustly applied to naturally occurring text.",,
5,DA04B0FE-FEA5-4840-8E7B-2E4DF0A13919,Working Together: Constraint Programming and Cloud Computing,"This proposal combines two active, important research streams in cloud computing and constraint programming, both of which will realise significant and sustained benefits from working in concert. Constraint programming is a proven technology for solving complex combinatorial problems. However, the inherent difficulty of these problems means that performance can be variable, often requiring tuning by an expert to obtain best results. One approach to obtaining more robust performance is to employ a portfolio of solvers with complementary strengths. The scalable resource offered by the cloud is perfectly suited to the deployment of such portfolios and presents the opportunity to employ large solver portfolios to tackle challenge problems of exceptional difficulty. Conversely, a major concern in cloud computing is how to deploy an application on the available infrastructure so as to maximise performance and minimise operating costs. Added complexity arises when dealing with Big Data scenarios where it is important to run computation as closely (in terms of network distance) as possible to the data, in order to minimise network latency and maximise the performance of an application. This is a difficult combinatorial problem with a large set of variables including: public cloud provider, cloud configuration, geographical region, pricing etc. to which constraint programming is ideally suited.

Our two primary research streams in ICT will interact and work together with a third in astronomy to deliver a solution to a major challenge application: scheduling telescope observations to measure the abundance of planets throughout the Milky Way. If successful, the benefit to astronomy is clear, but our two primary streams will also benefit greatly from a major evaluation of their ability to work together to solve a large, complex problem.",,"This proposal combines two active, important research streams in cloud computing and constraint programming, both of which will realise significant and sustained benefits from working in concert. Constraint programming is a proven technology for solving complex combinatorial problems and cloud computing is emerging as the dominant paradigm for hosting and delivering services over the Internet, both of which have very wide applicability. As a result of working together our research has a wide set of potential beneficiaries. 

Impact to the Economy: 

According to a recent Bloomberg report the total Worldwide cloud computing market could potentially be worth $270 billion by 2020. This project will foster collaboration between the complementary areas of cloud computing and constraint programming. The ideas and software produced as a result of this innovative collaboration will help ensure that the UK is on the forefront of the highly lucrative cloud computing market. In addition, the increased robustness of constraint solving gained from parallel constraint solver portfolio deployment, and the ability to scale constraint solving to problems of exceptional difficulty will be very valuable - combinatorial problems arise in many areas throughout the economy.

Regarding application and exploitation, there are currently no comprehensive services (discussed further in the case for support) that suggest the optimal public cloud provider and cloud configuration in order to maximise an application's performance and reduce operating costs. The technology developed as a result of our Working Together in ICT grant has the potential to be spun out into a cloud computing startup company and contribute directly to the UK economy. If this avenue is to be pursued we will source the correct government funding streams or venture capital investment. 

Our Working Together in ICT project will likely be the start of a long lasting collaboration between the areas of cloud computing and constraint programming. Inward investment will be met with further follow on project sourced from UK (e.g., EPSRC), European (e.g., FP7) and International research funds (e.g., Qatar National Research Fund) as well as industrial sources. 

Impact to People: 

This project will employ two Research Assistants working at the intersection of cloud computing and artificial intelligence. The core team (P.I and Co-I's) will work closely with two Research Assistants funded directly from the project. This collaboration will generate new ideas around these diverse areas of Computer Science and help create a unique skill set which will lead to further research projects, training (through our workshops discussed in WP4.3) and potential knowledge exchange and furthering the training of skilled people for both academic and non academic disciplines.

Impact to Society: 

Our major application focuses on gravitational micro lensing which provides the only technique known for detecting signatures of small planets (down to even Lunar mass) in wide orbits around their host stars. Embracing the amazing diversity of planets in the Milky Way is a requirement for understanding the origin, role, and possible fate of planet Earth. While comparative planetology beyond the Solar System addresses most fundamental philosophical questions, it also opens a window for gaining new insight on the interaction between ourselves and our planet, in particular on processes that shape the composition of the Earth's atmosphere, critical to life and the future of humanity."
6,384BBB2C-DA68-4D1C-A7D7-969F6F709F4E,Natural Language Processing Working Together with Arabic and Islamic Studies,"Summary 

This is an interdisciplinary project which addresses the ICT call of &quot;working together&quot; by aligning ICT expertise and research interests from Computational and Corpus Linguistics, with Humanities research streams in Arabic and Islamic Studies, focusing on the Qur'an as a core text. It is also an international collaboration between the Universities of Leeds and Jordan, and further addresses the &quot;working together&quot; call via incoming and outgoing mobility in the form of Visiting Researcher placements in the School of Computing at Leeds (incoming) and the Centre for the Study of Islam in the Contemporary World at Jordan (outgoing). This agreement is proactive and novel, and has high impact, ensuring knowledge transfer from different methodological perspectives and cultures.

The study of Tajwid or Qur'anic recitation is a sub-field and taught module* in Islamic Studies programmes at both universities and elsewhere, and the original insight informing this project is to view Tajwid mark-up in the Qur'an as additional text-based data for computational analysis. This mark-up is already incorporated into Qur'anic Arabic script, and identifies prosodic-syntactic phrase boundaries of different strengths, plus gradations of prosodic and semantic salience through colour-coded highlighting of pitch accented syllables, and hence prosodically and semantically salient words. 

The Computational Linguistics Module in Year 1 entails development and evaluation of software for generating a phonetically-transcribed, stressed and syllabified version of the entire text of the Qur'an, using the International Phonetic Alphabet (IPA). This canonical pronunciation tier for Classical Arabic will be informed and evaluated by Arabic linguists, Tajwid scholars, and phoneticians, and published in an updated version of the open-source Boundary-Annotated Qur'an Corpus [1], [2], preferably for LREC2 2014. The software will also be re-usable for Natural Language Engineering applications for Modern Standard Arabic, and for constructing dictionaries for Arabic language learners. 

The Text Analytics Module in Year 2 implements statistical techniques such as keyword extraction3 to explore semiotic relationships between sound and meaning in the Qur'an, invoking a Saussurean-type view of the sign as '...a bi-unity of expression and content...' [5]. Our investigation entails: (i) text data mining for statistically significant phonemes, syllables, words, and correlates of rhythmic juncture [6], [7]; and (ii) interpretation of results from interdisciplinary perspectives: Corpus Linguistics (ICT); Tajwid science, plus Tafsir or Qur'anic exegesis (Islamic Studies); Arabic (Language and Literature); and Phonetics and Phonology (Linguistics). 

In terms of ICT applications, the team will collaborate with stakeholders and beneficiaries to develop an associated or follow-on funding proposal for the UK Research Councils, to include publication of project software as an advanced corpus-query and visualization tool for Islamic Studies and Humanities scholars, plus Arabic language learners. This again represents an extension of the &quot;working together&quot; theme. 

Finally, our approach is interdisciplinary and pioneers stylistic analysis of sound and rhythm encoded in writing as a semiotic system for religious and other literary texts. As such it is entirely novel and has direct implications for research-led teaching in both partner institutions plus a broad cross-section of research groups and user communities, namely: Natural Language Processing and Artificial Intelligence; Qur'anic and Islamic Studies; Arabic Language and Literature; Linguistics and Phonetics; Digital Humanities; and Psychology. 

All references appear in Case for Support.",,"Impact Summary 

Novelty and Originality: The original insight in this project [1] is to use Tajwid recitation markup of chunk boundaries in the Qur'an, plus other orthographical features, as untapped sources of text data for computational analysis. A principal objective is to establish quantifiable, linguistically rigorous links between the prosodic markup and the semantics of the Qur'an. This falls within exegetical science (Tafsir); it will make an original contribution to the Tafsir literature, which as yet provides no detailed analysis in this area, and be of interest to educated pious Muslims worldwide.
 
Debate and Controversy: Another objective is to develop a Tajwid-IPA and this might be seen as controversial since the Muslim mainstream has traditionally not treated any Qur'anic transliteration as the Qur'an proper. Any suggestion of a movement away from Arabic script to another language system would be seen as unacceptable by the mainstream - despite the advantages of doing so - and so this would almost certainly open up a debate, placing at least traditionalists and modernists in opposition to one another. 

Sustainability: The project delivers re-usable algorithms for phoneticizing Arabic script for use in Arabic speech and language applications and lexicography. The Boundary-Annotated Qur'an Corpus [1] with canonical pronunciation tier will be the largest resource of its kind for training Arabic statistical language models and for Arabic linguistics and Islamic Studies. Re-usable algorithms for quantitative analysis of implicit prosody [3] in text will inform research on the link between text and spoken form in other languages. New interdisciplinary techniques for exploring religious and literary texts will inform AI research on deep text analytics. For Islamic Studies this research will open up the possibility of developing a new Qur'anic orthography comfortably rooted within modern phonetics, thus bringing an old oral tradition into a dynamic academic area of study. Current editions of the colour-coded Tajwid Qur'an are still inadequate in terms of providing the full gamut of Tajwid-phonetic values. This research is a stepping-stone towards a new Tajwid-IPA which we envisage would be welcomed by Arab linguists (though resisted by traditionalists). In this respect, we believe it to be a path-breaking research project.

Beneficiaries and how they will benefit: Beneficiaries span a broad cross-section of research groupings and user communities.
 
Science and Engineering: AI/NLP researchers who want a rich, gold-standard dataset for machine learning; language technologists for re-usable software to incorporate into systems; computational and corpus linguists as developers and users of annotated corpora and associated software; psycholinguists for conceptual models of prosody and language as a semiotic system. 

Humanities: Richly annotated corpora are tools of the trade for Corpus linguists, Arabic scholars and researchers in Applied Linguistics, and can be exploited by lexicographers for dictionary construction; online Qur'anic resources have been identified as a priority by groups such as the Muslims in Britain Research Network and the UK Islamic Studies Network [4]. 
 
Economic: re-usable software for generating IPA transcriptions of Arabic words has market potential for publishers of dictionaries and language learning materials; the project aims to exploit re-usable software and annotated online resources for an industry-standard corpus-query and visualization tool. 

Societal: The research will generate interest in the Middle East particularly in key centres of Islamic scholarship and is sustainable beyond the scope of this project, since a principal long-term objective is to provide a complete Tajwid markup system which can be used by institutions of Islamic learning in Britain - and worldwide.

References in Case for Support."
7,70FDA4B9-B409-494A-8189-4256C1C93096,Foundational Structures for Compositional Meaning,"Words are the building blocks of sentences, yet meaning of a sentence goes well beyond meanings of the words therein. Indeed, while we do have dictionaries for words, we don't seem to need them to infer the meaning of a sentence from meanings of its constituents. Discovering the process of meaning assignment in natural languages is one of the most foundational issues in linguistics and computer science, whose findings will increase our understanding of cognition and intelligence and may assist in applications to automating language-related tasks, such as document search as done by Google.

To date, the compositional logical and the distributional probabilistic models have provided two complementary partial solutions to the problem of meaning assigning in natural languages. The logical approach is based on classic ideas from mathematical logic, mainly Frege's principle that meaning of a sentence can be derived from the relations of the words in it. The distributional model is more recent, it can be related to Wittgenstein's philosophy of `meaning as use', whereby meanings of words can be determined from their context. The logical models have been the champions on the theory side, whereas in practice their probabilistic rivals have provided the best predictions. This two-sortedness of defining properties of meaning: `logical form' versus `contextual use', has left the question of `what is the foundational structure of meaning?' even more open a question than before. This project has ambitious and far-reaching goals; it aims to bring together these two complementary concepts to tackle the question. And it aims to do so by bridging the fields of linguistics, computer science, logic, probability theory, category theory, and even physics. Its scope is foundational, multi and inter disciplinary, with an eye towards applications. 

Meaning assignment is a dynamic interactive process involving grammar and logic as well as meanings of words. Both of the two existing approaches to language miss a crucial aspect of this process: the logical model ignores meanings of words, the distributional model ignores the grammar and logic. We aim to model the entire dynamic process alongside the following three strands of integration, foundations, and applications. 

(I) In integration we develop a process of meaning assignment that acts with the compositional forms of the logical model on the contextual word-meaning entities of the distributional model. 

(II) In foundations, we go beyond classical logical principles of compositionality and context-based models of meaning to develop more fundamental processes of meaning assignments based on novel information-flow techniques, mainly from physics, but also from other linguistic approaches and other models of word meaning, such as ontological domains and conceptual spaces. 

(III) In applications, we evaluate our theories against naturally occurring data and apply the results to practical issues based on meaning inference and similarity, e.g. in search. To be able to work with logical connectives in Google, one needs to re-enter them by hand in the `advanced search' tab, by manually decomposing the logical structure of the sentence and moreover providing the extra context for their different meanings. This is fundamentally non-compositional and goes against the spirit of automated search. It is exactly here that the lack of compositional methods in meaning assignment causes practical problems and where our compositional methods become of use. Hence, we aim to put forward our results to tackle such problems, e.g. to be able to use our sentence similarity models for paraphrasing, question-answering, and retrieving documents that have the same meaning and/or are about the same subject. Our proposed partnership with Google, ensures access to real life data and helps implementation and applicability of our methods in small and large scales.",,"On the knowledge side, the proposed research will cause significant scientific advances across different disciplines of logic, linguistics, mathematics, physics, and computer science. This is by modeling the process of cognition and natural language generation and developing new mathematics, logic, and high level diagrammatic tools.

The project has 3 partners, from Computer Science in Cambridge, Cognitive Sciences and Artificial Intelligence in Utrecht, and industry in Google. These extend the geographical boundaries of the impacts of the project from UK to Europe and the US, but also from academia to industry. I also have on-going collaborations with experts from these various disciplines in venues including UK, Italy, France, USA, and Canada. 

On the economy and social side, internet with its huge pool of services and data has become an inseparable part of our daily lives. The theoretical results of this project will be put forward to improve the quality of services on the internet. At the moment documents are identifies as bags of their words. If the relationships between the words is also taken into account, language processing tasks will hugely benefit, for instance tasks such as information retrieval from text and document search. As a result, new techniques for applications such as question answering and textual entailment will be developed, better answers to online questions will be provided, and more comprehensible summaries of news and articles will be constructed automatically. The partnership with Google and Cambridge is exactly towards following and realizing this pathway. 

From the other side of the spectrum, the results will help us understand the nature of intelligence and language understanding. This has conceptual importance of its own, will improve the quality of human life in due time, by facilitating mutual understanding in society and across societies of different languages. 

Finally, the proposed theoretical setting is based on using simple diagrammatic techniques to depict mathematical structure and logic. The simplicity and accessibility of these methods provides the public with a chance to understand advanced academic developments, a chance which will have an impact on educating the society. We have had open sessions to introduce Computer Science research to high school students and especially to girls in Oxford. The diagrammatic methods and their application areas caused much discussion with and within the students. 

On the academic side, apart from publishing articles and attending already-established workshops and conferences, I have asked for funding to organize two workshops. This is to fill the interdisciplinary gap and bring together researchers from the different disciplines of the project, so that we can discuss and disseminate ideas and results and help start a multi-subject community across these different fields. I will also organize the interdisciplinary seminar series of the logic group of computing lab at oxford. These are open to all academic fields and also the public. Other impacts are through training and teaching. I have asked funding for a doctorate student and plan to continue lecturing my field of expertise based on the project. I have already been invited to give advanced lecture series about the subject in Utrecht and in a Masters course in Cognitive Science in University of Latvia."
8,668FD25F-C9E2-477C-82AD-FD3C73B26F36,Learning the structure and dynamics of human environments to support intelligent mobile robot behaviour,"Advances in mobile robot technology over the last 10 years has made the use of service robots (i.e. robots performing tasks for, or with, humans) in the workplace increasingly possible. The problem of building maps of environments that do not change over time has been solved for many application environments, enabling robots to move around in these places for ever-increasing durations. However, real environments do change over time as their inhabitants move around and move furniture and objects as they do so. The resulting changes to a robot's world make it difficult for it to run reliably, and thus there is a danger that we will not be able to create service robots that are able to perform useful tasks in realistic situations. 

The proposed research treats the fact that a robot's environment changes regularly as an opportunity rather than a challenge. We will develop systems that are able to extract reliable, significant patterns from the changes observed by an existing intelligent robot (the Dora the Explorer robot from the CogX project). In particular we will develop two approaches. The first will capture how easy or difficult it is for a robot to move through particular parts of its map at particular times of day (e.g. due to humans getting in the way). The second approach will capture how the positions of objects in rooms change over time (e.g. the desk in my office never moves, but my chair tends to move around in front of the desk, but never near the door). Taken together these approaches will allow a robot to improve its performance on typical service robot tasks such as searching for an object in a building, whilst avoiding certain areas at certain times of day (e.g the corridor by a canteen during lunchtime), all in dynamic environments.

Our research will be informed by an advisory board of experts in reasoning about space and robot learning, and also by the security company G4S who are interested in using mobile robots to assist security guards. Our results could help them by allowing robots to choose better patrol routes through buildings, and to learn how the objects in a room are typically arranged (allowing them to spot when things change due to a burglary or other incidents). As well as presenting our results through the usual scientific channels, we will also demonstrate our finished robot system at the Thinktank science museum in Birmingham, giving the public a chance to learn more about state-of-the-art robots and AI, whilst also testing our systems in a challenging environment.",,"This project will develop innovative approaches to making intelligent mobile robots adapt to human-populated environments. There are a wide range of applications where such abilities are necessary to ensure a robotic solution provides appropriate service, including security, healthcare (portering and monitoring), and logistics (robots finding and delivering items). As such, the results of this project will create impact by enabling companies interested in robotic solutions to take a further step towards fully autonomous, adaptive service robots.

An example of such a company is G4S, the UK and Ireland's largest security solutions group. We have been working with them to understand their requirements for mobile robots, and their needs have led in part to this research. For example, our results could be used to allow security robots to detect (statistically) unusual variations in object positions in rooms (potentially caused during a security violation). G4S's interest is evidenced by a letter of support for this project, and we will build on this by meeting with David Ella, CTO of G4S Technology, during the project to present him our results, and also to discuss potential follow-on activities such as a Knowledge Transfer Partnership. 

As the demographics of our society change, in particular the ratio between the number of people surviving into old age and the number of people training to care for them, more roles in our lives will have to be filled by autonomous systems such as robots, or will require their assistance. The proposed research will have impact by enabling robots to perform more efficiently and effectively in human-populated environments (i.e. the environments where they will be required), thus benefitting society by enabling better service robots.

In order to ensure this impact can be made, it is necessary for the public in general to be more aware of the potential of service robotics, and also have a better understanding of what they can, and cannot offer. By engaging the public on the topic of autonomous robots now, we can start discussing the technological, societal and ethical issues surrounding their wider adoption. We will do this by running a week-long installation of our robot at Thinktank, Birmingham's science museum. Thinktank's interest in this installation is described in a letter of support for this project.

We will increase the impact of our research by making all of our results -- papers, software and data -- available online in a free and open manner. This will include open-source implementations of the new approaches we develop, and an open-source release of the full robot software system we will implement these approaches within. This will create impact by allowing industrial and academic groups developing intelligent mobile robots to build directly on our work, speeding up the process of creating and deploying such systems in real-world environments."
9,39B380A8-9F91-4289-92C8-6B2C31371B72,Computational Creativity Theory,"Computational Creativity is the study of how to build software which takes on some of the creative responsibility in arts and science projects. We are at a stage where software can generate pictures, melodies, jokes and poems, can invent new words and discover new and interesting mathematical theorems, and regularly helps scientists to make important discoveries. This kind software can be used autonomously, or in collaboration with creative people. It is also used in cognitive modelling projects, to shed light on aspects of human and animal creativity. In the last decade, Computational Creativity has come of age, as evidenced by special issues of publications such as the Minds and Machines journal and the AI magazine, and the first International Joint Conference on Computational Creativity, which replaced 10 years of successful workshops at major AI conferences. 

The proposed Leadership Fellow, Simon Colton, is a recognised expert in Computational Creativity, and has been working in the field since 1996. He is unique in having been involved in successful applications of creative software to four different domains, namely mathematical invention, video game design, graphic design and the visual arts. His mathematical theory formation software, HR, has produced theorems and concepts published in the mathematical literature; his visual art software, The Painting Fool, has produced pictures that have been exhibited and attracted much public attention; and research being done in the Computational Creativity group that he leads at Imperial College is helping video games companies to design the next generation of adaptive, personalised games.

A number of authors, such as Boden, Wiggins and Ritchie, have introduced formalisms which help us to be more precise about the creativity of software. However, there is no agreed upon theory which can describe the behaviour of software with sufficient acuity, coverage and formality that enables accurate comparison of implementations. In short, we have no generic way of saying that software B is more creative than software A. This has held back our field, because with no concrete and formal measures of the creativity of the software we build, it has been hard to put forward falsifiable scientific hypotheses that one approach is more creative than another, hence it has been difficult to progress, and to show progress. 

With this Fellowship, we propose to change this situation, by developing Computational Creativity Theory (CCT). This will comprise a series of models, each of which contains some conceptual definitions and some calculations involving those definitions which can be used to compare and contrast the creativity of software. The foundational models will make more precise the notion of a creative act and the impact they can have, and the more acute models will cover aspects of creative behaviour including intentionality, interpretation, imagination, appreciation and affect. To model computer creativity sufficiently well, we generalise past the merely generative and past usual AI notions of value, into new areas where software is expected to invent its own aesthetic and utilitarian measures, and frame its creations by describing its motivations, intentions, methods and innovations and by putting its work into historical and cultural contexts.

The proposed programme of research has the development of CCT at its heart. This is informed by a series of practical projects involving applications to creative language, music, visual arts, mathematics and games, and covering modes of creativity including realtime generation, assistive technologies and creative collaborations. By building and disseminating CCT, we will help to bring Computational Creativity research into a new era, where formal notions of creativity underpin software systems which really enrich our cultural lives.",,"Creativity is a hugely emotive, loaded and often confusing and contradictory term. While the creative industries of a country are as important to its economy as its manufacturing output, we are only beginning to understand the value of innovative practice in the workplace. Scientists, educationalists, engineers, politicians, leaders of industry, philosophers and artists all study creativity for different reasons, whether it is as an intellectual challenge, to increase productivity or to drive through policy changes. The popular conception of human creativity is confused and often steeped in mythology and romantic notions based on ill-defined terms such as imagination and inspiration. Given all these factors, our study of the controversial notion that software can be creative has the potential to seriously impact business, education and culture. In particular, our proposal to impose a concrete formalism able to describe creative behaviour in such a way that numerical comparisons can be used to measure creativity, is likely to be disruptive, and to have a wide impact.

In September 2010, the British Council in Madrid organised a public event to mark its 70th anniversary. The proposed Fellow, Simon Colton, was asked to speak at the event, alongside Ramon Lopez de Mantaras, a high profile Spanish AI researcher with similar interests in creativity studies. The public interest in the notion of software being creative is sufficiently high that Spain's two most popular newspapers, El Pais and El Mundo both covered the event, and ran articles in their weekend editions which covered Colton's work on The Painting Fool project (which aims to build a software system which is one day taken seriously as a creative artist in its own right). The combined print circulation of the newspapers is around 700,000 and they are the most popular Spanish language newspapers on the internet. Hence we hope that the issues of Computational Creativity were firmly planted in the minds of tens or hundreds of thousands of people. Again, due to our covering notions of creativity, our work has similarly been discussed in the New Scientist, More4 TV news and the Metro Newspaper. We will continue to react to press enquiries by being as forthcoming as possible about our work and the creativity issues it raises. Moreover, we will pro-actively write articles for the popular and specialist press, and regularly print and disseminate summary information about our work to governmental, educational and industrial organisations, in addition to releasing iPhone/Android/iPad applications for general consumption.

Software which can act in creative ways is clearly of value to the creative industries and the wider Digital Economy. We have already collaborated on funded projects with the Emote, Introversion, Lionhead, NestorGames and Rebellion games companies, in addition to Universal Music and Sony. Two of the project partners are from industry, and we will work closely with them to embed our research ideas in their working practice. As an example of the kind of industrial impact we expect from the Fellowship, in summer 2010, the proposed Fellow was asked to sit on a steering group looking at the future of digital tools for the creative industries, organised by the Creative Industries Knowledge Transfer Network (CIKTN). This led to the publication of a beacon project report, which was widely circulated to creative industry firms, educational establishments and governmental bodies. On the inside cover was a quote from the proposed Fellow: &quot;We are approaching an exciting time when computers will be powerful enough, and software sophisticated enough, for computers to be our creative partners&quot;. Again, we hope that this led to Computational Creativity issues being raised in creative industry firms and wider organisations, and working with the CIKTN to meet new industry partners and disseminate our work widely form important pathways for impact that we have planned."
10,CC89BA4F-7CDB-4797-9FFA-B3221AE799AE,UCT for Games and Beyond,"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.",,
11,9BC99088-C040-43AD-87DE-9A38D25C617E,Mining the Network Behaviour of Bots,"The botnets phenomenon has quickly become a major security concern for all the
Internet users. In fact, not only has it rapidly gained popularity among the
mass media, but it has also received the attention of the research community
interested in understanding, analyzing, and detecting bot-infected machines.
Once infected with a bot, the victim host joins a botnet, a network of
compromised machines that are under the control of a malicious entity. Botnets
are the primary means for cyber-criminals to carry out criminal tasks,
such as sending spam mail, launching denial-of-service attacks, or stealing
personal data such as mail accounts or bank credentials.

Clustering and correlating network events represent the state-of-the-art when
it comes to detecting and understanding the botnets phenomenon from a network
perspective. While effective, such approaches rest on weak foundations being
vulnerable to easy-to-perform (time and network) obfuscation attacks.

The goal of this project is to build on the promising results of our previous
work to explore novel machine-learning techniques to make the state-of-the-art
more accurate and robust against evasions and advanced malware. Exploring the
possibilities of advanced malware (and thus bots) to enable the development of
novel mathematical techniques to address such threats is not a mere academic
exercise. On the contrary, it is of paramount importance to build robust and
hard-to-elude mitigation approaches; something we currently lack, as
acknowledged by the research community at large.

On the cyber security side, we will develop techniques to analyze the network
traffic generated by a bot sample. Our analysis will focus on inferring the
interesting part of a bot's network behaviour to automatically generate models
that faithfully describe it. Our analysis aims at being independent from the
underlying botnet infrastructure, payload-agnostic, and able to pinpoint
legitimate-resembling malicious activities. The network flows of a monitored
bot will be initially filtered to remove well-defined attack patterns. The
remaining flows will be clustered using a number of network features and
suitable similarity functions. Clusters whose size exceeds a given threshold
will then be analyzed for periodicity: bots tend to engage in similar network
activities that have interflow intervals that either are sampled independently
from a potentially unknown probability distribution, or belong to a small
number of well-defined clusters. Once clusters exhibiting interesting
periodicity patterns are identified, they can be used, along with their network
features, for detecting (or understanding the behaviour of) bots in a mixed
population containing both compromised and clean hosts.

On the machine learning side, we propose to explore the use of conformal
prediction developed by our team to make such cluster-based analysis more
accurate and robust against arbitrary obfuscation-based evasion attacks. A
powerful clustering method is based on nonparametric probability density
estimation. A recent work proposes a computationally efficient method of
nonparametric density estimation based on conformal prediction and inherits its
properties of validity. We plan to explore the use of this method for the
purpose of robust clustering. A theoretical challenge is to spell out and study
the properties of robustness for this clustering method that are inherited from
the validity of the study mentioned above. In addition, the property of
validity of conformal predictors is usually established under the randomness
assumption; we will explore how this assumption can be relaxed. In addition,
the property of validity can be used to control the number of &quot;alarms&quot;
(predicting that a host is compromised) raised by a bot detection algorithm.
This is valuable in situations where alarms have to be investigated by human
experts but the available manpower is limited.",,"This project is concerned with the development of novel
obfuscation-resistant techniques to analyze and detect core network
behavioral traits of bot-infected computing devices. 

We will organize a two day workshop on the subject of machine learning
and malware analysis. The workshop will be aimed at bringing together
all the project collaborators, academic researchers and industry
practitioners with interest in the project's topics. The goal of the
workshop is to narrow the gap that nowadays exists between security
research carried out in academia and industry to face common threats.

The workshop will be held at the end of the second year of the project
and members of the advisory board (see next) will be encouraged to
attend. The workshop will include a number of invited talks from
representative of research industry laboratories, including---but not
limited to---McAfee Labs (UK), Symantec Research Labs (Europe), VU
University Amsterdam (The Netherlands), and FORTH-ICS (Greece), for
which this project has already received strong letters of support.

We will also appoint a panel of experts---mostly from the
industry---who will provide advice on practical problems and needs,
and help to promulgate the results of our research.

The Information Security Group at Royal Holloway, University of London
has considerable experience in working with industry. Over the last
ten years it has convened expert panels on public key infrastructures
(the &quot;PKI Club&quot;), authentication and identity management (the &quot;AIM
Club&quot;) and, most recently, on cyber security. These panels,
comprising experts from industry and government circles, meet
regularly to discuss the challenges facing industry and government,
focusing on a different aspect at each meeting. We anticipate that the
experts advisory board for this project would operate in a similar way.

The composition of the board will be determined in the first few
months of the project. At least one member of the advisory board will
be invited to give an industry-centered talk at the workshop. The
board will meet four times, once within six months of the start of the
project and at the end of each year of the project. 

We are confident our project will produce key results to overcome
advanced malware, which are of paramount interest to academic and
industry security researchers. However, not only the developed machine
learning techniques will improve our understanding of bot-related
threats, but they will also likely be applicable to other important
areas, such as System Biology, where time-series data---and clustering
in general---has to be examined."
12,F066F6B0-65C5-4CFA-9E62-493CBA854C6B,Network on Computational Statistics and Machine Learning,"The aim of this network is to establish the UK as the world leading authority in the joint area of Computational Statistics and Machine Learning (CompStat &amp; ML) by advancing communication, interchange and collaboration within the UK between the disciplines of Computational Statistics (CompStat) and Machine Learning (ML).

The UK has tremendous research strength and depth that is widely acknowledged as world leading in both the individual areas of Computational Statistics and Machine Learning. Despite each of these fields of research developing, largely, independently and having their own separate journals, international societies, conferences and curricula both areas of investigation share a common theoretical foundation based on the underlying formal principles of mathematical statistics and statistical inference. As such there is a natural diffusion of concepts, research and individuals between both disciplines. This network will seek to formalise as well as enhance this interchange and in the process capitalise on important synergies that will emerge from the combined and shared research agendas of CompStat &amp; ML.",,"Fit to EPSRC Strategic Priorities: 

This proposed Research Network fits well within the EPSRC area of Statistics and Applied Probability identified as a priority 'grow' area for support. It meets squarely the stated aim of EPSRC to encourage 'greater connectivity with other research areas and facilitate multidisciplinary research'. As identified in the 2010 International Review in Mathematical Sciences there is fragility in the discipline of statistics in terms of a shortage of people. This proposed Network grant will be ideally suited to develop some of the most promising areas of research in statistics and assist in bringing on the new generation of young researchers required to grow Statistics in the UK.

Furthermore EPSRC has very recently identified that the &quot;The interface between the mathematical sciences and ICT is extremely important and offers potential for high impact research. There are some well established connections between these disciplines but there are opportunities for new links to be developed.&quot; In the EPSRC ICT Theme, increasing the connections between Mathematical Sciences and ICT aligns to the Working Together cross-ICT priority and this Network proposal addresses this priority directly.


Communications and Engagement

The main ways in which it will be ensured that the beneficiaries can access the potential of the research output from this Network will be via the public access to materials through the dedicated website, further communication streams will be via Facebook and LinkedIn where professional contacts will seek specific research and technology expertise. The availability of the Research roadmap as it evolves will communicate the emerging research agenda to external agencies as well as direct adopters and developers of research results. 

The annual Network workshops will be an excellent means with which to communicate developments within the Network and act as a means of establishing new collaborations within and without the Network to further widen access to Network activities.

In addition joint publications in the main journals and presentation at the main conferences of each of the respective areas covered will be pursued to ensure persistence of visibility of the outcome of Network based collaborations. Furthermore special sessions which will be jointly organised and supported by the network at the main leading international meetings (e.g. JSM, ISBA, NIPS, ICML) will provide an excellent way in which to propagate the joint research themes emerging from the Network."
13,2E695DA0-F323-439D-A737-29E59BE6D301,Creative Code Generation for Interactive Media,"Computational Creativity research is a branch of Artificial Intelligence where we investigate ways in which software can enhance human creativity, as well as ways in which software can be autonomously creative. Researchers working in this field often build software that creates artefacts of some sort - from paintings to poems, soup recipes to sonatas. In more recent times, our attention has turned to higher level issues, such as how software can evaluate its work, show intentionality and imagination, and how it can frame its own work to add value. 

Writing software is a difficult and creative skill which can only be performed by people after much training. As such, while it would seem a natural fit, there has been no serious study of automatic program generation from a Computational Creativity perspective. With this project, we will address this shortcoming, by applying the approaches and methodologies of more than a decade of work on simulating creative behaviour to the problem of automatically generating and testing interactive, multi-media software artefacts. 

We will examine ways in which we can get software to write programs, and how we can make this process as creative possible. We will test our ideas by building a new system that can write interactive media programs (IMPs), ranging from videogames to first-person-perspective experiential art installations. This will be based on our successful existing co-operative co-evolution software which has generated well-received games in a fully autonomous way, but will hugely extend its creative abilities. In particular, the new system will plan, write and edit new program code directly, testing and evaluating what it writes before adding it to whatever IMP it is currently trying to create. This code might describe a new object, a new control or scoring mechanism, or how to produce a new musical or visual effect. 

We will also look at how creative code generators like our IMP designer can create software that isn't quite finished. These unfinished IMPs will be able to rewrite their own code as people interact with them, to change themselves as they are used. They will self-modify not only in reaction to user responses, but also in reaction to external factors in the world, such as international news or social network trends. We hope to show that these programs are perceived as more surprising, inventive and novel, due to their ever-changing nature.

Importantly, we'll be trying to make automated code generation a creative process. In Computational Creativity research, the software we build does not just generate things - it can make decisions about how to generate something, and communicate why it made those decisions. We will look at how our IMP designer can decide whether something it has made is new and interesting or not, and give it ways to communicate with programmers and users, to tell them about what it has created. We will test our approaches via a crowd sourcing methodology, whereby the feedback from thousands of people interacting with the IMPs will be analysed and subjected to machine learning exercises in order to determine the truth of certain hypotheses, and to produce partial audience models to be used to improve the quality of the output.

We believe that this project will have much impact on Computational Creativity research and Artificial Intelligence in general, as it will bring to the fore new issues in the field, most notably questions around software writing software, the automatic production of &quot;unfinished&quot; artefacts which self-modify and the spectrum from entertainment to thought provocation in interactive media. Moreover, we believe that this project will have much impact in the broader arenas of the public perception of computing and the creative industries, as it will highlight in a very tangible way - the automatic generation of games and artworks - the massive potential for computers to become our creative partners in the future.",,"Software is at the heart of the Digital Economy, and nowhere is that more evident than in the production and consumption of interactive media. The UK's videogame industry is the largest in Europe, contributing over &pound;2.9bn to the UK economy annually, and according to a survey by the Entertainment Research Association, consumption of interactive media and games eclipses both the film and music industries in the UK. Both the film and the games industry already rely on software to generate static content such as visual and audio assets. The use of creative software to make novel contributions to a cultural artefact such as a game through code generation would increase the value of those artefacts significantly. Our research programme will both help foster long-term interest in code generation research within the industry, and provide new software and publications that form foundational body of work for the industry to begin using straight away. 

One specific instance where this is likely to supply a large economic and knowledge benefit is within the small-scale independent game development community, where the presence of tools that can creatively engage in a game design process would greatly increase output and commercial success among its members. The independent games community within the UK is one of the best and most prolific in the world. As major publishers and developers struggle in the economic climate, and with independent game developers finding it difficult to qualify for recently-announced tax breaks, cutting-edge research is a meaningful way in which these major contributors to the economy can be bolstered within the UK. We have budgeted to consult with an independent games designer in order to ensure that our research results do exactly that.

Taking a broader view, our work has the potential to impact on the creative expressivity of the general populace, by investigating techniques that can drive a new generation of assistive game design tools. As ubiquitous computing continues to alter the digital landscape and the population becomes increasingly connected through social networks, more people turn to technology as a means to express themselves. However, the technical barrier to entry for expression through digital media such as games and interactive artworks is still very high, in spite of some notable attempts to provide simple design tools. The current state of the art for such tools suffers from two weaknesses - a dependence on programming, and a lack of intelligent assistance that can provide helpful contributions to the creative process. In her book, Rise Of The Videogame Zinesters, prominent industry critic and game designer Anna Anthropy asks us to &quot;imagine a future where creating a game is as easy as writing a story or drawing a picture&quot;. We believe that developing autonomous and semi-autonomous creative software, as per our proposed research programme has the scope and impact to help make this imagined future a reality.

Computational Creativity projects tend to attract attention from journalists, as their audiences are interested in non-standard technological futures, such as those where we share our creative universe with computers. In the past few years, we have taken much advantage of such interest to spread word of our research. The general public will continue to benefit from our ongoing efforts to disseminate our research outcomes as widely as possible. We will distribute high-quality games and interactive media, present summaries of our research at an accessible level through our project websites and blogs, and continue to spread awareness of Computational Creativity and wider topics in computer science by engaging with print and broadcast journalists on a regular basis when appropriate. In this way, we hope to increase public perception of the potential for creativity in software, and to make the idea of automated creative collaborators more accessible across society."
14,92BBE859-B3A4-480D-8494-09CE689B8F7D,An Active Learning Approach to Network Inference,"The understanding of pathway structures is crucial to our understanding of the functional organisation of genes and proteins. Abnormally functioning pathways underlie many human diseases. Given the extent of noise in biological datasets, and limited amounts of data available, unsupervised determination of network topology is a substantially under-determined inference problem. Consequently, in this project, we focus on network completion, which is a supervised learning task involving multiple types of data. Starting from a set of known links and non-links we construct a classifier which predicts and ranks further possible functionmal links for experimental validation. There will be a parallel experimental programme following an active learning approach to network inference, that is, predictions of functional links will be investigated which will, in turn, be used to improve the predictor. To provide a focus for the accompanying laboratory work, by the medical researchers associated with the project, our principal aim will be the discovery and validation of pathway structures associated with hypertension. Hypertension is the most common cause of preventable disease in the developed world.",,"The project has a wide range of potential beneficiaries. The proposed innovations in machine learning, in multiple kernel and active learning, could be applied across a wide range of disciplines, from recognizing hand-written digits, to face identification, text categorisation, bioinformatics and database marketing, for example. Thus the proposed contributions have potential applications well beyond the indicated biomedical application. Other topics we cover are also of interest within machine learning, and associated application domains, such as network inference, data cleaning, outlier detection and learning with label noise, for example. The project would also be of interest to bioinformatics researchers and medical researchers interested in network inference. The inference of transcriptional regulatory networks is very important for advancing our understanding of the complex regulatory mechanisms within cellular systems: many diseases are associated with abnormally functioning pathways. Finally, the project would be of interest to medical researchers with an interest in the hypertensive state. Though the emphasis of the project is algorithmic innovation and a novel approach to network inference, we have necessarily had to focus on a specific biomedical context. Professor Murphy's prime interest is the hypertensive state and so the associated biological context is the understanding of the cellular regulatory networks associated with this condition. This, in itself, is a very important goal given that hypertension is a significant contributor to premature mortality and morbidity and, for the vast majority of cases (about 95%), the cause is unknown (essential hyperytension). Hypertension is a major econmic burden for society since it is a major cause of stroke, heart failure, renal
failure, blindness and myocardial infarction."
15,6A7E82FE-D85A-41FB-9287-E91F937A5C62,LINE-TRACK: technology to improve overall yield during the manufacturing process,"The project aims to devise innovative technologies and engineering solutions in food &amp; drink manufacturing. Process yield
losses are currently identified by the mass balance method; at the end of the production run the output is compared to the
input of the various process ingredients and materials. This method only identifies the losses after the event and does not
determine where the losses have occurred and so required improvements are difficult to identify accurately. The next
product run materials may be different and so the identified improvements may not be fully applicable. The proposed
tracking technology called LINE-TRACK adopts the principles of prognostics to measure the yield losses in real time. LINETRACK
can be used to 1) identify the points where losses occur; 2) help identify root causes of the losses; 3) alert the
operators as soon as the losses are above specified targets &amp; 4) directly intervene in the system with the ultimate goal to
avoid losses occurring.",,"ECONOMIC. CCE will benefit from cost avoidance in increased yield with decreased effluent and energy. LVS improves
sustainable growth with a new product which can be added to existing products, and by selling LINE-TRACK to overseas
markets, it will give positive impact on UK GDP and trade deficit. LINE-TRACK will open up business opportunities beyond
food and drink sector, combining the strength of LVS and Cranfield.
SOCIAL. For 51 years CCE Sidcup has been a significant manufacturing facility for the local and national manufacturing
community with a current workforce of 333 employees. Being competitive whilst ensuring a sustainable business for the
next 50 years will allow CCE to support the social infrastructure of the Sidcup local community, job security, better work
environment and a strong contributor to UK manufacturing output.
ENVIRONMENTAL. The project gives benefits in terms of reduction of effluent, energy and packing losses and ultimately
the carbon footprint. We envisage that the benefits can be achieved within 1 year of the project execution."
16,638BF415-2124-463A-A44F-63409C75C694,Evo-Bots - From Intelligent Building Blocks to Living Things,"One of the grand challenges in robotics - and an enormous driver for technology - is to make robots more like living systems. If this was achieved, robots would act more autonomously, become more flexible, and be able to repair themselves. Life-like robots could even serve as models of natural organisms. They could be used to help answer three of the Top 25 Big Questions facing science over the next quarter-century (see 125th-anniversary issue of Science): Are we alone in the universe? How and where did life on Earth arise? How far can we push self-assembly?

Our long-term vision is to create the first non-biological living system through evolution in the natural world. This project takes arguably a radical approach: creating living systems without using the building blocks of biological systems. Rather, the building blocks, called evo-bots, will be synthesised from scratch. It is expected that evo-bots, similar to RNA/DNA, can give rise to novel forms of life. 

Evo-bots are expected to be a game changer in robotics. They are mobile robots that control when to move; however, their direction of motion is entirely dictated by their environment. We believe that this trade-off between mobility and extreme simplicity is an ideal compromise that will enable the fabrication of massively distributed robotic systems composed of millions of units. This will pave the way for a whole new range of applications, for example, in water engineering (micro-robots for inspection tasks) and clinical/healthcare technologies (micro- or nano-robots to operate inside the human body).",,"Who might benefit from this research?

The general public, society as a whole, animals (e.g., farming/laboratory), industry and academia in the UK and world-wide.

How might they benefit from this research?

In the short-term (1-3 years), the project will contribute to increasing public awareness and understanding of science: in particular, the question of what constitutes life, under what conditions may life emerge, and what constitutes evolution. Involving the public at an early stage will trigger an ethical debate on synthetic lifeforms, the potential co-existence of biological and non-biological life-forms on Earth, and its wider implications.

In the medium-term (3-10 years), the project will contribute to wealth creation and economic prosperity by helping companies to realise new products based on energy-autonomous micro- to nano-scale robotic swarms (e.g., for cleaning surfaces, inspecting pipes, repairing materials). Moreover, it can contribute to general health and well-being by helping new technologies to establish in micro-medicine (e.g., for non-invasive treatments). Similar to a conventional drug, our robots could target and possibly manipulate specific regions while being propelled through the media in which they are suspended. The main advantage of such a design over self-propelled robots is its simplicity. The robots will need several orders of magnitude less power, and they can possibly even harvest the required energy from their environment itself. They can be smaller in size than any existing autonomous mobile robot. Yet, when acting as a swarm of thousands to millions of units, such a system has the potential to solve problems reliably even if operating in confined spaces that are difficult or impossible to reach for humans.

In the long-term (10-50 years), non-biological species may co-exist with biological species, forming a symbiosis that is specifically designed to help human or animal welfare (e.g., to serve humans or animals, to help reduce the number of farm animals and laboratory animals). In principle, this includes relationships where one species benefits without affecting the other (commensalism) or by harming the other (parasitism), and it is therefore important to start a dialogue about the possible merits and dangers that are associated with such technologies.

The project will offer the research team advanced training in engaging with the public, policy makers, industry and scientists of a range of disciplines."
17,887E7C5B-F693-4C21-AD21-6A0DC21498D5,"ReDites: Real Time, Detection, Tracking, Monitoring and Interpretation of Events in Social Media","The project will develop a demonstrator capable of bringing together isolated strands of research on event detection, monitoring, interpretation, tracking and visualisation into a single system capable of situation awareness. The outcome will be an end-to-end demonstration from detecting an event/incident, through to being able to see what is going on, characterising/summarising the event, tracking, and monitoring.",,"The project will enable the Intelligence Services, MOD, police and Emergency Services to improve their situational awareness by the identification, interpretation and monitoring of events while they are happening. It utilises people's desire to broadcast observations and opinions on the events around them, from simply describing and discussing the evolution of situations, to informing others of their whereabouts and safety and organising themselves. The Arab Spring uprisings are a well documented example of how social media was of fundamental importance in the organisation of the protests and dispersal information. However, while such communication has proven benefits, there is also a risk of anti-social and criminal activities (e.g. to incite violence during the UK riots in August 2011).

Recent governmental reports continue to emphasise the importance and potential benefit of monitoring social media. In the USA the Department of Homeland Security reports on Media Monitoring and Terrorism state &quot;Social Media outlets provide instant feedback and alert capabilities to rapidly changing or newly occurring situations&quot; [1], and that there has been a &quot;...marked a shift in the use of social media in disasters. More than ever before, government agencies turned to mobile and online technologies.&quot; [2]. In the UK, a report considering the financial constraints on policing stated that &quot;...[police] need to use existing resources more smartly and free them up to act on intelligence - not be bogged down by sifting through information. Most importantly they need to use technology wisely to extract actionable intelligence from mass social media data. The police will only be able to bring the current profusion of data under control through the use of analytics, enabling them to action intelligence and use it to help prevent a range of crimes....&quot; [3].

The technology proposed in this project will combine efficient analytical algorithms, implemented on robust distributed technologies, to enable harvesting of real-time information directly from massive, incomplete, contradictory, noisy and distributed social media data. We will build on technology and know-how developed within the various partners' Daisy projects, integrating and improving the technologies to provide a comprehensive and coherent approach to social media event mining. Moreover, we will create demonstrators of capabilities that are made possible by the research to showcase the project outcomes to potential beneficiaries:

(i) The project will developed state of the art in social media mining technologies that aim to be commercialised and applied in subsequent projects. Currently social media monitoring is focusing on trend rather than event detection, the project will enable tools to move beyond these limitations.

(ii) The project will provide a feasibility study on the use of social media event monitoring for intelligence. The technology developed in the project will enable an insight into security related events as they arise, both national and international. We plan to test the developed technology on data collected during emergencies and crises that develop during 2013, demonstrating how future events could be monitored.

(iii) Both individual citizens and society at large will benefit from the results of the project through improved security nationally and internationally. More directly, the ability to gain intelligence through the use and analysis of social media may be reused, e.g. for education and to encourage civic engagement.

[1] DHS Analyst's Desktop Binder -- DHS National Operations Center Media Monitoring Capability, 2011

[2] Fraustino, J.D., B. Liu and Y. Jin. &quot;Social Media Use during Disasters: A Review

of the Knowledge Base and Gaps,&quot; Final Report to DHS, 2012

[3] http://www.policingtoday.co.uk/social_media_in_public_security_finding_a_way_forward_22353.aspx

Social Media in Public Security: Finding a Way Forward, April 2013"
18,F0F8F872-18D2-451A-A960-0B03B77BABA5,Bayesian Inference for Big Data with Stochastic Gradient Markov Chain Monte Carlo,"We are in the midst of an information revolution, where advances in science and technology, as well as the day-to-day operation of successful organisations and businesses, are increasingly reliant on the analyses of data. Driving these advances is a deluge of data, which is far outstripping the increase in computational power available. The importance of managing, analysing, and deriving useful understanding from such large scale data is highlighted by high-profile reports by McKinsey and The Economist as well as other outlets, and by the EPSRC's recent ICT priority of &quot;Towards an Intelligent Information Infrastructure&quot;.

Bayesian analysis is one of the most successful family of methods for analysing data, and one now widely adopted in the statistical sciences as well as in AI technologies like machine learning. The Bayesian approach offers a number of attractive advantages over other methods: flexibility in constructing complex models from simple parts; fully coherent inferences from data; natural incorporation of prior knowledge; explicit modelling assumptions; precise reasoning of uncertainties over model order and parameters; and protection against overfitting. 

On the other hand, there is a general perception that they can be too slow to be practically useful on big data sets. This is because exact Bayesian computations are typically intractable, so a range of more practical approximate algorithms are needed, including variational approximations, sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). MCMC methods arguably form the most popular class of Bayesian computational techniques, due to their flexibility, general applicability and asymptotic exactness. Unfortunately, MCMC methods do not scale well to big data sets, since they require many iterations to reduce Monte Carlo noise, and each iteration already involves an expensive sweep through the whole data set.

In this project we propose to develop the theoretical foundations for a new class of MCMC inference procedures that can scale to billions of data items, thus unlocking the strengths of Bayesian methods for big data. The basic idea is to use a small subset of the data during each parameter update iteration of the algorithm, so that many iterations can be performed cheaply. This introduces excess stochasticity in the algorithm, which can be controlled by annealing the update step sizes towards zero as the number of iterations increases. The resulting algorithm is a cross between an MCMC and a stochastic optimization algorithm. An initial exploration of this procedure, which we call stochastic gradient Langevin dynamics (SGLD), was initiated by us recently (Welling and Teh, ICML 2011). 

Our proposal is to lay the mathematical foundations for understanding the theoretical properties of such stochastic MCMC algorithms, and to build on these foundations to develop more sophisticated algorithms. We aim to understand the conditions under which the algorithm is guaranteed to converge, and the type and speed of convergence. Using this understanding, we aim to develop algorithmic extensions and generalizations with better convergence properties, including preconditioning, adaptive and Riemannian methods, Hamiltonian Monte Carlo methods, Online Bayesian learning methods, and approximate methods with large step sizes. These algorithms will be empirically validated on real world problems, including large scale data analysis problems for text processing and collaborative filtering which are standard problems in machine learning, and large scale data from ID Analytics, a partner company interested in detecting identity theft and fraud.",,"The Bayesian approach to data analysis offers significant advantages over conventional techniques. It has become very popular over the past twenty years thanks to Markov chain Monte Carlo (MCMC) algorithms which allow us to carry out Bayesian computation for complex models. Unfortunately, current MCMC methods do not scale to big data sets and consequently Bayesian techniques are almost never used in a data rich environment. At a time where the amount of data available is growing exponentially fast, this project proposes to unlock the strengths of Bayesian analysis for big data by developing new MCMC-type algorithms that can scale to billions of data items.

The methodologies developed in this proposal will allow the development of Bayesian approaches to large scale data analysis problems now frequently encountered in healthcare, industries and services to the general public. In the medium term, the applications to collaborative filtering, text processing, and to the data of our industrial partner ID Analytics will benefit the general public with more powerful search engines, recommender systems, better credit card scoring techniques and improved methods for identity fraud detection. Other industrial beneficiaries include business analytics, finance, pharmaceuticals, and the defence industry, where the developments of this proposal will have a significant impact on how large scale data can be analysed.

The longer term benefits of this project are also closely linked to the RCUK &quot;Digital Economy&quot; programme. For example the &quot;digital hospital&quot; component of this programme involves the real-time accurate data fusion and tracking of patients. It could directly benefit from the techniques we aim to develop in this proposal."
19,5CF85773-A755-4E9C-895B-34D34F968F85,Bayesian Inference for Big Data with Stochastic Gradient Markov Chain Monte Carlo,"We are in the midst of an information revolution, where advances in science and technology, as well as the day-to-day operation of successful organisations and businesses, are increasingly reliant on the analyses of data. Driving these advances is a deluge of data, which is far outstripping the increase in computational power available. The importance of managing, analysing, and deriving useful understanding from such large scale data is highlighted by high-profile reports by McKinsey and The Economist as well as other outlets, and by the EPSRC's recent ICT priority of &quot;Towards an Intelligent Information Infrastructure&quot;.

Bayesian analysis is one of the most successful family of methods for analysing data, and one now widely adopted in the statistical sciences as well as in AI technologies like machine learning. The Bayesian approach offers a number of attractive advantages over other methods: flexibility in constructing complex models from simple parts; fully coherent inferences from data; natural incorporation of prior knowledge; explicit modelling assumptions; precise reasoning of uncertainties over model order and parameters; and protection against overfitting. 

On the other hand, there is a general perception that they can be too slow to be practically useful on big data sets. This is because exact Bayesian computations are typically intractable, so a range of more practical approximate algorithms are needed, including variational approximations, sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). MCMC methods arguably form the most popular class of Bayesian computational techniques, due to their flexibility, general applicability and asymptotic exactness. Unfortunately, MCMC methods do not scale well to big data sets, since they require many iterations to reduce Monte Carlo noise, and each iteration already involves an expensive sweep through the whole data set.

In this project we propose to develop the theoretical foundations for a new class of MCMC inference procedures that can scale to billions of data items, thus unlocking the strengths of Bayesian methods for big data. The basic idea is to use a small subset of the data during each parameter update iteration of the algorithm, so that many iterations can be performed cheaply. This introduces excess stochasticity in the algorithm, which can be controlled by annealing the update step sizes towards zero as the number of iterations increases. The resulting algorithm is a cross between an MCMC and a stochastic optimization algorithm. An initial exploration of this procedure, which we call stochastic gradient Langevin dynamics (SGLD), was initiated by us recently (Welling and Teh, ICML 2011). 

Our proposal is to lay the mathematical foundations for understanding the theoretical properties of such stochastic MCMC algorithms, and to build on these foundations to develop more sophisticated algorithms. We aim to understand the conditions under which the algorithm is guaranteed to converge, and the type and speed of convergence. Using this understanding, we aim to develop algorithmic extensions and generalizations with better convergence properties, including preconditioning, adaptive and Riemannian methods, Hamiltonian Monte Carlo methods, Online Bayesian learning methods, and approximate methods with large step sizes. These algorithms will be empirically validated on real world problems, including large scale data analysis problems for text processing and collaborative filtering which are standard problems in machine learning, and large scale data from ID Analytics, a partner company interested in detecting identity theft and fraud.",,
20,BEAA5363-C415-4295-B2DA-93A0B716AC21,Automatic Generation of Scientific Theories,"Creativity is at the heart of scientific research, both in the natural and the social sciences. It is also at the basis of innovation in business and industry. Psychological research suggests that the Darwinian mechanisms of variation and selection are the key constituents of creativity. Similarly, philosophers of science such as Popper have noted the evolutionary character of scientific knowledge. However, with the exception of preliminary work by the PI, there has been no attempt to use computers to evolve scientific theories using mechanisms based on natural evolution. 

The aim of this research is to fill in this gap by proposing a novel way to generate theories. Theories will be automatically generated by computer programs using algorithms based on natural evolution. The success of the theories will be estimated by the extent to which they account for the experimental data, and the best theories will be selected for the next generation, possibly modified by mutation or crossover. The method will be applied to experiments in cognitive psychology, health psychology and behavioural economics. 

Currently, the development of scientific theories is considered the exclusive province of humans. This project will genuinely be transformative as it will challenge this conception and change the way theories are viewed in the social sciences: from static abstract objects to concrete objects that can be manipulated by computer programs. The proposed method will lead to more efficient ways to exploit empirical data for the development of theoretical knowledge, with implications for applied research in the long term.",,"The current project is primarily a piece of basic rather than applied research, and its main beneficiaries, at least in the short and medium term, will be other academics. Our impact strategy will therefore emphasise academic channels, including a dedicated website, conference presentations and publications, and articles in peer-reviewed journals. In addition, a workshop on automatic generation of scientific theories will be organised at the University of Liverpool to disseminate our findings, promote our novel methodology and receive comments from colleagues. Finally, the implementation of our methodology as well as a sample of the models that it will develop will be made publicly available on our website. These online resources will encourage discussion, spur new developments in cognitive psychology, cognitive science, evolutionary computation and philosophy of science, and provide a forum for critical, comparative and interdisciplinary research.

Our research is also likely to have impact outside of academia, but only in the longer term, which makes it difficult to foresee the exact nature of this impact at this point in time. The better our understanding of natural and artificial phenomena, the better the applications that can be developed. As our methodology has the potential to develop better theories than those currently used and thus to make better use of empirical data, it should have impact for applications as well.

We are also aware that our inter-disciplinary research, due to its transformative and ambitious nature, will be of interest to the public at large. We will therefore ensure wider understanding of the issues involved through press releases in newspapers, public-oriented webpages, public lectures, radio and TV. Note that the applicants already have an excellent track record of publicizing their work to the general public, and their work has been described in national and international newspapers (e.g. The Times, The Financial Time, NRC-Handelsblad, La Naci&oacute;n, Die Zeit), magazines (e.g. Times Higher Education, Scientific American, New Scientist, Psychology Today), radio and TV (e.g. BBC, HBO, National Public Radio, Arte)."
21,43B1F45D-CB2E-40C6-A7A7-EA3F1D8C5ADB,Bayesian Inference for Big Data with Stochastic Gradient Markov Chain Monte Carlo,"We are in the midst of an information revolution, where advances in science and technology, as well as the day-to-day operation of successful organisations and businesses, are increasingly reliant on the analyses of data. Driving these advances is a deluge of data, which is far outstripping the increase in computational power available. The importance of managing, analysing, and deriving useful understanding from such large scale data is highlighted by high-profile reports by McKinsey and The Economist as well as other outlets, and by the EPSRC's recent ICT priority of &quot;Towards an Intelligent Information Infrastructure&quot;.

Bayesian analysis is one of the most successful family of methods for analysing data, and one now widely adopted in the statistical sciences as well as in AI technologies like machine learning. The Bayesian approach offers a number of attractive advantages over other methods: flexibility in constructing complex models from simple parts; fully coherent inferences from data; natural incorporation of prior knowledge; explicit modelling assumptions; precise reasoning of uncertainties over model order and parameters; and protection against overfitting. 

On the other hand, there is a general perception that they can be too slow to be practically useful on big data sets. This is because exact Bayesian computations are typically intractable, so a range of more practical approximate algorithms are needed, including variational approximations, sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). MCMC methods arguably form the most popular class of Bayesian computational techniques, due to their flexibility, general applicability and asymptotic exactness. Unfortunately, MCMC methods do not scale well to big data sets, since they require many iterations to reduce Monte Carlo noise, and each iteration already involves an expensive sweep through the whole data set.

In this project we propose to develop the theoretical foundations for a new class of MCMC inference procedures that can scale to billions of data items, thus unlocking the strengths of Bayesian methods for big data. The basic idea is to use a small subset of the data during each parameter update iteration of the algorithm, so that many iterations can be performed cheaply. This introduces excess stochasticity in the algorithm, which can be controlled by annealing the update step sizes towards zero as the number of iterations increases. The resulting algorithm is a cross between an MCMC and a stochastic optimization algorithm. An initial exploration of this procedure, which we call stochastic gradient Langevin dynamics (SGLD), was initiated by us recently (Welling and Teh, ICML 2011). 

Our proposal is to lay the mathematical foundations for understanding the theoretical properties of such stochastic MCMC algorithms, and to build on these foundations to develop more sophisticated algorithms. We aim to understand the conditions under which the algorithm is guaranteed to converge, and the type and speed of convergence. Using this understanding, we aim to develop algorithmic extensions and generalizations with better convergence properties, including preconditioning, adaptive and Riemannian methods, Hamiltonian Monte Carlo methods, Online Bayesian learning methods, and approximate methods with large step sizes. These algorithms will be empirically validated on real world problems, including large scale data analysis problems for text processing and collaborative filtering which are standard problems in machine learning, and large scale data from ID Analytics, a partner company interested in detecting identity theft and fraud.",,
22,67ED2CD2-4561-45C6-B5B0-9B7D2ED75674,Privacy Dynamics: Learning from the Wisdom of Groups,"We propose to study privacy management by investigating how individuals learn and benefit from their membership of social or functional groups, and how such learning can be automated and incorporated into modern mobile and ubiquitous technologies that increasingly pervade society. We will focus on the privacy concerns of individuals in the context of their use of pervasive technologies, such as Smartphones and personal sensors which share data in the Cloud.

We aim to contribute to research in three areas: 

(1) software engineering of adaptive systems that guide their users to manage their privacy; 

(2) development of machine learning techniques to alleviate the cognitive and physical load of eliciting and personalising users' privacy requirements; and 

(3) empirical investigation of the privacy behaviour of, and in, groups, in the context of both collaboration and conflict.

The ability to control and maintain privacy is central to the preservation of identity. In recent years, social psychologists have made a core distinction between personal identity (which refers to what makes us unique, as individuals, compared to other individuals) and social identity (which refers to our sense of ourselves as members of a social group and the meaning that group has for us). In the latter case, our sense of who we are can be derived from our membership of social groups. Identity is not fixed, but is rather the outcome of a dynamic process. We can move from a personal to a social identity (and back again) depending on the context. We can move between different social identities (for example, as a male, a father, a worker, a football fan, English, British, etc). Identity matters because it provides a prism through which we perceive the world, experience events, decide how to act, and understand our relationships to other people. It tells who is and who is not of us, who is for us and who is against us. Understanding the identity process is therefore key to assessing the impact that privacy and security policies have on people's behaviours. This is essential in order to be able to deliver systems that can express and analyse users' privacy requirements and, at runtime, self-adapt and guide users as they move from context to context.

Broadly speaking, our proposed project asks the following two questions and attempts to answer them from both a social psychology and a computing perspective:

Can privacy be a distributed quality (across 'the group')?
If so, under what conditions might this be the case?

Can the group protect the privacy of the individual?
If so, how does the group manage the privacy-related behaviour of its members?

The research challenges for the project are to devise non-intrusive yet rigorous ways in which to study privacy, both using pervasive technologies (such as life-logging cameras and biometric sensors) and in order to deliver more effective privacy management. At the heart of the project is a hypothesis that individuals are able to better manage their privacy by adopting or learning from the 'wisdom of groups' - we use this term as an acknowledgement of the crowd sourcing movement, also adapted by others in the catchphrase 'wisdom of friends'. Our novelty is in extending this idea to exploit the wisdom of particular subsets of people - groups whose positions and knowledge are more nuanced than a crowd. Our technical challenge is to investigate what we call the privacy dynamics of individuals as they relate to their membership of social, professional or other groups, to develop computational (machine learning) techniques that support such dynamics, and then to deliver privacy management capabilities interactively, autonomously, and adaptively as individuals' contexts change.",,
23,0DE840C8-F1D0-487E-8542-14C7F442D25F,MAPTRAITS: MACHINE ANALYSIS OF PERSONALITY TRAITS IN HUMAN VIRTUAL AGENT INTERACTIONS,"Research findings suggest that personality traits such as extraversion, agreeableness, and openness to experience, are tightly coupled with human abilities and behaviour encountered in daily lives: emotional expression, linguistic production, success in interpersonal tasks, leadership ability, general job performance, teacher effectiveness, academic ability, as well as interaction with technology. In fact, human users tend to anthropomorphise computers and virtual agents, treating them as social beings, and interpreting their behaviour similarly to daily human-human interactions.

The problem of assessing people's personality is very important for multiple research and business domains such as computer-mediated staff assessment and training, human-computer and human-robot interaction. Despite a growing interest and emphasis on personality traits and their effects on human life in general, and recent advances in machine analysis of human behavioural signals (e.g., vocal expressions, and physiological reactions), pioneering efforts focusing on machine analysis of personality traits have started to emerge only recently: (i) there exist a small number of efforts based on unimodal cues such as written texts/ audio/ speech/ static facial features, (ii) despite tentative efforts on multimodal personality trait analysis, the dynamics (duration, speed, etc.) of multiple cues, which have been shown to be important in human judgments of personalities, have mostly been neglected, (iii) although personality analysis research suggests that a trait exists in all people to a greater or lesser degree (i.e. a person can be anywhere on a continuum ranging from introversion to extraversion), none of the proposed efforts have attempted to assess personality traits continuously in time and space (i.e., how a person can be rated along the multiple trait dimensions at a given interaction time and context), and (iv) how machine (automatic) traits analysis can be utilised for personalised, social and adaptive human - virtual agent interaction has not been investigated.

Overall, both the common everyday technology (e.g., personal PCs, smart phones) and the more sophisticated systems people use nowadays (e.g., computer games, assistive technologies, embodied virtual agents, etc.) lack the capability of understanding their human users' personality and behaviour, and of providing socially intelligent, adaptive and engaging human - computer interaction.

To address these issues and limitations, MAPTRAITS project will bring around a set of audio-visual tools that can analyse and predict human personality traits dynamically from multiple nonverbal cues and channels (i.e., upper body, head, face, voice and their dynamics) in continuous time and trait space. There is no prospect of building a perfect system for automatic analysis of personality traits that can be used in all possible application domains in 12 months' time. Therefore, as a proof-of-concept, the MAPTRAITS technology will be developed for automatic matching of virtual agent and user personalities, to automatically model what type of users would like to engage with what type of virtual agents to the aim of user engagement enhancement. The motivation for choosing this application area lies in its significance: (i) Research has shown that people's attitudes toward machines and conversational agents is based on the perceived personality of the agent, and their own personality, and (ii) humans are social beings, and currently their everyday life revolves around interacting with computers, virtual agents and robots that are getting increasingly popular as companions, coaches, user interfaces to smart homes, or household robots.",,"The MAPTRAITS project will focus on machine analysis of personality traits. It will bring around a set of audio-visual tools to analyse and predict human personality traits dynamically from multiple nonverbal cues and channels in continuous time and trait space (represented with multiple trait dimensions), to match virtual agents with human users. 

The line of research targeted in this funding application is highly interdisciplinary and the research outcomes can be spun into the outer society in various ways. Relevant beneficiaries are identified based on the expected term of impact. 

In the short term, the output of the research proposed will benefit national and international research groups working in the area of social signal processing, human behaviour analysis and interpretation, and embodied and relational agent community and applications. The most immediate application of the proposed research is in the areas of embodied conversational agent and relational agent design by addressing issues related to personalised, adaptive and engaging user - agent interaction, and by providing such agents with the means of analysing and understanding the nonverbal behaviour and personality of their users, to guide the agent's social interaction and personalisation ability. This in turn will improve their effectiveness in various application domains related to learning and health care, namely virtual tutoring, elderly care, cyber coaching and cyber therapy. From an interdisciplinary point of view, the knowledge gained will impact the understanding of the role of the personality traits in human - agent interactions. 

In the medium term, a system capable of assessing human personality traits displayed in naturalistic and spontaneous settings (non-verbal behaviours expressed through facial expressions, body gestures, and vocal cues) has the potential to alter significantly other existing technologies, and become applicable and expandable to multiple domains. Examples of such domains are: (1) recommending interfaces to people (matching people's personality traits with household robots, or virtual agents); (2) personal wellness technologies (prescription of exercise programs taking the patient's personality into account, prescription of therapies for people suffering from personality disorders, and guided therapy); (3) e-learning (to assess a student's personality and engage the student in studying that is most suited to his or her personality); and (4) pre-employment assessment (matching personality traits with job requirements)."
24,FE7F4BC0-E73A-40D2-9800-513C34D6B6A4,BTaRoT: Bayesian Tracking and Reasoning over Time,"In this project we will provide new advances in computational methods for reasoning about many objects that evolve in a scene over time. Information about such objects arrives, typically in a real-time data feed, from sensors such as radar, sonar, LIDAR and video. The tracking problem for such scenarios is a well-trodden area, studied for many decades by many researchers. The new and exciting part of this project is in automated understanding of the `social interactions' that underlie a multi-object scene. Can we learn the emerging network structure that develops between objects, in terms of things like who is following who, where is a particular group of objects heading (danger zone or friendly air-field?), has an object left one group and joined another, has a new set of network interactions suddenly come into force? We also seek to integrate this kind of deeper understanding of a complex scene with a simultaneous handling of all of the sensor information available and the decision-making tasks that are required (which sensors to swich on/off, whether an object is friendly or a source of danger, whether an object behaves like a land-rover or a civilian car). 

These sophisticated and difficult problems can all be posed very elegantly using probability theory, and in particular using Bayesian theory, a generic inferential and decision-making methodology that allows one to infer hidden information about a system given data from sensors and some prior beliefs about general behaviour patterns of objects. While generic and straightforward to pose, there are substantial challenges for our problem area in terms of how to pose the underlying prior models (what is a good way to model the random behaviour of networked objects in a scene?), and how do we carry out the very demanding computational calculations that are required for many-object scenes? These modelling and computational challenges form a major part of the project, and will require substantial new theoretical and applied algorithm development over the course of the project. We will develop novel computational methods based principally around Monte Carlo computing, in which very carefully designed randomised data are used to approximate very accurately the integrations and optimisations required in the Bayesian approach.

The outcomes from this ambitious project could cause a paradigm shift in tracking methodology if successful, moving away from the traditional viewpoint of a scene in which objects move independently of one another, towards an integrated viewpoint where object interactions are automatically learned and used in improved decision-making processes. We anticipate that the impact will be substantial across a wide range of related disciplines, from ecology and animal behaviour studies through to economic and social networking.",,"BTaRoT is expected to generate scientific innovations aimed at processing heterogeneous sensor data and knowledge extraction for high-dimensional systems. As such, the program will generate considerable impact for a wide range of academic and non-academic beneficiaries, principal amongst whom are: 
a) The sensor signal analysis community including academia and industry;
b) Our collaborating industrial partner, directly;
c) The research community, particularly in the areas of signal processing and engineering;
d) The project personnel: the EPSRC funded postdoctoral researchers and PhD students from both teams in Cambridge and Lancaster;
e) Society in general.
The project impact will involve both novel theoretical developments and knowledge transfer of the new methodologies to industry. A strong team is formed which has already an existing collaboration. The partners have been able to leverage their relationship with the wide range of users both in the academic area and industry. The project will afford new partnerships to be formed based on the EPSRC funding. Since the partners are actively involved in many external organisations and networks we will be able to link strongly with these organisations to strengthen the research base. BTaRoT seeks to develop a suite of objective simulation tests to benchmark the performance of developed methods and to draw on its existing industrial collaboration with QinetiQ. The PI in Lancaster will also exploit all benefits of InfoLab21, which hosts specialised ICT companies and hence this is a unique environment for dissemination and knowledge transfer of the obtained results.
A number of specific activities will be undertaken: 
 Secondment and visits of RAs or staff from partners or users.
 Regular plenary workshops to bring together the consortium and wider users group to exchange information and update users on progress.
 A dedicated web site will be developed to enable access to latest results and progress. Latest news and significant advances will be considered for release to the wider media as appropriate."
0,A837044A-84FD-4363-978C-2223EFF24049,BTaRoT: Bayesian Tracking and Reasoning over Time,"In this project we will provide new advances in computational methods for reasoning about many objects that evolve in a scene over time. Information about such objects arrives, typically in a real-time data feed, from sensors such as radar, sonar, LIDAR and video. The tracking problem for such scenarios is a well-trodden area, studied for many decades by many researchers. The new and exciting part of this project is in automated understanding of the `social interactions' that underlie a multi-object scene. Can we learn the emerging network structure that develops between objects, in terms of things like who is following who, where is a particular group of objects heading (danger zone or friendly air-field?), has an object left one group and joined another, has a new set of network interactions suddenly come into force? We also seek to integrate this kind of deeper understanding of a complex scene with a simultaneous handling of all of the sensor information available and the decision-making tasks that are required (which sensors to swich on/off, whether an object is friendly or a source of danger, whether an object behaves like a land-rover or a civilian car). 

These sophisticated and difficult problems can all be posed very elegantly using probability theory, and in particular using Bayesian theory, a generic inferential and decision-making methodology that allows one to infer hidden information about a system given data from sensors and some prior beliefs about general behaviour patterns of objects. While generic and straightforward to pose, there are substantial challenges for our problem area in terms of how to pose the underlying prior models (what is a good way to model the random behaviour of networked objects in a scene?), and how do we carry out the very demanding computational calculations that are required for many-object scenes? These modelling and computational challenges form a major part of the project, and will require substantial new theoretical and applied algorithm development over the course of the project. We will develop novel computational methods based principally around Monte Carlo computing, in which very carefully designed randomised data are used to approximate very accurately the integrations and optimisations required in the Bayesian approach.

The outcomes from this ambitious project could cause a paradigm shift in tracking methodology if successful, moving away from the traditional viewpoint of a scene in which objects move independently of one another, towards an integrated viewpoint where object interactions are automatically learned and used in improved decision-making processes. We anticipate that the impact will be substantial across a wide range of related disciplines, from ecology and animal behaviour studies through to economic and social networking.",,
1,89F630E8-5988-4BB0-8B3C-01EDFCDDEAB3,Privacy Dynamics: Learning from The Wisdom of Groups,"We propose to study privacy management by investigating how individuals learn and benefit from their membership of social or functional groups, and how such learning can be automated and incorporated into modern mobile and ubiquitous technologies that increasingly pervade society. We will focus on the privacy concerns of individuals in the context of their use of pervasive technologies, such as Smartphones and personal sensors which share data in the Cloud.

We aim to contribute to research in three areas: 

(1) software engineering of adaptive systems that guide their users to manage their privacy; 

(2) development of machine learning techniques to alleviate the cognitive and physical load of eliciting and personalising users' privacy requirements; and 

(3) empirical investigation of the privacy behaviour of, and in, groups, in the context of both collaboration and conflict.

The ability to control and maintain privacy is central to the preservation of identity. In recent years, social psychologists have made a core distinction between personal identity (which refers to what makes us unique, as individuals, compared to other individuals) and social identity (which refers to our sense of ourselves as members of a social group and the meaning that group has for us). In the latter case, our sense of who we are can be derived from our membership of social groups. Identity is not fixed, but is rather the outcome of a dynamic process. We can move from a personal to a social identity (and back again) depending on the context. We can move between different social identities (for example, as a male, a father, a worker, a football fan, English, British, etc). Identity matters because it provides a prism through which we perceive the world, experience events, decide how to act, and understand our relationships to other people. It tells who is and who is not of us, who is for us and who is against us. Understanding the identity process is therefore key to assessing the impact that privacy and security policies have on people's behaviours. This is essential in order to be able to deliver systems that can express and analyse users' privacy requirements and, at runtime, self-adapt and guide users as they move from context to context.

Broadly speaking, our proposed project asks the following two questions and attempts to answer them from both a social psychology and a computing perspective:

Can privacy be a distributed quality (across 'the group')?
If so, under what conditions might this be the case?

Can the group protect the privacy of the individual?
If so, how does the group manage the privacy-related behaviour of its members?

The research challenges for the project are to devise non-intrusive yet rigorous ways in which to study privacy, both using pervasive technologies (such as life-logging cameras and biometric sensors) and in order to deliver more effective privacy management. At the heart of the project is a hypothesis that individuals are able to better manage their privacy by adopting or learning from the 'wisdom of groups' - we use this term as an acknowledgement of the crowd sourcing movement, also adapted by others in the catchphrase 'wisdom of friends'. Our novelty is in extending this idea to exploit the wisdom of particular subsets of people - groups whose positions and knowledge are more nuanced than a crowd. Our technical challenge is to investigate what we call the privacy dynamics of individuals as they relate to their membership of social, professional or other groups, to develop computational (machine learning) techniques that support such dynamics, and then to deliver privacy management capabilities interactively, autonomously, and adaptively as individuals' contexts change.",,"The proposed research aims to have impact in four distinct areas:

(1) practitioners and professional services: by involving industry partners in an iterative user-centred design and evaluation processes during the course of the project, we will enable professional software engineers to include privacy considerations in their development practices. Additionally, the project will deliver a research-based automated adaptive privacy engineering environment and software architecture geared towards designers of ubiquitous computing systems.

(2) society and culture: we will engaging communities of end-users of the mobile privacy management technologies by involving them in our field studies and publicising our results through a variety of public-facing media. This will have the impact raise awareness of privacy issues relating to next-generation technologies in society at large, and lead to better adoption of these technologies.

(3) public policy: we will build on past work (e.g. our contribution to ENISA's lifelogging privacy report) to contribute to European initiatives to increase policy-makers' understanding of privacy issues relating to ubiquitous computing technologies. We will also continue our engagement with relevant European agencies and Information Commissioners to promote best practice and influence policy.

(4) economic: working with the respective university's technology transfer offices, we will develop a commercialisation strategy aimed at creating a viable route to market for the framework and mechanisms and offer consultancy services based on its capabilities.

We will design and deliver a demonstration of the privacy engineering tools at conferences such as ACM SIGCHI, ICSE and intermediary organisations such as the British Computer Society RE Specialist Group. Our experience has shown that much of this research is particularly accessible and relevant to the wider scientific community as well as the general public. The skills of the Media and Public Relations teams at all three institutions will be used to enable the research investigators to make research results accessible to the wider public through major newspapers, magazines, Internet publishing and broadcast media.

We will set up a project committee consisting of the applicants, university technology transfer officers, and senior members of industrial partners to maintain a register of impact pathways, opportunities and risks to decide the most important forms of dissemination to maximise impact.

Additionally, any peer-reviewed publications arising from this grant will be registered on the Open University's open access institutional repository - Open Research Online (ORO) at http://oro.open.ac.uk. ORO is now one of the largest repositories in the UK. The site receives an average of 40,000 visitors per month from over 200 different countries and territories and has received over 2.2 million visitors since 2006. It enables access to research outputs via common search engines including Google, by using the OAI (Open Archives Initiative) Protocol for Metadata Harvesting."
2,EC5CA48E-E752-4079-A91A-6C65A9BF4840,Statistical Natural Language Processing Methods for Computer Program Source Code,"Complex software systems involve many components and make use of many external libraries. Programmers who work on such software must remember the protocols for using all of those components correctly, and the process of learning to use a new component can be time consuming and a source of bugs.

We believe that there is a major untapped resource that can help address this problem. Billions of lines of code are readily available on the Internet, much of which are of professional quality. Hidden within this code is a large amount of knowledge about good coding practices, for example, about avoiding error-prone constructs or about the best protocol for using a particular library. We envision a new type of programming tool, which could be called data-driven development tools, that aggregate knowledge about programming from a large corpus of mature software projects, for presentation within the development environment. Just as the current generation of IDEs helps developers to manage their code, the next generation of IDEs will help developers to learn how to write better code.

Fortunately, there is a research field that has already developed a large body of sophisticated tools for analyzing large amounts of text: namely, statistical natural language processing. The long-term strategic goal of this project is to develop new natural language processing techniques aimed at analyzing computer program source code, in order to help programmers learn coding techniques from the code of others. There is a large area for research here that has been almost completely unexplored.

As a first step in this research area, in this project we will focus on automatically identifying short code fragments, which we call idioms, that occur repeatedly across different software projects. An example of an idiom is the typical construct for iterating over an array in Java. Although they are ubiquitous in source code, idioms of this form have not to our knowledge been systematically studied, and we are unaware of any techniques for automatically identifying idioms. The main objective of this project is to develop new statistical NLP methods with the goal of automatically identifying idioms from a corpus of source code text. We call this research problem idiom mining, and it is to our knowledge a new research problem.

This is an interdisciplinary project that draws from statistical NLP, machine learning, and software engineering. The research work of this project is primarily in statistical NLP and machine learning, and will involve developing new statistical methods for finding idioms in programming language text.",,"The work in this proposal has the potential for substantial economic benefits in the long term. This project is an applied one with the general goal of building tools that help developers to create software more accurately and with less cost. The UK has one of the strongest software sectors in Europe. For example, in 2008 the UK accounted for 25% of European software companies. By making it possible to develop software at lower cost, we hope that this will benefit companies that sell software by lowering their costs. We hope that these tools would have special benefit to the many companies that develop custom software systems for their own in house use, by lowering the cost of these infrastructural projects.

We also hope that software developers themselves benefit, by having new tools that make their jobs easier and more enjoyable. It has been our personal experience as a professional software developer, that programmers find it extremely important to have good development tools, and even enjoy using them. This is evidenced by the fact that in many cases, programmers voluntarily spend their own time working on tools; development tools comprise many of the most successful open source projects, such as the Gnu command-line utilities, gcc, and Eclipse. We hope that development tools based on code mining have the potential to have an elusive &quot;magic&quot; quality, by finding patterns that programmers recognize but didn't realize existed---with the effect of making software developers happier and more productive."
3,2B8E12BD-E1B6-4F17-A59E-CBD8BBD335BB,EPSRC IRC 'SPHERE' - a Sensor Platform for HEalthcare in a Residential Environment,"The UK's healthcare system faces unprecedented challenges. We are the most obese nation in Europe and our ageing population is especially at risk from isolation, depression, strokes and fractures caused by falls in the home. UK health expenditure is already very substantial and it is difficult to imagine the NHS budget rising to meet the future needs of the UK's population. NHS staff are under particular pressure to reduce hospital bed-days by achieving earlier discharge after surgery. However this inevitably increases the risk that patients face post operative complications on returning home. Hospital readmission rates have in fact grown 20% since 1998.

Many look to technology to mitigate these problems - in 2011 the Health Minister asserted that 80% of face-to-face interactions with the NHS are unnecessary. 

SPHERE envisages sensors, for example:
1) That employ video and motion analytics to predict falls and detect strokes so that help may be summoned.
2) That uses video sensing to analyse eating behaviour, including whether people are taking their prescribed medication.
3) That uses video to detect periods of depression or anxiety and intervene using a computer-based therapy.

The SPHERE IRC will take a interdisciplinary approach to developing these sensor technologies, in order that:

1) They are acceptable in people's homes (this will be achieved by forming User Groups to assist in the technology design process, as well as experts in Ethics and User-Involvement who will explore issues of privacy and digital inclusion).

2) They solve real healthcare problems in a cost-effective way (this will be achieved by working with leading clinicians in Heart Surgery, Orthopaedics, Stroke and Parkinson's Disease, and recognised authorities on Depression and Obesity).

3) The IRC generates knowledge that will change clinical practice (this will be achieved by focusing on real-world technologies that can be shown working in a large number of local homes during the life of the project).

The IRC &quot;SPHERE&quot; proposal has been developed from day one with clinicians, social workers and clinical scientists from internationally-recognised institutes including the Bristol Heart Institute, Southampton's Rehabilitation and Health Technologies Group, the NIHR Biomedical Research Unit in Nutrition, Diet and Lifestyle and the Orthopaedic Surgery Group at Southmead hospital in Bristol. This proposal further includes a local authority that is a UK leader in the field of &quot;Smart Cities&quot; (Bristol City Council), a local charity with an impressive track record of community-based technology pilots (Knowle West Media Centre) and a unique longitudinal study (the world-renowned Avon Longitudinal Study of Parents and Children (ALSPAC), a.k.a. &quot;The Children of the Nineties&quot;). 

SPHERE draws upon expertise from the UK's leading groups in Communications, Machine Vision, Cybernetics, Data Mining and Energy Harvesting, and from two corporations with world-class reputations for research and development (IBM, Toshiba).",,"The SPHERE IRC aims to have a profound impact on the health and well-being of people with a wide range of different health challenges and the UK's growing elderly population. The beneficiaries will also include families, carers, health and social services professionals involved in all stages of care. SPHERE targets significant health challenges; obesity, depression, stroke, falls, cardiovascular and musculoskeletal diseases.

SPHERE's approach includes a substantial amount of technology demonstration. This is specifically designed to ensure that benefits to key stakeholders are brought about - these beneficiaries include clinicians, the NHS and UK industry, which will be able to build not only on the SPHERE technology platform, but will be able to access the body of knowledge and understanding built up through 5 years work with communities and user groups. It is impossible to overstate the importance of this; home healthcare technology is inherently highly invasive and technologies that are not designed from the outset with user participation from a wide range of demographics are doomed to fail. Furthemore, industrial pull-through will only be achievable if technology meets the needs of doctors, nurses, social workers, care-givers and the NHS generally - SPHERE is ideally-situated to take those needs on board.

Specific industrial beneficiaries include the project partners IBM and Toshiba who by participating have strongly indicated their interest in the outcomes, but also companies including Phillips, Texas instruments, Intel and Microsoft all of whom are already marketing, or are known to be researching, ICT heathcare solutions, services and/or telehealth/home sensor devices. There are also a number of companies in the UK and elsewhere developing ICT products for assisted living e.g. &quot;Sensormind&quot; and &quot;Just Checking&quot;.

SPHERE already includes engagement with local government through the participation of Bristol City Council. Bristol has an excellent - and growing - reputation for Smart City research and has a &quot;Futures Group&quot; that has a remit to evaluate the opportunities for ICT in the delivery of the city's services (see the letter of support). Bristol City Council is participating in several EU funded technology projects for this reason, and with the City's growing statutory responsibility for the health of its citizens, Bristol City Council (like other local authorities) is keenly interested in the role of technology in healthcare. The City Council works closely with national government and is an ideal point of contact.

SPHERE has a stated objective to engage with policy-makers and this will be delivered though the eminent and influential co-investigators that are such a valued part of the IRC. Many of these co-investigators already have considerable track-record in interfacing with policy-makers in the NHS and in Government."
4,FE1E93FD-EEEF-4705-8450-51A4B5272714,Personalised Medicine through Learning in the Model Space,"In order to achieve the goal of truly personalised healthcare and disease treatments tailored specifically for each individual patient, we should be able to understand why a disease appears or progresses, how does it happen, where it would happen and in how long this will happen. It is not an easy task.

Mathematics is playing an ever-increasing role in the area of health and medicine, through the use of predictive modelling, statistics, and virtual simulations. Such mathematical tools are becoming invaluable in testing the feasibility of therapeutic procedures and medical devices prior to clinical trials. Furthermore, over the coming years computer models coupled to patient-specific diagnostics will be used in real time in the clinical environment to directly advise on treatment strategies. 

Given the wealth of (many times) disconnected biological, epidemiological and environmental information on a disease and adding on top of this the multiple paths that we as individuals can follow (a change in lifestyle, a geographical change, etc.) and our own individual characteristics (genes, anatomy, weight, age, etc.) it is not surprising that personalised models are difficult to achieve. There is data, information and knowledge that we must be able to connect via mathematical approaches in order to represent the mechanisms of the disease and the unique journey that we all follow. From a modeller's perspective, this is an incredible conundrum: what is important/ what is not? how do I formulate the cause-effect relationships with this disparate data if I don't understand how one risk factor or variable relates to another?

The aim of this project is to be able to 'guide' the modeller from the data and to provide personalised models for diagnosis and treatment. Starting from an already existing (partial) explanation of the disease constructed in a mechanistic mathematical way (explanation-based or hypotheses driven), the information should lead the modeller. In order to do this in a systematic way, we propose that the information will be built into so-called &quot;data-driven&quot; models: i.e, models that fit the data but don't explain why. These &quot;data-driven&quot; models are &quot;intelligent&quot;: they learn from the data and information that they have. If these &quot;data-driven&quot; models could learn in the same space that the mechanistic models try to explain, there is a possible path of common understanding of these two approaches that could potentially exist. And this is the path that we intend to explore and define.

The different levels in personalised medicine that will be considered in this project are the following:
- Cell &amp; organ level: in the context of this project, with 'cell &amp; organ level' we mean the behavior of individual cells (cell level), the joined behavior of all cells in a tissue (tissue level) and the combined behavior of the tissues in an organ (organ level).
- patient level: with 'patient level' we mean the properties and processes of organs and patients, part of which can be observed through online monitoring, visual inspection, therapy records, etc.
- care level: with care level we mean the whole of actions of nurses and doctors, the behavior of the support systems, the applicable guidelines and policies, etc. which are external to the patient but have a significant impact on his condition.
The developed methods will allow one to perform the following prediction and inference tasks:
- Assessment of risk of a range of potential complications.
- Early warning for and diagnosis of such conditions.
- Simulation of effects of possible treatments for individual patients.",,"This project directly addresses the strategic priority of novel treatment and therapeutic technologies by developing new patient-centred model-based predictive and diagnostic tools. It will have an impact on health and quality of life by allowing personalised healthcare for chronic health conditions. In Great Britain, around 17.5 million adults may be living with chronic disease, 45% of those will suffer from more than one condition. 80% of GP consultations relate to chronic disease and patients with chronic disease and complications use over 60% of hospital bed days. Chronic disease costs the NHS &pound;7 for every &pound;10 spent on patient care and the incidence of chronic disease in the over 65s is predicted to double by 2030; the potential social and financial impact of reduced medication, reduced unplanned admissions and reduced hospital stays is clear. Programmes of work on early intervention in both physical and mental health conditions and modelling processes that accurately categorise patient health states would enhance efforts to detect illness at the 'sensitive' period in the evolution of the disorder. Our system will aid clinical decision making in our increasingly overstretched healthcare services.

The potential beneficiaries of this work encompass academia, clinicians and patients; delivering substantial mathematical novelty while also being of high clinical value. This will be achieved in a highly multidisciplinary and complementary team across several institutions that specialise not only in cutting edge methodologies but also in solving real clinical problems. While we will perform the ground work in the context of two exemplar applications, the general methodology has the potential to serve a broad range of clinical (or indeed non-clinical) applications. 

To achieve impact, we will exploit our local links across our institutions, enabling us to foster strong collaborations between the mathematical, machine learning and clinical communities. These collaborations are vital to ensure the translational value of this work. Clinicians will be involved throughout the whole project, from the early phases (including collection of the appropriate data), through the refinement of mechanistic models and learning methods in the model space, up to the evaluation of the overall system. Specific translational links include the University of Birmingham's links to the Queen Elizabeth Hospital and UCL's collaboration with University College Hospitals."
5,2E0CA60C-1C5A-42F0-A126-EDEFB026BEDB,Computational approaches to cognition: the origins of social and causal reasoning in children and primates,"Imagine watching as a fellow traveler approaches a ticket machine, rubs a pound coin carefully on the machine's side, and then inserts it into a slot before retrieving a ticket. When it's your turn, what do you do? Inserting a coin seems important, but perhaps you would rub it first too - after all, that guy must have had a reason to do so. 

From an early age, children are quick to copy another's action, and from very little feedback they can screen off unnecessary actions and focus on what works. But in a context like this they usually copy all the actions in a sequence, even those that seem unlikely to matter. Chimpanzees don't do this: if they can see that an action is pointless, they don't copy it. Of course, both species are intensely social beings, and their causal learning takes place in a rich social context. A natural question is therefore how social interaction informs and influences learning. But surprisingly, we know very little about how an actor's intentions and the observer's prior physical knowledge feed into children's causal judgments, and still less about how, if at all, our primate relatives take advantage of either source of information. 

We propose to try and disentangle how learners of different primate species, including our own, weigh these sources of information when they decide what to copy. We will first use a computational model that tries to predict how learners should learn from different kinds of evidence. We will then conduct a study examining what learners of different species actually do. In the study, some participants will see two purposeful actions precede an outcome (as in the man rubbing and then inserting the coin). Others will witness the same two actions, but the demonstrator will appear to produce the outcome accidentally (e.g. ignoring the ticket, and continuing to explore the environment). A third group will see a pedagogical demonstration (imagine the man catching your eye and pointing at the coin before rubbing it: would you be more likely to copy him?). 

Cutting across this comparison, we will vary mechanical plausibility. For half of the learners the first action will be particularly unlikely to have causal power (imagine the man rubbing one coin and then inserting a different one). We will measure the tendency of children, apes and monkeys to produce the two actions after the demonstrations. If they integrate some prior physical knowledge, they will be less likely to copy both actions if one is mechanically implausible. However, if they integrate social knowledge, they should be more likely to copy both actions if the actor did them on purpose, and this may lead them to override any previous bias. Follow up studies will look at related questions, such as how different primate species learn to cooperate and coordinate joint behaviors. (Think of helping someone move a heavy sofa. Both people not only have the shared overall goal of getting the sofa out, they have intermediate goals like avoiding bumping into the walls or getting stuck in doorways, and each person has a separate role in achieving those goals). 

The fact is that although all primate infants seem to learn from observation, something about human children is different, and allows them to acquire a working knowledge of the multitude of artifacts developed in their culture, from crayons to ipads, with breathtaking speed. Meanwhile other primates such as chimpanzees spend several years getting to grips with just a few skills, such as the use of a hammer stone to crack nuts, despite many hours of close observation. Understanding what differs may give us helpful insights into what sorts of things children are uniquely adapted to learn. The research will therefore provide important insights into how children learn from teachers, both formally in the classroom and informally in the world.",,"Public engagement with science
The evolutionary origin of cognition is an exciting area of scientific inquiry, and one that has broad public interest. Members of the public will benefit from this research by being able to observe and learn about the research while it is in progress at venues such as the Edinburgh zoo and local libraries and science museums. Scientific research is generally conducted in laboratories and is relatively invisible to the public. Allowing members of the public to observe and participate in research will raise awareness of and interest in cognitive science research and scientific inquiry in general. We particularly expect this work to benefit preschool and school age children, by allowing them to both observe and actively participate in science. 

Increased understanding of children's learning abilities
In the medium and longer term, we expect that the results of the proposed experiments will improve our understanding of children's cognitive abilities and in particular, will have broad implications for how pedagogy and expertise can influence children's learning. Previous research by the PI and others has already generated interesting implications for teaching, suggesting that direct instruction can enhance but also has the potential to interfere with learning. The proposed research will therefore provide important insights into how children learn from teachers, both formally in the classroom and informally in the world.
 
Benefits to teachers, schools and educational policy makers
Even in domains such as causal learning, where explicit instruction is not needed for learning to take place, children are influenced by the information they receive from others. This research will inform our understanding of the impact instructors' words and demonstrations can have on children, and we expect that in the longer term our results will contribute to advances in education policy, especially approaches to early instruction in preschool and grade school settings, and changes in teacher training and education. 

Benefits to parents and families
The proposed work looks at the role of what is called &quot;natural pedagogy&quot; in children's learning. Natural pedagogy is the informal teaching stance that most adults intuitively take when interacting with young children. This instinctive teaching approach is used not only by teachers in the classroom, but also by parents and other caregivers in their day to day lives. Therefore, results from this research may benefit caregivers and the children they care for, by helping us to understand the best ways for them to encourage children's natural learning abilities. 

Benefits to zoo animals and primates in captivity
The proposed experiments with primates at the Edinburgh Zoo involve challenging social and physical reasoning problems. Participating in these experiments provides primates with species appropriate stimulation and cognitive enrichment. In the longer term, we expect that the research proposed here will increase our understanding of non-human primate cognition and social behavior, allowing for the design of improved, species appropriate enrichment activities, social interactions and housing structures in zoos and other situations where primates are kept in captivity. Boredom, social isolation and high stress levels are serious health and welfare problems for captive animals, especially social and intelligent animals such as primates. Results of this research will help zoos and animal caretakers to avoid or mitigate these issues.

Benefits to primate conservation and policy
We expect the results of this research to contribute to the understanding of primate cognition and primate social behavior. The primates involved in this research belong to species that are threatened or endangered. Developing appropriate and effective animal management, conservation and breeding programs, requires understanding species-typical behavior to be successful."
6,255BBB61-5D94-429A-B98D-F6E10E907599,Learning to learn how to design drugs,"A key step in developing a new drug is to learn quantitative structure activity relationships (QSARs). These are mathematical functions that predict how well chemical compounds will act as drugs. QSARs are used to guide the synthesis of new drugs.

The current situation is:
1) There is a vast range of approaches to learning QSARs.
2) It is clear from theory and practice that the best QSAR approach depends on the type of problem.
3) Currently the QSAR scientist has little to guide her/him on which QSAR approach to choose for a specific problem. 

We therefore propose to make a step-change in QSAR research. We will utilise newly available public domain chemoinformatic databases, and in-house datasets, to systematically run extensive comparative QSAR experiments. We will then generalise these results to learn which target-type/ compound-type/ compound-representation /learning-method combinations work best together. 

We do not propose to develop any new QSAR method. Rather, we will learn how to better apply existing QSAR methods. This approach is called &quot;meta-learning&quot;, using machine learning to learn about QSAR leaning. 

We will make the knowledge we learn publically available to guide and improve future QSAR learning.",,
7,C6E71579-E632-4B9E-84C3-1F5F706422B2,Learning to learn how to design drugs,"A key step in developing a new drug is to learn quantitative structure activity relationships (QSARs). These are mathematical functions that predict how well chemical compounds will act as drugs. QSARs are used to guide the synthesis of new drugs.

The current situation is:
1) There is a vast range of approaches to learning QSARs.
2) It is clear from theory and practice that the best QSAR approach depends on the type of problem.
3) Currently the QSAR scientist has little to guide her/him on which QSAR approach to choose for a specific problem. 

We therefore propose to make a step-change in QSAR research. We will utilise newly available public domain chemoinformatic databases, and in-house datasets, to systematically run extensive comparative QSAR experiments. We will then generalise these results to learn which target-type/ compound-type/ compound-representation /learning-method combinations work best together. 

We do not propose to develop any new QSAR method. Rather, we will learn how to better apply existing QSAR methods. This approach is called &quot;meta-learning&quot;, using machine learning to learn about QSAR leaning. 

We will make the knowledge we learn publically available to guide and improve future QSAR learning.",,"Knowledge Production for Better Drug Design

The proposal is an ambitious one: its goal to change the way drug-design research is done, and to make it more efficient and cost-effective.

The most important deliverable of the proposal is to produce knowledge (QSAR-KB) about how to better apply quantitative structure activity (QSAR) methods for drug design. The beneficiaries of this knowledge will drug design practitioners both in industry and the increasing number in the public sector.

We will communicate this knowledge to these beneficiaries by: 
1) Publishing in high-impact journals. 
2) Talking at drug-design conferences. 
3) Project Website

The other project deliverables will also be of direct utility to drug design beneficiaries as they will be made openly available. These will enable drug-designers to:
1) Better develop and evaluate new QSAR methods.
2) Develop their own meta-QSAR systems for application to their own commercially sensitive or proprietary chemoinformatic databases. 
To achieve these applications of the project deliverable we will actively seek to transfer our knowledge and knowhow about meta-learning to the drug-design community. 

We have excellent contacts with the pharmaceutical industry, which we will use to ensure that our research is exploited commercially. ALH is leading the UK partners in the IMI Lead Factory consortium (Dundee, Oxford and Biocity Scotland), which expects to be finalized by the end of 2012. The IMI Lead Factory project is a 160 Million euro public-private consortium consisting of seven major Pharmaceutical companies that proposed to offer 120 industrial scale HTS projects to European academic over the next 5 years with the creation of a screening file of 500,000 novel compounds. 

In addition 2012 ALH founded, Ex Scientia Ltd., a new spin out company focusing on developing novel machine learning methods to automate drug design (Besnard et al., Nature, In Press; PCT/GB2010/05194,0). 


Broader economic benefits

The problem of how best to learn QSARs is of great industrial and medical importance. Drug development is arguably the most important applications of science in the UK. The average cost to bring a new drug to market is ~&pound;500 million. A successful drug can earn &pound;billions a year, and as patent protection is time-limited, even an extra week of protection can be of great financial significance. The UK (both academia and industry) is a leader in QSAR research and chemoinformatics in general as can be seen by its publication record. This project aims to help to maintain this lead.


Health and health sector benefits
 
The deliverables of the proposal will, over the medium term, lead to the faster development of better, and cheaper pharmaceuticals. This will increase the effectiveness of health services. Cost reduction in the delivery of existing services thus freeing up resources to use elsewhere in the health system;


Research capacity building 

The project will train three PDRAs in areas of crucial importance to the future science and industry: drug-design, machine-learning, and the semantic web."
8,7201EBC9-F120-428B-8051-130BE4A6E86B,Privacy Dynamics: Learning from the Wisdom of Groups,"We propose to study privacy management by investigating how individuals learn and benefit from their membership of social or functional groups, and how such learning can be automated and incorporated into modern mobile and ubiquitous technologies that increasingly pervade society. We will focus on the privacy concerns of individuals in the context of their use of pervasive technologies, such as Smartphones and personal sensors which share data in the Cloud.

We aim to contribute to research in three areas: 

(1) software engineering of adaptive systems that guide their users to manage their privacy; 

(2) development of machine learning techniques to alleviate the cognitive and physical load of eliciting and personalising users' privacy requirements; and 

(3) empirical investigation of the privacy behaviour of, and in, groups, in the context of both collaboration and conflict.

The ability to control and maintain privacy is central to the preservation of identity. In recent years, social psychologists have made a core distinction between personal identity (which refers to what makes us unique, as individuals, compared to other individuals) and social identity (which refers to our sense of ourselves as members of a social group and the meaning that group has for us). In the latter case, our sense of who we are can be derived from our membership of social groups. Identity is not fixed, but is rather the outcome of a dynamic process. We can move from a personal to a social identity (and back again) depending on the context. We can move between different social identities (for example, as a male, a father, a worker, a football fan, English, British, etc). Identity matters because it provides a prism through which we perceive the world, experience events, decide how to act, and understand our relationships to other people. It tells who is and who is not of us, who is for us and who is against us. Understanding the identity process is therefore key to assessing the impact that privacy and security policies have on people's behaviours. This is essential in order to be able to deliver systems that can express and analyse users' privacy requirements and, at runtime, self-adapt and guide users as they move from context to context.

Broadly speaking, our proposed project asks the following two questions and attempts to answer them from both a social psychology and a computing perspective:

Can privacy be a distributed quality (across 'the group')?
If so, under what conditions might this be the case?

Can the group protect the privacy of the individual?
If so, how does the group manage the privacy-related behaviour of its members?

The research challenges for the project are to devise non-intrusive yet rigorous ways in which to study privacy, both using pervasive technologies (such as life-logging cameras and biometric sensors) and in order to deliver more effective privacy management. At the heart of the project is a hypothesis that individuals are able to better manage their privacy by adopting or learning from the 'wisdom of groups' - we use this term as an acknowledgement of the crowd sourcing movement, also adapted by others in the catchphrase 'wisdom of friends'. Our novelty is in extending this idea to exploit the wisdom of particular subsets of people - groups whose positions and knowledge are more nuanced than a crowd. Our technical challenge is to investigate what we call the privacy dynamics of individuals as they relate to their membership of social, professional or other groups, to develop computational (machine learning) techniques that support such dynamics, and then to deliver privacy management capabilities interactively, autonomously, and adaptively as individuals' contexts change.",,
9,74954CAF-B7F6-4D01-ADF2-3E493BAC12DB,Accelerated Coordinate Descent Methods for Big Data Problems,"Much of modern society and economy, in the United Kingdom and elsewhere, is moving in the direction of digitization and computation. Humankind is now able to collect and store enormous quantities of digital data coming from sources such as health records (e.g., IBM ``Watson'' project, MRI/CT scans), government databases (e.g., e-Government, GORS: government operational research service), social networks (e.g., Facebook, Linked-IN, delicious), online news (e.g., New York Times article database), corporate databases (e.g., bank records, Amazon.com) and the internet. Global society is, as a consequence, facing many unprecedented challenges and opportunities. One of the biggest of these has to do with the ability (or rather, lack thereof) to distill, understand and utilize in an optimal way the information contained within these gigantic data sources. The main technology for this is to &quot;form an optimization problem'' and then solve it using a well-chosen optimization algorithm in a suitable computing environment (e.g., a multicore workstation, GPU-enabled machine, cloud).

In this project we aim to contribute to a breakthrough in our ability to solve optimization problems arising from big data domains via developing, analyzing and implementing new accelerated parallel coordinate descent (CD) methods. Since in big data problems the data is typically highly structured, well-designed CD methods can have very low memory requirements and arithmetic cost per iteration---often much smaller than the dimension of the problem. This is in sharp contrast with standard methods whose arithmetic complexity of a single iteration depends on the dimension at least quadratically.

Our research objectives are:

1. Acceleration Theory. We will analyze the iteration complexity (i.e., give bounds on the number of iterations/steps needed to achieve a prescribed level of accuracy) of new parallel coordinate descent methods accelerated using the following 4 strategies: a) nonuniformity (of the frequency with which individual coordinates are updated), b) asynchronicity (of updates and computation), c) distribution (of data and computation to nodes of a cluster) and d) inexactness (of certain operations and computations the algorithm depends on).

2. Stochastic Gradient Descent. We will analyze theoretically and test numerically the relationship between parallel coordinate descent (CD) methods and parallel stochastic gradient descent (SGD) methods.

3. ACDC Code. We will implement the accelerated algorithms in a code which we will make publicly available.",,"The pathways to impact document details the ways in which we plan to achieve non-academic impact. The Case for Support details the academic impact.

Here we offer a brief summary:

Academic Impact:
- close research communities: optimization, operational research, numerical analysis, computer science, machine learning, compressed sensing, high performance computing, ITC
- more distant research communities: biology, astronomy, civil engineering, topology design, signal processing
- individuals: Prof Nesterov (Louvain), Prof Srebro (Chicago), Martin Takac (Edinburgh), PDRA, Prof Gondzio (Edinburgh), Prof Recht (Madison), Prof Wright (Madison), Dr Forgan (Astronomy)

Non-academic Impact:
- ACDC code (publicly available efficient code based on the algorithms developed in Module 1)
- SAS Institute (Dr Polik, Dr Chari, Dr Griffin)
- Western General Hospital (Dr Green)
- Arup (Mr Simpson)

Pathways:
- writing papers in top journals
- presenting at top conferences and workshops
- website with code at code.google.com + links to it from various places
- meetings with and feedback from SAS Institute at conferences (at least 1-2 x per year) and via teleconferencing facilities
- visit to SAS Headquarters in Cary, North Carolina
- impact workshop (to be held in May 2015)
- supervision of 2 MSc dissertations (Summer 2013 and Summer 2014)

Since structured convex optimization, randomized algorithms and data mining are important and fast growing fields, this proposal is timely for establishing and growing an internationally competitive research group in this area based in the UK. 

Quote from EPSRC website: &quot;For UK research to have maximum impact, our researchers need to work with the very best on the global stage.&quot; We strongly believe that, given the quality of the network of researchers involved and, in particular, of the visiting researchers, this proposal falls into this category."
10,C0A363B8-4B54-4A92-AAB4-1974BD58C1B1,Unifying audio signal processing and machine learning: a fundamental framework for machine hearing,"Modern technology is leading to a flood of audio data. For example, over seventy two hours of unstructured and unlabelled sound-tracks are uploaded to internet sites every minute. Automatic systems are urgently needed for recognising audio content so that these sound-tracks can be tagged for categorisation and search. Moreover, an increasing proportion of recordings are made on hand-held devices in challenging environments that contain multiple sound sources and noise. Such uncurated and noisy data necessitate automatic systems for cleaning the audio content and separating sources from mixtures. On a related note, devices for the hearing impaired currently perform poorly in noise. In fact, this is a major reason why six million people in the UK who would benefit from a hearing aid, do not use them (a market worth &pound;18 billion p.a.). Patients fitted with cochlear implants suffer from similar limitations, and as the population ages more people are affected. 

It is clear that audio recognition and enhancement methods are required to stop us drowning in audio-data, for processing in hearing devices, and to
support new technological innovations. Current approaches to these problems use a combination of audio signal processing (which places the audio data into a convenient format and reduces the data-rate) and machine learning (which removes noise, separates sources, or classifies the content). It is widely believed that these two fields must become increasingly integrated in the future. However, this union is currently a troubled one, suffering from four problems. 

Inefficiency: The methods are too inefficient when we have vast amounts of data (as is the case for audio-tracks on the web) or for real-time applications (such as is necessary in hearing aids)
Impoverished models: The machine learning modules tend to be statistically limited.
Unadapted: The signal processing modules are unadapted despite evidence from other fields, like computer vision, which suggests that automatic tuning leads to significant performance gains 
Distorted mixtures: The signal processing modules introduce non-linear distortions which are not captured by the machine learning modules.

In this project we address these four limitations by introducing a new theoretical framework which unifies signal processing and machine learning. The key step is to view the signal processing module as solving an inference problem. Since the machine-learning modules are often framed in this way, the two modules can be integrated into a single coherent approach allowing technologies from the two fields to be completely integrated. In the project we will then use the new approach to develop efficient, rich, adaptive, and distortion free approaches to audio denoising, source separation and recognition. We will evaluate the the noise reduction and source separations algorithms on the hearing impaired, and the audio recognition algorithms on audio-sound track data.

We believe this new framework will form a foundation of the emerging field of machine hearing. In the future, machine hearing will be deployed in a vast range of applications from music processing tasks to augmented reality systems (in conjunction with technologies from computer vision). We believe that this project will kick start this proliferation.",,"The signal processing and machine learning methods developed in this project are keystone technologies upon which upstream research depends. In particular, the project will have a significant impact for the health-care and digital industries. The following specific groups will benefit from the research:

The hearing impaired: hearing aid users

Six million people in the UK who would benefit from a hearing aid do not use them (a market worth 18 billion p.a.). This group of people is expanding rapidly as the population ages (the number of people aged 65 or older is expected to double by 2050). One of the main reasons why hearing aids are not as widely used as they should be is that they perform poorly in noisy environments. The efficient and adaptive noise removal systems developed for hearing aids in this project will address this key issue. This proposal will therefore contribute to the EPSRC Healthcare Technologies theme. Prof. Moore will be involved in translating the research into hearing aids, including providing access to hearing impaired patients for testing.

The hearing impaired: cochlear implantees

Cochlear implants allow the profoundly deaf, who get little or no benefit from a normal hearing aid, to gain awareness of environmental sounds and, in most cases, understand speech without lip-reading. 8000 people in the UK currently use cochlear implants and there are 1000 new implantees each year. Again, perhaps the major limitation of the current devices is their poor performance in noisy environments. The efficient and adaptive noise removal systems developed in this project aim to address this key issue thereby contributing to the EPSRC Healthcare Technologies theme. Dr. Carlyon will be involved in translating the research into cochlear implants, including providing access to implanted patients for testing.

Audio search and information retrieval: digital-industries and society

Many companies preside over large, uncurated collections of audio data and they would benefit from methods for searching and categorizing the data. For example, over seventy two hours of unstructured and unlabelled sound-tracks are uploaded to internet sites every minute and this number continues to grow. Automatic systems are urgently needed for recognising audio content so that these sound-tracks can be tagged (possibly at precise times throughout the clips) for categorisation and search. Often the audio-tracks are recorded with video and so the audio tags can also be used to search the video. The audio recognition technology developed in this project therefore has wide commercial application to information retrieval. Similarly, the BBC's public space project is attempting to organise and unlock the archives of the BBC, making them accessible to the public. The audio-recognition technologies will make significant contributions to this project, thereby improving public services and enhancing life.

Audio denoising and source separation: digital-industries and society 

Poor quality audio data is becoming common place. For example, 3hrs of recordings made on hand-held devices are uploaded to YouTube every minute. These recording are often made in challenging environments that contain multiple sound sources and noise. Such noisy data necessitate automatic systems for cleaning the audio content and separating sources from mixtures. This project will provide tools for this purpose. Moreover, upstream technologies such as Automatic Speech Recognition (ASR) and Audio Diarisation (AD) systems perform poorly in noisy environments. As such the noise removal methods developed in the project can be coupled with these approaches to improve performance. Since modern approaches to ASR and AD are probabilistic, this raises the possibility of integrated approaches that jointly estimate the noise at the same time as interpreting the content. The advisory group, and in particular Prof. Gales, will advise on possible technology transfer"
11,70165E9A-6C24-49DB-8AAC-E04A6DBA8040,Modelling Discourse in Statistical Machine Translation,"Automatic translation of human languages is an increasing necessity in our global society: large amounts of text are constantly produced in various languages and fast, cheap and accurate translation into a number of other languages is required to foster business and communication within and across nations. This high demand for translations cannot be fulfilled by human translators because of its sheer volume, cost and the lack of skilled professionals. 

Different Machine Translation (MT) approaches have been proposed to automate translation. The most widely adopted approach is Statistical MT (SMT): the broad availability of free, open source SMT systems, along with significant improvements in their quality in recent years, has made SMT a very promising technology. This is evidenced by the many commercially successful SMT systems, such as those developed by Google, Microsoft and IBM. 

Despite its recent success, SMT systems are still far from producing translations that reach human quality levels. A major limitation is that they translate sentences one by one, in isolation, without resorting to any information about the context in which such sentences appear. This leads to systems that are computationally feasible; however, more advanced approaches that overcome this limitation are needed to improve SMT quality and make it a de facto translation technology. The context surrounding a sentence -- its discourse -- contains information about dependencies connecting words or expressions across sentences. Neglecting such connections can lead to incoherent and inconsistent translations:

-- Humans use different words to refer to the same concepts in different sentences. If the links between these words are not identified, sentences can be incoherently translated. E.g.: in &quot;The man bought a leather bag&quot; and &quot;It was soft&quot;, Bing Translator misses the connection between &quot;it&quot; and &quot;bag&quot;. It produces for Portuguese &quot;[...]. *Ele *foi *suave&quot;, rendering a completely inadequate meaning: &quot;He went smooth&quot;.

-- The same text can appear in different sentences. If the links between these occurrences are not identified, they can be translated inconsistently. E.g.: in &quot;He took cash from the bank&quot; and &quot;The bank was far away&quot;, only the first sentence has enough information about the correct meaning of &quot;bank&quot;, and thus the second occurrence gets translated as &quot;*margem&quot; in Portuguese (river bank).

SMT is a young area and researchers have so far focused on overcoming issues within sentence boundaries. Most of these issues have been addressed to a large extent in recent years and it is now time to turn to discourse-level challenges. Very few attempts to deal with these challenges have been proposed. These are limited to pre- or post-processing strategies. 

This project aims at explicitly modelling discourse level relationships across sentences in SMT at translation time without compromising the scalability of existing approaches. The proposed approach includes (i) a novel framework to model discourse level relationships by learning valid transitions across sentences based on rich linguistic information for both source and target languages and (ii) a constraint-based inference algorithm to use these relationships to guide the translation process while keeping it tractable. By decoupling model learning and inference, a basic SMT model will augmented at inference time with document-wide constraints representing expected discourse relationships that are too expensive or unavailable at model learning time.",,"This project has major potential impact in Machine Translation (MT) research and use as it proposes a significant change in the way translations are produced: in the context of a document, as opposed to sentence by sentence, in isolation. The impact of this project spans four main areas:

-- Economy: It is expected that improvements resulting from discourse-informed MT will yield better quality translations, which will have a strong impact in the translation industry and among industrial users of multilingual content, with the potential to further reduce translation costs and turnaround times. It is estimated that more than 40% of Language Service Providers worldwide already use MT as part of their translation workflow, and that this results in 30-40% cost reduction and 70% productivity increase. In addition, 28% of the large corporations worldwide use MT to translated their content. Low quality translations due to, among other things, inconsistencies and incoherences is reported as the main reason preventing an even wider adoption of MT. Improvements in translation quality resulting from using discourse can thus magnify the usefulness of MT. 

-- Society: Better quality translations will also affect individuals who use translations. These include millions of users who benefit from freely available online systems such as Google Translate and Microsoft Bing Translator. These systems are popular among users on the internet for a number of purposes, from the gisting of content in foreign languages to enabling communication (through chats, forums, etc.) with speakers of other languages. For these end-users, translation can have a big impact, as evidenced during the Haiti earthquake in 2010, when Microsoft and Google built basic Statistical MT systems for Haitian Creole in four days. These systems were successfully used to help the relief efforts by improving communication between locals and support teams.

-- Knowledge: This project will advance the state of the art research in Statistical MT (SMT) and the use of linguistic information for this problem, which is a recent and promising direction. The findings of this project will also impact research in a number of related fields: other approaches to MT, namely the rule-based and example-based approaches, which also translate sentences in isolation, and could benefit from the general methodology for discourse processing proposed in this project; NLP for other cross-lingual applications, by providing a bilingual discourse framework that could be adapted to such applications; discourse processing, by providing better understanding of how discourse models can affect bilingual applications; translation studies, by providing a framework to represent and study several linguistic phenomena related to (human or machine) translation.

-- People: The project will have a positive impact on the careers of the PI and RA, who will both gain additional knowledge and experience in discourse modelling, constrained-based learning frameworks and development of scalable software. It will also provide greater exposure for both researchers, allowing the PI to consolidate her position as a new lecturer and raise her profile nationally and internationally in Natural Language Processing (NLP) circles. Having a first UK funded project is a very important step in the PI's academic career. Despite her recent success with EU/US grants, the PI has never been involved in UK-funded projects. Given her goal of establishing herself as one of the leading MT researchers in the UK (and subsequently worldwide), it is essential that she engages in such projects. An EPSRC first grant is the ideal opportunity to start this process: its duration and scale will help ensure the success of this endeavour."
12,DD1BF54A-0742-4286-B16B-E6845E5F2241,WISER: Which Ecosystem Service Models Best Capture the Needs of the Rural Poor?,"It is widely acknowledged that poor rural communities are frequently highly dependent on ecosystem services (ES) for their livelihoods, especially as a safety net in times of hardship or crisis. However, a major challenge to the understanding and management of these benefit flows to the poor is a lack of data on the supply, demand and use of ecosystem services by the poor, particularly in the developing world where dependence on ES is often highest. Recent work suggests that errors associated with the commonly used global proxies (eg. benefits transfer) are likely to be substantial and therefore confuse or worse, misdirect, policy formulation or management interventions (e.g. perverse subsidies). Given these issues, recent improvements in integrated modelling platforms - in some cases founded on desktop process-based models - which aim to provide improved and dynamic maps of current and future distributions of ES have much to offer ES-based poverty alleviation interventions and policy.
While these next generation process-based models appear to have a role to play in ES-based poverty alleviation efforts, the level of sophistication and data needs that is required to deliver policy relevant information is poorly understood. It is, for example, unclear whether even the most sophisticated process-based biophysical model is able to provide sufficiently accurate information for regional- or local-scale policy decision making when based on globally available datasets. Similarly, there has been no attempt to quantify the degree to which disaggregation of beneficiaries is necessary within integrated modelling platforms to provide information on managing natural assets that is relevant to the poorest people. Such analyses are vital to ensure that next generation models produce useful and credible results as efficiently as possible - that is, with a minimum investment in data collection and bespoke model development.
We will evaluate the effectiveness of a range of current modelling approaches of varying degrees of complexity for mapping at least six ecosystem services - crop production, stored carbon, water availability, non-timber forest products (NTFPs), grazing resources, and pollination - at multiple spatial scales across sub-Saharan Africa. We will assess model performance based on two broad metrics: model data requirements and the usefulness to decision-making. Firstly, we will evaluate the data requirements of each modelling tier, using data availability, spatial resolution and uncertainty to score in the intensity of the required inputs. Those models with intensive data requirements will be scored poorly. Secondly, we will evaluate the usefulness of the model in a decision-making process using statistical binary discriminator tests. We will use the same approach to evaluate the impact of consideration of beneficiaries on decision making by comparing the biophysical model outputs with both socioeconomic measures and models also using binary discriminator tests. 
Our goal in this project is to ascertain the degree of complexity of modelling that needs to be applied to map ES at resolutions that are useful for poverty alleviation. The findings of this project will enable decision makers to: 1) best use existing ES models to inform national and regional land use/cover change policies supporting ES management and promoting equality and justice amongst the beneficiaries of these services; and 2) set priorities determining where scarce resources should be invested to improve effective management of ES. Thus, WISER may help improve the lives of the approximately 400 million people living in poverty in sub-Saharan Africa by evaluating the tools available to policy makers in this region.",,"The developmental impact of this project will be to contribute to poverty alleviation for the approximately 400 million people living in poverty in sub-Saharan Africa by improving the tools available to policy makers to manage ecosystem services (ES) to improve poor livelihoods in this region. More specifically, the findings of this project will enable decision makers to: 1) best use existing ES models to inform national and regional land use/cover change policies supporting ES management and promoting equality and justice amongst the beneficiaries of these services; and 2) set priorities determining where scarce resources should be invested to improve effective management of ES.
The main pathway to impact of WISER will be through improving the reliability of the tools and models that are used to manage the ES used by the rural poor in sub-Saharan Africa. Such an improvement in modelling will have direct impact on poverty alleviation by giving regional, national and international policy makers (the primary beneficiaries) the confidence to translate the multiple potential management actions that can be compared with such tools into evidence-based concrete policy actions which will allow alleviation of poverty through sustainable use of natural resources. In particular, WISER's tests of models that represent both the access to and utilisation of ES have the potential to have major impacts on poverty alleviation, given that this sort of disaggregation of beneficiaries of ES has been mostly overlooked in ES models, yet is clearly vital in devising pro-poor policies for ES management. The project is structured to ensure that the modelling needs of different types of policy makers are incorporated into the specific tests that we will conduct within the course of the project.
We will engage with both governmental and non-governmental organisations throughout the project via stakeholder interviews and workshops. WISER will actively listen to stakeholders throughout the project so they can identify their perceived needs for ES models: e.g. what outputs do they require and what spatial and temporal scale is needed? We will then use the results of these stakeholder engagements to inform our model comparison methods, ensuring we evaluate the ability of the ES models to fulfil the requirements of such decision makers. We will maintain regular communication with the stakeholders to help to build trust and provide the stakeholders with a sense of ownership and understanding of our results. 
Our project team has a strong, grounded understanding of policy decision making at all levels. Our project partners include national staff members from Government Agencies, NGOs and civil society. Through collaboration with our project partners WISER will have access to local, regional and global dissemination networks that will help ensure that WISER deliver long lasting, poverty alleviating benefits, far beyond the lifetime and geographic span of the project.
The secondary beneficiaries include academics and researchers working in cognate fields. Results will be disseminated to academics via conferences, submission to professional newsletters and publication in international peer-reviewed journals. We aim publish in open access journals communicating research to the widest possible audience. Similarly, our data will be publicly available via an online repository."
13,51F7B7BD-573E-4125-A28A-68ACF2A0279A,Semantic Annotation and Mark Up for Enhancing Lexical Searches (SAMUELS),"As humanities datasets get ever larger, researchers have a pressing need for more sophisticated techniques of analysis. The most significant issue in big data research into textual datasets is that our primary methodology for searching, aggregating and analysing them relies not on concepts or meanings, but rather on word forms. These forms are imperfect and evasive proxies for the meanings they refer to, and with 60% of word forms in English referring to more than one meaning, and some word forms referring to close to two hundred meanings, the irrelevant &quot;noise&quot; which appears when searching using word forms grows with the size of the texts being searched.

In big data contexts, this problem cripples research, making any sort of detailed analysis entirely intractable and requiring impossible amounts of manual intervention. In this project, we will deliver a system for automatically annotating words in texts with their precise meanings, enabling a step-change in the way we deal with large textual data. The system is based around the unparalleled Historical Thesaurus of English, which contains 797,000 words from across the history of English arranged into 236,000 hierarchical categories of meanings alongside each word's dates of known use. The annotation software will take a text and provide for each word it contains an XML annotation giving the word meaning's Historical Thesaurus category code. The system will automatically disambiguate word meanings using a range of state-of-the-art computational techniques alongside new context-dependent methods unlocked by the Thesaurus's dating codes and its uniquely detailed and fine-grained hierarchical structure.

Textual data tagged in this way can then be accurately searched and precisely investigated, with any results also able to be aggregated at a range of levels of precision, without the need for manual intervention. A major part of the project is also the development of new techniques for working with semantically-aggregated and disambiguated data. Project partners will conduct research on resources including the Hansard Corpus, consisting of over 2.3 billion words of text, the Oxford English Corpus, the world's largest stratified corpus of modern English, and the EEBO-TCP corpus of 40,000 early modern books. As part of our work on changing the nature of how we deal with data on this scale, we will mine these text collections for frequently-occurring or statistically unusual concepts, will take advantage of our ability to search large datasets for terms realised by ambiguous word forms (such as &quot;union&quot; in the particular context of industrial relations rather than any of the other 33 possible meanings of this word), and will examine the data as a whole from a distant-reading perspective in order to look for striking or significant patterns of meaning changes across time.

These research projects based on tagged data will also drive the development of our tools for using this data, with teams of researchers across the UK and abroad providing a range of different demands on the data, ensuring a variety of needs and use-cases are catered for in the development of the project. In this way, we are committed to producing a set of compelling, fruitful, and practical research outcomes using semantically-tagged data during the lifetime of the project, in order to demonstrate the value of our approach and to help ensure the work of the project is as widely utilised and exploited as possible.

By doing all of this, we will enable new and transformative techniques of exploring, searching and investigating large-scale cultural, literary, historical and linguistic phenomena in big humanities datasets; through this project, it will be possible to place meaning - rather than word forms - at the heart of digital humanities research into text.",,"Education
The modern teaching of history and of literature often operates thematically, with students introduced to specific themes across a range of time periods. Open access through the BYU site to our annotated data will provide ways for students and teachers to explore the literature in EEBO and the political and historical information in Hansard through semantic categories which relate to the broader themes they are studying. We will provide suggestions for use of the data in teaching at upper secondary and tertiary levels, and will particularly highlight and support educational uses of the aggregated data produced in the Hansard project, as well as the Time Machine project. We also see potential for SAMUELS in areas such as improving written style through increased awareness of semantic patterning, grading reading materials according to age and topic suitability, and as a reading aid for older or complex texts.

Writers
Professional writers frequently use the print and online Historical Thesaurus for information about word usage at various periods, for the sake of authenticity in their writing or of novel ways to express concepts. The Time Machine project, linking words in supplied texts to the period during which those words are known to have been used, will be a further key resource for writers, particularly those working on historical fiction. Those writing about topics such as politics, history, and the development of debating styles will find a useful resource in the Hansard project.

Third Sector
Our work on Hansard has already led to discussions with third sector agencies such as theyworkforyou.org, who aggregate and display information to the public about their MP's activities. We will offer these agencies access to the aggregated data produced in the Glasgow Hansard project to enable them to display data on the topics and concepts most often discussed by their elected representatives across time. The original Hansard data was provided to Glasgow by the UK Parliamentary Service, and our enhanced version will be offered to them for their own uses. Libraries such as the National Library of Wales have indicated the need for a resource such as SAMUELS to aid cataloguing and other topic-related activities.

Commercial Implications
While non-commercial and academic access to the annotation system's website will remain free of charge, Oxford University Press, the experienced contractual operator for commercialising the Historical Thesaurus dataset, will partner with Glasgow and Lancaster to seek opportunities in the private sector for a semantic annotation system. One key commercial opportunity is with large providers of historical texts, who would benefit from semantically-aware searches, in particular from the ability to search using concepts in areas where users are unlikely to know historical synonyms for modern terms. This group includes the large private sector providers of legal texts and historical case material (eg WestLaw, HeinOnline or LexisNexis) as well as newspapers' historical archives, and database developers such as Gale, Cengage, and ProQuest. The project therefore has the potential to benefit the economy by stimulating income and creating further commercial outputs across time, and we will work with our partners to achieve this where feasible.

Data Mining
Finally, we see potential impact in SAMUELS for groups which need to mine large data sources for socially or commercially valuable information, including researchers in such fields as health informatics, marketing sentiment analysis, and wide-scale cultural analysis. Our project, with its advantages over plain lexical searches, will be an additional tool for these groups, permitting deeper and more productive mining of their data by aggregating a range of data sources in meaning-structured ways. Such work will also further expose the usefulness of the rich and unparalleled Historical Thesaurus lexical database for their research."
14,FA40B4DB-E290-45C2-B66D-0D1510016CFF,"ENGAGE : Interactive Machine Learning Accelerating Progress in Science, An Emerging Theme of ICT Research","Our vision is to establish and lead a new theme in ICT research based on Interactive Machine Learning (IML). Our expansion of IML will give scientists and non-ICT specialists unprecedented access to cutting-edge Machine Learning algorithms by providing a human-computer interface by which they can directly interact with large scale data and computing resources in an intuitive visual environment. In addition, the outcome of this particular project will have a direct transformative impact on the sciences by making it possible for non-programming individuals (scientists), to create systems that semi-automatically detect objects and events in vast quantities of A) audio and B) visual data. By working together across two parallel, highly interconnected streams of ICT research, we will develop the foundations of statistical methodology, algorithms and systems for IML. As an exemplar, this project partners with world leading scientists grappling with the challenge of analysing enormous quantities of heterogeneous data being generated in Biodiversity Science.",,"This research project ultimately will have a broad impact across a range of disciplines and contribute to the strategic development of the EPSRC portfolio in ICT. By the nature of research that reaches across a number of disciplines within ICT and beyond to other sciences, there is a range of beneficiaries as detailed below.

Machine Learning
The ML academic and industrial community will benefit from the investigation of the proposed methods, and systems for IML, which have far wider impact than the application areas being targeted by this research. While Stream.A. of the proposal will develop, analyse and apply new ML methodologies specifically within the IML framework, there is considerable hope that this will lead to further substantial cross-fertilisation of ideas within the ML research community pertaining to user interaction and the formal quantification of information and uncertainty inherent in the synthesis of user and system as a whole. 

Computer Vision
Only a few individual groups in the Computer Vision community have put effort into building interactive systems. Dissemination of our findings and prototypes from this project will help focus the CV community on challenges beyond the typical objectives of &quot;just&quot; making algorithms run real-time: the labeler(s) providing the training data must be modeled just like other variables. Most importantly, to maintain our field's track record of transferring technology beyond academia, we must plan ahead to models and algorithms that perform online learning as specialized users become more sophisticated and demanding. 

Communications and Engagement

The project will directly impact ICT and scientific communities via workshops, publications, public software releases, and the training of highly qualified personnel. Further details about academic value are explained in the main proposal, so the focus here is on impact beyond the ICT academic sphere.

As a major new area of ICT is being established, it is important that a community is built to foster the interface between the various disciplines that this research in IML will have impact on, to share early results, stimulate enquiry and adoption of the research results, as well as to encourage a wider community to engage in this area of research. To engage researchers and users of IML systems, we will organize two qualitatively different styles of workshops. The first set of workshops will be carried out in the top venues for ML (NIPS), CV (ICCV), and Human Computer Interaction (CHI). These workshops will perform the crucial task of bridging the separate research communities to create the strong inter-community bonds necessary for long-term research in IML. 

The second style of workshop will be a hands-on workshop to introduce our IML tools to non-programming scientists who can apply them in their own work. These workshops will be modelled after similar, highly successful endeavours supporting open-source software by the Blender Foundation, a non-profit corporation that has created a number of computer animated short films."
15,52E28BD8-ECD8-4FAC-8FCD-D1960F4E5233,Network on Computational Statistics and Machine Learning,"The aim of this network is to establish the UK as the world leading authority in the joint area of Computational Statistics and Machine Learning (CompStat &amp; ML) by advancing communication, interchange and collaboration within the UK between the disciplines of Computational Statistics (CompStat) and Machine Learning (ML).

The UK has tremendous research strength and depth that is widely acknowledged as world leading in both the individual areas of Computational Statistics and Machine Learning. Despite each of these fields of research developing, largely, independently and having their own separate journals, international societies, conferences and curricula both areas of investigation share a common theoretical foundation based on the underlying formal principles of mathematical statistics and statistical inference. As such there is a natural diffusion of concepts, research and individuals between both disciplines. This network will seek to formalise as well as enhance this interchange and in the process capitalise on important synergies that will emerge from the combined and shared research agendas of CompStat &amp; ML.",,"Fit to EPSRC Strategic Priorities: 

This proposed Research Network fits well within the EPSRC area of Statistics and Applied Probability identified as a priority 'grow' area for support. It meets squarely the stated aim of EPSRC to encourage 'greater connectivity with other research areas and facilitate multidisciplinary research'. As identified in the 2010 International Review in Mathematical Sciences there is fragility in the discipline of statistics in terms of a shortage of people. This proposed Network grant will be ideally suited to develop some of the most promising areas of research in statistics and assist in bringing on the new generation of young researchers required to grow Statistics in the UK.

Furthermore EPSRC has very recently identified that the &quot;The interface between the mathematical sciences and ICT is extremely important and offers potential for high impact research. There are some well established connections between these disciplines but there are opportunities for new links to be developed.&quot; In the EPSRC ICT Theme, increasing the connections between Mathematical Sciences and ICT aligns to the Working Together cross-ICT priority and this Network proposal addresses this priority directly.


Communications and Engagement

The main ways in which it will be ensured that the beneficiaries can access the potential of the research output from this Network will be via the public access to materials through the dedicated website, further communication streams will be via Facebook and LinkedIn where professional contacts will seek specific research and technology expertise. The availability of the Research roadmap as it evolves will communicate the emerging research agenda to external agencies as well as direct adopters and developers of research results. 

The annual Network workshops will be an excellent means with which to communicate developments within the Network and act as a means of establishing new collaborations within and without the Network to further widen access to Network activities.

In addition joint publications in the main journals and presentation at the main conferences of each of the respective areas covered will be pursued to ensure persistence of visibility of the outcome of Network based collaborations. Furthermore special sessions which will be jointly organised and supported by the network at the main leading international meetings (e.g. JSM, ISBA, NIPS, ICML) will provide an excellent way in which to propagate the joint research themes emerging from the Network."
16,1112FED7-125C-45F3-940E-2834BAE7EB09,Symbolic Support for Scientific Discovery in Systems Biology,"The aim of this proposal is to advance the UK's world-leading research on the automation of science by developing novel Artificial Intelligence (AI) support for an existing laboratory robot called Eve (whose predecessor Adam was popularised by Time Magazine and Science in 2009). The purpose of this project is to develop a new logic-based reasoning tool that will allow robots to correct errors in their knowledge. Unlike prior work aimed at extending knowledge that is incomplete, we argue such machines also need the ability to revise knowledge that is incorrect. Indeed, we suggest the capacity to make (and learn from) mistakes is an indispensible part of scientific reasoning. Thus our goals are to realise this ability in a software system for automating intelligent inference about scientific theories and experiments and to demonstrate its benefit in a genuine application of Eve. We believe this will pave the way to a new era in which Robot Scientists will be more productive, more cost-effective, and better able to assist humans in all parts of scientific method.

This project is based on the hypothesis that ground-breaking advances in a field of AI known as Answer Set Programming (ASP) can be used to develop a novel form of (multi-semantic meta-logical) reasoning that will give Robot Scientists the ability to continuously revise and extend their knowledge. Evidence to support this claim is provided by 2 preliminary studies which link the applicant's previous work on the integration of abductive and inductive inference with the robot Adam and a leading ASP system called Clasp. The 1st study showed how a combination of non-monotonic and non-deductive logic can be used to revise a state-of-the-art metabolic model of yeast metabolism in order to fit data seen by Adam; but it also showed a further combination of meta-logical and multi-semantic logic was needed to design new experiments for testing the proposed revisions. The 2nd study suggests how a combination of features recently included in Clasp can be used to do this. Hence this proposal affords a timely opportunity to draw together and build upon these complementary strands of research in a way that will open the door to exciting new opportunities for scientific discovery in systems biology.

The most direct beneficiaries of this work will be our collaborators in the Robot Scientist group (now at the Univ. of Manchester) as our software will enable their robot to correct mistakes in its knowledge and thereby allow the continual evolution of scientific models through many cycles of analysis and experiment. This will represent a major step towards Robot Scientists that participate more effectively in science. By making our tools portable we hope to facilitate their application in other tasks that will benefit from their enhanced reasoning abilities. These tasks include planned follow-on work in the modelling of social insect behaviour (previously studied by our research group) and the automation of some aspects of legal reasoning (recently formalised in argumentation theory). We also plan to study probabilistic extensions of this research that can be built on the logical foundations we will lay. Once our system has been deployed on the Robot Scientist, we also hope to use data generated by planned applications of Eve in high-throughput drug screens to improve our understanding of living organisms.",,"Although the main goals of this project and its most immediate extensions are primarily of an academic nature, their successful realisation could provide the basis for longer-term contributions to both industry and society. Since our work involves the automation of the abstract mechanisms of scientific inference (for automatically correcting errors in formalised knowledge and for designing experiments to test those corrections) it has the potential to be incorporated into a wide range of different scientific fields. We chose to focus initially on the domain of symbolic systems biology to exploit the expertise of our collaborators and facilitate future integration into pharmaceutical applications. This is because the Robot Scientist Eve has been specifically designed to support high-throughput drug screens that are likely to be of both medical and scientific interest; and while they are mainly intended to identify leads for new candidate drugs, such screens will also generate large amounts of data that our methods could potentially use to identify and correct errors in current biological models of the underlying organisms. We believe the possibility of interleaving medical and scientific investigations in this way could represent a major potential benefit of Robot Scientists that our work will help to realise. 

In general, since industrial progress often depends upon scientific progress, it follows that any means of improving the efficiency of scientific method (such as Robot Scientists with the ability to automate the intellectual aspects of science as well as the experimental ones) could have significant benefits. At the Univ. of Bristol we have plans to exploit our reasoning methods for the automation of certain types of legal contractual reasoning (which could have subsequent applications in e-commerce) and the study of social insect behaviour (which could have long term implications in agriculture). We see our work as part of a growing trend where formal methods from computer science are increasingly adopted into the practice of science; and which has been publicised by the &quot;2020 Science Group&quot;, commissioned by Microsoft Research in 2006 to explore key scientific challenges over the next decade. Given the recent uptake of experimental automation in both science and industry, now is the time to invest in intelligent methods for scientific reasoning so that the experimental and analytical aspects can be synergistically integrated in a way that truly addresses the challenges of 21st century science. 

Many of these themes have been explored in a series of workshops organised by Dr Ray on the role of abduction and induction in artificial intelligence and their application to science (AIAI'05/06/07/09). The proceedings of most recent edition (http://www.cs.bris.ac.uk/~oray/AIAI09/) at the International Joint Conference on AI in 2009 reveals a range of related issues and applications the proposed research could benefit. These include scientific policy (e.g. Bradley's invited talk on open notebook science), publication models (e.g. Poole's paper on semantic science), and a spectrum of uses ranging from basic science (e.g. Brodaric's paper on geological knowledge evolution) to national security (e.g. Josephson's invited talk on military situation awareness). Last year, by extending the scope of these earlier workshops, the author organised the 1st International Symposium on Symbolic Systems Biology - a new field at the intersection of formal methods and systems biology - which reveals more potential applications in synthetic biology (e.g. Phillips's work on cell programming) and agriculture (e.g. Muggleton's work on food webs). All of these formal approaches can benefit from the type of logical inference this proposal seeks to develop."
17,B467A384-A403-448B-B7A6-0FF47A21FF94,"Digital Conservation: Birds, Blogs and Volunteers","Governments and NGOs worldwide devote considerable efforts to wildlife conservation. Building public support for such initiatives often involves web-based approaches, which facilitate bringing the natural world closer to people. Web-based approaches can be particularly effective for, often harder to reach, younger audiences. Our Natural Language Generation (NLG) research has investigated the use of automatically generated narratives for communicating complex data to audiences. We developed an exciting new website using NLG to bring up-to-date information about wild animals in their natural environment to both nature and technology enthusiasts. We believe this work pioneers communication of nature conservation through new media.

For Blogging Birds to become a conservation tool, however, it is required to create an actual following. We propose the following three activities and give indicative budget implications in brackets. 
1. Working with the RSPB to learn how best to reach large audiences. 
2. Engaging with user communities, thereby exposing target audiences with our current website and variations thereof to capture what dimensions are most important to actual users. 
3. Building in new functionality based on the above two activities, most likely including social media and several ways in which users can interact with the tool and its users.",,"We have developed ground-breaking technology that helps to tell real-time stories of Scotland's satellite-tagged red kites without any human input. Raw location data from satellite tags are supplemented with environmental data which are automatically analysed to detect patterns; these are ecologically interpreted and directly converted into a blog using Natural Language Generation routines. This automated blogging system is the first of its kind and enables large amounts of data to be instantly converted into readable text. Simply by flying around with a tag on its back, a red kite is allowing a computer to write the story of its life - through weekly blogs about how and why it explores the landscape around it.

The Blogging Birds site (http://redkite.abdn.ac.uk) is developed within the dot.rural Digital Economy Hub at the University of Aberdeen, through collaboration between computer scientists and ecologists working in partnership with conservationists at the Royal Society for the Protection of Birds (RSPB). Our launch of the very first version of 'Blogging Birds' on 22 August 2013 clearly caught the attention of the press; this was helped by a feature article in 'The Conversation' viewed more than 16,000 times. Extensive articles appeared in widely read magazines such as 'Wired', E&amp;T and PopSci, as well as in newspapers (e.g. Telegraph, Scotsman, Irish Times) and on radio (BBC Radio Scotland live and Out of Doors - 7 min). The Chronicle of Higher Education (circulated to all US academics) wrote an in-depth piece highlighting the technology angle of this innovation. New Scientist wrote a glowing column and AOL made a 1 minute video clip professionally bringing out the essence of Blogging birds (see http://redkite.abdn.ac.uk/press.html for all named sources). This large-scaled press activity led to unprecedented dot.rural twitter activity: more than 1.3 million retweets. Blogging Birds was portrayed as 'fascinating', highly innovative' and 'impressive', and supported by readers' comments. The principle of this innovation took society 'by storm' and generated a huge amount of interest in both the principle of 'birds telling their own story', the reintroduction of this species and the underlying technology. Indeed, our NLG blogs are among the first to inform and inspire a general (rather than a specialist) audience and seemed to have done so with reasonable success. The technology magazine 'Wired' complimented that, when &quot;Reading through the blogs, they flow extremely naturally, seem very erudite and even formulate questions relating to activity of the birds&quot;.

Scrutinising the Google Analytics of our website, over 5000 people have visited Blogging Birds from all around the world. Among the top of all NLG sites in terms of use Blogging Birds has provided many people with an encounter of this frontier technology. Importantly, we also managed to attract a following: several hundred people visit the site regularly (&gt;10x in the 6 weeks since launch). It is this aspect which has our particular interest. For Blogging Birds to become a 'new conservation tool' with a wider regular following the site should not only be attractive and dynamic but also interactive, allowing users to engage and contribute material to enhance the blogs. This will create a more engaged community of users who are keen to 'follow' the lives of these charismatic red kites over time."
18,11D4A902-81D6-4B66-8D22-15C1CC9BD402,Testing Autonomous Vehicle Software using Situation Generation,"Autonomous vehicles (AVs) must be controlled by software, and such software thus has responsibility for safe vehicle behaviour. It is therefore essential that we rigorously test such software. This is difficult to do for AVs, as they have to respond appropriately to a great diversity of external situations as they go about their missions. 
 
It is possible to find faults in an AV software specification by testing its behaviour in a variety of external situations, either in reality or in computer simulation. Such testing may reveal that the specification ignores certain situations (e.g. negotiating a motorway contraflow lane) or defines behaviour that is unsafe in a subset of situations (e.g. its policy for adapting to icy surfaces leads to unsafe speed control in crowded urban environments). 
 
This project will test the hypothesis that testing based on coverage of possible external situations (&quot;situation coverage&quot;) is an effective means of finding AV specification faults. We will test the hypothesis by creating a tool that generates situations for simulated AVs, both randomly and using heuristic search, and assessing whether higher situation coverage correlates with greater success at revealing seeded specification faults. (For the search, the fitness function will be based on the situation coverage achieved) 
 
The project will draw on previous work on test coverage measures, on search-based testing, and on automated scenario generation in training simulations. To assess the effectiveness of the approach, we will use a small but practically-motivated case study of an autonomous ground vehicle, informed by the advice of an advisory panel set up for this project.",,"The short-term result of the project will be a technique, supported by proof-of-concept tools, for AV testing by means of situation generation. Developers of AVs and AV algorithms will be able to use developments of this technique to get better understanding of the limitations, weaknesses and risks in their technology. 
 
In the medium term, development of this and similar techniques will increase industry's ability to justify the safety of their AV systems, and thus convince UK regulators to allow more adventurous (inherently risky) technologies and systems, thus maintaining our competitive edge. This will have benefits for potential users of AVs, including the military, law enforcement, and many industries. 
 
These techniques will be adaptable to non-vehicle domains, such as service robotics, which will benefit the potential users of such robots - industry, retail and people with physical disabilities. 
 
A broader scientific benefit will be a general insight into the kind of situations that give rise to problems for AV controllers, and also the kinds of specification faults in them that are difficult to find. This will stimulate researchers in AV technologies to resolve these problems, thus helping to advance what is possible. 
 
In the long term, situation coverage could eventually form part of certification criteria for autonomous vehicle control systems (as MC/DC test coverage does for software under the DO-178B standard for avionics). This would allow its use across many industries and domains."
19,0E213EF8-A19B-45CA-A68D-7F96861E743D,Bayesian Models of Grammar Induction and Translation,"The processes by which humans learn the rules that govern what is and is not a valid sentence in a language constitute an enduring theme of linguistic research. The development of computational models able to reproduce these processess holds great promise for both increasing our understanding of how children learn languages, and the development of advanced language technologies for processing online data.

The fields of Computational Linguistics and Machine Learning seek to provide the technologies necessary to enable people to interact seamlessly with the vast quantities of multilingual text published each day on the world wide web. Core amongst these technologies are those that assign syntactic structure to text (parsing) and automatically translate between languages (machine translation). Traditionally researchers have relied upon supervised machine learning techniques to build their systems, first annotating data by hand with the desired output of the system, then training the system to replicate and generalise from these annotations on new data. However this process of hand annotation is both time consuming and expensive, and as a result such data only exists for dominant languages (e.g. English).

The research programme set out for this fellowship will provide a solution to the problem of obtaining syntactic analyses for large quantities of real world language data. The overarching aim of this project is to develop large scale and language independent algorithms for learning syntactic structure from unannotated text using techniques from non-parametric Bayesian probability. In tandem a new syntactic model of machine translation will be developed to evaluate and validate these algorithms. 

The project will consist of two major components:
1. Develop scalable models of unsupervised grammar induction suitable for use with languages exhibiting a wide range of morphological and syntactic phenomena.
2. Develop a syntactic model of translation based upon the analyses produced by the unsupervised model. This system will be composed of a source reordering model which reorders the input conditioned on the induced syntactic structure, and a phrasal translation model which maps the reordered source to a translation.

The specific scientific contributions of this project are:
1. The first accurate large scale grammar induction algorithms applicable to a wide range of language processing tasks.
2. New advanced machine learning algorithms for latent variable induction and approximate inference within Bayesian non-parametric models.
3. An investigation of the cognitive implications of the developed grammar induction algorithms, which are considerably more powerful than those previously used for language acquisition simulations within computational cognitive science.
4. An extrinsic evaluation of the induced grammars within a novel machine translation system.
5. A state-of-the-art open source machine translation system capable of producing high quality translations for a much larger range of languages than those handled by current systems.

The success of this research programme will have wide ranging impacts beyond the core contribution of a large scale syntactic induction system; from advanced new algorithms for machine learning and machine translation, to a powerful new tool for simulating child language acquisition within cognitive science.
The aims of this project are adventurous but the contribution of an effective and scalable model for Grammar Induction will be transformative for a wide range of text processing applications.",,"The long term impacts of the proposed research program span both the scientific and commercial spheres. The availability of computational models capable of automatically learning the syntactic structure of human languages will allow us to build sophisticated language technologies with minimal supervision while also furthering our understanding of human language acquisition. 

Current language technologies have been extensively adopted but are limited by the difficulties involved in developing systems for new languages or domains. The wide availability of high quality unsupervised language processing technologies that can be affordably built from raw language data holds extraordinary promise for reducing communication barriers in many sectors of the community. The research funded by this proposal will also have a significant impacts on a range of related research fields such as Machine Translation, Information Extraction, Cognitive Science, and Machine Learning.

Commercial:
Britain's economic strength is founded on its origins as a trading nation and the development of high quality language technologies hold the potential to greatly strengthen the competitiveness of British companies doing business in a global online commercial environment. Leading international technology companies, such as Google, Microsoft, IBM etc., rapidly absorb the latest advances in research to improve their online information processing and translation systems, advancing a revolution in the ability of individuals and companies to communicate across language divides on the web. Currently existing commercial language technology systems achieve acceptable performance for a limited number of closely related languages, typically European languages. The development of the algorithms described in this proposal will broaden these horizons by leading allowing systems to be quickly build for any language for which raw textual data is available.

Public:
The British people form a multicultural and multilingual society with a continuing need for high quality language services. Government departments are large producers of multilingual information in their role of communicating with the public. While much progress has been made in automatically extracting information from and translating between English and some European languages, other languages of great significance to British Society, such as Hindi and Urdu, lack the availability of high quality annotated data required to build the current generation of supervised systems. The proposed research will address such limitations by designing machine learning algorithms to directly learning the structure of languages using widely available raw unannotated data.

Research Community
In the wider Computational Linguistics research community the cutting-edge of research is focused on integrating sophisticated linguistic resources, such as treebank trained parsers, into translation systems. While such systems hold great promise for translations involving English, the high expense (currently millions of pounds) of creating such resources for other languages is a barrier to such technologies being widely applicable to all the languages of the world.
The successful execution and dissemination of the proposed research will serve to demonstrate the potential for unsupervised systems to realise similar performance gains without the need for language specific resources. 
In addition to contributions specific to Computational Linguistics, the algorithms developed by this research will push the boundaries of current structured Machine Learning technologies, providing general algorithms which will have wide applicability to research in other areas of artificial intelligence, such as the discovery of structure in images, genetic sequences etc."
20,68DC07AD-96E8-4229-B281-1A9E06458839,Trajectories of Depression: Investigating the Correlation between Human Mobility Patterns and Mental Health Problems by means of Smartphones,"Depression does not only affect the personal life of individuals and their families and social circles but it has also a strongly negative economic impact as shown in several reports. According to a recent study, workers in the United Kingdom suffer high levels of depression than those anywhere else in Europe. The survey found that 1 in 10 employees had taken time off at some point in their working lives because of depression problems. Novel strategies for tackling the problem of depression and preventing suicides are needed. We believe that new emerging technologies, in particular mobile ones, together with the possibility of mining large amount of data in real-time can help to tackle this problem in new and more effective ways. 

Existing interview-based studies have shown that depression is significantly associated with a marked decline of physical activity. The goal of this project is to investigate how mobile phones can be used to collect and analyse mobility patterns of individuals in order to understand how mental health problems affect their daily routines and behaviour and how potential changes can be automatically detected. In particular, mobility patterns and levels of activity can be quantitatively measured by means of mobile phones, exploiting the GPS receiver and the accelerometers embedded in the devices. The data can be extremely helpful to understand the behaviour of a depressed person, and in particular, to detect potential changes in his or her behaviour, which might be linked to a worsening depressive state. By monitoring this information in real-time, health officers and charity workers might intervene by means of digital behaviour intervention delivered through mobile phones or by means of traditional methods such as by inviting the person for a meeting or by calling him or her by phone.

In order to support these novel applications, it is necessary to build mathematical tools for analysing the mobility traces in real-time for the detection of gradual or sudden changes related to the emotional states of the individual. More specifically, we plan to devise analytical techniques for studying the relationships between human mobility patterns and emotional states. We plan to use existing datasets of human mobility and to collect data by means of a smartphone application distributed to people affected by depression. This can be considered as a sort of pilot study for a wider deployment of these technologies and it will provide a sound theoretical basis for further studies in this area. Finally, a key aspect of the proposed research work is the implementation of mechanisms for preserving the privacy of the individuals involved in the study.",,"We believe that the proposed research programme can have a significant societal impact. Indeed, depression is a major issue in the United Kingdom and has profound psychological, social, and economic implications for individuals, families and communities. The long-term impact will be in novel methods of intervention and support, based on the outputs of the proposed work. Indeed the analytical techniques investigated in &quot;Trajectories of Depression&quot; can be used as a basis for novel systems for providing real-time support to individuals when they really need it without a direct interaction with health officers. By doing so, individuals that are not collaborative because of their condition can also receive prompt help and attention in order to prevent a worsening of their condition and, in some cases, suicide attempts. Moreover, the techniques can also be used to concentrate resources on individuals that really need help at a certain point in time. To summarise, the project can really lead to the improvement of the well-being of thousands of individuals and it will improve the effectiveness and efficiency of delivery of Social Welfare services.

It is worth noting that Dr. Paul Patterson, Public Health Research Programme Manager at Youthspace, Birmingham and Solihull Mental Health NHS Foundation Trust is a member of the Advisory Committee and will help to steer the project in directions that will provide a direct benefit to the communities in the coming years. Dr. Patterson and the colleagues at the Stirling Suicidal Behaviour Research Laboratory will also help in establishing links to charities that are active in this area (such as Mind and The Samaritans).

Given the highly interdisciplinary nature of this work, the proposed research programme will benefit various communities in Computer Science, Health Sciences and Psychology. As far as Computer Science is concerned, we believe that the project will lead to novel insights and contributions in different areas including Ubiquitous Computing, Data Mining, Systems, and, potentially Security as discussed in the &quot;Academic Beneficiaries&quot; section. With respect to Health Sciences and Psychology, for the first time, researchers will have the possibility of accessing quantitative data to validate theories and models that have been evaluated only by means of qualitative information, such as interviews. This can really lead to a step change in studying depression, especially in terms of its evolution over time and its impact on the daily life of individuals. It will provide new ways for providing direct help to them, for example through Digital Behaviour Change Interventions through mobile phones.

We are also planning an outreach programme for disseminating the finding of the project among young people. Activities in schools might also be an occasion for discussing issues related to depression and suicides in collaborations with charities working on these themes. The project will benefit from the fact that the School of Computer Science at the University of Birmingham is heavily involved in the Computing at Schools initiative. We also plan an exhibit at the Thinkthank Science Museum in Birmingham. We plan to engage with students in our Undergraduate and Master programmes by offering for example guest lectures on topics related to the project, discussing the use of mobile technology and large-scale data mining.

Finally, we believe that the outputs of this project can potentially be translated into commercial products in the future. We plan to work closely with NEC, our industrial project partner, in order to explore uses of the findings of &quot;Trajectories of Depression&quot;. The company has also expressed a strong interest in the computational aspects of the project, since it provides an interesting application scenario for investigating issues related to large-scale real-time processing and mining of personal data, in particular in the healthcare sector."
21,02F15393-CA17-4236-940A-3883C6B2ADC7,Digital Economy Doctoral Training Network,"In 2009 seven Centres for Doctoral Training (CDTs) were launched under the Digital Economy (DE) programme supported by Research Councils UK. Since the award of these centres there have been a number of collaborative activities between Centres, as well as a series of meetings for Centre Directors and Managers, that have enabled sharing of best practice, consolidation of resources and contributed to the excellence and impact of the work done within the CDTs in Digital Economy.

The proposed network will formalise collaborative activities to be conducted by current CDTs and those that will be funded under the DE priority area within the 2013 CDT call. In addition, the establishment of this network offers the opportunity to open some of the collaborative initiatives to those involved in PhD study and supervision from across the DE programme, encompassing work done in DE hubs and individual DE-related projects. 

The Network will provide resource to support rapid response to this changing landscape, and enable DE Postgraduate Research (PGR) students to work with each other and engage with external activities. The Network will consolidate and expand the network of DE PGR students, ensuring that a collegiate and engaged group of post docs lead DE activities in industry and academia in the future. CDTs remain a relatively new concept within UK postgraduate education, and conducting research in the multidisciplinary space of digital economy presents particular novel challenges that particularly benefit from collaboration between colleagues and students. 

Activities within the network will be coordinated under five themes: 1. Sharing Best Practice; 2. Large Scale Shared Events; 3. Sustainable Impact; 4. New Collaborative Initiatives; 5. Community Outreach and Dissemination. The Network will enable activities that would not have been possible to run by individual Centres for Doctoral Training or Hubs alone, and which take particular advantage of the links between Digital Economy related activities at different institutions. Events will comprise both activities that have already been identified as of value to DE doctoral students, such as the summer school, hackfests or Digital Economy Young Entrepreneurs Scheme, and those which are proposed by members of the DE research community and supported by the network. Funding will support a Network Administrator to help run and plan events and manage bids to the network.",,"Commercial private sector beneficiaries will include many businesses who will engage with PhD students during their research and employ DE PhD students once they have completed their PhDs. Specific mechanisms for engaging the commercial private sector include participation in the Digital Economy Young Entrepreneurs Schemes, sponsorship of individual events, speaking at events such as the DE summer schools and providing case examples for events such as the proposed Hackfests. 

The wider public will benefit as they engage with DE research through public engagement activities such as Digital Shoreditch or individually led events by small groups of PhD students. It is hoped that the events will engage the public in a variety of ways including dedicated outreach face to face events and production of high quality digital interactive material that will enable the public to engage with and increase their understanding of DE research. It is also hoped that events will be in a range of national (and possibly international) locations to disseminate research and that they will access a range of user groups, including school children.

Public sector beneficiaries will primarily consist of organisations with pursuit of new knowledge or research as a core activity (e.g. libraries), museums and galleries. and government bodies. These beneficiaries will be on a national and regional level. They will be able to easily access case studies of research and activities that will increase their abilities to engage with Centres for Doctoral Training and individual PhD researchers within the topics of Digital Economy. 

Overall, the aim of the network is to support PhD students within the DE research area and thus have a positive impact on their research quality. DE research in general has a core focus on the impact of technologies and systems on society, communities and industry, and thus the network has will contribute to work done in all of these areas."
22,912BB5EB-E68E-4BDE-88C7-16A438C8642C,Automated identification of optimal data-specific organelle clusters using freely available protein annotations,"Organelle proteomics is the systematic study of proteins and their assignments to sub-cellular compartments like organelles and macro-molecular complexes. It is a growing field in importance and popularity and over the last few years has gained a large amount of attention due to the role played by organelles in carrying out defined cellular processes. 

The most information-rich datasets are generated using high accuracy mass-spectrometry (MS), a technique that allows to identify and quantify the proteome content in complex biological samples. These datasets are high quality rich sources of data that have been mined using a variety of robust supervised statistical machine learning (ML) methods which have shown to yield valuable protein-organelle predictions (BBSRC: BB/G024618/1 and BB/H024247/1). These classification methods require as set of tenth expert-curated ground-truth marker proteins of know localisation and then match proteins of unknown localisation to organelles based on their MS data resemblance with those of marker proteins. However, there are still inherent issues that limit the optimal application of such contemporary classification methods: (1) the limited number of organelle markers and the reliance on time-consuming manual curation and (2) the limited number of organelle classes that systematically underestimates the sub-cellular diversity recorded in the datasets. 

In this proposal we aim to improve protein-organelle association via the application of different state-of-the-art methods to remove the need to ground-truth marker proteins to accurately assign proteins to a broader set of sub-cellular compartments. These unsupervised approaches will be looking specifically for patterns in the organelle proteomics data. We will also make use vast amounts of freely available protein annotation data like the Gene Ontology. These annotations, while prone to erroneous or misleading information, are available for tens of thousands proteins, describing all organelles identified so far. The amount of anntation data allows to overcome its uncertainty and investigate the sub-cellular environment at a much more meaningful diversity. In addition, the proposed methods will allow complete automation of the data analysis, thus permitting the treatment of more and bigger datasets.

The development of a framework that will support this annotation to guide the extrapolation and elucidation of patterns in the MS data will lead to the creation of optimal organelle proteomics datasets which will be deposited in a public access proteomics data repository through the main ProteomeXchange submission portal. These tools will be made freely available as open-source software for the use of the whole proteomics community.

The work proposed in this grant will be implemented by a multidisciplinary team bringing together expertise in state-of-the-art mass-spectrometry based proteomics approaches (KSL), database annotation (CD), contemporary pattern recognition methods (AP, TB, SBH and LG), computational bioinformatics and code development (LG) and applied mathematics (LMS). LG, KSL and LMS have worked together previously on organelle proteomics grants that resulted in the release of the current state-of-the-art toolkits for organelle proteomics data analysis.","Localisation of proteins inside cells is of paramount importance to study their function, refine our comprehension of sub-cellular process and organisation and understand the effect of perturbations at the sub-cellular level. Various dedicated experimental designs based on biochemical separation and quantitative mass-spectrometry have been described and refined over the years. The major break-through in terms of organelle proteomics data analysis consists in the application of state-of-the-art supervised machine learning (ML) techniques. These techniques utilise the quantitative profiles of the proteins and permit optimal classification of proteins of unknown localisation based on the definition of sub-cellular markers. These markers represent proteins of known localisation, identified through manual database mining, literature search and, most crucially, expert curation. Manual curation of a dataset containing thousands of proteins is however, although currently the most reliable solution, an extremely time consuming task. Furthermore, the quest for tens of highly reliable markers per organelle favours large, well characterised organelles at the expenses of smaller, less studied compartments, leading to systematic under-representation of the true organelle diversity in the experimental data. Our project proposes a major shift in the analysis of organelle proteomics data by abandoning supervised ML which requires rigid sets of highly reliable markers and instead employ unsupervised and semi-unsupervised approaches relying on the vast amount of freely available database annotations such as, for example, the Gene Ontology. These novel approaches will allow to (1) automate the analysis of our datasets without the expensive manual curation and (2) assess the true cellular diversity that underpin such experiments at a much finer scale. These techniques will be made accessible in the frame of the open source pRoloc framework for organelle proteomics data analysis.","Who will benefit from this research?

The developments proposed in this project will benefit the organelle proteomics community in particular as we will develop and share improved tools to analyse such data. The proteomics field as a whole will also benefit as our methods and software, although focused on organelle proteomics data, have a much wider scope and impact and can be applied in other fields. Computational biologists will also benefit from the open-source organelle proteomics analysis methods and the quality software that will be distributed to the wider community. Cell biologists, both academic and within the pharmaceutical sector will also immensely benefit as this proposal underpins the interface of modern omics technologies and more classical cell biological methodologies.

Our work is targeted to experimentalist users who will use our tools to analyse their data, as well as computational scientists and developers who want to re-use or adapt our methods and software infrastructure to new projects and topics.

How will they benefit from this research?

The toolkit will ensure unprecedented mining of proteomics data produced from widely-used gradient-based proteomics approaches, enabling unprecedented insight into the underlying sub-cellular diversity of these data. In addition, it will provide a benchmark upon which to add new data analysis methods as the technology and data annotation progresses. The sophisticated statistical machine learning methods will be made available for the statistical programming environment R and the Bioconductor project and will inter-operate with existing complementary software. Our novel methods will no doubt be applicable in other omics areas of research due to the inherit cross-disciplinary nature of computer science, mathematics and machine learning that underpins many areas of computational biology. Lastly, the project will contribute knowledge and scientific advancement in the form of the dissemination of data and improvement of the analyses of complex multivariate data to facilitate interpretation and understanding of relevant biological processes. Fully characterised organelle proteomics datasets will be deposited in publicly accessible databases (via the ProteomeXchange portal) upon publication of the peer-reviewed research outputs and the detailed analysis methodologies will be documented and distributed with software releases to facilitate application of our methods to new datasets and use cases. 

The research staff will benefit from the multi-disciplinary research environment and extend their national and international research network through on-going collaborations. In addition to the benefits of improved tools and data, the academic beneficiaries will also be invited to workshops that will be organised in the frame of the European FP7 project to promote our approaches.

What will be done to ensure that they have the opportunity to benefit from this research?

The algorithms and tools developed in this proposal will be implemented in the R statistical programming environment (www.r-project.org) and will be deposited to the Bioconductor suite of bioinformatics software. The algorithms will be implemented as independent modules that will be contributed to and compatible with current the pRoloc analysis framework (developed by LG and LMS in BBSRC: BB/H024247/1 and BB/G024618/1), to form a freely available open-source toolkit for the analysis of organelle proteomics data. It is envisaged that these manuscripts will be submitted to high impact journals with large general readership, such as Nature Methods and Nature Biotechnology. KSL, LG and CD are invited to give numerous talks at all the top proteomics and computational conferences world wide, thus they will endeavour to publicise the work described here at such events."
23,CBA2EF53-BDC6-40AA-B4A0-D59698606625,MACACO: Mobile context-Adaptive CAching for COntent-centric networking,"Finding new ways to manage the increased data usage and to improve the level of service required by the new wave of smartphones applications is an essential issue. The MACACO project proposes an innovative solution to this problem by focusing on data offloading mechanisms that take advantage of context and content information. Our intuition is that if it is possible to extract and forecast the behaviour of mobile network users in the three dimensional space of time, location and interest (i.e. 'what', 'when' and 'where' users are pulling data from the network), it is possible to derive efficient data offloading protocols. Such protocols would pre-fetch the identified data and cache them at the network edge at an earlier time, preferably when the mobile network is less congested, or offers better quality of service. Caching can be done directly at the mobile terminals, as well as at the edge nodes of the network (e.g., femtocells or wireless access points). 

Building on previous research efforts in the fields of social wireless networking, opportunistic communications and content networking, MACACO will address several issues in this space. The first one is to derive appropriate models for the correlation between user interests and their mobility. Lots of studies have characterised mobile nodes mobility based on real world data traces, but knowledge about the interactions with user interests in this context is still missing. To fill this gap, MACACO proposes to acquire real world data sets to model mobile node behaviour in the aforementioned three-dimensional space. The second issue addressed is the derivation of efficient data-offloading algorithms leveraging the large-scale data traces and corresponding models. Firstly, simple and efficient prediction algorithms will be derived to forecast the node's mobility and interests. Then, MACACO will provide data pre-fetching mechanisms that both improves the perceived quality of service of the mobile user and
noticeably offloads peak bandwidth demands at the cellular network. A proof of concept will be exhibited though a federated testbed located in France, Switzerland and in the UK.",,"The MACACO project is totally aligned with the expected types of impact of the second topic of the CHIST-ERA 2012 call. If successful, MACACO will foster new &quot;services enabling the emergence of innovative network technologies&quot; by providing the required context- and content-aware models and protocols to manage the increased data usage required by the new wave of smartphones applications. By doing so, MACACO aims to reinforce the European scientific excellence in the mobile service provision and helping European carriers to offload their cellular traffic.

Additionally, by detecting and modelling the correlations between user mobility and the traffic demand he/she generates, MACACO aims to strengthen the research field of mobile networks and human behaviour prediction and &quot;develop a deeper fundamental and comprehensive understanding of new enhanced communication network architectures&quot;. Moreover, by facilitating development of new context- and content-aware applications, the project expects to foster significant innovation for industry. Novel and improved services will also have a significant for final users, given the fundamental role played by mobile Internet nowadays. In fact, several companies (from small start-ups to corporations) can benefit from improved wireless broadband services and from innovative content delivery mechanisms.

As also required by the call goals, MACACO &quot;brings together researchers and research communities working on distinct network layers and on content and context extraction in the broader framework of a content- and context-adaptive communication networks&quot;. As stated before, the consortium was carefully constituted to gather partners that are pretty complementary and qualified to address the context-content correlation and related data offloading challenge. This constitutes one of the strengths of the project, which could not be conducted with the participation of only one or few of the involved partners. Thus, in addition to the technological impact, MACACO will have a significant impact in terms of competence building. The partners will combine research and experience in a wide set of areas to gain unique competence, which will be brought forward to other European partners through the dissemination and exploitation activities of the consortium."
24,B878C460-3291-4ED3-82C6-D053497EE323,Automated identification of optimal data-specific organelle clusters using freely available protein annotations,"Organelle proteomics is the systematic study of proteins and their assignments to sub-cellular compartments like organelles and macro-molecular complexes. It is a growing field in importance and popularity and over the last few years has gained a large amount of attention due to the role played by organelles in carrying out defined cellular processes. 

The most information-rich datasets are generated using high accuracy mass-spectrometry (MS), a technique that allows to identify and quantify the proteome content in complex biological samples. These datasets are high quality rich sources of data that have been mined using a variety of robust supervised statistical machine learning (ML) methods which have shown to yield valuable protein-organelle predictions (BBSRC: BB/G024618/1 and BB/H024247/1). These classification methods require as set of tenth expert-curated ground-truth marker proteins of know localisation and then match proteins of unknown localisation to organelles based on their MS data resemblance with those of marker proteins. However, there are still inherent issues that limit the optimal application of such contemporary classification methods: (1) the limited number of organelle markers and the reliance on time-consuming manual curation and (2) the limited number of organelle classes that systematically underestimates the sub-cellular diversity recorded in the datasets. 

In this proposal we aim to improve protein-organelle association via the application of different state-of-the-art methods to remove the need to ground-truth marker proteins to accurately assign proteins to a broader set of sub-cellular compartments. These unsupervised approaches will be looking specifically for patterns in the organelle proteomics data. We will also make use vast amounts of freely available protein annotation data like the Gene Ontology. These annotations, while prone to erroneous or misleading information, are available for tens of thousands proteins, describing all organelles identified so far. The amount of anntation data allows to overcome its uncertainty and investigate the sub-cellular environment at a much more meaningful diversity. In addition, the proposed methods will allow complete automation of the data analysis, thus permitting the treatment of more and bigger datasets.

The development of a framework that will support this annotation to guide the extrapolation and elucidation of patterns in the MS data will lead to the creation of optimal organelle proteomics datasets which will be deposited in a public access proteomics data repository through the main ProteomeXchange submission portal. These tools will be made freely available as open-source software for the use of the whole proteomics community.

The work proposed in this grant will be implemented by a multidisciplinary team bringing together expertise in state-of-the-art mass-spectrometry based proteomics approaches (KSL), database annotation (CD), contemporary pattern recognition methods (AP, TB, SBH and LG), computational bioinformatics and code development (LG) and applied mathematics (LMS). LG, KSL and LMS have worked together previously on organelle proteomics grants that resulted in the release of the current state-of-the-art toolkits for organelle proteomics data analysis.","Localisation of proteins inside cells is of paramount importance to study their function, refine our comprehension of sub-cellular process and organisation and understand the effect of perturbations at the sub-cellular level. Various dedicated experimental designs based on biochemical separation and quantitative mass-spectrometry have been described and refined over the years. The major break-through in terms of organelle proteomics data analysis consists in the application of state-of-the-art supervised machine learning (ML) techniques. These techniques utilise the quantitative profiles of the proteins and permit optimal classification of proteins of unknown localisation based on the definition of sub-cellular markers. These markers represent proteins of known localisation, identified through manual database mining, literature search and, most crucially, expert curation. Manual curation of a dataset containing thousands of proteins is however, although currently the most reliable solution, an extremely time consuming task. Furthermore, the quest for tens of highly reliable markers per organelle favours large, well characterised organelles at the expenses of smaller, less studied compartments, leading to systematic under-representation of the true organelle diversity in the experimental data. Our project proposes a major shift in the analysis of organelle proteomics data by abandoning supervised ML which requires rigid sets of highly reliable markers and instead employ unsupervised and semi-unsupervised approaches relying on the vast amount of freely available database annotations such as, for example, the Gene Ontology. These novel approaches will allow to (1) automate the analysis of our datasets without the expensive manual curation and (2) assess the true cellular diversity that underpin such experiments at a much finer scale. These techniques will be made accessible in the frame of the open source pRoloc framework for organelle proteomics data analysis.","Who will benefit from this research?

The developments proposed in this project will benefit the organelle proteomics community in particular as we will develop and share improved tools to analyse such data. The proteomics field as a whole will also benefit as our methods and software, although focused on organelle proteomics data, have a much wider scope and impact and can be applied in other fields. Computational biologists will also benefit from the open-source organelle proteomics analysis methods and the quality software that will be distributed to the wider community. Cell biologists, both academic and within the pharmaceutical sector will also immensely benefit as this proposal underpins the interface of modern omics technologies and more classical cell biological methodologies.

Our work is targeted to experimentalist users who will use our tools to analyse their data, as well as computational scientists and developers who want to re-use or adapt our methods and software infrastructure to new projects and topics.

How will they benefit from this research?

The toolkit will ensure unprecedented mining of proteomics data produced from widely-used gradient-based proteomics approaches, enabling unprecedented insight into the underlying sub-cellular diversity of these data. In addition, it will provide a benchmark upon which to add new data analysis methods as the technology and data annotation progresses. The sophisticated statistical machine learning methods will be made available for the statistical programming environment R and the Bioconductor project and will inter-operate with existing complementary software. Our novel methods will no doubt be applicable in other omics areas of research due to the inherit cross-disciplinary nature of computer science, mathematics and machine learning that underpins many areas of computational biology. Lastly, the project will contribute knowledge and scientific advancement in the form of the dissemination of data and improvement of the analyses of complex multivariate data to facilitate interpretation and understanding of relevant biological processes. Fully characterised organelle proteomics datasets will be deposited in publicly accessible databases (via the ProteomeXchange portal) upon publication of the peer-reviewed research outputs and the detailed analysis methodologies will be documented and distributed with software releases to facilitate application of our methods to new datasets and use cases. 

The research staff will benefit from the multi-disciplinary research environment and extend their national and international research network through on-going collaborations. In addition to the benefits of improved tools and data, the academic beneficiaries will also be invited to workshops that will be organised in the frame of the European FP7 project to promote our approaches.

What will be done to ensure that they have the opportunity to benefit from this research?

The algorithms and tools developed in this proposal will be implemented in the R statistical programming environment (www.r-project.org) and will be deposited to the Bioconductor suite of bioinformatics software. The algorithms will be implemented as independent modules that will be contributed to and compatible with current the pRoloc analysis framework (developed by LG and LMS in BBSRC: BB/H024247/1 and BB/G024618/1), to form a freely available open-source toolkit for the analysis of organelle proteomics data. It is envisaged that these manuscripts will be submitted to high impact journals with large general readership, such as Nature Methods and Nature Biotechnology. KSL, LG and CD are invited to give numerous talks at all the top proteomics and computational conferences world wide, thus they will endeavour to publicise the work described here at such events."
0,A8259FE5-95C4-4D2D-9008-22FB4D019FAA,Structured machine listening for soundscapes with multiple birds,"In this Early Career fellowship I will establish a world-leading capability in automatic inference about songbird communications via novel &quot;machine listening&quot; methods, working collaboratively with experts in machine listening but also experts in bird behaviour and communication. Automatic analysis has already shown benefit to researchers in efficiently characterising recorded bird sounds, but there are still many limitations in applicability, such as when many birds sing together. The techniques developed will specifically be designed to handle noisy multi-source audio recordings, and to infer not just the presence of birds but the structure of the signals and the interactions between them. Such methods will be a leap beyond the current state of the art in bioacoustics, allowing researchers to study not just sounds recorded in the lab under controlled conditions, but also field recordings and archive recordings found in public audio archives.

I will develop my techniques through specific application case studies. First through collaboration with David Clayton, an international expert on zebra finch behaviour and genetics, who recently moved his lab to my proposed host institution. The zebra finch is an important &quot;model organism&quot; in biology, because its genome is fully sequenced and it is a useful bird for probing aspects of songbird vocal development. I will collaborate with the Clayton lab to develop methods for automatically inferring the social interactions implicit in audio recordings of zebra finch colonies. Second, I will conduct international research visits to collaborate with other research groups who analyse bird sounds and bird social interactions. Third, I will study the case of automatically detecting bird activity in arbitrary sound archives, such as the soundscape recordings held by the British Library Sound Archive.

Importantly, not only will I apply modern signal processing and machine learning techniques, but I will also develop new techniques inspired by this application area. This fellowship is not about contributing from one field to another, but about building up UK research strength in this cross-discplinary research topic. In order to make the most of this possibility, I will host research workshops and an open data contest to serve as focal points for research attention, and I will also conduct a public engagement initiative to engage the widest possible enthusiasm for this exciting field of possibility.

This fellowship directly aligns with the &quot;Working Together&quot; priority, which is EPSRC's current overriding priority for ICT fellowships.",,"The prime beneficiaries outside my immediate field will be in research fields benefitting from the structured analysis of animal sounds and interactions. For example the improved techniques in zebra finch analysis will complement ongoing research into songbird genetics and individual differences, or research into conversational interactions in linguistics: the availability of more structured naturalistic data about animal communication could provide stronger empirical foundation to considerations of the evolution of communication systems. (This impact overlaps to some extent with that described in the Academic Beneficiaries section.)

The availability of these sound analysis techniques is also of interest to wildlife monitoring organisations such as the British Trust for Ornithology (BTO). They largely use manual surveying by volunteers and professionals to quantify the distributions of species: however, if high-quality automatic analysis were available their work could be made more efficient. Current academic and commercial software (examples include Raven, XBAT, Seewave, Praat, Sound Analysis Pro) allow users to inspect and detect bird sounds but are unable to analyse communication networks, nor can they use models of communication interactions to ensure high-quality detection. Analysing not just the presence of bird song and calls, but the networks of interaction between them, could be used as an indicator of the population health, reflecting issues such as habitat fragmentation which can impact the viability of a bird population. Downstream, detailed analysis of animal sound can thus form a strong evidence base for ecological policy decisions.

The application to audio archives shows another direct route to impact. Large audio collections such as those in the British Library Sound Archive are highly valuable to society, yet a lot of their value remains locked away because there is very little metadata associated that would facilitate different types of query. This research will directly enable the unlocking of some of this value, helping people to discover the presence of birds in large audio archives which may not be annotated for their bird sounds, indeed may have been collected for entirely different reasons.

The fellowship will also have an impact on the public understanding of bird sounds, bird social interactions, and signal processing and machine learning. These will be explicitly encouraged through the public engagement activities: through engaging talks, articles and exhibits I will aim, not to place the technology between the public and the birds, but to enchant the public with both the wonders of technology and the wonders of bird vocal communication."
1,B78473C2-84D7-4254-B536-9DD3EF116B46,MyLifeHub: An interoperability hub for aggregating lifelogging data from heterogeneous sensors and its applications in ophthalmic care,"Visual impairment is one of the most feared forms of medical disability, which imposes a great social and economic burden on our society. In the UK, the number of visually impaired people is almost 2 million, with total annual costs estimated over &pound;13,000 million. Notably, age-related increase of visual impairment has been well-documented and is set to be on constant rise owing to the growing ageing population nowadays. Visual impairment has significant impact on quality of life (QoL), as reduced visual acuity seriously affect patients' daily and social activities, with substantial increase of risk of mortality, fracture and falls, depression and other emotional distress. 

A variety of responsive instruments for quantifying functional impairment related to vision have been developed. Also, there are well-recognised generic instruments for the assessment of QoL in general health terms. These instruments contain questionnaires referring to a broad range of physical, social and psychological aspects, offering the basis for establishing the QoL profiles of the individuals under concern.
Any changes in the QoL profile, for instance, the increase or drop of the level of physical and social activities before and after an eye surgery, can be used as important indicators for the outcome of the treatment. 

However, QoL assessment through written questionnaires has several significant drawbacks. Many answers often rely on participants' memory over a long period of time; people may read differently into the questions with their own interpretations; often there is no way of validating the truthfulness of many responses. These limitations raise serious questions on the reliability and validity of the measurements. 

Remarkably, the rapid advance of the Internet of Things (IoT) technology grants us opportunities to build QoL profiles of individuals with increased reliability and validity by monitoring their lifelogging data captured by a variety of IoT assets (namely objects, sensors, mobile apps, web-objects, etc.) with constant connectivity and interaction in a pervasive network. MyLifeHub is such an attempt with focus on the interoperability of the IoT assets, aiming at a common, interoperable and internet-based environment for long-term lifestyle information for individuals. The system will keep users well informed about their daily activities, diet, sleep, mood, blood pressure, pulse, etc., enhancing self-awareness in health and encouraging positive attitudes towards lifestyles. Data sharing among different users will also be enabled to allow for experience exchange and to build healthcare social-networks among users. 

Especially, MyLifeHub will feature new techniques enabling simultaneously and long-term quantifying the functional impairment related to vision underpinned with smart glasses (e.g. Google-Glass), which provide wearable sensors to connect with the environment through RFID, infrared, Bluetooth or QR code, allowing for a constant monitoring of the behaviours of people's vision. 

MyLifeHub will be utilized as a platform to assess the impact of visual impairment on the QoL of ophthalmic patients both in general health terms and in vision specific terms. The research will be conducted &quot;in the wild&quot; through direct exposure to potential beneficiaries. Our clinical collaborator, Moorfields Eye Hospital (MEH), is the largest eye hospital in the UK and earns a reputation worldwide. The outcome of MyLifeHub will be evaluated by the end users (namely MEH and its patients), allowing for the assessment and forecast of the transformational impact of the IoT technologies in real world terms.",,"The potential beneficiaries of MyLifeHub are in 4 categories, including those from (1) ophthalmic care, (2) general healthcare, (3) the academic community for IoT research and (4) industry.

Visual impairment imposes a great social and economic burden on individuals and society. In the UK, it is estimated that almost 2 million people have sigh loss, among which around 360,000 people have severe and irreversible conditions. This number is set to increase dramatically, reaching 4 million by 2050. Visual impairment affects people at all ages especially the elderlies, causing significant problems in the increase of mortality, falls, and emotional distress, and many social problems unemployed and isolation. The cost of ophthalmic care is very high in the UK, for example, expenditure directly linked to eye health was over billions from 2009 to 2010. 

MyLifeHub is an attempt to build quality of life(QoL) profiles of individuals with increased reliability and validity by constantly monitoring their lifelogging data using the latest IoT technologies. The long-term collection of lifelogging data from a spectrum of IoT assets will offer a completed set of evidence for reliable QoL assessment in ophthalmic care.

From a more general perspective beyond ophthalmic care, MyLifeHub also contributes to general healthcare by bringing fundamental transform to healthcare: 
1) A long-term lifestyle record of individuals offer a significant input for personalised medicine, allowing individualised risk prediction and treatment that leads to enhanced reliability and timeliness of medical decisions. 
2) Citizens/Patients receive great benefit by active involvement in self-healthcare. MyLifeHub keeps citizens informed about their daily activities and health, encouraging positive attitudes towards healthy lifestyles. This is supported by a rapid growing of public interest in self-healthcare. 
3) Healthcare and associated social welfare costs are increasing exponentially due to growing ageing population and prolonged life expectancy, and they will soon become unsustainable unless we change the way in which people are supported. In many cases, there is a need to shift medical care from institutions to the home environment. 

From the academic perspective, IoT research under new settings equipped with newly developed ICT gadgets from a very wide variety of background, such as lifelogging devices, smart glasses and mHealth apps, etc. brings new opportunities in addressing new challenges within the area of IoT and its technology enablers (e.g. scalable data mining, visualization and security). In addition, MyLifeHub also supports to build an infrastructure to maximise the yield of biomedical research expenditure by supplying healthcare providers with more comprehensive patient information.

MyLifeHub also comes with great commercial potentials. There is a growing public interest in self monitoring for transformative healthcare, evidenced by strong market indications: e.g. consumers used to self-monitor health conditions will account for more than 80% of wireless devices in 2016; more than 97,000 mobile apps available related to health, with the top 10 mobile health apps generating up to four million free and 300,000 paid downloads per day."
2,8B3690A5-807B-481D-8ABD-A8E8E9836A65,Quantified-self for obesity: Physical activity behaviour sensing to improve health outcomes from surgery for severe obesity,"It is widely assumed that physical activity affects weight loss outcomes for severely obese patients, but there is a scarcity of robust research on this subject. We propose to use smartphone sensors and advanced data mining techniques to conduct detailed investigations addressing this important question. The research participants will be obese people having bariatric (or weight-loss) surgery (e.g. gastric bypass), but our results will also benefit other people with weight problems and patients with other conditions where exercise is helpful.

In England just over a quarter of adults were classified as obese in 2010. This group is more likely to suffer from a range of illnesses (e.g. type-2 diabetes) and to have a lower life expectancy. Surgery is recommended for those with severe and complex obesity that has not responded to other therapies, and is highly cost effective in achieving weight loss, overcoming associated illnesses and promoting longer term health. However long-term success is far from guaranteed, with up to 15% of gastric bypass and 50% of gastric band procedures being ultimately unsuccessful. 

Obese people often lead very sedentary lives, both before and after surgery. Research has shown that even small long-term increases in routine physical activity could be very significant for weight loss, so we are very interested in how we can motivate people to do that little bit more in their daily lives.

Patients attending the Imperial Weight Centre (IWC) are reminded to exercise during their hospital visits, but what they ideally need is a personal trainer to encourage them every day. Recognising this, patients have asked us if there are any devices that can help, and so we began our research into how sensors and mobile phones can seamlessly track activity and deliver timely, personalised feedback and encouragement.

IWC Patients have tried wristbands such as the Nike Fuelband - but despite initial enthusiasm the novelty soon wears off. These devices do not provide sufficiently detailed or meaningful information. Smartphone apps such as MyFitnessPal are also popular, but soon become tedious since users must log everything they eat or do: many trying them did not persevere for more than a few days. 

With the advent of new apps it is now possible to track physical activity effortlessly just by carrying around one's smartphone, using its inbuilt sensors. Data is processed in the &quot;internet cloud&quot; where it can be analysed by new software we are developing. These apps also produce a complete daily &quot;storyline&quot; detailing a user's travels, and the amount and type of activity at each location. Our pilot users have been delighted to be able to see their physical activity progress and said that they felt motivated to challenge themselves to do more each day.

This project sets out to objectively monitor physical activites on a daily basis so that we can follow almost 1000 patients over protracted periods of time and throughout their weight loss journey. We will use advanced data mining tools to understand individual differences and responses to surgery in terms of physical activity and how these relate to weight loss and weight maintenance over time. We shall use our analysis and understanding of behaviour change methods to devise ways to encourage users to do better and thereby achieve longer and healthier lives. For example, individualised prompts could incorporate weather and location information to suggest suitable walks on fine days, support positive goal setting or inspire competition with other users. This project will pave the way for further behavioural studies, for example using emerging wearable-sensor technologies and should offer long-term benefits for obese people and others with many different types of health problems, where exercise helps - lifestyle recommendations and advice can be produced that will be more personalised and useful for individuals looking to optimise their health.",,"Patients, including users and previous users of NHS bariatric services, and their clinicians will benefit from this research in two ways:

1) Findings from the study will result in an improved understanding of individual differences in physical activity, motivational factors and weight management behaviours in severely obese people so that lifestyle recommendations and advice can be produced that will be more personalised and useful for individuals looking to manage their weight. 

2) They will gain knowledge of, and access to, specific tools to help them increase their physical activity, assuming that the study produces evidence of their efficacy.

Being able to monitor the effects of physical activity on an individual's health is important for many other patient groups. There are indications of increasing incidence of cardio-vascular disease and lifestyle-related cancers in the general population, as well as alarmingly high estimated prevalence of diabetes and obesity. Exercise also benefits people with arthritis or backpain and is increasingly recognised as highly effective in supporting mental health.

More broadly we expect to be able to demonstrate the benefits of using smartphone sensor technologies to seamlessly collect healthcare-related information from people with the minimum of effort for both user and researcher. This should benefit many patient groups and researchers nationally and internationally. 

In the UK general population over half fail to meet the recommended levels for moderate intensity physical activity (30 minutes or more on 5 days per week). And since these are self-reported measures the numbers are likely to be far higher (Health Survey for England-2008). The estimated direct cost of physical inactivity to the NHS across the UK based on 5 conditions (coronary heart disease, stroke, diabetes, colorectal cancer and breast cancer) is &pound;1.06 billion (Allender et al, 2007) while it has been estimated that the wider cost is &pound;6.5 billion per year in England alone through sickness absence and premature deaths in the workforce (Ossa D and Hutton J, 2002). Increasing physical activity also supports other social agendas; e.g. journeys made on foot reduce carbon emissions and help create safer, higher quality streets. By conducting this study and publicising its results we should influence many more individuals to make use of free physical activity tracking software to help them become more active and lead healthier lives.

In our previous research with a cross-section of 500 London commuters with smartphones, many of whom were overweight or obese, (SBRI Competition: &quot;Changing behaviour to reduce the impact of obesity ....&quot;) we showed that the majority would benefit from and would be willing to use an App like &quot;Moves&quot;: 42% were overweight; Most exercise for less than 150 mins/week; Some took brisk walks, but few for more than 30 mins at a time; Most did not walk near their workplace; Approx. half of the sample said they would use an App like Moves. 100 users were followed up with a 2nd survey: Of these over 60% were motivated to walk more and over 70% were willing to set themselves goals using an App like Moves.

Smartphone technology may improve targeting to harder-to-reach groups, including men and young people. These are two groups that do not necessarily access conventional healthcare services but are particularly likely to be 'early adopters' of novel smartphone technologies.

The Principal Investigator for this project, Lord Darzi, is the Executive Chair of WISH, the World Innovation Summit for Health, with major themes in obesity and empowering patients. Since obesity is a truly global pandemic and physical activity is a key influencer in obesity and other global non communicable conditions we anticipate that the innovations in this project can help to influence other healthcare systems and their policymakers as they investigate low cost technologies for patient self monitoring and care."
3,4CAAA117-7708-4265-A7DE-175CA468F111,Detailed and Deep Image Understanding,"Computer vision, the technology that allows machines to understand the content of image automatically, is fuelling a revolution in digital image processing. For example, it is now possible to use computers to search billions of images and millions of hours of video in the Internet for a particular content (Google Googles), interpret gestures and body motions to play games (Microsoft Kinect), automatically focus cameras on faces, or build smart cameras that can monitor hazardous industrial equipment on a 24h basis.

If not for their scale, these tasks would appear trivial to a human. However, vision is computationally exceptionally challenging, to the extent that more than half of our brain is dedicated to this function alone. Since this complexity cannot be met by hand-crafting software, vision architectures are nowadays learned automatically from million of example images, leveraging advanced machine learning and optimisation technologies. Despite recent terrific successes, however, machine vision still pales in comparison to vision in humans. Probably the most disappointing restriction is that these systems can address a single task at a time, such as deciding whether a particular image contains, say, person. Recognising a different concept, for example a dog, or addressing a different task, for example outlining rather than recognising the person, requires learning a new system from scratch, wasting time and effort.

My research idea is to transform existing architectures into repositories of 'visual knowledge' that can be reused and extended incrementally to address multiple tasks and domains, greatly improving the efficiency, scalability, and flexibility of the technology. The key scientific challenge is to understand how visual information is encoded in state-of-the-art vision systems. In fact, since these are learned automatically rather than being hand-crafted, it is currently unclear what information is captured by them and how it is represented. An in-depth investigation will explicate this formally and quantitatively and will be the basis to share and integrate visual knowledge between a growing number of concepts and tasks, including ones not addressed by the initial design of the system. At the same time, identifying fine-grained information will allow a system to obtain a more detailed, comprehensive, and meaningful understanding of the content of images.

The potential for impact is huge as the proposed research will enhance core computer vision technology that already powers countless applications. For example, computers will be able to search images by matching more detailed queries expressed using a far richer visual vocabulary; software will be extensible to new domains and tasks with minimal effort; and computer vision systems will be able to explain in explicit, intuitive terms how they understand images.

The research outcomes will be evaluated in the most rigorous manner on international benchmark data and protocols. Research results will be made available to a widespread technical audience by distributing open source software implementing the new technology. The project is also likely to have a strong academic impact, consolidating the leadership of the UK in computer vision, a strategic competitive area in the digital economy.",,"Automatic image understanding has a tremendous impact on a wide spectrum of cutting-edge applications. Recent computer vision and machine learning breakthroughs have made it possible for companies such as Google and the BBC to offer tools that can search and organise very large media collections automatically, in some cases as large as the whole Internet (Google Googles); at the same time, this technology can be used to index personal photo collections (Google plus). Vision technologies are now deployed in public spaces in intelligent surveillance cameras that can automatically match thousands of people to a particular description. The Microsoft Kinect sensor, that was largely developed in the UK, allows players to interact with video games using gestures and body motion rather than a controller. Major enterprises that have historically relied manual inspection to verify the safety of their plants are now looking at computer vision as a way to make visual inspection work continuously, on a large scale, and on a quantitative foundation. These are just a few examples; by constructing machines that can interpret the content of images automatically, we can open major avenues to innovation in countless application domains and create new business opportunities.

The key benefit of the proposed research is the creation of a new generation of computer vision systems that will more powerful and flexible, capable of efficient adaptation to an ever expanding array of problems and application domains. These systems will also be capable of extracting more refined information from images; for example, where current technology may be able to detect a person, a vehicle, or some other object in an image, the proposed advances will make it be possible to extract detailed information about these objects as well, such as particular facial or body features, or whether the wheels of the car are steered in a particular direction. The impact of these advances is broad: for example, in a content search application it will be possible to formulate more refined queries to pinpoint exactly the desired content; or it will be possible to index new types of searchable content with minimal modification to an existing system. In an industrial inspection application faults may not only be detected, but also diagnosed and explained to an engineer by highlighting salient features in an image.

Example areas that can ultimately benefit from the proposed advances are exemplified by current collaborations of the PI with industrial partners such as XRCE/Xerox, BBC, and BP. The PI is transferring existing state-of-the-art vision technology to these partners to: recognise detailed properties of objects such as the breed of an animal (XRCE), search large-scale video databases for specific contents (BBC), and industrial monitoring (BP). The proposed research will enable substantial improvements in the underlying techniques, supporting ultimately finer-grained characterisation of visual objects, an understanding of more typologies of visual contents, and the ability to mark image features relevant to a particular visual assessment. The PI will pursue follow-up collaborations with these partners to create practical applications of the technology developed in this proposal to the EPSRC.

A successful outcome of this research will have not just a national but also an international impact. As suggested above, this research is likely to be of interest to international business and research centres such as XRCE/Xerox. At the same time, it is likely to attract the interest of the international academic community."
4,29026DF1-96B4-4474-8A7C-BFF49F1423E7,MyLifeHub: An interoperability hub for aggregating lifelogging data from heterogeneous sensors and its applications in ophthalmic care,"Visual impairment is one of the most feared forms of medical disability, which imposes a great social and economic burden on our society. In the UK, the number of visually impaired people is almost 2 million, with total annual costs estimated over &pound;13,000 million. Notably, age-related increase of visual impairment has been well-documented and is set to be on constant rise owing to the growing ageing population nowadays. Visual impairment has significant impact on quality of life (QoL), as reduced visual acuity seriously affect patients' daily and social activities, with substantial increase of risk of mortality, fracture and falls, depression and other emotional distress. 

A variety of responsive instruments for quantifying functional impairment related to vision have been developed. Also, there are well-recognised generic instruments for the assessment of QoL in general health terms. These instruments contain questionnaires referring to a broad range of physical, social and psychological aspects, offering the basis for establishing the QoL profiles of the individuals under concern.
Any changes in the QoL profile, for instance, the increase or drop of the level of physical and social activities before and after an eye surgery, can be used as important indicators for the outcome of the treatment. 

However, QoL assessment through written questionnaires has several significant drawbacks. Many answers often rely on participants' memory over a long period of time; people may read differently into the questions with their own interpretations; often there is no way of validating the truthfulness of many responses. These limitations raise serious questions on the reliability and validity of the measurements. 

Remarkably, the rapid advance of the Internet of Things (IoT) technology grants us opportunities to build QoL profiles of individuals with increased reliability and validity by monitoring their lifelogging data captured by a variety of IoT assets (namely objects, sensors, mobile apps, web-objects, etc.) with constant connectivity and interaction in a pervasive network. MyLifeHub is such an attempt with focus on the interoperability of the IoT assets, aiming at a common, interoperable and internet-based environment for long-term lifestyle information for individuals. The system will keep users well informed about their daily activities, diet, sleep, mood, blood pressure, pulse, etc., enhancing self-awareness in health and encouraging positive attitudes towards lifestyles. Data sharing among different users will also be enabled to allow for experience exchange and to build healthcare social-networks among users. 

Especially, MyLifeHub will feature new techniques enabling simultaneously and long-term quantifying the functional impairment related to vision underpinned with smart glasses (e.g. Google-Glass), which provide wearable sensors to connect with the environment through RFID, infrared, Bluetooth or QR code, allowing for a constant monitoring of the behaviours of people's vision. 

MyLifeHub will be utilized as a platform to assess the impact of visual impairment on the QoL of ophthalmic patients both in general health terms and in vision specific terms. The research will be conducted &quot;in the wild&quot; through direct exposure to potential beneficiaries. Our clinical collaborator, Moorfields Eye Hospital (MEH), is the largest eye hospital in the UK and earns a reputation worldwide. The outcome of MyLifeHub will be evaluated by the end users (namely MEH and its patients), allowing for the assessment and forecast of the transformational impact of the IoT technologies in real world terms.",,
5,08D3C73F-598B-4976-9E1F-48D172A29683,EPSRC-Royal Society fellowship engagement (2013): Combining Constraints and Verification,Please refer to attached Royal Society application,,Please refer to attached Royal Society application
6,82F836D5-3182-4F4E-9D72-63EAB68A9910,"Assuming Identities Online: description, development and ethical implications.","Preventive policing of serious crime sometimes involves deception and disguise. A case in point is the prevention of abuse arising from paedophile grooming and peer to peer networks where abuse images of children are discussed and exchanged. The preventive techniques by police investigators include assuming identities of existing community members, and of children, so that interventions and arrests can be made. Often, there are tight time constraints associated with this process - investigators have only a small window in which to learn and assume the identity in question before arousing suspicion in their target(s). The training that undercover online investigators currently receive, although broadly informed by linguistic theory, is in need of development. Furthermore, the time constraints mean that a semi-automated system to assist in identity assumption would represent a crucial contribution to the investigative toolkit.

Research taking a computational approach to the analysis of online communications has thus far focussed overwhelmingly on the structural elements of Computer Mediated Discourse (CMD), such as typography, orthography and other low level features, with little to no attention being paid to the socially situated discourses in which these features are embedded. The Centre for Forensic Linguistics (CFL) - a research centre within Aston University combining leading-edge research and investigative forensic practice - and Lexegesys - a consultancy and technology company specialising in developing and implementing data analysis solutions, recently collaborated on a project that was successful in automating the process of identification and extraction of low-level features for the purposes of attributing authorship of unknown texts within the context of Twitter. Yet CMD has widely been recognized to operate on a number of linguistic levels, such as those of meaning, of interaction, and of social practice. Outside of the computational linguistic field, the characteristic features of CMD are understood as resources that users draw on in the construction of identities in particular contexts, and CMD constitutes social practice in and of itself rather than simply being shaped by social variables. 

Taking an inductive approach, which is to say that the phenomena of interest, rather than a specific theoretical paradigm, are primary, this research aims to bridge the gap between complex theories of the discursive construction of online identities on the one hand, and computational approaches to analysing online communications on the other. A small scale study CFL and Lexegesys are currently engaged in is addressing the challenges of automation at the pragmatic and interactional levels, working towards the semi-automated identification of phenomena such as indirect speech acts and topic management. 

The work is extremely practical and is informed by real-world police investigations. A partner in the project, the West Midlands Police, Technical Intelligence Development Unit is crucially committed to providing data and operational insights. In addition to empirical applied linguistics, the project conducts proof-of-concept work for software that will assist in an ethical use of assumed identities in policing. Furthermore, it will involve an assessment of the ethical and policy implications for policing and security of complexity in online identity performance. 

This proposal was previously submitted to the AHRC, and is resubmitted here on their advice.",,"A major intended impact of the research is improved policing and a better protection of children online. The most significant beneficiaries of this research will be the children. 

More direct beneficiaries include police officers engaged in online undercover work and specifically their work targeting paedophilic picture exchanges and online grooming activity. There are two separate aspects of impact within this task. The first is the development of policy concerning, and ethical conduct of investigations into, on-line paeodophile grooming, and the second is improved performance in the task, i.e. successful prosecutions of paedophile groomers. Different groups of police will benefit from the research project taken as a whole; locally, it will be the West Midlands Police (WMP) unit engaged in the project, but as this unit has representatives on the relevant committee of the Association of Chief Police Officers (ACPO) and holds the national training contract for online undercover operations, and as this training is increasingly available to international police, the impact will be rapidly disseminated nationally and potentially internationally. 

In addition to this route for dissemination of impact the project outputs include the provision of individual training days and a training manual for the wider policing and security community and we would hope to attract to this training those with interests in investigation of high tech crimes, fraud, gangs and counter terrorism.

The research will also be of benefit to organisations such as the Child Exploitation and Online Protection Centre (CEOP) in its efforts to build up intelligence that informs their operational deployments and steers their educational programmes. Charitable organisations such as Barnardo's Cut Them Free campaign, which is working for strong and targeted policing to prevent sexual abuse and prosecute it when it occurs, will also benefit from the targeted preventative methods that will form the outcome of this research. The project also feeds into the work of the Virtual Global Taskforce, which seeks to build an effective, international partnership of law enforcement agencies, non government organisations and industry to help protect children from online abuse."
7,1D0CE9E2-74FB-4AAA-BD6B-F96B283FB61A,Connected High Street,"Linking data about the wide range of goods that are stored in the databases of high street shops offers positive outcomes for consumers, the wellbeing of salesperson as part of the UK's rapidly expanding service sector, health of the high street and the UK economy. Despite the tremendous connections that are made through shopping experiences on the internet, the high street remains locked in a 19th Century paradigm in which the cash register is the only interface between material goods, the customer and the stock inventory (database) (Carrier 1995). Shoppers work hard to make their own connections between disconnected shops, salespeople are reduced to scanning barcodes or fixing self-checkout machines, and data between all parties remains in silos. The vision of a 'frictionless shopping experience' (Brynjofsson and Smith 2000) that part an Internet of Things promise (http://www.youtube.com/watch?v=_xNhL39uD7I), in which technology can reduce the bottleneck of the cash register is far from being realised, instead shoppers have turned to 'showrooming' to make their lives easier (Campbell 2013).

This timely project explores the potential for reconfiguring the traditional organisation of customer, salesperson, cash register, tangible things and database, allowing shops 'stacks' of both immaterial and material processes to share data that will improve social and economic conditions. That high streets are in trouble is documented by the Portas Review (2011), which describes the impact that internet shopping, out of town shopping centres and the economic downturn have had upon on these spaces of social, economic and environmental exchange (see Miller 1998). One ramification is decreased employment opportunities for local young people. This year the Grimsey Review (2013) offered a deeper 'digital' critique of the state of the High Street, embracing a Digitally Economic perspective &quot;To strengthen the high street, we need to increase the number of mutual connections between the nodes or network participants (retail, services, local government, job centres and all others). The more mutual connections, the more adaptive the high street network becomes in response to changes in the success of individuals shops and services.&quot; (Grimsey 2013:17). 

Our project builds on research and innovation that NCR Consumer Experience (Cx) have initiated in response to the advent of ubiquitous computing in which every shopper carries a cash register in the form of a smart phone. Equipped with a suite of applications, shoppers are able to make purchases, compare prices, track goods, acquire vouchers and group together with friends or strangers to get better deals, all contributing to the consumers range of tactics to make the most from the high street. This project aims to use this sophisticated user knowledge to inform new models for interaction with physical artefacts and their connected data, to improve the high street experience and recover values and relationships that are core to shopping.

The research partnership is completed through the investigators expertise and experience in handling the remaining vital component in the context of the connected high street: things. The extraordinary number of products available in a typical high street at any one time is a material manifestation of the big databases and shop inventories that are connected to each thing. Making visible the scale of the goods in the high street to the shopper, through patterns, correlations and recommendations is a critical step in developing a more connected high street. Through a better understanding of how this data can support the shopper and the salesperson to connect 'things that want to be together', new models of shopping will emerge and reinvigorate the role of things, people and architectures. This Internet of Things project is firmly located within a tenet that the re-thinking of things, data and people might unpack and ameliorate established practices.",,"Primary impact aims:
We aim to provide networked and physical prototypes co-designed by high street users, providing a springboard for the high street to change from a retail space to a connected retail space that will have resonance for 10-50 years. New models of social economy will encourage more people to shop on high street, potentially reviving local economies. By enhancing the social capital of the high street we aim to provide new forms of social engagement with the potential to change how people use the high street. We will respond to the well-documented social and economic problems facing UK high streets by aiming to provide new models and practices that will inform government policy. 

In-project impacts:
The primary impact objective for the collaboration is to extend NCR's technical design portfolio with prototypes for new shopping experiences. NCR are world leaders in point of sale technologies and as the high street transforms they require strategies that open up new market opportunities. The proposed project benefits from being closely associated with Dr. Rogers at Dundee who has worked closely with NCR Research through projects and supervision of PhD students. The second objective is to develop truly user-led solutions toward an Internet of Things. Given the amount of material 'things' and immaterial data that is in the high street, the project focus offers an apt context in which to develop co-designed solutions through ethnographic engagement with participants. The third objective is to develop tangible prototypes that demonstrate the potential for connected high streets that are in direct response to both the Portas Review and the &quot;Digitally Economic&quot; perspective offered by the Grimsey Review. 

Enhancing quality of life: 
'As broadband and 4G continue to &quot;collapse the distance&quot;, we are free to ignore the constraints of the past and the notion that physical space is necessary for shopping. We can find a path with new technologies to create the best combination of both physical and virtual shopping through the hybrid high street. This considers the need for convenience and value, but also for the social aspect of face-to-face communing' (Grimsey 2013).The project aims to develop sensitive solutions that are led by users and not by the apparent promise of new technology, seeking to redress this balance and enhancing the quality of the shopping experience through creative design methods. 

Ensure users contribute to and gain from the project:
The project requires the involvement of high street shoppers and salespeople. The project's team benefits from experience in parallel domains of recruitment and retention with community members (e.g. work in the past with Oxfam managers, staff and shoppers etc.) Given the project is committed to exploring user-led shopping opportunities between objects and data, the pathways to impact will be informed by studies of interaction and engagement, an approach which casts a wider net over what an Internet of Things for the high street might be. We will develop 'pop-up' demonstrations in shopping contexts to expose the prototypes to parties beyond the participatory group. The pop-up events will provide material for documenting and disseminating the ideas to wider communities and will provide key content for conference and exhibition applications. We will design web and print publicity material to be used from the start of the project and will utilise contemporary social media supported by illustrations, photographs and videos to document the innovations and effect of the research. Professional and public institutions (such as the Pro Retail conference, Independent Retail Show, SXSW and the Internet of Things Council events) will be used to disseminate our findings and communicate with non-academic communities. The team has strong links with media channels including BBC Digital Human, New Scientist and WIRED magazine to support the dissemination of key ideas."
8,CBC410DA-9B0E-4F3F-B7DA-6B52B725C454,Temporal forensic analysis of digital Camera sensor imperfections for picture dating,"The research proposal aims to investigate the temporal variations of camera sensor imperfections in order to establish, for any given digital camera, a theoretical model that allows the analyst to estimate the acquisition date of digital pictures. This will advance the field of forensic science, with the potential for applications in high profile cases that require the extraction of evidential information for courtroom purposes. In this project, the analyst is assumed to have the camera device or a set of trusted pictures taken by the same camera and whose acquisition time is known. The goal is then to date one or more images supposedly obtained from the same source and whose acquisition time is unknown. This application will obviously help forensic investigators analyse incidents and link different events.
The novelty of the proposed approach lies in (i) New techniques for efficiently estimating the sensor pattern noise for the purpose of temporal analysis (ii) Classification and temporal analysis of three types of pixels, namely, normal pixels, abnormal but not yet defective pixels, and defective pixels. (iii) Use of machine learning techniques to model the temporal behaviour of camera sensor imperfections.",,"This proposed research would be of great help to forensic scientists and investigators in police forces throughout the UK and beyond since the extraction of informaion from digital images can be used for courtroom purposes. It should also have a positive economic impact in future by enabling the leading UK industrial companies to develop internationally competitive products for picture dating as this has not yet been considered in the UK and elsewhere. In a local context, we will transfer the skills and techniques gained during our investigation to other group members in the Conputer and Electronic security Systems Lab at Northumbria University and to our collaborators in this project. 

The results of this research project will be disseminated to the research community through publications at key international conferences and in high impact journals. Furthermore, under the main CESS website, a new webpage will be designed and updated regularly during the project for providing up-to-date details, experimental results, and free access to the standard dataset."
9,6A6CB343-A835-4C56-B30F-0C519A5C7CFE,Innovative tools to enable exploration of complex and specialised data sets,"In the age of Big Data, knowledge workers - individuals, companies and organisations whose primary focus is knowledge
and information extraction and usage - find it increasingly difficult to search for and identify accurate and relevant
information. In particular, in the domain of scientific literature and IP search, where the underlying corpora are growing at a
huge rate, this is a daunting task and human expertise and involvement remain critical. This project aims to develop a suite
of techniques and methods that will enable users to search for and identify relevant information within a corpus more
efficiently and effectively. The methods developed will deploy semantic-based analysis, domain and lexical linguistic
ontologies in order to first understand the user needs based on the underlying domain of application and subsequently
enable more accurate information retrieval through enhanced search and cross-reference of information. In addition, the
project aims to offer advanced user services through sharing of search strategies which will be identified by observing and
understanding patterns in users' search behaviours.",,"The project represents a highly integrated research proposal that will generate significant impact both for the company and
within the intended application domain, but also more generally within the area of semantic information extraction from
multiple information resources.
Commercial Impact
The project will have direct impact for CambridgeIP and their work in providing efficient and effective services in the area of
scientific search including patent search and analysis. Both partners have significant expertise and experience in the areas
of document and corpora analysis and information extraction, but the combination of skills of CambridgeIP members and
the research expertise and experience of the academic staff from the University of Essex, who are also the leading
members of the ESRC Data Research Centre for Smart Analytics, is envisaged to bring about significant and tangible
outcomes.
More specifically, the project is envisaged to bring about significant improvements to the user services that CambridgeIP
currently delivers, increase the efficiency of the provided services as well as potentially increase their user base
significantly including providing access to SMEs to services and products that were previously accessible only to wellfunded
companies and organisations. At the heart, we do propose to develop methods and techniques which will allow firms to gain access to timely and affordable intelligence about their technological space. Enhanced access to intelligence
will strengthen a firm's R&amp;D strategy and accelerate the commercialisation of inventions which would be dormant
otherwise.
Systems developed in this project will initially be demonstrated in the areas of Bioinformatics and Electronics to start with,
but we envisage extending it to the Automotive, Telecoms and Nanotechology (including Graphene) technology areas.
These are areas where clusters of inventors already exist in the UK and elsewhere. To this end, CambridgeIP's client base
in this space will be direct beneficiaries, but we will also work within the Eastern Academic Research Consortium (Eastern
ARC: this is a research collaboration which involves the Universities of Essex, Kent and East Anglia) and the University of
East Anglia in particular to identify other potential users and beneficiaries.
There will be significant interaction between CambridgeIP staff and University of Essex researchers throughout the project
as well as other practitioners, companies and organisation via the ESRC Data Research Centre for Smart Analytics.
Beyond the impact of the provision of the CambridgeIP services, the developed methods and techniques would be generic
in nature and may be applied to other application areas where information from multiple resources needs to be extracted
and linked (e-commerce, e-government and healthcare applications for instance).
Academic impact
Key academic groups this research will impact on are research groups in Information Retrieval in the Universities of
Sheffield and Glasgow as well as the University of Strathclyde. The University of Essex through the Language and
Computation Group as well as the ESRC Data Research Centre for Smart Analytics have links and relationships with these
research groups and we are planning to disseminate our research results to researchers in these institutions through
dedicated seminars which will provide the opportunity to meet other key academics in this research space. Seminars and
workshops, which will be organised within the ESRC Data Research Centre for Smart Analytics, will provide another means
to ensure academic impact to other key partners, researchers and practitioners. As the techniques developed will also be
more generic they also have the potential to impact on the work carried out by the UK Data Archive which is hosted at the
University of Essex."
10,F2CA944C-5D73-41EE-A516-7CFE39E74EE2,In the Hands of the Analyst: Unlocking the value of social media for professional market research,"Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.",,
11,35F0F1B3-B80F-4D1C-AE54-EC3FEFE82FBF,Fareviz,"Fareviz will develop the business opportunity to explore the fares database/routing guide for the spatial and temporal
structure of commercial fares decisions made by TOCs so we can analyse and visualise the:
- structure of the 3.9m fares for the 500K journeys between main stations (flows) offered by TOCs/PTEs
- permitted routes through routing points for the 500K flows and any associated restrictions
- fare gradients around regulated fare zones and peak periods
- anomalous routes involving sub-optimal journeys for passengers
- creation of custom routes for scenic tours or season ticket cost reduction
- development of custom reporting for Regulators, PTEs and DfT on fare increases and regulated fares
The opportunity is to improve journey planning, provide new insights on the fares system, offer new save money on fares
and identify anomalies through exploration of this data outside the context of the ticket retailing process.",,"The impact of the project falls in the following areas:
- creation of new digital services around the open fares database
- improvement of accountability in the regulagion of fares on the railway"
12,3F9D91D0-A2C9-4842-AEDE-91929057E4F4,Nonparametric Learning for Situated Data-to-Text Generation: Helping People to Understand Uncertain Data,"Information overload is a pervasive problem in many environments, particularly those in which human decision making is based on extensive data sets. Data-to-text systems have been shown to successfully address this problem by automatically generating textual descriptions of the underlying data. However, when translating (numerical) data into words, an appropriate level of precision needs to be chosen. The following example is from a system which summarises medical time series data for neonatal care: &quot;At 17:24 T1 is 35.7 and T2 is 34.5C&quot; (Gatt et al., 2009). This summary is clearly targeted to experts, such as doctors or nurses, which need precise information for decision making. However, other users, such as visiting parents might be more happy with a description such as &quot;In the evening your baby had normal temperature.&quot; 

In this project, we will build a data-to-text system that automatically determines the appropriate level of precision for a given context by using statistical machine learning methods. These methods can learn an optimal generation policy from real data and promise to be more robust to new situations than hand-written rules by human experts. 
We will also investigate novel feedback-based non-parametric state estimation methods to reduce the data annotation cost for data-to-text systems. Typically, the first step in creating such systems is to manually interpret and align the raw data sources. However, this step is very costly as human experts need to trained for this task. Our new methods promise for data-to-text systems to be rapidly applied to new domains. 

The domain we will be targeting for this initial project is pedestrian navigation, where the task is to translate uncertain user positions into walking instructions. The underlying data uncertainty here arises from several sources, such as the user's speech signal, the GPS location, estimated viewshed, walking direction and speed. We will integrate and test our learnt data-to-text generation strategy by integrating it in an existing system and running an evaluation with real users.
 
One of the outcomes of this project is a data-driven linguistic view on the question of &quot;how to communicate uncertainty&quot;, which is an active interdisciplinary research area, including researchers from medicine, law, environmental modelling and climate change.
In future work we will also investigate how the proposed framework transfers to new domains, such as natural language generation from medical data, weather forecasts, or output from complex environmental models.",,"The overall aim of this research is to provide better interfaces for people to understand &quot;big&quot; data more intuitively. As such, the outcomes of this research have three main impact beneficiaries: (1) academic research investigating how to (automatically) communicate data, (2) informing policy makers how to communicate their findings, and (3) the general public needing to make decisions based on (uncertain) data. 

(1) Within the academic community, this proposal aims to bridge the gap between two disciplines, which are both concerned with decision support: data-to-text Natural Language Generation (NLG) systems, and interdisciplinary research working on communicating uncertainty. While other disciplines, such as medicine, environmental modelling, climate change or weather forecasts strongly promote the need for communicating data uncertainty, data-to-text systems still assume that their underlying data is precise and correct. If automatic data-to-text systems are to be widely used within decision support, they must have mechanisms to communicate uncertain data in an effective way. This research will contribute a principled study and data-driven framework for generating descriptions of underlying data uncertainty.

(2) The developed models will not only be beneficial to academics from other disciplines, but also to policy makers, such as the International Panel of Climate Change (IPCC) for example. Currently, the IPCC prescribes a standardised mapping of data uncertainty into words, which is widely recognised and applied beyond climate change research. However, the guidelines by the IPCC are not grounded in linguistic research and have been reported to be problematic in their use. In future, these guidelines could be informed by the outcomes of this research.

(3) Finally, the long-term beneficiary of this research is the general public, who in their daily life have to make decisions based on vast amounts of unstructured information becoming more readily available. For example, the British government recently announced in a white paper that it will be greatly expanding the amount of data which it shares with the rest of us (http://www.guardian.co.uk/politics/2012/jun/27/public-services-data-published-price). However, most people lack the skills and tools to access and interpret large data sets. The overall aim of this research is to provide better direct access to data through user-friendly interfaces, which help people to understand data more intuitively and support decision making.

For a description of how these impact goals will be realised and how their success will be measured, please see &quot;Part III: Pathways to Impact&quot; of this proposal."
13,FA9903DD-FE9A-4DC9-ADA6-AED74CF674B7,An Integrated Audio-Symbolic Model of Music Similarity,"Similarity is a central aspect of music. It can help as a benchmark for testing theories of music perception and cognition, it is useful to answer for practical applications, such as music retrieval and recommendation, and it is an overarching question that relates to many aspects of music. However, theories of musical similarity have mostly focused on symbolic representations, where musical structures such as melodic and harmonic development are addressed but the aspects or expressive performance, such as micro-timing and timbre are ignored. On the other hand, audio based models, typically distances based on audio features, can capture details of the performance such as timbre, dynamics and tempo changes, but little of the musical structure as it unfolds over time.

Recent progress in audio transcription and alignment and the availability of music analysis tools for music collections with audio and symbolic content, which are being developed in the Digital Music Lab (DML) project, are changing the landscape of research in music. Large datasets of acoustic and symbolic music data that are generated or unveiled through the DML and &quot;Optical Music Recognition from Multiple Sources&quot; Big Data projects encourage an approach that can combine symbolic and audio based analyses into a joint similarity mode. This opens a great potential to new tools for research in music information retrieval and musicology, as the interaction between symbolic structure and acoustic information such as timbral texture has rarely been addressed and it could reveal aspects that have been unnoticed or unexplained so far. If successful, it might contribute to breaking the glass ceiling in music recommendation. 

The aim of this project is to develop an initial framework and conduct experiments on an integration of symbolic melodic and structural similarity models with audio based models. The models ability to capture various notions of similarity will be evaluated on cultural information in music collections (e.g. genre, style, composer) as well as user annotations or ratings of similarity. This work will build on the experience of the participants in modelling audio similarity (Wolff &amp; Weyde 2013), audio transcription and the integration of symbolic and audio based models (Benetos, Ewert &amp; Weyde 2014), symbolic melodic similarity (Marsden 2012) and probabilistic music and performance modelling (Spiro, Gold &amp; Rink 2010, Abdallah et al 2012).",,"This project offers an opportunity to transform musicological research, specifically the research areas of music similarity and structure analysis. The proposed work will enable access to musicologists, technologists, British Library users, and the wider music listening public to large collections of music recordings and scores, and tools for comparing, classifying, and clustering music collections, enabling large-scale systematic musicology research. In specific, the beneficiaries of this project include those directly involved, and those involved through our partner the British Library (BL), and from any likely commercialisation of project outcomes.
Those directly involved are:
(1) Users of the British Library, in particular of the Digital Music Collections
(2) The British Library, in particular the British Library Labs and the British Library Sound Archive
(3) Musicologists, musicians and music enthusiasts accessing the project-created online resources
Those indirectly involved are:
(4) Music technologists, in particular developers working on music recommendation and similarity
(5) Potential licensees and adopters of the music analysis tools showcased by this project
(6) Customers of licensees and adopters, i.e. the wider music listening public

These different constituents benefit in differing ways. Users of the BL will have access to large collections of audio recordings and scores (the latter also automatically aligned to the recordings), as well as high-level annotations of musical structure. From a collection-level, users will be able to visualise collections, as well as compare and cluster large groups of music recordings and scores. The BL will also be able to improve their service and infrastructure, and will be able to exploit the vast amount of data which already exist in the BL Sound Archive, and also link it to its large corpus of transcribed scores (from printed sources) that are being made available through the Optical Music Recognition project.

Musicologists will be able to access this resource online from BL services but also from the online services developed as part of the DML project, and will benefit from automated tools for accessing audio recordings, scores, and high-level information regarding structure of individual music pieces, as well as collection-level analysis tools for music similarity. Musicians will have the opportunity to practice and study using an online dataset providing recordings, scores, and structural information. Music technologists will realise the impact that audio-score integration can lead to improved systems for music recommendation. Potential licensees of music analysis tools will benefit from software that jointly analyse symbolic and audio information for music, and provides reliable similarity measures for music recommendation applications. Customers of music technology tools will benefit from a more insightful organisation of their music collections and will be able to more easily locate music that matches their interests.

We expect beneficiaries (1) and (2) to gain significant benefit during the lifetime of the project, while beneficiaries (3) will receive increased benefit as the main project outcomes are disseminated. Finally, beneficiaries (4)-(6) should see benefit after the end of the project.

The project includes regular interaction between researchers and partners, as well as partners on the DML and Optical Music Recognition projects. We will organise a workshop towards the end of the project, aimed at musicologists and music technologists, where the created datasets and tools for music similarity will be presented. Project documentation and press items for engaging with users and the wider public will also be created, along with a project-specific website, blog, press, and social media presence. Mechanisms to present the project to the public will be sought in conjunction with the Press and Publications Offices of City, UCL and Lancaster."
14,99F59449-79D9-4DD6-9672-84EF46DB0722,Verifiable Autonomy,"Autonomy is surely a core theme of technology in the 21st century. Within 20 years, we expect to see fully autonomous vehicles, aircraft, robots, devices, swarms, and software, all of which will (and must) be able to make their own decisions without direct human intervention. The economic implications are enormous: for example, the global civil unmanned air- vehicle (UAV) market has been estimated to be &pound;6B over the next 10 years, while the world-wide market for robotic systems is expected to exceed $50B by 2025.

This potential is both exciting and frightening. Exciting, in that this technology can allow us to develop systems and tackle tasks well beyond current possibilities. Frightening in that the control of these systems is now taken away from us. How do we know that they will work? How do we know that they are safe? And how can we trust them? All of these are impossible questions for current technology. We cannot say that such systems are safe, will not deliberately try to injure humans, and will always try their best to keep humans safe. Without such guarantees, these new technologies will neither be allowed by regulators nor accepted by the public.

Imagine that we have a generic architecture for autonomous systems such that the choices the system makes can be guaranteed? And these guarantees are backed by strong mathematical proof? If we have such an architecture, upon which our autonomous systems (be they robots, vehicles, or software) can be based, then we can indeed guarantee that our systems never intentionally act dangerously, will endeavour to be safe, and will - as far as possible - act in an ethical and trustworthy way. It is important to note that this is separate from the problem of how accurately the system understands its environment. Due to inaccuracy in modelling the real world, we cannot say that a system will be absolutely safe or will definitely achieve something; instead we can say that it tries to be safe and decides to carry out a task to its best ability. This distinction is crucial: we can only prove that the system never decides to do the wrong thing, we cannot guarantee that accidents will never happen. Consequently, we also need to make an autonomous system judge the quality of its understanding and require it to act taking this into account. We should also verify, by our methods, that the system's choices do not exacerbate any potential safety problems.

Our hypothesis is that by identifying and separating out the high-level decision-making component within autonomous systems, and providing comprehensive formal verification techniques for this, we can indeed directly tackle questions of safety, ethics, legality and reliability. In this project, we build on internationally leading work on agent verification (Fisher), control and learning (Veres), safety and ethics (Winfield), and practical autonomous systems (Veres, Winfield) to advance the underlying verification techniques and so develop a framework allowing us to tackle questions such as the above. In developing autonomous systems for complex and unknown environments, being able to answer such questions is crucial.",,"Within 20 years, we expect to see fully autonomous vehicles, aircraft, robots, devices, swarms, and software, all of which will (and must) be able to make their own decisions without direct human intervention. This project will be relevant across many of these applications, from autonomous vehicles, such as driver-less cars and unmanned air vehicles, through to robotics, both in the home and in wider society. Since the control of autonomous systems is increasingly taken away from humans: how can we be sure that they will work; how can we be confident that they are safe; and how can we trust them? All of these are impossible questions for current technology. We cannot say that such systems are safe, will not deliberately try to injure humans, and will always try their best to keep humans safe. Without such guarantees, these new technologies will neither be allowed by regulators nor accepted by the public.

Consequently, the core problem with this technology is being able to effectively verify the inherent autonomy. This is the focus of our project and its potential impact reaches across all those in academia, industry, and government involved in the development, social acceptance, and legality of truly autonomous systems.

Specifically in WP6 and WP7 we have outlined our initial network of collaborating companies, covering Robotic Assistants (Elumotion) and Automotive (MIRA), later with (if sufficient progress) Nuclear Robotics (NNL), and potentially in Underwater (Thales), and Space Exploration (European Space Agency). However, we intend to broaden the industrial aspect and promote and expand the companies aware of, and developing, &quot;verifiable autonomous systems&quot;. This promotion will build on our existing networks, for example the industrial members of the Autonomous and Intelligent Systems Partnership; industrial members of the Autonomous Systems National Technical Committee; industrial members of the British Automation and Robot Association; and industrial participants with the Virtual Engineering Centre. 

We aim to engage closely with governmental agencies, professional societies, European and international institutions to contribute to the public debate on issues influenced by autonomous systems, leveraging and deepening our existing memberships and influence in such bodies. We aim to develop a suitable activity within one of the relevant learned societies to increase engagement with the scientific and policy communities.

We have already seen, particularly through Winfield's work on public engagement, how autonomous systems (typically, Robotics) can spark the public's imagination. We will build on this, expand to other forms of autonomous systems (UAVs and driver-less cars are natural possibilities), and target public engagement events such as the Cheltenham Science Festival and the British Science Festival (Winfield has already been involved with the former) later in the project. 

In summary, through deeper engagement later in the project we aim to impact upon broader industry, and both knowledge transfer and business development branches of government. The team, together with their respective universities, have effective mechanisms for dealing with the media, with access to press offices who regularly publicise work nationally and internationally to non-academic audiences."
15,E4977336-A445-48CB-A84D-8F670EE1C3A7,Verifiable Autonomy,"Autonomy is surely a core theme of technology in the 21st century. Within 20 years, we expect to see fully autonomous vehicles, aircraft, robots, devices, swarms, and software, all of which will (and must) be able to make their own decisions without direct human intervention. The economic implications are enormous: for example, the global civil unmanned air- vehicle (UAV) market has been estimated to be &pound;6B over the next 10 years, while the world-wide market for robotic systems is expected to exceed $50B by 2025.

This potential is both exciting and frightening. Exciting, in that this technology can allow us to develop systems and tackle tasks well beyond current possibilities. Frightening in that the control of these systems is now taken away from us. How do we know that they will work? How do we know that they are safe? And how can we trust them? All of these are impossible questions for current technology. We cannot say that such systems are safe, will not deliberately try to injure humans, and will always try their best to keep humans safe. Without such guarantees, these new technologies will neither be allowed by regulators nor accepted by the public.

Imagine that we have a generic architecture for autonomous systems such that the choices the system makes can be guaranteed? And these guarantees are backed by strong mathematical proof? If we have such an architecture, upon which our autonomous systems (be they robots, vehicles, or software) can be based, then we can indeed guarantee that our systems never intentionally act dangerously, will endeavour to be safe, and will - as far as possible - act in an ethical and trustworthy way. It is important to note that this is separate from the problem of how accurately the system understands its environment. Due to inaccuracy in modelling the real world, we cannot say that a system will be absolutely safe or will definitely achieve something; instead we can say that it tries to be safe and decides to carry out a task to its best ability. This distinction is crucial: we can only prove that the system never decides to do the wrong thing, we cannot guarantee that accidents will never happen. Consequently, we also need to make an autonomous system judge the quality of its understanding and require it to act taking this into account. We should also verify, by our methods, that the system's choices do not exacerbate any potential safety problems.

Our hypothesis is that by identifying and separating out the high-level decision-making component within autonomous systems, and providing comprehensive formal verification techniques for this, we can indeed directly tackle questions of safety, ethics, legality and reliability. In this project, we build on internationally leading work on agent verification (Fisher), control and learning (Veres), safety and ethics (Winfield), and practical autonomous systems (Veres, Winfield) to advance the underlying verification techniques and so develop a framework allowing us to tackle questions such as the above. In developing autonomous systems for complex and unknown environments, being able to answer such questions is crucial.",,
16,09378966-1E66-44EB-8920-84C293AEE1A2,Advancing Consumer Protection Through Machine Learning: Reducing Harm in Gambling,"The global gaming market is worth $500bn, with internet gaming growing at 16% YOY. As gambling continues to expand
globally across digital channels there is growing concerns in increased problem gambling. Bet Buddy and City University
have the opportunity to develop a solution to predict problem gambling behaviour, leveraging City University Machine
Learning expertise together with Bet Buddy domain exprtise in problem gambling and software egnineering. This would
enable Bet Buddy to apply machine learning techniques not yet deployd in area of growing market value and solving social
problems. Bet Buddy won its first major contract in 2013 with the Ontario Lottery and Gaming Corporation (OLG) in
Canada, and has now signed re-seller agreements with the two of the world's biggest gaming software providers, making
product exploitation achievable. This project will advance Bet Buddy's product offering by applying Artificial Intelligence (AI)
techniques to enable Bet Buddy to offer its products to more clients globally, solve emerging problems in new forms of
gambling (e.g. machine gambling), and to target new but related industries e.g. trading and social casino gaming.",,"Financial Support: Financial support adds value in several ways, both financial and non-financial. Firstly, a TSB grant
provides opportunity to unlock financial support from the Cass Entrepreneurship Fund, a &pound;10m venture capital fund
associated with Cass Business School, and funded by Peter Cullum, CBE, one of the UK's most successful
entrepreneurs.To date, City University and the Peter Cullum Centre for Entrepreneurship have been instrumental in the creation, development and growth of Bet Buddy. Funding via a prestigious UK body such as the TSB is greatly preferred
over market development capital alternatives, with regards to protecting the trust built up over the previous 2 years. The
total financing package unlocked would provide the required support to deliver research and enhanced product to Bet
Buddy's first major client, Ontario Lottery and Gaming Corporation, as well as provide Bet Buddy with the capability to
capitalise on growing market demands for innovative products in this field, addressing the previously outlined market and
penetration strategy. Non-financial support generated by a TSB grant is extremely important, and has material impact in
Bet Buddy optimizing its development and growth strategy. Support from the TSB would provide a very positive message,
both among the UK gaming community and Bet Buddy's intended export sales targets, that the government is committed to
finding new and innovative ways to address public health concerns. Specifically, those linked to the rapid expansion of the
gambling sector and other industries where consumers can be at risk of developing destructive behaviors. A partnership
between Bet Buddy and City University/Cass Business School, along with with financial support rom the TSB would also
reinforce the public - private relationship often required to credibly address complex public health issues. Should TSB
funding be unssuccful then it is likely that Bet Buddy would miss out on the opportunity of growth as demand grows from
UK and international markets to existing competitors (e.g. Playscan) and possible new entrants as the pace of product
development would lag the opportunities."
17,CC43EAD5-224F-4F01-8904-D07E9092AE69,Bayesian Analysis of Competing Cyber Hypotheses,"Cyber security is recognised as important at the highest levels of international government. President Obama has said that &quot;the Cyber threat is one of the most serious economic and national security challenges [the US] face as a nation&quot;. Even the &pound;650M in additional funding that accompanied the UK's Cyber Security Strategy is dwarfed by the &gt;&pound;10B estimated annual cost of cyber-crime to the UK economy. Additionally, we see links to &quot;transnational organised crime&quot; (cyber-crime is lucrative and widespread) as well as &quot;Terrorism&quot; (state-sponsored cyber-warfare is increasing) and &quot;Ideologies and beliefs&quot; (anti-establishment hacktivists, eg Anonymous, are also resorting to cyber-attack to express their views).

Companies such as HP help organisations who are subjected to cyber attacks to protect their assets and information from such attacks. These cyber defence companies achieve this using a combination of hardware and software augmented with human effort. Allocating human effort to activity is critical since inappropriate allocation can result in human time being wasted or attacks going unchallenged. Time pressure, the presence of ambiguous information and the high stakes involved can then degrade the human judgement associated with this allocation process.

Psychologists understand that such pressures degrade human decision making and similar issues have been found to exist in other domains. Indeed, Pearl Harbour and the Cuban Missile Crisis were each the result of failures in the intelligence process that can be traced back to human analysis errors educating decision making. 

Motivated by such experiences, in the 1970s, the CIA developed a technique, &quot;Analysis of Competing Hypotheses&quot; which encourages analysts and decision makers to avoid the pitfalls that can be associated with intelligence analysis. This technique involves consideration of multiple candidate explanations for what is being observed. The hypotheses are then assessed (and iteratively refined) using the observations to discriminate between likely and unlikely hypotheses. While the technique has proven its utility, for it to work effectively, it is important that the hypotheses considered include the &quot;possible&quot; not just the &quot;probable&quot; explanations. Unfortunately, &quot;possible&quot; and &quot;probable&quot; aren't precisely defined in this context.

However, a recent advance in the statistics literature, &quot;Sequential Monte Carlo Samplers&quot;, exhibits many of the same features as Analysis of Competing Hypotheses. Sequential Monte Carlo samplers are typically applied in contexts where a computer (not a person) generates the hypotheses and assesses them. However, just like Analysis of Competing Hypotheses, they consider a population of hypotheses, assessed against data and then iteratively used to spawn a new population of hypotheses. Crucially, the analogous concept to the notion of &quot;possible&quot; and &quot;probable&quot; hypotheses is both well defined and well understood.

We propose to adapt Sequential Monte Carlo samplers to become part of Analysis of Competing Hypotheses. We further propose to apply and demonstrate a tool embodying the technique in an operational cyber security context.

If successful, this project would develop techniques that would ensure that decisions made in operational cyber security settings were well motivated. Where those decisions relate to the allocation of human analyst resources to activities, this would improve the efficiency of cyber security operations. The technology will position the UK at the forefront of the state-of-the-art in this high priority application domain.",,"HP Labs are a partner in this proposal with the explicit purpose of maximising the project's impact. HP Labs is the long term corporate research entity for HP and has a team dedicated to innovation in security. That team include experts in the UK and US who are specifically interested in improving decision making in operational contexts relevant to cyber security. 

HP is a major global player in security and offers a wide range of products and services to enterprises and governments internationally. These offerings include extensive human analyses and decision making inside SOCs. HP are keen to explore the potential to use technology to mitigate biases encountered in such decision making and improve the effectiveness of such analysts. Indeed the US Department of Homeland Security (DHS) is funding HP Labs (Princeton) to conduct research as part of a project funded via Dartmouth University and with George-Mason University looking at issues that are closely related to this proposal. 

HP provides products for use in other people's SOCs, as well as providing SOCs as a service to its customers: HP runs SOCs for organisations that range from SMEs through corporate enterprises to government organisations. This positions HP as a route through to a global market related to SOCs specifically and cyber security in general.

The primary impact for this project will therefore be the use of tools, developed and then prototyped in his project, that can improve the effectiveness of SOCs which HP either provides as a service or supports via the provision of security products. 

By developing such prototype tools in this project, the security of companies associated with such SOCs will be improved; this project will not only impact HP directly, but HP's customers indirectly. In so doing, this project will improve the cyber security of organisations across the UK and right across the wider global marketplace."
18,2296688B-1615-4C8E-9310-71D61F7A2AD4,Improving Target Language Fluency in Statistical Machine Translation,"Recent years have seen great improvement in the quality of statistical machine translation (SMT). Automatic translation has benefitted from increasing amounts of monolingual and translated data, from advancements in core modelling algorithms, and from a growing understanding of how best to integrate automatic translation into large-scale language processing systems. Despite these improvements, even the best SMT output is rarely of human quality. Any casual inspection of MT output will quickly find syntactic and semantic errors that only a machine would make. New modelling techniques, capable of extracting the best possible models from all available data, are needed.

This proposal aims to overcome one of technical barriers to delivering 'human quality' statistical machine translation (SMT): the production of grammatical output. We propose here to use multiple grammars in SMT. One grammar is focused on translation of the source language, as in current practice. The second grammar is focused on production, with the aim of producing fluent and grammatical sentences in the target language. We will develop a decoding framework in which translation and production are closely linked but independent processes driven by these two grammars. Our systems will be based on state-of-the-art syntactic SMT, and our aim will be to dramatically improve the fluency of the translation output, particularly in situations where the original source language text is noisy and difficult to translate fluently.

This work will be of value to UK industry. The UK translation and interpretation market was estimated at EURO 290M - EURO 434M in 2009, and UK localisation and language service providers are strong competitors in the worldwide language industry, forecast to grow to EURO 16B by 2015. Reducing the cost of high-quality translation is a concern for this industry which we will address directly, in that improving target language fluency is a key factor in translation post-editing efficiency. 

In academia, SMT systems are now used to build systems incorporating speech recognition, speech synthesis, and dialogue systems. Researchers at Edinburgh University, Heriot-Watt University, Oxford University, Sheffield University are among universities with groups working on these problems. Our project will enable SMT researchers to apply their expertise in translation grammar induction, large-scale language modelling, and parameterisation to target language production.

Motivated by these needs, our research hypotheses are that: (1) modelling techniques from syntax-based SMT can be used to build stochastic production systems; (2) production quality can be improved using 'Big Data' and machine learning statistical modelling techniques; and (3) target language production systems can be integrated into syntax-based statistical machine translation systems using risk-based decoding procedures, yielding improvements in translation quality, robustness, and fluency. The novelty in this proposal is in: (1) the use of separate grammars for syntax-based statistical machine translation, one grammar for translation and a second for production; (2) coupling them into a risk-based consensus decoding procedure; (3) incorporation of phrase-based production grammars and search procedures; (4) an explicit focus on fluency.

Our research will yield new models and algorithms in the form of open source software and systems. We take the view that the best pathway to economic impact for this type of research is by: publishing research results; releasing software and data under generous Open Source licenses for unconstrained use by industry; and by training students and PDRAs who can take their skills and knowledge from the university to industry. We believe this is the broadest and surest way to enhance the research capacity, knowledge and skills of businesses and organisations. All results of this research project will be distributed in the public domain.",,"Our pathways to impact will follow our close links with the UK language technologies industry; with international sponsored research programmes; our participation in the Cambridge Language Sciences Initiative; and our involvement in undergraduate and post-graduate teaching in the Department of Engineering and Computer Laboratory at the University of Cambridge. Our impact activities are: 
1 Publication, Release of Open Source Tools and Data, Participation in International Evaluations
Our main focus is academic impact in the form of traditional peer-reviewed publication. However, the machine translation and natural language processing (NLP) research communities rely heavily on shared experimental infrastructure. Substantial research effort within the field is devoted to: distribution of open source software; creation of common data sets to train and evaluate systems; workshops and short courses for research students; and participation in international competitions evaluated by impartial third-party judges. Publication impact is greatly enhanced when research results are used in a good evaluation system and when the software and data underlying the publication are released with accompanying tutorial documentation. This enables others in the field to quickly replicate and assess the published results. We will participate in all these activities.
2 Involvement of Undergraduates, MPhil Students, and PhD Students in Sponsored Research
- PhD students in the Cambridge Engineering Department and the Computer Laboratory will be offered opportunities to work on the project.
- Students on the Cambridge Advanced Computer Science MPhil are given a project option. We will offer suitably scoped research projects based on topics in this proposal. These will be closely tied to the main themes of this proposal, with suitable resources and a limited agenda so that students can finish successfully within their course
- Cambridge Engineering students are required to do a Fourth Year Project. We will offer projects in Information Engineering to interest students. Projects are scoped for the skills and abilities of students. We offer software engineering projects, e.g. iOS/Android applications for mobile interaction with server-based SMT systems. Research-minded students can undertake projects on specific technologies or translation problems.
3. Cambridge Language Sciences Initiative 
Cambridge Language Sciences was established in mid-2012 as a University-level Strategic Research Initiative. These Initiatives are intended to influence national and international research, policy and funding agendas; to strengthen internal cross-disciplinary research collaborations; and to provide a platform for large-scale funding applications, recruitments and international research partnerships. Human Language Technologies has been named as one of the key Research Themes within the initiative. There are regular seminars, workshops, and discussion groups, which are widely attended, and to which we will contribute research results from this project.
4. Interaction with Industry
We take the view that the best pathway to economic impact for this type of research is by: publishing research results; releasing software and data under generous Open Source licenses for unconstrained use by industry; and by training students and PDRAs who can take their skills and knowledge from the university to industry. All results of this research project will be distributed in the public domain. This is the best way to ensure they can be easily used by industry. We have two additional, specific paths to commercialisation and exploitation of scientific knowledge:
- Close ties to Language Weaver / SDL plc, a UK language technologies and language services provider 
- Cambridge Enterprise Limited is a wholly owned subsidiary of the University of Cambridge responsible for the commercialisation of technology arising from University departments."
19,20FB69FA-7B65-48F9-8863-40254FCE11FC,Transfer Learning for Person Re-identification,"Person re-identification is an important task in distributed multi-camera surveillance. This is currently performed manually at great economic cost, and with high error rates due to operator attentive gaps. In this project we aim to achieve fast accurate and robust automated person re-identification that can be deployed to any given camera network scenario, without any expensive calibration steps.

Automated person re-identification is the task of associating people based on images captured in video across diverse spatially distributed camera views at different times. This is challenging because the articulation of the human body and variety of viewing conditions such as lighting, angle and distance means that observed appearance typically differs more for the same person in different views than it does for different people. At the same time, it is an important task to solve because re-identification underpins many key capabilities in visual surveillance such as multi-camera tracking. This in turn is a key capability for end-user organizations which need video analytics to achieve a variety of ends including retail optimization, operational efficiency, public safety, security, infrastructure protection and terrorism prevention. Moreover, it is important to automate re-identification because the manual process in large camera networks is both prohibitively costly and inaccurate due to attentive gaps.

Current state of the art re-identification systems use machine learning techniques to produce models for re- identifying across a particular pair of cameras based on manual annotation of person identity in those cameras. However, this is not scalable in practice, because every unique pair of cameras would need calibration with training data. In this project, we will develop new machine learning models that can automatically adapt re-identification models created for an initial set of source cameras to address the re-identification problem in each new pair of cameras without requiring new annotation. This will dramatically improve the practical impact of re-identification technology by making it significantly more accurate as well as cheaper and easier to deploy.",,"In the short to medium term, beneficiaries of advances in re-identification (ReId) specifically include:

* UK ReId research community. Despite initial groundbreaking impact in the early days of the field, the UK is in danger of loosing leadership to Asian competitors which are investing heavily (CUHK alone 7 people working on ReID).

* Video-content analytics companies who could commercialize the ReId IP outcomes of this project, includ- ing both SMEs like VSL (see included letter of support) as well as big industry such as BAE and IBM (who have submitted a flurry of patents in this area recently). ReId technology is used in a variety of downstream analytics tasks such as long-term tracking and person search, thus impacting the breadth of the analytics industry.

* End-user organizations tasked with video analytics for retail optimization, operational efficiency, public safety, security, infrastructure protection and terrorism prevention. Previous end user partners with whom we have partnered via VSL for trailing this type of research outputs include: Tesco, BAA (Heathrow Airport), BAE, UK MOD and London Metropolitan Police. As two specific examples:

- The London Met would like to use ReId technology for, among other tasks, automatic long-term monitoring of well known professional thieves. This is in order to catch them in the act so as to be able to secure arrests, convictions and deportations. Without this technology, the manpower does not exist to consistently keep tabs on these criminals, allowing thefts to go on unfettered. The thieves are fully aware of the Met's limitations and brazenly industrialize and professionalize their activities as a result.

- BAA have trialled person ReId for both security and operational efficiency reasons: In security they would like to perform long-term tracking of the whereabouts of someone leaving an illegally parked vehicle (a potential car-bomb danger). This is so they can intercept this person as soon as possible and resolve the situation quickly so as to minimize both risk of &quot;true positives&quot; and disruption to the airport by the numerous &quot;false positives&quot; (most violators are not of malicious intent, but all need to be dealt with just in case). On efficiency, monitoring how long it takes airport users to get through various checks and queues is key to optimizing operational processes. Without ReId technology, this information is actually quite hard to get. Thus ReId technology is being trialled to provide this information to help improve procedure efficiency.

- In each case, ReId technology has not yet fulfilled its promise well enough for operational deployment. This is because of limited practicality (annotation requirements) and the inter-related issue of significantly weaker performance out of the lab (change of domains). These are exactly issues which this proposal will address (annotation requirements, WP1), performance improvements (all WPs, but especially WP3).

* Citizens will enjoy more efficient service, increased safety and reduced crime resulting from more effective ReId.

* Other QMUL research groups. Imminent projects include: a Spanish collaboration on gait-based ReId will exploit our results (gait recognition needs to transfer across covariates of ground surface type); and an FP7 project on UAV-based surveillance will test our ReId algorithms from an overhead perspective: in which zero view annotation models are an intrinsic requirement since the view changes dynamically.

In the longer term, unsupervised transfer learning outcomes have potential to impact to a wide variety of applications including for example, transferring music recognition models across various acoustic conditions (work being done at QMUL's C4DM), transferring medical diagnosis and risk assessment Bayesian networks (work being done at QMUL's RIM).

These impacts will be achieved via trials through VSL and various collaborations and seminars - please see PTI for details."
20,2D2C24A1-FB2A-40B5-B7F7-94A94D006297,Verifiable Autonomy,"Autonomy is surely a core theme of technology in the 21st century. Within 20 years, we expect to see fully autonomous vehicles, aircraft, robots, devices, swarms, and software, all of which will (and must) be able to make their own decisions without direct human intervention. The economic implications are enormous: for example, the global civil unmanned air- vehicle (UAV) market has been estimated to be &pound;6B over the next 10 years, while the world-wide market for robotic systems is expected to exceed $50B by 2025.

This potential is both exciting and frightening. Exciting, in that this technology can allow us to develop systems and tackle tasks well beyond current possibilities. Frightening in that the control of these systems is now taken away from us. How do we know that they will work? How do we know that they are safe? And how can we trust them? All of these are impossible questions for current technology. We cannot say that such systems are safe, will not deliberately try to injure humans, and will always try their best to keep humans safe. Without such guarantees, these new technologies will neither be allowed by regulators nor accepted by the public.

Imagine that we have a generic architecture for autonomous systems such that the choices the system makes can be guaranteed? And these guarantees are backed by strong mathematical proof? If we have such an architecture, upon which our autonomous systems (be they robots, vehicles, or software) can be based, then we can indeed guarantee that our systems never intentionally act dangerously, will endeavour to be safe, and will - as far as possible - act in an ethical and trustworthy way. It is important to note that this is separate from the problem of how accurately the system understands its environment. Due to inaccuracy in modelling the real world, we cannot say that a system will be absolutely safe or will definitely achieve something; instead we can say that it tries to be safe and decides to carry out a task to its best ability. This distinction is crucial: we can only prove that the system never decides to do the wrong thing, we cannot guarantee that accidents will never happen. Consequently, we also need to make an autonomous system judge the quality of its understanding and require it to act taking this into account. We should also verify, by our methods, that the system's choices do not exacerbate any potential safety problems.

Our hypothesis is that by identifying and separating out the high-level decision-making component within autonomous systems, and providing comprehensive formal verification techniques for this, we can indeed directly tackle questions of safety, ethics, legality and reliability. In this project, we build on internationally leading work on agent verification (Fisher), control and learning (Veres), safety and ethics (Winfield), and practical autonomous systems (Veres, Winfield) to advance the underlying verification techniques and so develop a framework allowing us to tackle questions such as the above. In developing autonomous systems for complex and unknown environments, being able to answer such questions is crucial.",,
21,7FD00A2D-A88B-45E0-A326-E4C4707AAE87,"Towards managing risk from climate change through comprehensive, inclusive and resilient UK infrastructure planning","The UK, along with most other nations in Europe, is encountering a paradigm shift as the funding, management, and protection of infrastructure is increasingly expected to be managed through partnership-led governance ensuring better informed and more viable, long-term decision making. This shift in emphasis from 'top-down' direct government, to 'bottom-up' stakeholder engagement is particularly pronounced in managing risks to critical infrastructure and the environment.

Current methodologies and tools, such as surveys and stakeholder focus groups, aimed at consultation with stakeholder organisations and citizens are however generally limited in scope and insufficiently open or adaptive. Thus, they do not effectively inform the complex planning processes underlying comprehensive, multi-faceted infrastructure development planning as undertaken for example in the Thames Estuary. In order to inform and reconcile planning approaches to heterogeneous challenges such as environmental risk protection, economic viability (e.g., fisheries) and ecosystem management, new consultation methods and novel ways of combining multi-stakeholder views with quantitative data are urgently needed.

The Thames Estuary Partnership (TEP) and its partners are facing the challenge of stakeholder integration in the planning of imminent, major infrastructural development in the context of large scale projects in the areas of pollution (sewers), flood protection and ecosystem management. 
This project is designed to leverage, apply and evaluate - in the Thames Estuary Infrastructure context, cutting edge methodologies and software tools for value-based, data-driven planning methods developed and tested as part of the recent, interdisciplinary EPSRC funded project &quot;Towards Data-Driven Environmental Policy Design&quot; (TDDEPD). 
Building on novel developments in human data capture and computer science, these techniques enable the rapid and comprehensive capture of qualitative data; e.g., stakeholder opinion and their integration with quantitative data sources such as sensor measurements (e.g., rainfall levels) and process outputs. 
While these planning methodologies were developed in a very different topical (i.e. environmental protection planning) and geographic (Western Australian wetlands) context, this project will explore their adaptation and application to the context of infrastructural planning in the Thames Estuary. In order to establish their viability, the project brings together an exceptional team, led by the TEP and including the Horizon Digital Economy Research Institute and School of Computer Science from the University of Nottingham (Horizon) and integrating key expertise in planning contributed (as an in-kind contribution) by the Department for Parks and Wildlife by the Western Australian Government.

The ambitious project will deliver a novel value-driven methodology for infrastructure planning, including an adapted framework for stakeholder engagement. A digital platform enabling the capture and processing of both stakeholder input and (often uncertain) quantitative data (e.g., water levels) will be developed and will provide essential management support tools such as sensitivity analyses for potential infrastructure changes. The latter will also directly support the evaluation of the proposed approach and will enable addressing a concrete use case within the project time frame (Clean Seas Please). 

TEP believe that a values-based approach to managing infrastructure, risk, and habitat creation will be the only effective way forward. They believe that using the methods to be developed in this work by a team with a strong track record and significant expertise will transform the way they interact with those to whom they are responsible at the levels of policy setting, and policy implementation and will enable the comprehensive planning and development of infrastructure in the future.",,
22,0A42C60C-B515-45E2-8089-115E16F731AB,Communicating And Visualizing Erosion-associated Risks To Infrastructure (CAVERTI),"Soil erosion is a major environmental problem, affecting agriculture, the natural environment and urban areas through sediment-related damage to roads, buildings and infrastructure such as water, gas and electricity supply networks and its impact on flood risk, water quality, loss of the nutrient-rich upper soil layers, sedimentation of waterways and eutrophication of water bodies. There is significant interest amongst stakeholders with whom the project team have engaged in protecting assets such as pipes and gauges for transport and monitoring, where to locate new assets and the costs of sediment removal. These issues highlight the need for a better understanding of where erosion is happening, how to reduce erosion risk and for effective ways of communicating mitigation strategies to stakeholders such as farmers, land managers and policy-makers.
The proposed work will identify the parts of the landscape most susceptible to soil erosion in order to target those areas for mitigation using a new soil-erosion modelling approach developed in two earlier NERC-funded projects. Empirically measureable transport distance and virtual velocity of soil particles are used to account for the difference in water and sediment transport in a scale-independent way. A cell-based and particle-based approach are combined to simulate the movement and 'life history' of individual sediment particles, making it possible to determine where soil eroded from a particular site is located after a specified period of time. The model is thus capable of identifying, for example, where the soil entering a river came from and how long it took to travel from its source site to the river, addressing precisely the need identified by our stakeholders.
The partner organization in this proposal is the Wear Rivers Trust which is responsible for conservation, protection, rehabilitation and improvement of the landscape and watercourses of the whole River Wear catchment. The Trust have identified the problem that silt moving into Brancepeth Beck near Durham represents a direct threat to a range of local infrastructure including roads, bridges, culverts, electricity pylons and buried services and an indirect threat to wider infrastructure downstream such as urban areas, roads, bridges, distribution networks, power generation facilities, sewerage, drainage and communications systems. There is thus considerable interest in mitigation measures that could be introduced on farms in the locality to reduce the risk of silt movement during heavy rainfall events. 
This project will use Brancepeth Beck as a case study in risks to infrastructure due to soil erosion, addressing directly the problem of sediment transport through applying the soil erosion model and developing new decision support tools in partnership with local farmers, the Wear Rivers Trust and other stakeholders. The Decision Support Matrix (DSM) approach will be used, involving the development of a range of visualization and communication tools to help assess and compare the risks associated with different farming practices and explore options to manage runoff. The DSMs will be developed through direct engagement with stakeholders employing a Participatory Action Research approach, ensuring that the examples and language used makes sense to end-users. The DSM approach has proven effective at increasing stakeholders' confidence in making decisions to make landscapes more resilient. DSMs have been taken up widely in the UK by bodies such as the EA and Defra, and have been successfully employed within wider decision support frameworks alongside modelling at multiple scales. 
Engagement with the Wear Rivers Trust and farmers with whom they work indicates a strong interest in reducing soil erosion risk through identifying the source of sediment using models and in developing communication and visualization tools using the DSM approach.",,
23,44EB5807-0933-4A5D-823E-9EF76E549659,Data Science for the Detection of Emerging Music Styles,"When music was still sold on physical carriers such as CDs or LPs, to maximise profits music outlets needed to carefully fill their limited shelf space with the items most likely to bring in the most income, which, assuming equal cost and physical footprint, will be the most popular titles to their clientele. This business model is known as a Blockbuster strategy, and involves heavy investment and promotion of a few select products.

There was some hope that this would change in today's digital economy with minimal overheads for the artists (minimal recording and reproduction costs), retailers (no limits on shelf space, cheap promotion and distribution), and consumers (practically unlimited choice). The expectation was that retail patterns would shift to a business model of selling `less of more', taking the focus away from the elite few and allowing smaller, less well-known artists to prosper. This is known as the theory of the Long Tail (coined by Chris Anderson): while some artists still get the lion's share of the revenue, the tail of less popular music would lengthen and fatten.

Surprisingly, the opposite was found to be true: the tail has become even skinnier, with an even smaller proportion of artists able to make a living from their music. Research by The Harvard business review in 2008 found that 1% of artists account for 32% of total plays on the online radio station Rhapsody, with 10% making up 78% of plays. Similar figures have been quoted by music licensing company PRS music for both illegal peer-to-peer network sharing services and legal downloads, finding for example that 75% of the music stocked by online stores did not find a single buyer.

A well-known explanation for this is given in the book &quot;The Paradox of Choice&quot;, where Barry Schwartz observes that having too many options tends to be paralysing instead of liberating. Applied to the popular music market: as searching for new interesting music comes at a cost to consumers (at least an opportunity cost), they will often play it safe to avoid disappointment: they will either listen to the same old bands over and over again, or at best they will try what is recommended to them by trusted parties (friends, or automatic systems that recommend songs liked by people similar to you). As a result, the rich get richer, and revenue concentrates on the hugely popular few.

This makes it increasingly hard for new music trends to gain a foothold in the music industry. Even if a pioneering band's music has a genuine potential of ultimately appealing to large consumer groups, there is only a small chance that it will ever emerge from the skinny tail of popular music. As a result, creative innovation in popular music is stymied, and new emergent music styles disappear before becoming sustainable.

Thus the following question begs an answer: is it possible to detect emergent music styles at an early stage, in a scalable (and thus automated) way, characterising it in terms of its innovative audio features, demographics of the fan base, and their geographical location. Today, for the first time, all stars necessary for doing this are aligned. We have access on a large scale to the audio of a number of bands of the order of a million (e.g. on SoundCloud), and we have access to their fan base and their properties through social media (e.g. Twitter). The subject of this proposal is to gather this data, and to develop the data mining techniques needed to discover new emerging music styles at a very early stage.

This proposal would thus provide the tools necessary for an entirely new way of recommending music that is able to put in the spotlight music that is truly original, currently budding among a small set of fans with a specified demographic and geographical location. Rather than oppressing new trends (as current recommendation strategies do), it would make it possible to actively promote them, and in this way to give new air to creativity.",,"This proposal has the potential to impact on the following groups:

- The music industry as a whole,
in increasing the efficiency of the music market and the creative economy more generally -- a market that has traditionally been of great relevance to the UK economy and in which the UK has long played a leading role. This will ultimately increase the competitive position of the UK in the international creative economy.

- Music analytics companies (e.g. MusicMetric, uPlaya, SoundOut, Next Big Sound)
can benefit from this research if they integrate the research results into their platforms, in offering a more open-ended way of exploring what is going on in music scene.

- labels and their A&amp;R departments more specifically (major as well as independent labels) as well as companies representing groups of labels (e.g. Merlin Network, The State51 Conspiracy),
in providing them more effective means to search for new market opportunities that would otherwise go unnoticed. This will help them save costs and be more effective at the same time.

- Music players / streamers, music recommendation services (e.g. Spotify, Deezer, We7, Last.fm, iTunes, Nokia Music, etc.),
in making new features possible (e.g. a player / recommendation system recommending music based on a demographic profile and geographical location of the user, instead of just recommending music similar to other music they, their friends, or people similar to them like).

- Companies providing business intelligence and marketing services (e.g. SDL/Alterian, HP/Autonomy, IBM),
in providing an understanding of how demographics relates to music preferences, and this very early on in the lifecycle of a music style. Of course, also for marketing of music specifically our results would be useful. For example, assuming that Liverpool and Manchester have a similar demographic, an emerging music style in Liverpool may well catch on in Manchester as well.

- Music venues, festival organisers,
in giving them original ideas for the organisation of music events and strategically booking bands that will appeal to their clientele.

- Charities and public organisations providing subsidies for music / arts,
in helping them decide where to invest so as to maximise diversity balanced with chances of success.

- Music consumers,
in ensuring a more creative music industry, where new talent is given a fair chance.

- Other vertical markets (besides the music industry) that would benefit from detecting emerging trends from a body of interrelated data, or from finding patterns in relational data more generally.
For example MusicMetric.com is actually a front-end dedicated to the music vertical market of semetric.com, a data analytics company. In a similar way, the data mining results are not limited to this vertical market and will be of use also in other vertical markets. A few examples are:
 * Detecting emerging trends in research publications and collaborations.
 * Detecting emerging trends in the job market.
 * Detecting emerging trends in lifestyle and consumption patterns.
In these examples data will not always be publicly available, but often will be accessible to certain market players."
24,80583049-1C5A-4EC2-87C2-363A9FE43314,Large scale spatio-temporal point processes: novel machine learning methodologies and application to neural multi-electrode arrays.,"Large scale spatio-temporal data sets are becoming increasingly available due to progress in data gathering technology. In this proposal we are concerned with event-based data: data points are spatial and temporal coordinates of an event, as opposed to analogue measurements of a variable. Such data is pervasive in a number of applications, ranging from epidemiology to social sciences, and poses considerable computational issues: the data is intrinsically high dimensional (indeed infinite dimensional if working in a continuous time framework) and nonlinear, and it is often a noisy observation of complex dynamical processes. Scalable data modelling solutions for this data type are urgently needed, and will require novel research in computational statistics and machine learning. 
In this proposal, we will address fundamental methodological problems motivated by an application of great relevance in a biomedical scenario: recordings of neural electrical activity by High Density Multi Electrode Arrays (HD-MEA). These are electronic chips with many (&gt;1000) recording channels, which are used to measure electrical activity in a range of in vitro preparations, and enable simultaneous measurement of the spiking activity of thousands of neurons. This novel technology (commercially developed within the last five years) has the potential to enable scientists to answer fundamental questions on how neurons communicate between each other, as well as having direct translational potential as an effective tool to test in vitro the impact of drug treatment over neuronal function. Providing data modelling tools for such data is challenging: HD-MEA recordings are a prime example of big data (data rates of ~3GB/minute) where complex behaviours rule out simple scalable models.
In this exciting multi-disciplinary project, we propose to treat an HD-MEA data set as a realisation of a spatio-temporal point-process (a random set of points, i.e. neural spikes), and use and develop techniques from Bayesian statistics and machine learning to infer salient dynamical properties of the biophysical process underlying the data. The major challenges which will be addressed are concerned with devising statistical machine learning methods which can accommodate non-linearities and that can scale to the large size of HD-MEA data, while still giving biologically meaningful insights. In particular, we will focus on determining from data the connectivity of the network of neurons, neuron-intrinsic dynamics, and how chemical (e.g. drugs administered to the culture) and other stimuli influence the electrical response and network properties of the culture. Addressing these challenges will entail considerable work on approximate Bayesian inference for large-scale spatio-temporal point processes, generating methodologies which will be general and applicable to many other domains of science and engineering.
The project brings together the machine learning and systems biology expertise of the PI and named RA, the neuroscience expertise of the coI as well as strong collaborative ties with international experimental groups and industrial players, making the team of researchers ideally suited to tackle this challenging project.",,"This project will achieve impact both from delivering widely applicable methodological advances, and through an important application of direct biomedical significance:
- academic beneficiaries will include researchers in computational statistics, machine learning and computational neuroscience, as well as people working in closely related disciplines and applications. The major channels for achieving such impact will be publications, attendance and presentation at conferences/ workshops, and organisation of workshops.
- industrial beneficiaries will include the project industrial partner 3Brain Gmbh, who will directly benefit from the analytical methodologies developed in the project, as well as companies interested in collection and analysis of spatio-temporal data (a burgeoning field involving some major global players such as Google and Microsoft). In the longer term, the improved analytical tools will increase the usefulness of the HD-MEA technology in drug testing and medical research, leading to possible long-term impact on the pharma sector. Impact on the direct application will be achieved through the planned collaboration with the project partner. Broader methodological impact will be achieved through nurturing existing contacts (e.g. the applicant currently holds funding from Microsoft) and through presenting at machine learning conferences which are usually well attended by industrial researchers.
- societal beneficiaries include biomedical practitioners and the general public, who in the long term are likely to indirectly benefit from the availability of strong drug prototyping technologies, as well as from the insights in fundamental neuroscience we are likely to obtain. Furthermore, this research uses mathematical tools to address fundamental question on the working of neural cells which are likely to appeal to the general public in outreach efforts. We will pursue this avenue by participating and instigating outreach activities within the School of Informatics, the University of Edinburgh and more generally by exploiting high profile Edinburgh-based events such as the science festival."
0,DD4B4EEF-AE44-43CD-B621-6486F445EE40,Audio Data Exploration: New Insights and Value,"The &quot;Audio Data Exploration: New Insights and Value&quot; project is a collaboration between Audio Analytic Ltd. and the
Centre for Digital Music &amp; Centre for Intelligent Sensing at Queen Mary University of London (QML). Compared to
mathematical, textual or visual data, audio data has remained largely underexploited and undervalued, and thus represents
an opportunity to grow innovation and to develop new markets. While Automatic Speech Recognition and Music Analysis
are now creating some industrial value, R&amp;D needs to be conducted to tackle the challenges posed by Automatic
Environmental Sound Recognition defined in a broader sense. The newly developed advanced audio data analysis and
modelling techniques will create value across a variety of applicative domains. While proven markets include Professional
Security and Home Security, a range of novel markets can be developed in relation to Multimedia Database Indexing,
Environmental and Industrial Monitoring, the Internet of Things and more. The project will gather the newly developed
audio analysis and modelling techniques into a demonstrator instantiated as a &quot;Personal Audio Space Indexer&quot;.",,"ECONOMIC IMPACTS
Audio Analytic, the lead partner in this project, will directly benefit from the transfer of state-of-the-art academic research on
sound recognition into their product range. This will enable Audio Analytic to reinforce its position as a world leading
supplier of sound recognition applications. (Medium term time scale.) The company is planning to quadruple its size after
entering the consumer market, thus creating new employment in the UK (Medium term).
A wide range of players from of diversity of markets spanning professional security, home security, media indexing,
environmental/industrial monitoring and the internet of things will be able to offer a distinctive feature in the form of reliable
and smarter sound event detection as part of their products and applications. These impacts will come through both
existing and newly developed commercial relationships between Audio Analytic and such companies (Medium to long
term).
SOCIAL IMPACTS
Improved public safety: Members of the public looking to protect their safety and security will benefit from the deployment
of the new smart sound recognition as part of home security solutions and smart city solutions, to protect against events
such as, and not limited to: break-ins (detection of glass break sound, burglar alarm sound), aggressions (gunshots, calls
for help etc.), fire (detection of smoke alarm sound), domestic accidents (detection of SOS keywords, specific accident
sounds) etc. Companies and organisations looking to improve security and safety in the workplace, transport systems or
public spaces will benefit from the availability of solutions for sound-based safety and security monitoring.
Environment and public expense: In the case of accident detection, the faster response time enabled by the system may
mean shorter hospital stays and related reductions in NHS expense and CO2 emissions. In the case of environmental
monitoring: the proposed system will make it possible to compute indicators of environmental health through acoustic
monitoring.
Wellbeing of vulnerable adults: On a longer timescale, vulnerable adults in the home such as the elderly, and their families
or carers, will benefit from future enhancements to include sound monitoring for assistance calls to respond to SOS
keywords, or identification of departure from regular sound activity patterns in the home to provide reassurance and
support. Deaf members of the public may also see their quality of life improved by sound indicators that would describe
particular audio events arising in their environment. While some of these impacts are outside the scope of the current
project, the demonstrator will cover enough general sound recognition cases to enable the long term expansion of market
segments by Audio Analytic.
ADDITIONAL IMPACTS
People: The postdoctoral researcher (PDRA) will benefit through gaining skills in knowledge transfer and application of
research technology in a commercial setting, applicable to their future career. (Short term.)
Transfer of technology best practice: Academic research in the domain of sound recognition will benefit from access to a
real-life platform and real-life data to evaluate novel sound recognition algorithms. This project will set a precedent for best
practice in algorithm evaluation, maths optimisation and transfer of technology between the academia and the industry.
(Medium to long term.)"
1,9770F490-0045-47FE-A60D-C75A6F9EB323,Large-scale Unsupervised Parsing for Resource-Poor Languages,"This project focuses on the automatic induction of grammatical structure from raw text. Automatic inference of the syntax of sentences is an old problem in natural language processing, which originates in studies attempting to build computational models for the way humans learn language.

This problem is still far from being solved. There is yet no fully-fledged computer program that takes raw text and returns a computational representation of its syntax (for example, identifying the noun phrases, the verb phrases, the prepositional phrases, and how they relate to each other in the text).

This research aims to make a major step toward building such a system. The goal is to derive a new algorithm that recovers, at least partially, the syntax of raw text. The algorithm is based on the assumption that words which frequently tend to co-occur should usually be linked, not just semantically, but also syntactically. For example, if the word &quot;deep&quot; often co-occurs with the word &quot;puddle&quot;, the algorithm will assume that &quot;deep&quot; tends to modify the word &quot;puddle.&quot;

The algorithm is based on a new learning paradigm developed in the machine learning community called &quot;spectral learning&quot;. This paradigm has many advantages, most notably, its well-motivated mathematical component. This means that we can derive mathematical proofs that guarantee that the algorithm will be able to learn the syntax of a language if the algorithm is exposed to sufficiently large enough amounts of raw text.

Such proofs are important partially because they explain the learnability of language by humans. If these proofs show that we do not require much data to learn syntax, they can shed light on humans' ability to learn language from (relatively) short exposure to language through their childhood.",,"From the societal/educational perspective:

Language is the main topic of research in NLP, and this research area offers an approachable topic in computer science for such a broad audience. This is especially true for syntactic parsing. Most of us have an intuitive notion of what syntax means, as well as understanding of computer applications. Synthesising these two together should therefore be comprehensible to a young audience, and can potentially give them a window to further inspect ideas about computation and language.

Our goal will be to develop a website that makes all of the material we develop accessible to such a crowd. We will explain the basics of computational parsing to this crowd, and build a demo website that will enable this crowd to experiment with our syntactic parsers in various languages.

From the economic perspective:

The development of our algorithms can be very useful for the language technology industry. Syntactic structures are core algorithms used in natural language analysis and applications, including machine translation, information retrieval, information extraction and question answering.

Availability of parsers for resource-poor languages has the potential to advance such applications for these languages. One of the greatest impediments for creating natural language applications for various languages has been the lack of tools which analyse language at its basic level, such as syntax. Making our parsing tools more mature for companies that develop natural language applications for resource-poor languages can therefore be very valuable for these companies."
2,57C99FC6-DF22-463A-958F-E5056B2B0191,Computational inference of biopathway dynamics and structures,"The mathematical modelling of regulatory interactions and signalling processes in living cells is a growing research area, aiming to elucidate the molecular mechanisms that give rise to complex biological phenomena. Examples include circadian clock models describing how plants predict the length of night and adjust their metabolism to prevent carbon starvation before dawn, or carcinogenesis models aiming to explain how aberrant cellular signalling pathways lead to tumour growth and metastasis. Ambitious current approaches in systems biology aim to develop mechanistic models of the relevant cellular networks, using methods from chemical kinetics and control theory. However, due to the large number of chemical kinetic parameters, inference remains an extremely challenging problem, restricting current applications to small a priori identified model pathways. The objective of the proposed research project is to advance the current state of the art of computational/statistical inference in mechanistic models, with a particular focus on applications in systems biology. To this end, we aim to explore and hone a portfolio of methods combining ideas based on (i) gradient matching, (ii) modularization, (iii) parallel tempering, and (iv) focus statistics. The idea of gradient matching (i) is to avoid a computationally expensive explicit solution of the differential equations and instead infer kinetic parameters that give the best agreement between the gradients predicted from the differential equations and those obtained from the tangent to the interpolant of the data. The aim of modularization (ii) is to decompose a complex system into a collection of simpler weakly coupled subsystems for which inference is less challenging, and then reduce inconsistencies between the subsystems in an iterative manner. Parallel tempering (iii) proceeds by carrying out inference on a set of several increasingly smoothed (tempered) versions of the mismatch function in parallel, as a way to avoid suboptimal local optima. As an alternative to smoothing, we aim to adopt ideas from approximate Bayesian computation (iv) and replace the data in parallel chains by sets of focus statistics to extract relevant patterns from the data. This mimics heuristic procedures that are currently carried out by biological modellers, who aim to find chemical kinetic parameters that match certain signatures in the data, like phase shifts, frequency variations, amplitude alterations, etc. All approaches, in isolation and in combination, aim to reduce the computational complexity at a high level of accuracy, thereby enabling an application of inference in mechanistic models to larger and more complex systems. For regulatory networks whose structure is a priori unknown, we precede the above procedures with novel structure learning algorithms from machine learning, aiming for fast search through network topology space based on abstract models of molecular interactions. For both structure learning with machine learning methods and kinetic parameter inference for mechanistic models we will exploit modern PC clusters for parallel processing. The results of our research will be implemented in a user-friendly software toolbox that will be integrated into GLAMA, a widely used systems biology and polyomic data analysis software package (http://www.brc.dcs.gla.ac.uk/systems/glama/), for maximal impact in the biological end-user community.",,"THE IMMEDIATE TARGET GROUP who will benefit from the proposed research are quantitative biologists working in a variety of disciplines, including (1) pathway medicine, (2) human biomonitoring, (3) molecular plant biology, and (4) ecology. 
1) Researchers in PATHWAY MEDICINE will benefit from the deeper understanding of pathogenesis at the molecular level. In particular, the research carried out in the project will lead to a more thorough insight into the consequences of a dysregulation of the pro-inflammatory Interleukin-6 biopathway, which is a potential causal factor in the development of cardiovascular diseases and diabetes.
2) HUMAN BIOMONITORING provides an important source of information on routes of intake of drugs or potentially toxic chemicals in the human body over a wide range of exposure conditions. The mathematical modelling relies on physiologically-based pharmacokinetic or toxicokinetic differential equation models that describe the transfer and perfusion of chemicals between different organs or lumped tissues. The improvement of parameter inference achieved with our work will enable exposure pathways to be identified more accurately. Our work will therefore contribute to improved toxicokinetic screening and risk assessment, to the benefit of the patient community. 
3) A challenging problem in MOLECULAR PLANT BIOLOGY is to understand the interplay between internal time keeping (circadian regulation) and metabolism in plants. In the last few years, substantial progress has been made to mathematically model the central processes of circadian regulation at the molecular level. By enabling faster parameter inference and model selection of larger systems, the proposed project will provide the tools for elucidating the detailed structure of the molecular regulatory networks and signalling pathways that enable the interplay between circadian and metabolic processes, to the immediate benefit of the molecular plant biology community. The long-term impact will be a better understanding and control of biomass production in plants, with the ultimate objective to improve food security and biofuel production. 
4) ECOLOGICAL SYSTEMS consist of complex interactions among species and their environment, the understanding of which has implications for predicting environmental response to perturbations such as invading species and climate change. However, the revelation of these interactions is not straightforward, nor are the interactions necessarily stable across space and time. By improving the methodology to scale up reliable parameter inference in mechanistic models of diverse species interactions to larger and more complex systems, the proposed project will help ecologists to predict critical regime shifts, i.e. the rapid transition from one stable community structure to another, often ecologically inferior, state. Understanding such regime shifts is particularly critical to assess the impact of climate change on biodiversity, and our work will therefore contribute to the development of more reliable decision support systems on which policy makers can base their risk assessment and adoption of mitigating measures. 
A SECOND TARGET GROUP are statisticians, informaticians and quantitative scientists, as our project contributes to the development of improved inference tools for computational statistics and machine learning. This includes nonparametric Bayesian modelling, the modularisation of complex systems, parallel tempering and annealing, emulation versus calibration, mixing of Markov chains, and approximate computation of Bayes factors.
While the primary focus of our project is systems biology, we note that mathematical modelling with differential equations is highly relevant to several other research areas, including chemical engineering, seismology, and epidemiology. The proposed project will therefore make an important methodological contribution that a variety of other scientific disciplines will profit from."
3,5E2D666A-53A0-4CB5-8FC0-4625A30FA5FD,Combining Qualitative and Quantitative Reasoning for Logic-based Games,"The use of game theoretic techniques in computer science is becoming ever more prevalent. One reason for this is that in many of the systems we want to build, participants cannot be assumed to be benevolent: instead, they must be assumed to be rational agents, acting in pursuit of their own personal goals. For such systems, game theory provides a natural analytical framework. In our work, we are interested in the automated analysis of such systems using techniques for model checking, which over the past two decades have proved to be enormously influential. In model checking, the idea is to express desirable system properties as logical formula, and then to check whether these properties actually hold of the given system. A key problem if we want to extend existing verification techniques to game theoretic settings is that the formalisms used in model checking do not allow us to directly represent the preferences or utilities of players (i.e., their goals). This project is directed at this problem. The basic idea is that we can use a formalism known as Lukasiewicz logic to express the &quot;utility function&quot; for players, which represent their preferences. Lukasiewicz logic is a non-classical, multiple-valued logic which has the attractive property that Lukasiewicz formulae can represent a very rich class of utility functions -- much richer than is possible using classical logic. The project will lay the theoretical groundwork for this new and exciting class of logically-specified games, and has the potential to greatly enrich the class of systems for which logic-based automated analysis techniques can be applied.",,"Although the direct applications of our research are downstream, we
can nevertheless identify several possible beneficiaries, as follows.

* First, our results will benefit that part of the computing R&amp;D
 community who must design and build systems and protocols relating
 to systems in which participants act in their self-interest. This
 is, we believe a large community. For example, the designers of
 online trading software must take economic considerations into
 account when they design their software; the designers of social
 media software and group decision-making software also need to take
 into account the preferences and self-interested behaviour of
 participants.

* Second, our results will benefit the growing international
 community interested in multi-agent systems generally, and in the
 automated analysis and verification of such systems in particular,
 who will gain a better understanding of the logical analysis of game
 theoretic systems. 

* Third, we expect our results to benefit the wider logic
 community: our work will further extend the range of frameworks that
 are amenable to logical analysis, and provide results that others
 can build on, and algorithms that can be refined and extended.

 As noted above, our project will deliver new formal models,
formalisms, and algorithms directed at the automated analysis of
distributed systems containing participants acting in pursuit of
personal goals. The concrete benefits we expect of our project include:

* understanding of the applicability of logic in general, and
 Lukasiewicz logic in particular, for the specification of game theoretic
 settings and protocols; 

* understanding of the kinds of domains for which logic-based game
 theoretic analysis is feasible and valuable; 

* the capability to capture using logic complex game theoretic
 scenarios, far beyond what is currently possible; and 

* the capability to automatically analyse game theoretic scenarios
 and protocols
 using a logical (Lukasiewicz) specification."
4,40E8222D-165F-4308-B112-68F7756240F8,"Poetry by Numbers, Then and Now: Metre, Mathematics, Machines and Manufacture","This project centres on a one-off piece of technology from the 1830s/40s: the Eureka Latin Verse Machine. This device, built to 'compose' in random sequence lines of poetry (each one arranged in Latin hexameters), was conceived and constructed by a Somerset inventor named John Clark. The 'programme' on which Clark's machine works, which had been in existence for over a hundred years when he decided to 'automate' it, consists of a table of letters so arranged that when a person selects from among them following a particular numerical sequence he or she is able to 'manufacture' a line of poetry that is at once 'correct' in terms of its metre, grammar and sense. The Eureka uses a system of wooden staves, metal wires and revolving drums that are activated when the machine is wound up by hand. Once one line of poetry is composed it appears in windows on the machine's front; the line is then scrambled and another appears in its place, ostensibly in random order, this process continuing until the machine runs out of motive power. The project is interested in uncovering and documenting the competencies, methodologies and skill sets needed for the construction of such a device, as well as the extent to which the convergence of these specialisms can be put to productive use in the current day to inform restoration projects relating to Britain's technological heritage. To that end, the project assembles experts from the key disciplines whose knowledge feeds into the working of the Eureka: a specialist in nineteenth-century versification (principal investigator); an expert in Classical studies of the Victorian period; a historian of nineteenth-century mathematics; a mathematician and computer scientist (co-investigator); engineering specialists working at Exeter's Centre for Additive Layer Manufacture (CALM); two conservators; and the archivist for the Alfred Gillett Trust (AGT), which owns the Eureka. This team of experts, co-ordinated by the principal and co-investigator, with other named experts advising on a sub-contractual basis to keep costs to a minimum, will assess the historical object and documents obtaining immediately and peripherally to it (e.g., notes on its construction and on Victorian prosody/programming/mathematics/Classics more generally) with a view to (1) understanding the Eureka's operation, (2) conserving the device itself and (3) returning it (where feasible) to a functional state. Given its age and uniqueness, however, the project will also (4) produce both a virtual and actual replica--the former using up-to-date computer programming, and the latter the procedures of 3-D printing in Exeter's CALM lab--that will allow for display and hands-on operation. A related outcome is the documenting of collaborative methods and manufacturing techniques for potential application in other restoration projects. The knowledge we gain about the use of 3-D printing for the construction of objects relating to the history of science will have transferable use across the museums and heritage sector, providing a model for best practice, as well as a detailed construction template. Once the core work supported by the grant has been accomplished, we envisage an exhibition of the machine, alongside its replicas and related examples of Victorian 'computing' technology, bringing the Eureka, which was famously exhibited in 1845, back to public view. Roughly 170 years later, a new generation can appreciate the collaboration of science and culture, with a fuller awareness of the array of techniques that came together--both then and now--to make possible computer-generated poetry. For this venture, which is likely to fall beyond the immediate timeframe of the grant, we have begun dialogue with the Royal Albert Memorial Museum (RAMM) in Exeter (which already maintains a collection of artefacts relating to Babbage's 'Difference Engine'), as well museums in Somerset (where the Euerka was originally built) and the Science Museum in London.",,"The project generates both academic and economic and societal impact--through its end products but also through its methods and processes, which will be carefully documented with a view to sharing with scholars, members of the heritage sector and the general public. Impact generation can be divided into four main areas: public exhibition and engagement, schools, undergraduates and the heritage and private sectors. 

Public exhibition appeals to the general public with a focus on science and technology in its historical and cultural context. The specific iterations of the Eureka realized through the project provide a focus for exploration of the foundations of computational poetry and language. Furthermore, the technologies involved (e.g., 3D printing and artificial intelligence) have captured the public imagination. Exhibition will fall outside the immediate timeframe of the grant, but we have begun dialogue with, in addition to the Alfred Gillett Trust (current proprietors of the machine), museums in Devon, Somerset and London. Ultimately, exhibition plans will depend on conservation outcomes, and for these reasons we have not factored exhibition plans into the application's Timetable or Resources.

Public engagement designates other ways of reaching a general audience. Given the wide appeal of the cultural and technical topics related to the Eureka, the project will seek to engage popular scientific and history magazines (e.g., New Scientist) and television programmes. Further, to secure the wider impact of the materials we produce, the project website will make available videos, case files, and software, enabling the public to co-create new material based on the project.

The project will invite local schools to view aspects of the restoration and replica construction. Exeter's Computer Science department already runs a series of workshops for schools, which will be extended to include ideas of randomness, language and poetry generation. We will work with the newly-founded Exeter Mathematics School, a sixth-form specialist mathematics school, which has strong links with the University of Exeter, but also with students taking English and Latin at GCSE and Royal Institution mathematics classes held at the university. In addition, the project will engage directly with the Computing at School (CAS) initiative, which seeks to promote and support computer science education in light of the introduction of the new ICT curriculum. Following previous conferences hosted by Exeter, we will co-ordinate another conference towards the end of the project that focuses on John Clark, the Eureka machine, language and chatbots.

Videos from a preliminary examination of the Eureka are already used by the CI in programming courses reaching over 200 undergraduates a year, and the PI presents prosodical documents relating to the Eureka in undergraduate poetry options. The project will also engage a group of undergraduate Engineering students in a final-year project and a final-year Computer Science student in making a 3D virtual simulation.

Of particular relevance to both academic and extra-academic beneficiaries are the methods underpinning the restoration and replication of the Eureka using additive layer manufacturing. The principal beneficiary here will be the heritage sector, particularly in relation to preservation, display and interpretation of historical artefacts since the industrial revolution. We will therefore pursue ways of disseminating the results of our study through professional trade bodies such as the Museum Association and the Institute for Conservation and publications such as Museum Practice. The working methods and techniques may be even more widely applicable, and the project thus presents a prototypical case study of the use of 3D printing for the replication of historical materials, whether by a publically funded museum or private corporation (e.g., a manufacturer of scientific instruments)."
5,B3242BBF-1A92-4859-9655-FFB91511D599,ALOOF: Autonomous Learning of the Meaning of Objects,"When working with and for humans, robots and autonomous systems must know about the objects involved in human activities, e.g. the parts and tools in manufacturing, the professional items used in service applications, and the objects of daily life in assisted living. While great progress has been made in object instance and class recognition, a robot is always limited to knowing about the objects it has been trained to recognize. The goal of ALOOF is to enable robots to exploit the vast amount of knowledge on the Web in order to learn about previously unseen objects and to use this knowledge when acting in the real world. We will develop techniques to allow robots to use the Web to not just learn the appearance of new objects, but also their properties including where they might be found in the robot's environment. 

To achieve our goal, we will provide a mechanism for translating between the representations robots use in their real-world experience and those found on the Web. Our proposed translation mechanism is a meta-modal representation (i.e. a representation which contains and structures representations from other modalities), composed of meta-modal entities and relations between them. A single entity represents a single object type, and is composed of modal features extracted from robot sensors or the Web. The combined features are linked to the semantic properties associated with each entity. The robot's collection of meta-modal entities is organized into a structured ontology, supporting formal reasoning. This representation is complemented with methods for detecting gaps in the knowledge of the robot (i.e. unknown objects and properties), and for planning how to fill these gaps. As the robot's main source of new knowledge will be the Web, we will also contribute techniques for extracting relevant knowledge from Web resources using novel machine reading and computer vision algorithms.

By linking meta-modal representations with the perception and action capabilities of robots, we will achieve an innovative and powerful mix of Web-supported and physically-grounded life-long learning. Our scenario consists of an open-ended domestic setting where robots have to find objects. Our measure of progress will be how many knowledge gaps (i.e. situations where the robot has incomplete information about objects), can be resolved autonomously given specific prior knowledge. We will integrate the results on multiple mobile robots including the MetraLabs SCITOS robot, and the home service robot HOBBIT.",,"ALOOF will change how we work with robots. Today, it is hardly believable that a robot could provide you with information about any object you show to it. After ALOOF we will be able to ask future robots to (1) find an object given a name, (2) find out the name for an unknown object and (3) find out other information about an object. ALOOF technology will enable any Web-connected device (e.g. from smart phones to robots) to learn from the Web, widening the range of applications that can profit from autonomous online knowledge gathering. This could range from driver assistance systems that could learn about locally unique buildings, signs, or sites, to Ambient Assistive Living (AAL) systems. In general, the capability to learn from the Web will become an essential tool for technology that must perform some task in loosely-structured and open-ended environments, thus creating impact (in terms of increased productivity and capability) in a wide range of fields, from supporting independent living to creating new shopping experiences.


In order to encourage the use of robots in new service scenarios, and to support the predicted growth in the service robot market, it is crucial that future service robots are autonomous and robust, and also as cheap and easy to deploy as possible. This cannot be said for current technology, as robots must be trained in advance for all objects they must work with (impossible in open-ended domains, as found in most service scenarios). ALOOF will change this. By using Web learning integrated with situated experience, robots can train themselves to recognize objects as they encounter them, thus minimising deployment overhead. By being founded on an approach that assumes missing knowledge is always part of a problem, ALOOF systems will be uniquely robust to failures in perception, using Web learning to fill in the gaps in their understanding. Successful demonstrations of these technologies will increase the uptake of robots based on UK technology in both targeted industries and beyond, increasing market share and UK competitiveness in the process."
6,4679AC35-15A5-4067-ACB9-0CA55C783097,Train unit scheduling optimisation,"Efficient passenger rail is a key factor of success for the UK economy. Growing and modernising the UK rail infrastructure such as the HS2, Crossrail and many other projects must be complemented by optimised operations planning to maximise its passenger carrying network capacity. 

After a timetable has been finalised, a fleet of train units is scheduled. Each train unit is assigned to serve some train journeys in a sequence and no timetabled journey is left uncovered. Obviously all the train connections for each unit must be feasible, i.e. the unit must be at the right place before its next scheduled departure is due, and the scheduling task is like solving a hugely difficult jigsaw puzzle that the minimum number of train units is to be used. Moreover, some timetabled journeys at peak times may demand more seats than a single train unit can provide. Thus the train units may be purposefully scheduled to overlap in their assignments to achieve the desired combined seat capacity.

Train operating companies are motivated to seek automatic optimised train unit scheduling methods for several reasons. There are very high costs to lease, operate and maintain the train units making them a critical resource for most UK train operating companies, how to spread the train unit resource amongst competing demands is a big challenge. The potential saving in an optimised train unit schedule is very attractive. Good train unit schedules can be derived manually based on experience and local knowledge, but that is usually a very time consuming and tedious process making it impractical to consider many what-if options available to the planners.

Existing research in passenger train unit scheduling is mainly from the Netherlands and Italy, whose models have not included some UK features and have tackled smaller problems than those usually found in the UK. This project builds on recent research in collaboration with ScotRail, which has led to promising results for part of the ScotRail network around Glasgow and Edinburgh. This project aims at yielding fully operable schedules in real life practice to demonstrate the validity and quality of research, and hence further more extensive industrial collaboration is planned.

This 36 month project consists of three parallel work streams. The planned research is grounded on an exact mathematical approach, under which advanced solution techniques and appropriate formulation variations are sought to improve and refine its computational performance. One research fellow will be responsible for this work stream.

While the mathematical approach has superior optimisation power, computational time escalates exponentially to becoming impractical beyond small to medium sized problem instances. In another work stream, the second research fellow will investigate a new method that could make a step change. Recognising that there is a practical limit on how large a problem instance the mathematical optimiser can solve 'comfortably' a heuristics is used to compress and transform, analogous to compression of an image file, the problem instance into a much smaller one for the mathematical solver to be applied. Over a number of cycles, more and more is learnt about the key data points to be retained in the compressed instance whereby the hybridised algorithm would converge to the optimal or very near optimal solution.

In the third work stream, both research fellows will be engaged in activities with our industrial collaborators to ensure that the most realistic model is built, the solution schedules produced are fully operable and testing and evaluation are as thorough as possible. The activities include short placements, regular contacts, on-site testing/evaluation and three seminar workshops that other train companies will also be invited.",,"This research will help train companies to produce optimised operational schedules for their train vehicle units. The scheduling process will be automatic and quick due to the advanced algorithms developed. The human planners therefore will have more time to put effort into trying out a much wider range of planning options as input to the scheduling process. Furthermore, heavy involvement of train companies and rail software and consultancy specialist Tracsis Plc in steering this project and at all the stages (problem modelling/formulation, testing and evaluation) will ensure that real life rather than simplified problems are solved. Therefore through this research, train companies will be able to plan more cost efficient and reliable services. The public will enjoy lower fares and better rail transport than otherwise. The impact in this respect would be especially important when UK passenger rail traffic is predicted to double in the next 15 to 20 years. Ultimately a good rail transport system will help bring down energy consumption and lessen environmental issues leading to a better society.

Network flow models and integer linear programming (ILP) are hugely useful for an extremely wide range of problem areas. The advancements of solution techniques in this research will therefore have significant impacts on the relevant research communities seeking ever faster computation. In particular, the branch-and-price framework incorporating convex hull computation and specialised branching and branch-tree traversal strategies will enable all-integer solutions to be found quickly.

ILP solvers often have the drawback of not performing satisfactorily as the size of the problem instance increases, and the difficulty increases exponentially. While this research will make contributions in directly improving the computational performance of ILP solvers, it will also be deriving a hybridised algorithm using iterative heuristics to compress the problem input in each iteration into a small scale instance before applying the ILP solver. The hybridised algorithm approach will be amenable to simple parallelisation."
7,8884E015-2406-4EA9-A34A-788D7212A2E1,Justified Assessments of Service Provider Reputation,"Justified Assessments of Service Provider Reputation (JASPR) aims to improve the way that services are discovered, selected and used by providing rich, personalised reputation assessments of services with the rationale behind those assessments. It is particularly targeted at giving small and medium-sized enterprises (SMEs) better exposure to large clients by reducing clients' reliance on extensive market histories or opaque online reviews that do not account for personalised needs. While large companies can rely on brand influence to bring clients to their services, it is difficult for SMEs to gain market access, especially newer businesses with little history for clients to draw on. This can mean that an SME is overlooked even when providing a service directly matching the client's needs. From the customer side, the project will allow more intelligent service procurement, based on rich reputation assessments reflecting the actual performance of providers, with less bias from branding and superficial reviews.

More generally, in any service-based system, an accurate assessment of reputation is essential for selecting between alternative providers. Existing methods typically assess reputation on a combination of direct experiences by the client being provided with a service and third party recommendations, with the reputation expressed as a numerical score or probability estimate. They do not allow the opportunity to interrogate an assessment to find out why a particular assessment is made, and so whether it is appropriate to a new service selection requirement, and they exclude from consideration a wealth of information about the context of providers' previous actions that could give useful information to a customer in selecting a service provider. For example, there may be mitigating circumstances for past failures, or a provider may have changed their organisational affiliation. These limitations are of particular significance in marketplaces involving both newer and more established service providers. New providers are often disadvantaged in a marketplace since a single negative review can disproportionately harm their reputation, and customers are unable to accurately assess the risk associated with new providers compared to those that are established. To make richer reputation assessments that take into account the context of past service provisions, this context must be modelled and recorded, and can be described as the provenance of the provision.

In the proposed project, we will use provenance records as a source of information on which a more nuanced reputation mechanism can be based. We will define the supporting algorithms and software infrastructure to allow this rich reputation information to be captured, analysed and presented to clients.",,
8,565CA9EA-96D7-4E30-BCDA-C59792C93125,Justified Assessments of Service Provider Reputation,"Justified Assessments of Service Provider Reputation (JASPR) aims to improve the way that services are discovered, selected and used by providing rich, personalised reputation assessments of services with the rationale behind those assessments. It is particularly targeted at giving small and medium-sized enterprises (SMEs) better exposure to large clients by reducing clients' reliance on extensive market histories or opaque online reviews that do not account for personalised needs. While large companies can rely on brand influence to bring clients to their services, it is difficult for SMEs to gain market access, especially newer businesses with little history for clients to draw on. This can mean that an SME is overlooked even when providing a service directly matching the client's needs. From the customer side, the project will allow more intelligent service procurement, based on rich reputation assessments reflecting the actual performance of providers, with less bias from branding and superficial reviews.

More generally, in any service-based system, an accurate assessment of reputation is essential for selecting between alternative providers. Existing methods typically assess reputation on a combination of direct experiences by the client being provided with a service and third party recommendations, with the reputation expressed as a numerical score or probability estimate. They do not allow the opportunity to interrogate an assessment to find out why a particular assessment is made, and so whether it is appropriate to a new service selection requirement, and they exclude from consideration a wealth of information about the context of providers' previous actions that could give useful information to a customer in selecting a service provider. For example, there may be mitigating circumstances for past failures, or a provider may have changed their organisational affiliation. These limitations are of particular significance in marketplaces involving both newer and more established service providers. New providers are often disadvantaged in a marketplace since a single negative review can disproportionately harm their reputation, and customers are unable to accurately assess the risk associated with new providers compared to those that are established. To make richer reputation assessments that take into account the context of past service provisions, this context must be modelled and recorded, and can be described as the provenance of the provision.

In the proposed project, we will use provenance records as a source of information on which a more nuanced reputation mechanism can be based. We will define the supporting algorithms and software infrastructure to allow this rich reputation information to be captured, analysed and presented to clients.",,"Decisions on which providers to use for particular services, such as suppliers of goods or utilities, are often based on reputation. This favours long-established providers with recognised brands, regardless of whether they provide the most appropriate or best value service. Reputation is often based on past customer reviews or recommendations which, while valuable, has important failings. For example, in the online world every review is typically treated equally (by the review site) regardless of its relevance to the particular need. Also, the aspects of a service influencing a past customer's rating may not be the aspects that are most important to the potential customer.

Our aim is to develop the algorithms and technology that allows decisions on service providers to be based on rich, personalised, relevant reputation information, where less information from past customers is required for reputation assessments because more useful and objective information is derived from each service provision. There are many areas in which the results of the proposed research can be applied, and this is expected to create opportunities for establishing new collaborations with business and industry. The methods developed for provenance-driven reputation can be applied in any situation where services must be selected, and can benefit stakeholders across the system, including: (i) better exposing smaller and newer businesses to potential custom, (ii) enabling organisations needing to make use of a set of providers, drawn from a wide pool of possibilities, to obtain better services tailored to customers' needs, (iii) providing algorithms and technology that can be used by service middleware providers in developing service management infrastructure, and (iv) offering end consumers better and more informed choice of services. Given the widespread adoption of service-oriented systems, these potential applications of the research indicate the strategic importance of the project. There is potential for IP generation, and the appropriate exploitation of IP will be considered as part of the project management.

The scientific and technology outputs of the project are generally applicable, and the involvement of Black Pepper Software (BP) will not only enable us to effectively evaluate the outputs, but will also provide technology impact in the areas of service middleware in general and for the logistics domain specifically. As a software company with extensive experience of service-oriented systems in a range of applications, BP's involvement will promote the adoption of the developed technologies and will expose the project results to industry, both in its own terms and as an exemplar of the value and benefits to be gained. BP are regular contributors to industry conferences and exhibitions and are well placed to support dissemination to industry. Through the case studies, supported by BP, the project will also make specific technology contributions that are applicable to the logistics domain. 

We aim to build new links that can lead to both potential application of the outputs through adoption by industry and to future research proposals with strong industry involvement. A reputation workshop will be organised to which relevant companies, who are the potential users of the research results, will be invited. In the medium term, it is envisaged that this project may result in, for example, technology transfer through new Knowledge Transfer Partnerships."
9,C810A568-A942-4C72-A593-8C3A852539C5,OptiMAM: Optimising Model-Driven Service Design via Stochastic Analysis Methods,"The project focuses on performance analysis and optimisation algorithms for Services Computing. Services Computing is a inter-disciplinary area at the interface between IT and business management that aims at maximising the business efficiency of IT service technologies. Over the last decade, enterprises have embraced services computing through the notion of service-orientation, a design pattern where business functions are organised into self-contained services. Service-orientation is now common, thanks to advancements in web services technologies and languages for business process modelling (e.g., BPMN) and their execution (e.g., WS-BPEL).

Although service-oriented architectures and business process management have been extensively adopted, the ability to optimise the design of the underlying activity workflows remains computationally challenging. As the complexity and layering of services grows, it becomes extremely challenging to establish the correct schedule of operations and the optimal dependencies between resources, activities and services. It is also difficult to understand the sensitivity of the solutions found to variability in execution times and costs, which are unavoidable due to resource contention, design-time uncertainty over the parameters, and potential involvement of humans in the processes.

The project will first develop a model-to-model transformation from a high-level workflow specification (e.g., BPMN) to layered queueing network (LQN) models, a class of stochastic models used for performance analysis. LQNs allow to mathematically analyse the relationships between resources, services and processes. Once in LQN form, the design plan will be analyzed using new stochastic analysis techniques to be developed within the project. Such techniques, which will be the main scientific innovation of the project, will apply for the first time matrix-analytic methods (MAM) to LQN analysis. MAM are queueing analysis techniques that allow to describe complex queueing systems, where service at resources can evolve in phases, similarly to the sequence of activities that are used in the workflows underpinning business processes and service-oriented architectures. Quite surprisingly, MAM techniques have never been applied to performance analysis problems in services computing, to the best of our knowledge. The goal of applying MAM to LQNs is to increase both the efficiency and accuracy of the evaluation of a single design scenario for a service-oriented system design, providing an efficient and accurate analysis method that can be coupled with optimisation for search purposes.",,"The project has both academic and industrial beneficiaries.

Academic beneficiaries

- This research will directly lead to production of knowledge in the areas of service-oriented computing, performance evaluation, optimisation and applied probability. These would benefit the academic communities working in these fields. 
- The project will also contribute to the training of the RA on service-oriented systems and BPMN modelling. These technologies are widely popular on the job market and could help the RA acquire a research position in industry after the end of the project. We plan to involve the RA in both research and dissemination activities, particularly through presentations in conferences or seminars.
- We will also organise 3 invited seminars at Imperial College London with invited experts from EU on service-oriented systems and matrix geometric methods. We will advertise the seminars internally and to targeted colleagues in UK and Europe, particularly in across the performance engineering community where the PI has an established network.

Industrial beneficiaries

- Our research could impact on the optimisation of service-oriented architectures and business processes in industry, thus promoting in the medium term heightened business efficiency and performance. There is a strong demand for such business process management solutions in the UK and internationally.
- Project partner is BOC, a company leader in service-oriented architectures, business process management and related software modelling technologies. BOC's main product is ADONIS, a business process analysis tool for analysis, simulation and evaluation of BPMN models. The tool is adopted in several companies in UK and EU for service-oriented system modelling. As stated in the support letter, BOC is willing to examine the project output for possible integration of our open source algorithms (as external royalty-free libraries) in ADONIS.
- Since we plan to release our results as open source, any company could integrate our results in a business process management or service engineering solution. Contributing to business process management would impact on operational and organisational change. BPMN models can describe various forms of human activities and therefore tools to optimise process structure would immediately foster better organisations. Conversely, contributing to service engineering would mainly impact on technological advancement of services computing.
- Beyond the area of service-oriented systems, the project could benefit the design of component-based software. This is because Line is integrated with the Palladio framework developed by KIT, a popular environment in the model-driven engineering community. This would benefit software architects and process engineers.

Further details, including dissemination and communication plans, are given in the Pathways to impact document."
10,C18F9241-D5F4-4CE2-AD64-246B944119BA,SIPHS: Semantic interpretation of personal health messages for generating public health summaries,"Open online data such as microblogs and discussion board messages have the potential to be an incredibly valuable source of information about health in populations. Such data has been rapidly growing, is low cost, real-time and seems likely to cover a significant proportion of the demographic. To take two examples, PatientsLikeMe has enjoyed 10% growth and now has over 200,000 users covering over 1500 health conditions; the generic Twitter service is expanding at a rate of 30% annually with over 200 million active users. Going beyond simple keyword search and harnessing this data for public health represents both an opportunity and a challenge to natural language processing (NLP). This fellowship proposal is about helping health experts leverage social media for their own clinical and scientific studies through automatic techniques that encode messages according to a machine understandable semantic representation. There are three major challenges this project seeks to address: (1) knowledge brokering: to develop algorithms to identify and code the informal descriptions of conditions, treatments, medications, behaviours and attitudes to standard ontologies such as the UMLS; (2) knowledge management: to create a structured resource of patient vocabulary used in blog texts and link it to existing coding systems; and (3) adding insight to evidence: to work with domain experts to utilize the coded information to automatically generate meaningful summaries for follow up investigation. At the technological level the fellowship seeks to pioneer new methods for NLP and machine learning (ML). Social media remains a challenging area for NLP for a variety of reasons: short de-contextualised messages, high levels of ambiguity/out of vocabulary words, use of slang and an evolving vocabulary, as well as inherent bias towards sensational topics. The fellowship seeks to harness the progress made so far in NLP for social media analysis in the commercial domain and develop it further to provide meaningful public health evidence. One key aspect not previously addressed is in the clinical coding of patient messages. Although knowledge brokering systems exist for clinical and scientific texts (e.g. the NLM's MetaMap), their performance on social media messages has been poor. The fellowship will utilise the rich availability of ontological resources in biomedicine together with ML on annotated message data to disambiguate informal language. Research will also aim to understanding the communicative function of messages, for example whether the message reports direct experience or is related to news, humour or marketing. If these problems are successfully overcome an important barrier to data integration with other types of clinical data will be removed. The advantage of providing health coding for social media reports is its potential for studying very-large scale cohorts and also in real-time early alerting of aberrations. In the fellowship I will research the potential for multi-variate time series alerting from semantically coded features, working with domain experts to evaluate across a range of metrics (e.g. sensitivity, timeliness, false alerting rates). A variety of approaches will be explored to generate real time risk summaries across social media sources. Two real-world applications have been chosen to take this forwards: early alerting for Adverse drug reactions (ADRs) and Infectious disease surveillance (IDS). Project outcomes will include fundamental technologies as well as open source algorithms, data sets and ontology. An exciting aspect of this fellowship is inter-disciplinary collaboration across stakeholders at all levels: scientists, public health experts and industry. Finally, participation will be opened up to the international community through the release of open source data. Colleagues working on social media technologies will be invited to participate in discussions with users at a new challenge evaluation workshop.",,"The SIPHS project aims to revolutionise how health experts leverage personal health evidence for their own clinical and scientific studies through automatic techniques that encode social media messages according to a machine understandable semantic representation. SIPHS will deliver state of the art knowledge extraction solutions for evidence relating to human diseases. This is highly relevant to a range of experts across domains such as public health, pharmacology and molecular biology.

Who will benefit from this research?
1. Public health experts performing infectious disease surveillance (IDS), situation awareness and risk assessment functions will benefit from becoming more efficient and having access to earlier warnings and greater coverage about health threats such as pandemic influenza, chemical/ biological/ radiological/ nuclear (CBRN) terrorist attacks;
2. Researchers and engineers in human language technologies, e-Science and information retrieval will benefit from software tools and data sets that can reliably encode social media messages for clinically important concepts;
3. The pharmaceutical industry and those involved in biotechnology and drug discovery will benefit from having access to a new and extensive database of evidence about adverse drug reactions and potentially novel therapeutic properties for licensed drugs;
4. Life scientists and clinicians involved in translational studies will benefit from having a novel database of evidence about phenotype associations to drugs and human diseases that links to the existing scientific and clinical data infrastructure through networks. As noted in Section 2(b) I reiterate that SIPHS is highly relevant to initiatives such as ELIXIR which coordinates and links European biomedical resources; 
5. The public will benefit from having improved technologies for early detection of health threats and improved understanding about those technologies through the PI's outreach activities, e.g. a public blog, participation in the Cambridge Science Festival, press releases and a Wikipedia page.

How will they benefit?
1. Building on Dr. Collier's existing global public health network, the PI will continue to work directly with public health experts at Public Health England, the CORDS network and at the WHO to deploy the proposed technologies and database. The innovative techniques advocated in this proposal extends proven high throughput techniques developed by the PI which successfully detected A(H1N1). The techniques supplement scarce human expertise, bring in evidence beyond national boundaries and cover segments of the population who may not interact with traditional sensor networks (e.g. patients who may not visit a GP). The novel techniques will be measured against existing human surveillance network standard;
2. The fellowship pioneers new methods for Natural Language Processing (NLP) and Machine Learning (ML) on social media. We propose to develop a novel combination of supervised and semi-supervised approaches on maximally rich NLP features in order to understand the context of personal health messages, ground layman's terms to clinical standards and provide timely alert summaries. Researchers and engineers will benefit from tools, data sets and techniques;
3. The technology in this proposal will help the pharmaceutical industry in the monitoring of patient reports for ADRs as required by EU and national regulations and to reveal novel therapeutics;
4. The database developed through the SIPHS project will generate high visibility in the lifescience and clinical communities. The integration of the different data resources and the automatic analysis of the social media will lead to benefits for the research community and the general public. If the problem of message coding in personal health messages is successfully overcome an important barrier to data integration - for example with data from clinical trials or electronic patient records - will be removed."
11,2F41C06B-718B-4145-8707-3D9492EBB042,Personalized Exploration of Imagery Database,"&quot;I want to see jackets which are stylish, but not too fancy. Say, 70% stylish.&quot;

This project aims to develop new techniques which can significantly improve data browsing experience in online shopping, dating, media recommendations, and many other applications. 

Two very common ways to explore large collections of imagery items, for instance, in online shopping, are to browse a hierarchy of items and to search with textual keywords. The returned results are browsed in lists, typically ordered by popularity. However, popularity is defined across all users as one homogeneous peoples, and users cannot sort by their own subjective criteria, e.g., by their own personal `style' for clothes; What is `stylish' to one person will be passe to another. Furthermore, there is no way to place items on a continuous scale, where the criteria amount for each item is known, e.g., how stylish a particular piece of clothing is to a user. 

Our goal is to develop new techniques which enable users to organize and explore imagery data based on their own subjective criteria at a high semantic level. This is a challenging problem: Many criteria are hard to quantify and a user may not even be able to articulate the criteria. 
We face this challenge by observing that even though users may not be able to specify their criteria quantitatively, or even fully describe them, they are still able to communicate their own notions by providing examples, e.g., &quot;this shoe is cooler than that one&quot;. Our goal is to build an algorithm that arranges a large corpus of visual data according to these examples. Once built, the arranged data can be browsed with an interface that exploits the learned criteria to navigate the continuous scale.

The key contributions of the proposed research will include 1) exploring different modes of user interaction and elaborate on reflecting the resulting knowledge to 2) a new algorithm that, by breaking the limitations of existing approaches, effectively and efficiently learns from user-provided examples and thereby makes personalized data exploration realistic.",,"This project aims to develop new techniques which can significantly improve data browsing experience by enabling users to organize data collections based on their own subjective, semantic-level criteria. If successful, these techniques can be directly used in many applications that use/require data exploration. In particular, online shopping will be the biggest beneficiary of this research. The UK is one of the largest and ever growing markets in online shopping: As of November 2013, online shopping increased 10% over the year 2012 and revenues reached a monthly record of &pound;10.1 billion. Specific application scenarios include
1) Finding the perfect chair for a user's room from thousands of possibilities across different styles, by ranking a small subset of chairs by preference.
2) Finding a tasty wine (in terms of personal preference) by trying a small number of different wines: Even novice users could easily establish their shopping portfolio, without having to gain knowledge of domain-specific keywords such as `Tannin' and `Tartaric Acid'.
This research will therefore, contribute to qualitative and quantitative growth of the online shopping market in the UK by attracting users with a significantly improved experience.

Online shopping is only an example of many data browsing applications. Additional application examples are
- Online dating (&pound;170 million market in the UK): Attractiveness is personal-- ranking a small subset of people would help to tailor the personal matches you received by a personal appearance attractiveness scale.
- Media recommendation (e.g., Netflix, iTunes, Kindle): Ranking a few films, albums, or books in an online library to quickly organize the entire collection by your preference.

Our strategy for realizing such a browsing system is to make advances in machine learning, computer vision, and HCI. In particular, one of key technical contributions of this project will be an improved algorithm for semi-supervised learning. Since semi-supervised learning is nowadays extensively used in diverse areas including data mining, social networks analysis, robotics, and genetics, in the long-term, this project will impact on a much broader range of economic and academic activities which may benefit from these techniques.

Furthermore, our techniques will have a societal impact by helping people save time: if successful, users would no longer have to spend hours hunting for just the right item. Users could sort by their particular criteria, and have a good chance of finding it within a small amount of time. Collectively, this saves people a lot of time, and makes the shopping experience or more generally, the data browsing experience, much more pleasant."
12,FA2CA31A-77F2-414B-97C9-B8279143CE42,DILiGENt: Domain-Independent Language Generation,"We propose a two year project to develop a novel data-driven methodology to rapidly create high quality NLG systems for new domains, by combining recent advances in three domains: 
(1) advances in statistical models for NLG, 
(2) crowdsourcing methods for natural language data collection, which have shown first promising results in related fields, such as Machine Translation, and 
(3) recently developed imitation learning algorithms for structured prediction. 

The project team combines expertise of two leading research groups in these areas:
At Heriot-Watt University, we recently demonstrated the potential for data-driven statistical NLG in limited domains. In order to make this framework domain-independent we will leverage recent machine learning models, developed by researchers at the University College London. These models learn by imitating the actions a human expert would perform to generate NL utterances, which we collect via a tightly integrated crowdsourcing procedure. The outcome of this work is a framework which will allow the rapid development of NLG systems for new domains, and thus accelerate the impact NLG technology has on the market. 

We will showcase this framework on a dataset provided by the BBC, where we address the problem of generating weather reports for over 20,000 individual locations. Currently, the BBC website features only 10 reports written by meteorologists. Each of these reports covers a rather large area of the country (e.g. East of England), and thus of little interest to their users who are usually interested in the weather in a particular location (e.g. Norwich).

In a second, more ambitious step, we will explore how this framework scales to more complex interactive dialogue settings, where generation has to account for discourse phenomena, such as long-distance discourse relations or syntactic coordination. This will be evaluated in a shared task challenge for generation in interactive systems, hosted by Heriot-Watt University.

In sum, this project will further our understanding of domain-independent language generation, as well as deliver substantial and novel resources to support future research in this area (in the forms of code and data), and practical implementations of NLG systems in a wide-range of domains, from weather reports to natural language interfaces.",,"(Please note that part of the following text is taken form the National Importance section of Part II)

Machine Learning for Natural Language Processing (NLP) is an area which has begun to generate positive economic impact, and start-ups and large companies such as Microsoft, Amazon, and Yahoo! are making substantial, new investments in this field. The UK has one of the highest proportion of world-leading NLG research centres. However, in contrast to other areas of NLP, the UK is still under-represented in using machine learning approaches for NLG. This proposal will help to strengthen this research strand in the UK.

From an end-user importance point of view, this work will be of interest to a wide range of businesses and companies in the UK (see letters of support). NLG creates more flexible output for interactive natural language interfaces. Repetitive linguistic output is one of the main problems for intelligent personal assistants (see for example Apple's Siri or Google Voice). As such, NLG technology has a strong potential for commercialisation, as the successful company ARRIA NLG has demonstrated. 

The overall aim of this research - the rapid development of NLG systems for new domains - is to accelerate the impact that NLG technologies have on the market. To this end, we will assess our framework in a real-world applications, for example generating local weather reports for the BBC website. Furthermore, we will actively seek ways to commercialise this research, such as patenting and licensing of the framework that we develop.

From a societal importance point of view, NLG technology directly contributes to addressing key UK societal challenges: text simplification and data-to-text systems will be increasingly used to enable easy access to information across society. This will remove barriers to the use and understanding of information from large volumes of data. NLG systems have also been used in modern health technology, especially in the areas of personalised and localised healthcare."
13,230A1009-CA07-4CD4-B506-7D1B24E0FE1B,"Developmental algorithms for robotics: Understanding the world of objects, interactions and tools.","This research project is a psychologically-inspired investigation of an analogy of infant play as the central mechanism for autonomous, self-motivated robots that learn the local physics of their world.

We note that infants and children at play exhibit exactly the kind of autonomous learning that would be very desirable in robotics. Infant play has a major role in the acquisition of new skills and cognitive growth. Noticing that early infants spend hours in play, we have designed a computer analogy of infant play and this project is an in-depth investigation into the use of play as a means of building subjective understanding of the physics of the local world.

The project will implement a play generator algorithm on an iCub humanoid robot and perform experiments with a wide range of scenarios involving varieties of objects. This includes playing solitarily with objects to learn their properties, and interactive play with a human participant. We also include experiments with tool use (using one object as a tool for acting on another) to investigate how objects may become extensions of self.

A panel of selected scientific experts on infants and play will provide their psychological expertise throughout the project and will also assist with the design of a series of matching experiments that will compare results from the robot model with those from selected psychological experiments on infants.

The data from the experiments will be analysed and interpreted to shed light on a set of scientific issues. When we report on the results we will also extract some general principles for robot learning through play. We will examine the applicability of these principles in new robotic and intelligent systems developments. For example, we anticipate particular applications in areas such as assistive technology and home care where the re-programming of mass-produced systems is not feasible. We believe technology with a developmental approach will have wide implications and provide an alternative to &quot;building robots&quot; by establishing the idea of &quot;developing robots&quot; for applications.",,"The scientific output of this project will produce new algorithms and methods for a form of robot learning based on development. The knowledge and understanding gained will be interpreted in terms of principles for autonomous operation and the safe handling of novel experiences. Apart from the academic beneficiaries, we see scope for considerable societal and economic impacts.

There are three immediate fields that will benefit: (1) the robotics and associated manufacturing industries; (2) the healthcare, medical, and domestic care sectors; and (3) science education and public awareness.

(1) Industrial robotics is now a mature field but most robot devices in real-world environments are still controlled by either fixed programs or human operators. Examples include robot assistants, search and rescue robots, and surveillance and military robots. Unlike factory robots, it is often the case that real-world robots cannot be programmed to cover every task contingency. Each new task scenario demands knowledge appropriate to that specific case and it would be desirable for the robot to accumulate this knowledge rather than rely on the operator every time. Our results will address how this can be achieved and how operators may shape or train robots to improve their performance, thus achieving some operational autonomy, at least for part of the task. Such autonomy is very desirable for reducing operator stress and costs, increasing functionality, and extending application areas. This could provide a technological advance that has major benefits for robot product innovators, commercial robotics companies, and application developers.

(2) Assistive technology is a current world priority and a major challenge, driven by the demographic shift towards care for the aging population. This crucial need for flexible, adaptive technology involves all kinds of assistive, service, healthcare and domestic applications. As for (1) these are all real-world robotics scenarios that require flexibility in coping with the physical world, and the capability to adapt to the many and varied domestic needs and personal environments. Providing some autonomy in such domestic support devices should have a considerable impact on the quality of people's lives and bring real benefits to a range of organisations both public (e.g. NHS) and private sector (service and technology providers). The benefits will include; releasing demand for care staff, increasing the independence of users, generating new products and service industry possibilities, and increasing UK competitiveness in the service and healthcare domains.

Our project partners, QinetiQ, will be actively engaged in identifying new application opportunities in both the above fields. They are especially interested in new ways of task configuring, calibrating and controlling robotic systems that are cheaper to implement and more robust.

(3) Our use of the local Technocamps organisation (http://www.technocamps.com/) will require new materials to be prepared for school pupils aged 11-19. The Technocamps staff will prepare these and this will result in the availability of several workpackages of material. These will have wide applicability for public awareness of science promotions and will be made openly available."
14,3F733FF4-A286-4B77-9CC2-FE058AD8D5EC,Adaptive Automated Scientific Laboratory,"Our proposal integrates the scientific method with 21st century automation technology, with the goal of making scientific discovery more efficient (cheaper, faster, better). A &quot;Robot Scientist&quot; is a physically implemented laboratory automation system that exploits techniques from the field of artificial intelligence to execute cycles of scientific experimentation. Our vision is that within 10 years many scientific discoveries will be made by teams of human and robot scientists, and that such collaborations between human and robot scientists will produce scientific knowledge more efficiently than either could alone. In this way the productivity of science will be increased, leading to societal benefits: better food security, better medicines, etc. The Physics Nobel Laureate Frank Wilczek has predicted that the best scientist in one hundred years time will be a machine. The proposed project aims to take that prediction several steps closer.

We will develop the AdaLab (an Adaptive Automated Scientific Laboratory) framework for semi-automated and automated knowledge discovery by teams of human and robot scientists. This framework will integrate and advance a number of ICT methodologies: knowledge representation, ontology engineering, semantic technologies, machine learning, bioinformatics, and automated experimentation (robot scientists). We will evaluate the AdaLab framework on an important real-world application in cell biology with biomedical relevance to cancer and ageing. The core of AdaLab will be generic.

The expected project outputs include:

- An AdaLab demonstrated to be greater than 20% more efficient at discovering scientific knowledge (within a limited scientific domain) than human scientists alone.
- A novel ontology for modelling uncertain knowledge that supports all aspects of the proposed AdaLab framework.
- The first ever communication mechanism between human and robot scientists that standardises modes of communication, information exchange protocols, and the content of typical messages. 
- New machine learning methods for the generation and efficient testing of complex scientific hypotheses that are twice as efficient at selecting experiments as the best current methods.
- A significant advance in the state-of-the-art in automating scientific discovery that demonstrates its scalability to problems an order of magnitude more complex than currently possible.
- Novel biomedical knowledge about cell biology relevant to cancer and ageing.
 - A strengthened interdisciplinary research community that crosses the boundaries between multiple ICT disciplines, laboratory automation, and biology.

All outputs produced by the project will be made publicly available by the end of the project.",,"The proposed project has high potential for significant technological, economical, and societal impacts. This potential impact of robot scientists has been widely recognised, e.g. in the Nature editorial of 15.1.04 their potential synergistic collaboration with human scientists is stressed, &quot;an automated system that designs its own experiments will benefit young molecular geneticists&quot;; a reviewer of our article in Science (King et al, 2009) stated that the work was of &quot;historical significance&quot;; and Time magazine named it the 4th most significant scientific advance of 2009. The proposed project principally extends the previous work by developing a (semi-) automated framework for scientific discoveries by teams of human and robot scientists (AdaLab), making it far more applicable to general biomedical research. 

The AdaLab framework will contribute to realising Europe's 2020 strategy for smart, sustainable and inclusive growth. The project would contribute to the future development of (semi)-automated laboratories across Europe and wider. These intelligent laboratories have the potential to speed up the technological progress. Our cautious estimate is that the exploitation of the AdaLab framework in scientific laboratories would increase the efficiency of laboratory experimentation by 20% (see section 2). Such an increase would lead to more scientific discoveries, better technological solutions, and new products. 

Science is the greatest generator of economic wealth (through developments in technology), and the greatest driver of better health (through development in biomedical science). Therefore all EU citizens will potentially benefit from the proposed research. For example new better drugs could be delivered to the market faster and cheaper. Currently, ~25Billion euros is spent annually within the EU on pharmaceutical research. Most of this is spent on late-stage trials (which are less amenable to automation), but conservatively estimating that 10% is amenable to the AdaLab framework, then a 20% efficiency gain would result in savings of ~0.5Billion euros per annum."
15,34DE1848-99E9-44B6-9585-5998D33D6093,Adaptive Automated Scientific Laboratory,"Our proposal integrates the scientific method with 21st century automation technology, with the goal of making scientific discovery more efficient (cheaper, faster, better). A &quot;Robot Scientist&quot; is a physically implemented laboratory automation system that exploits techniques from the field of artificial intelligence to execute cycles of scientific experimentation. Our vision is that within 10 years many scientific discoveries will be made by teams of human and robot scientists, and that such collaborations between human and robot scientists will produce scientific knowledge more efficiently than either could alone. In this way the productivity of science will be increased, leading to societal benefits: better food security, better medicines, etc. The Physics Nobel Laureate Frank Wilczek has predicted that the best scientist in one hundred years time will be a machine. The proposed project aims to take that prediction several steps closer.

We will develop the AdaLab (an Adaptive Automated Scientific Laboratory) framework for semi-automated and automated knowledge discovery by teams of human and robot scientists. This framework will integrate and advance a number of ICT methodologies: knowledge representation, ontology engineering, semantic technologies, machine learning, bioinformatics, and automated experimentation (robot scientists). We will evaluate the AdaLab framework on an important real-world application in cell biology with biomedical relevance to cancer and ageing. The core of AdaLab will be generic.

The expected project outputs include:

- An AdaLab demonstrated to be greater than 20% more efficient at discovering scientific knowledge (within a limited scientific domain) than human scientists alone.
- A novel ontology for modelling uncertain knowledge that supports all aspects of the proposed AdaLab framework.
- The first ever communication mechanism between human and robot scientists that standardises modes of communication, information exchange protocols, and the content of typical messages. 
- New machine learning methods for the generation and efficient testing of complex scientific hypotheses that are twice as efficient at selecting experiments as the best current methods.
- A significant advance in the state-of-the-art in automating scientific discovery that demonstrates its scalability to problems an order of magnitude more complex than currently possible.
- Novel biomedical knowledge about cell biology relevant to cancer and ageing.
 - A strengthened interdisciplinary research community that crosses the boundaries between multiple ICT disciplines, laboratory automation, and biology.

All outputs produced by the project will be made publicly available by the end of the project.",,
16,8796CF17-3376-45C5-BC0D-CFF9661B9D6B,Rigorous Runtime Analysis of Bio-Inspired Computing,"Bio-Inspired Search Heuristics (BISHs) are general purpose randomized search heuristics (RSHs). Well known BISHs are Evolutionary Algorithms, Ant Colony Optimisation and Artificial Immune Systems. They have been applied successfully to combinatorial optimization in many fields. However, their computational complexity is far from being understood in depth. In this project the mathematical methodology will be developed to reveal where the real power of BISHs is in comparison with the traditional problem-specific algorithms. The project impacts the field of BISHs in several ways. A feature that distinguishes BISHs from most other algorithms is their population of individuals that simultaneously explore the search space. The first objective is to explain the performance of realistic BISHs for well-known combinatorial optimization problems through runtime analyses, highlighting the relationships between the solution quality and the exploration capabilities of the population. The second objective is to theoretically explain how BISHs can take advantage of the parallelisation available inherently in new technologies to achieve the population diversity required to produce solutions of higher quality in shorter time. The third objective of this project is to create a mathematical basis to explain the working principles of Genetic Programming (GP) and allow the effective and efficient self-evolution of computer programs. The fourth objective is to devise a suitable computational complexity model for the problem classification of BISHs. The enlargement of the established computational complexity picture with BISH complexity classes will enable the understanding of the relationships between traditional problem-specific algorithms and BISHs. Through industrial collaborators, the final objective is the direct exploitation of the theoretical results in real-world applications related to the combinatorial optimization problems studied in this project.",,"Although this project is theoretical in nature, it has significant benefits to practical applications of bio-inspired search heuristics (BISHs). Bio-Inspired Algorithms are applied to numerous real-world applications that involve an optimisation process. 
These are wide spread. Industrial beneficiaries include the engineering sector, the manufacturing sector, supply chain and logistics sector, the health sector, etc. All of these sectors have optimisation problems that share significant similarities with the combinatorial optimisation problems studied in this proposal. Major companies in these sectors that apply the considered bio-inspired algorithms include Rolls Royce, Honda, BMW, Network Rail, ST-Microelectronics, BT, TGV trains etc. 
Unfortunately, most of the claims about realistic EAs and any other bio-inspired algorithms are only justified by experimental work. Based on experimental results, it is difficult to understand whether a good performance of a bio-inspired heuristic
is due to the settings of the algorithms, the experimental setup or the inherent power of the bio-inspired heuristic itself. Hence, understanding where the real power of these algorithms lies is a key challenge towards future industrial development and economic success.

It is specifically noted in the 'Big Data Revolution and Energy-Efficient computing' section of the Eight Great Technologies document that &quot;We also have a particular opportunity in energy efficient computing. IT is an increasingly heavy user of energy ... Energy-use is driven by the number of calculations. Poor quality software come with a high energy cost. Smart algorithms which get to a result with less effort need less energy&quot;. This project will contribute directly to this goal in the context of bio-inspired computing, since studying the number of calculations required to reach a solution of a given quality is the main measure of performance considered in our work. In fact, the gained insights and guidance will lead to the design and application of more efficient variants of the algorithms and to better solutions to the problems.

Through our industrial partners (i.e., AT&amp;T, Network Rail, ACRC and CERCIA), the theoretical results obtained in this project will be directly exploited in real applications. This will allow faster dissemination rather than just relying on the scientific
publications of the highest quality that will be produced. Furthermore, the close collaboration with the industrial partners will allow the development of benchmark problems that capture characteristics present in real-world optimisation problems, thus feeding back into theory."
17,14E89B22-BBF6-41D6-B9B8-EE76492B8CAE,Babble: domain-general methods for learning natural spoken dialogue systems,"The demand for future conversational speech technologies is estimated to reach a market value of $3 billion by 2020 (Grand View Research, 2014). Our proposed technology will provide vital foundations and impetus for the rapid development of a next-generation of naturally interactive conversational interfaces with deep language understanding, in areas as diverse as healthcare, human-robot interaction, wearables, home automation, education, games, and assistive technologies.

Future conversational speech interfaces should allow users to interact with machines using everyday spontaneous language to achieve everyday needs. A commercial example with quite basic capabilities is Apple's Siri. However, even today's limited speech interfaces are very difficult and time-consuming to develop for new applications: their key components currently need to be tailor-made by experts for specific application domains, relying either on hand-written rules or statistical methods that depend on large amounts of expensive, domain-specific, human-annotated dialogue data. The components thus produced are of little or no use for any new application domain, resulting in expensive and time-consuming development cycles.

One key underlying reason for this status quo is that for spoken dialogue, general, scalable methods for natural language understanding (NLU), dialogue management (DM), and language generation (NLG) are not yet available. Current domain-general methods for language processing are sentence-based and so perform fairly well for processing written text, but they quickly run into difficulties in the case of spoken dialogue, because ordinary conversation is highly fragmentary and incremental: it naturally happens word-by-word, rather than sentence-by-sentence. Real conversation happens bit by bit, using half-starts, suggested add-ons, pauses, interruptions, and corrections -- without respecting the boundaries of sentences. And it is precisely these properties that contribute to the feeling of being engaged in a normal, natural conversation, which current state-of-the-art speech interfaces fail to produce.

We propose to solve these two problems together, by for the first time: 

(1) combining domain-general, incremental, and scalable approaches to NLU, DM, and NLG;

(2) developing machine learning algorithms to automatically create working speech interfaces from data, using (1). 

We propose a new method &quot;BABBLE&quot; in which speech systems can be trained to interact naturally with humans, much like a child who experiments with new combinations of words to discover their usefulness (though doing this offline to avoid annoying real users while doing so!). 
 
BABBLE will be deployed as a developer kit and as mobile speech Apps for public use and engagement, and will also generate large dialogue data sets for scientific and industry use.

This new method will not require expensive data annotation or expert developers, leading to easy creation of new speech interfaces that advance the state-of-the-art in interacting more naturally, and therefore more successfully and engagingly with users. 

New advances have been made in key areas relevant to this proposal: incremental grammars, formal semantic models of dialogue, and sample-efficient machine learning methods. The opportunity to combine and develop these approaches has arisen only recently, and now makes major advances in spoken dialogue technology possible.",,"There are 3 main arenas of impact for this work: commercial, societal, and academic. 
The major societal impact of this work is in widening access to information technology, and making it more efficient and engaging for users through the use of spoken conversation.
The core technology for natural conversational speech interfaces developed here has the potential to impact on several important groups of users: 

* Internet users, mobile App users, and wearable technology users via improved conversational interfaces for control and search;
* Visually impaired individuals via better speech interfaces; 
* The elderly and disabled, via automated assisted independent living using dialogue-based interaction;
* Users of immersive virtual reality interfaces (using speech for control and search);
* Computer Game players, e.g. via improved dialogue-based interaction with virtual characters; 
* Users of educational technologies, using conversational virtual characters for learning;
* People interacting with robots (spoken Human-Robot Interaction).

In all of the above areas, the important economic impact of the work is also in lowering costs for industry through the automation of speech interface development -- a central objective of this proposal. Companies will benefit through the lowering or removal of costs associated with hiring expert system developers and data annotators. These advances will make speech interfaces a more affordable technology, deployable more rapidly in wider contexts.
Academic and industrial researchers will also benefit via the provision of new, open resources for conversational system development.

To realise the full impact of the research, we will therefore focus on the following pathways:

* BABBLE mobile Apps: public engagement with the developed systems released as speech Apps on smartphones, mobile devices, and wearables (like Siri and Google Now);

* Open data releases: the large anonymised spoken dialogue data-sets generated by the project will be of interest to academic researchers and industry;

* BABBLE Toolkit: software and tools developed will be released for academic and industrial use;

* Interdisciplinary publications and research visits: combining research from two previously disconnected communities: wide-coverage grammars of Natural Language and statistical approaches to automated dialogue systems;

* Demonstrations: showcasing the BABBLE technology at events such as the Scottish Informatics and Computer Science Alliance (SICSA) demofests and EC ICT events, which bring together academia with industrial developers;

* Robotarium demos: showcasing BABBLE systems for human-robot interaction applications at the Robotarium (EPSRC Infrastructure Grant, 2013);

* Impact workshop: organised at the end of the project to amplify these avenues to impact, inviting researchers from both academia and industry;

* Spin-out company: the BABBLE technology will be integrated in to a spin-out speech technology company which is currently being developed at Heriot-Watt by the PI, under an &quot;Impact Acceleration&quot; grant.

To reach the general public as well as interested academic and industry researchers, we will also use a range of social media such as Twitter and YouTube, as we have done in the past with success for a number of our projects, as well as using traditional websites. We will also further develop our contacts with press and media, which have lead to a number of newspaper, radio, and television outputs describing the PI's research. Our advisory board member Dr. Matthew Purver, founder and Chief Data Scientist of Chatterbox, will advise us on technology transfer and commercialisation of dialogue and language technology (see letters of support)."
18,44191F33-803E-417F-839B-E24CA71876DC,Robustness-as-evolvability: building a dynamic control plane with Software-Defined Networking,"Highly available information networks are an increasingly essential component of the modern society. Targeted attacks are a key threat to the availability of these networks. These attacks exploit weak components in network infrastructure and attack them, triggering side-effects that harm the ultimate victim. Targeted attacks are carried out using highly distributed attacker networks called botnets comprising between thousands and hundreds of thousands of compromised computers. A key feature is that botnets are programmable allowing the attacker to adapt to evolve and adapt to defences developed by infrastructure providers. However current network infrastructure is largely static and hence cannot adapt to a fast evolving attacker.

To design effective responses, a programmable network infrastructure enabling large-scale cooperation is necessary. Our research will create a new form of secure network infrastructure which detects targeted attacks on itself. It then automatically restructures the infrastructure to maximise attack resilience. Finally, it self-verifies whether global properties of safety and correctness can be assured even though each part of the infrastructure only has a local view of the world.

Our research will examine techniques to collect and merge inferences across distributed vantage points within a network whilst minimising risks to user privacy from data-aggregation using novel privacy techniques. We make a start on addressing the risks introduced by programmability itself, by developing smart assurance techniques that can verify evidence of good intention before the infrastructure is reprogrammed.

We set three fundamental design objectives for our design: 
(1) Automated and seamless restructuring of network infrastructure to withstand attacks aimed at strategic targets on the infrastructure.

(2) A measurement system that allows dynamic allocation of resources and fine control over the manner, location, frequency, and intensity of data collected at each monitoring location on the infrastructure.

(3) Assurance of safety and compliance to sound principles of structural resilience when infrastructure is reprogrammed.

Our aim is to develop future network defences based on a smart and evolving network infrastructure.",,
19,3C8260C8-029C-4495-91E2-65C5E4988755,Foundations of Opinion Formation in Autonomous Systems,"The project concerns how groups of partially informed and self-interested agents (e.g., humans, robots), which are faced with a common problem, take a collective decision by exchanging their individual opinions to, possibly, reach a consensus. It aims at understanding how processes of opinion formation in groups behave and how they can be engineered in groups of artificial agents, like robots. 

The project capitalizes on techniques developed in the social and economic sciences, applying them to the artificial intelligence setting. It extends the state-of-the-art in the application of voting theory to artificial intelligence, addressing the process of opinion formation, and lays the theoretical groundwork for the development of collective decision-making techniques in autonomous systems.",,"The long term vision of the project is to enable advanced collective decision-making capabilities in autonomous systems. 

Given the theoretical focus of the project, its short and mid-term impact are likely to be exclusively academic. The project aims at developing a critical mass of theoretical results for a principled understanding of opinion formation in collective decision-making settings involving autonomous systems. The achievement of this aim will be disseminated primarily in AI venues, but venues in the social and economic sciences will also be targeted capitalizing on the support offered by the project partner, the Amsterdam Center for Law and Economics (ACLE).

The mid-term impact of the project will more naturally shift towards applications, even though remaining in an academic context. It will be facilitated by the Center for Autonomous Systems Technology (CAST) of the host institution. The project will benefit from the direct feedback with researchers active in this center, in particular on robotics, and will provide a natural pathway for the project's theoretical findings to benefit research in applications. 

Autonomous systems and multi-agent systems are currently a thriving area of research and development. The long-term impact that these systems, in all their incarnations (robots, autonomous vehicles, trading agents, intelligent sensors, and the like) are expected to have on the future of society is vast. The project contributes to their understanding and development, helping to shape their realization."
20,B153B16B-2923-4E40-9831-B31EF203BDE5,The Alan Turing Institute,"The work of the Alan Turing Institute will enable knowledge and predictions to be extracted from large-scale and diverse digital data. It will bring together the best people, organisations and technologies in data science for the development of foundational theory, methodologies and algorithms. These will inform scientific and technological discoveries, create new business opportunities, accelerate solutions to global challenges, inform policy-making, and improve the environment, health and infrastructure of the world in an 'Age of Algorithms'.",,The Institute will bring together leaders in advanced mathematics and computing science from the five founding universities and other partners. Its work is expected to encompass a wide range of scientific disciplines and be relevant to a large number of business sectors.
21,E3D9699E-BC50-48C8-A31B-003871DD668A,CSIT 2,"From the outset, CSIT's vision has been to establish a Global Innovation Hub for Cyber Security in order to promote growth
in this strategically important sector of the UK economy. There have been major achievements in the past five years during
CSIT's Phase 1. The Centre has grown to over 80 people and is now one of the largest academic centres of its type
internationally, and is increasingly recognised as a leader in cyber security technology research. CSIT has created a
unique 'Open Innovation' model with Industry and Government that is successfully 'bridging the gap' to economic impact.
CSIT has numerous International partnerships and research links including several FP7 and Horizon 2020 collaborative
research projects. CSIT has spun-out 3 new ventures, supported numerous start-ups and SMEs scale and nucleated the
emergence of a new cyber security cluster in Belfast (Over 900 new jobs announced in the last 5 years).
CSIT has delivered against Phase 1 and commits to scale activity and impact, raising the bar on innovation and growth
across the UK. The vision and strategy for Phase 2 can be summarised by, 'continuing to establish a global innovation hub
for cyber security, raising the bar on growth'. At the heart of this strategy CSIT will not lose focus on the Phase 1 innovation
hub model (accelerating new value creation, new venture creation and building capacity), as this is the engine from which
increased and future translation is possible. CSIT's plan will increase inputs to this growth engine by feeding the innovation
hub with partner innovators as well as CSIT's own innovation. CSIT will also extend activities to further supply skills,
training and people to the UK cyber security industry. CSIT's plan will also increase outputs from the growth engine by
formalising a Cyber PreAccelerator Programme. CSIT will also increase scale-up support for identified high growth
companies and provide leadership for the cyber security cluster that emerged from, and has been nucleated by CSIT. In
CSIT Phase 2 Research will be developed under the theme &quot;Securing our Digital Tomorrow&quot; creating the new technologies
needed for the seamless integration of electronic security into future Smart Cities and the Internet of Things (IoT).
Research Excellence will continue to be a focus within CSIT. CSIT Innovation and IP will be developed through various
Innovation Programmes, some of which will be core funded and some of which will be competitively won externally with
collaborative partners. CSIT has world-leading research expertise in a wide range of security research areas. With this
expertise, and through longer-term speculative PhD research, CSIT will address the security requirements of future Smart
City systems. However, from previous experience, CSIT realises resources should not be spread too thinly on such a vast
range of research topics. Therefore, CSIT has chosen to select three research programmes, it believes to be the most
exciting and innovative, with the most potential for commercial exploitation. Dedicated teams of researchers, PhD students
and engineers will drive the development of these programmes which will be reviewed annually in conjunction with our
industry partners and senior academic board members. The selected Research and Innovation Programme Themes
identified at the outset are: Secure Ubiquitous Networking, Device Authentication and Security Analytics and Autonomous
Sensor Security.
Cyber security is still an emerging discipline but one that is growing rapidly. This is presenting exciting opportunities for
research, new business and economic impact. CSIT is in a strong position to make further and significant contributions in
all of these aspects and help position the UK in terms of international research reputation as well as scaling economic
growth.",,"CSIT's unique capacity and capability in security intelligence for Smart Cities and IoT and strong collaborative partnerships
primarily through its open innovation model, ideally position CSIT to support UK economic growth in this sector. CSIT has
successfully demonstrated its awareness of key markets and players in the cyber security technology domain. Partnerships
established with CSIT Members, CSIT Associates, collaborative research partners (including FP7 and Horizon 2020) and
the emerging Belfast Cyber Cluster shows the relevance of CSIT's strategic engagements.
Over the next 5 years, CSIT will support significant progress towards increased market share and a new &pound;1Bn industry in
the UK, by: -
Supporting innovators create new ventures Targeting scale-up support to help companies grow business. Supporting the
creation of new jobs in the sector.
With an overall vision of, establishing a global innovation hub for cyber security and raising the bar on growth, CSIT will not
lose focus on the innovation hub, as this is the engine from which increased and future translation is possible. The
innovation hub has 3 key priorities.
Accelerating new value creation. Key to this strategic objectives are, the Open Innovation Model of Industry Membership to
inform the development of novel cyber security technologies with significant market potential (CSIT has 9 full member
companies, 14 associate members with a pipeline of additional potential members) and CSIT's permanent engineers who
work alongside academic researchers assisting with knowledge transfer, articulating capability, producing rapid prototypes
(proof-of-concept demonstrators) and understanding the drivers of industry.
Creating New Ventures. CSIT will continue to support UKTI and Invest NI in attracting new businesses in the form of FDI.
CSIT will also continue to develop concept ventures, giving due consideration to potential for CSIT delivery of the
technology without needing to 'spin-out' (i.e. licensed and supported directly via CSIT).
Building Capacity. CSIT will continue to spillover staff and students for the industry and also having a capacity of expertise
and skills within CSIT that can be accessed by partner companies to build new products and services and win new
business by accessing that resource within CSIT. Since 2009 15 PhDs engaged in CSIT research have graduated. Around
50% have taken up jobs in industry, the other 50% continuing to engage in research e.g. as PDRAs at CSIT or at
universities elsewhere. A further 41 students are now at various stages in their PhDs with a steady pipeline of 10+ PhDs
expected to graduate each year. As part of its commitment to CSIT Phase 2, Queen's has agreed to support a further 8
PhDs per annum. In addition, CSIT, in collaboration with colleagues at Queen's Institute for the Study of Conflict and Social
Justice, has recently been successful in obtaining &pound;1.05M from the Leverhulme Trust to fund a Doctoral Training
programme entitled the 'Leverhulme Interdisciplinary Network on Cybersecurity and Society - LINCS'. This will fund an
additional 30 PhDs over the next 7 years. This is in addition to PhDs supported by organisations such as GCHQ (currently
3) and by industry.
CSIT's plan to raise the bar on translation over the next five years will scale CSIT's Innovation Hub model (Accelerating
Value Creation, New Venture Creation and Building Capacity) for the benefit of the whole of the UK. CSIT will increase
activity with partner innovators (from startups, SMEs, government agencies and academia). CSIT will also increase the
supply of skills, training and people to the UK cyber security industry (as mentioned above). CSIT will also deliver a Cyber
PreAccelerator Program, increase scale-up support for partners identified with high growth potential across the UK, and
provide leadership for the emerging Belfast Cyber Cluster nucleated by CSIT."
22,09B4E060-AE65-435F-8856-5D2D0A5C32C1,Open Domain Statistical Spoken Dialogue Systems,"Spoken Dialogue Systems (SDS) encompass the technologies required to build effective man-machine interfaces which depend primarily on voice. To date they have mostly been deployed in telephone-based call centre applications such as banking, billing queries and travel information and they are built using hand-crafted rules.

The recent introduction of Apple Siri and Google Now has moved voice-based interfaces into the main-stream. These virtual personal assistants (VPAs) offer the potential to revolutionise the way we interact with machines, and they open the way to properly control and manage the emerging Internet of Things - the rapidly growing network of smart devices which lack any form of conventional user interface. However, current personal assistants are built using the same technology as limited domain spoken dialogue systems. They are not capable of sustaining conversational dialogues except within the selected limited domains which they have been explicitly programmed to handle.

Very recent work on statistical SDS has demonstrated that it is not only possible for such a system to adapt and improve performance within the domain for which it has been designed but it is also possible for the system to automatically extend its coverage to include new, hitherto unseen concepts. This suggests that it should be possible to build on the progress achieved in the development of limited domain statistical SDS to design a radically new form of spoken dialogue system (and hence VPA) which is able to extend and adapt with use to cover an ever-wider range of conversational topics. The design of such a system is the focus of this research proposal.

The key idea is to integrate the latest statistical dialogue technology into a wide coverage knowledge graph (such as freebase) which contains not only ontological information about entities but also the operations that can be applied to those entities (e.g. find flight information, book a hotel room, buy an ebook, etc. ).

The implementation of a single monolithic spoken dialogue system capable of interpreting and responding to every conceivable user request is simply not practicable. Hence, rather than simply trying to broaden the coverage of existing SDS, a novel distributed system architecture is proposed with three key features:

1. the three essential components of an SDS (semantic decoder, dialogue manager and response generator) are distributed across the knowledge-graph. In essence, every node in the graph has the capability to recognise when it is being referred to and have the capability to respond appropriately.

2. when the user speaks, all semantic decoders are listening, based on the activation levels of the decoder outputs, a topic tracker identifies which concept is in focus and activates its dialogue policy.

3. all components are statistical enabling them to be adapted automatically on-line using unsupervised adaptation. Data sparsity is managed by ensuring that the top level nodes in the class hierarchy have well-trained components. Initially, lower level more specialised concepts simply inherit the required statistical models from their super-classes. As the system interacts with users and more data is collected, lower level components acquire sufficient data to train their own dedicated statistical models.

The end result is a system that continually learns on-line. It starts with a limited and stilted conversational style, but the more it is used, the more fluent it becomes, and as users explore new topics, the system learns to adapt and extend its capability to handle those new topics. Since many users can be using the system simultaneously, learning can be fast and capable of accommodating live updates of the underlying data, all of which are characteristics that a virtual personal assistant must have to be genuinely useful.",,"The principal goal of this research is to extend the theory and practice of spoken dialogue systems to support conversational interaction in unrestricted open domains. This enabling technology is critical to the development of accessible and widely available general purpose human computer interfaces, especially virtual personal assistants (VPAs).

VPAs offer the potential to revolutionise the way we interact with machines. They are being introduced to the public via smart phones, but they are actually independent -- all that they really need is an audio channel to a remote server via the internet. This is a disruptive technology with a relatively low barrier to entry and high impact. VPAs have the potential to change not only the way we interact with machines, but also the infrastructure and economic models that underly much of the digital economy since they provide an opportunity to capture users in much the same way that Facebook and Amazon try to capture their users today. This work therefore has the potential to have a direct impact on UK competitiveness.

VPAs will also become essential because speech is the only way to properly control and manage the emerging Internet of Things -- the rapidly growing network of smart devices for which conventional user interfaces are either ergonomically difficult (e.g. Apple iWatch, Google glass) or inappropriate (e.g. home devices such as thermostats, fridges, etc.). Furthermore, users will need all of these devices to be integrated into their highly personalised digital worlds, with a consistent single-point of contact. VPAs are the obvious way to achieve this. Given the UK's dependence on service industry and the knowledge economy, it is essential that it has the technology and expertise to compete in this space.

In order to ensure that the research outputs of this project can be exploited to the benefit of the UK, the Cambridge Dialogue Group is working closely with a Cambridge-based SME called VocalIQ Ltd (www.vocaliq.com) in which Cambridge University is a major share holder. VocalIQ is developing automatic self-learning spoken interfaces for applications in a variety of areas including automobiles, home automation and education. VocalIQ will collaborate on system development, provide advice on commercial deployment issues and provide access to a platform to allow the prototype system to be tested on real users. This direct interaction with a local SME should ensure that the benefits of the research outputs are realised during and soon after project completion.

Through recent and current research projects, the group also has working collaborations with Yahoo Iberia (Mika), Toshiba Cambridge Research Laboratory (Stylianou) and General Motors Advanced Technical Centre (Tzirkel-Hancock).

The two members of research staff working directly on the project will further enhance their skills in machine learning, natural language processing, human-computer interaction and the system skills needed to make complex real-time systems accessible to large segments of the public. There will also be 3 research students associated with the project. All will develop similar skills which will eventually feed into the workforce.

Finally, Cambridge will be launching a new MPhil in Machine Learning and Speech and Language Technology in October 2015 and this project will provide a catalyst for Masters projects and eventually for further PhD projects."
23,53368443-8B03-4CCE-B505-334D8AA2E47A,Predicting musical choices using computational models of cognitive and neural processing,"Music consumption has shifted dramatically in recent years towards streaming from vast music libraries, overloading the user with the enormity of possible musical choices. This new landscape makes it imperative to develop intelligent tools to help listeners choose music to listen to. Research in music technology has traditionally followed a pure engineering approach, which has taken the field some way. However, progress is being hindered by the lack of a robust, scientifically grounded model of the listener, which can be used to inform digital music players (e.g., iTunes, Spotify, Last FM) about users' preferences for selecting music.

The proposed research addresses this gap by developing the scientific knowledge needed to create computational models which can predict listeners' musical choices from features of the music and electrical brain responses recorded using Electroencephalography (EEG). The principal idea is to develop a scientific understanding of the psychological and neural processes involved when a listener chooses music to listen to. The hypothesis is that accurate predictions of a listener's musical choices can be made using a combination of psychological principles, musical features and electrical brain responses recorded from the scalp. This research has two foundations: first, to conduct listener studies to identify those psychological principles, musical features and brain responses; and second, to use that knowledge to build a computational model that predicts a listener's choice of music.

The modelling approach includes three components to capture features of music that have an impact on musical choices. The first component uses acoustic features such as dissonance and temporal regularity extracted from the audio using signal processing methods. The second component takes a higher-level cognitive approach, extracting measures of complexity using information-theoretic models based on note-level representations of music. The third component extracts the emotional intentions of the music from affective textual analysis of the lyrical content. Understanding the exact nature and weighting of these features and how they impact on musical choice requires the detailed examination of the choices that listeners make when listening to music.

Therefore, investigating the behaviour of actual listeners is central to this research and two studies will be performed. The first focuses on collecting EEG data while participants listen to musical excerpts and select them for future listening. Machine learning methods will be used to predict the listeners' decisions using features of the time-varying neural response recorded prior to the choice being made. The purpose of the second user study is to collect data for predict modelling of choices from features of the music itself and attributes of the listener. This will involve a larger range of musical excerpts and a wider range of listeners than is practical for the EEG study. The objective is to understand the psychological processes involved in musical choice and to use this knowledge to refine, parameterise and optimise the computational models of musical choice.

The final stage of the research will develop an integrated predictive model of musical choice by combining predictive models using the musical signal with those making predictions from the neural signal. The development of such an integrated model is highly innovative. The advent within the last two years of affordable, multi-channel, wireless EEG headsets for the consumer market makes the possibility of using these devices to control media players and other interactive systems a real possibility. Therefore, the time is ripe to combine research in neuroscience, music cognition and machine learning, reflecting the unique interdisciplinary expertise of the PI, to understand the mapping between neural signals, musical structure and song selections.",,"By investigating and modelling the psychological and neural processes involved in musical preference and choice, this project holds benefits to several groups beyond academic researchers, both in the UK and internationally. These include:

1. Commercial Private Sector:
* Digital music software industry: understanding the psychological and neural processes involved in listeners making musical choices, will help the digital music industry develop better software for supporting and enhancing those choices, especially since this research will implement computational models that aim to predict choice;
* Advertising industry: Understanding the psychological and neural principles of musical preference and decision making can help the advertising industry understand how to use music most effectively for marketing products; 
* Musicians and concert organisers: who can use the principles of musical choice to help design better concert programmes and playlists for releases.

2. Public and third sectors:
* music educators: understanding of the psychological principles involved in musical preference and choice can facilitate the selection of repertoire and help design software for inspiring young children in musical education
* Independent organisations promoting STEM (science, technology, engineering and maths) to young people and underrepresented groups, by using music to generate interest in STEM careers.

3. The wider public:
* musical listeners will benefit from better software tools that match their intuitive preferences when making musical choices;
* Students and school children in technical, musical and psychological fields can benefit by creating links between scientific, technical and musical domains to better understand these disciplines.

The ways in which this research will target these beneficiaries is outlined in the Pathways to Impact document. For academic beneficiaries see Section 2 in the Case for Support."
24,2900CE8C-43EE-4135-92ED-996A056286D4,Enriching Metabolic PATHwaY models with evidence from the literature (EMPATHY),"In order to understand living systems, biologists have taken to generating predictive models of the system, allowing them to run computational experiments that reduce the number of more traditional, lab-based experiments that would previously be necessary to gain such an understanding. This approach follows that which is now commonplace in engineering, in which, for instance, aeronautical engineers will develop sophisticated models of aircraft and test safety aspects of the proposed design in a computer, long before developing the aircraft itself (or even putting it in a wind tunnel).

This biological modelling approach is named &quot;systems biology&quot; and has been employed successfully in a number of areas. The focus of this proposal is in modelling metabolism. Metabolism is the collection of interconnected chemical reactions that allow cells to extract energy and material from the nutrients that they consume and to grow. All free-living organisms necessarily have such metabolic systems. Thus, modelling human metabolism will allow us to understand the human body's healthy state, for instance as a function of ageing, and aid in the design of chemicals (whether nutrients or drugs) that can maintain human health.

In a similar vein, metabolic modelling is also being used in the development of cell factories, which are able to produce industrially relevant chemicals, which are commonly produced by the chemical industry through more traditional means, and often involve the use of oil as a feedstock. This approach (known as fermentation or &quot;industrial biotechnology&quot;) is not new - we have been fermenting yeast cells to produce alcohol for thousands of years - but traditional fermentation improvements, lasting decades in the case of penicillins, involved random mutation and selection, often coupled to the incorporation of harmful 'passenger' mutations. However, recent research has shown that metabolic network modelling methods provide a rational approach, both for mature fermentations and for new ones such as bio-isoprene for sustainable car tyre production. Thus, these methods have great value for the sustainable bioproduction of important substances, such as biofuels and fine chemicals.

Metabolic modelling therefore has much promise for health and environmental sustainability in this coming century. However, much of the information necessary for the building of these models is held in textbooks, patents and scientific journals, and large teams of researchers are required to search for, judge and extract this information before including it in the models. Thus, the traditional development of such models currently follows (and requires) a time consuming and expensive manual process. Modern methods allow this to be automated.

This process of extracting information from the literature can be greatly facilitated by the application of the methods of text mining. Text mining applies sophisticated algorithms to recognise relevant terms and sentences buried in text, and can be trained to recognise those passages of text within a large number of documents that may be relevant to a given application.

In this work, we will utilise text mining to extract information necessary for the construction of metabolic network models from the large number of scientific articles that are published daily. The results of these analyses will be presented to model developers, who will judge and extract this information to develop existing metabolic models further. A specific easy-to-use web application will be developed in order to allow a multiple users to contribute towards this model building process, irrespective of their background and previous experience of computational model building.

The results of this work will be more complete metabolic models, which will allow researchers to improve understanding of metabolism in a range of organisms, and therefore use this increased knowledge in applications of health and environmental sustainability.","Recently, the development of genome-scale metabolic models, and their analysis through constraint-based modelling, has increased dramatically, and has been applied to research in human health, drug discovery and biotechnology. These models provide computational and mathematical representations of metabolism in a wide range of organisms, allowing in silico predictions of metabolic processes.

Our recent work has automated the construction of draft models for over 2600 organisms from pathway databases. While providing a valuable starting point, these draft models require further manual curation, as current databases lack the coverage of metabolism required to produce detailed, predictive models. The curation process continues to be a time consuming and expensive affair, driven by the need to extract manually the missing details of metabolic processes from literature. Recent reconstruction efforts that have led to high quality models - such as those that we undertook in the development of yeast and human consensus metabolic networks - are heavily reliant on manual mining of literature.

This project attempts to reduce greatly the time and expense devoted to manual literature mining by developing infrastructure to support literature-driven model construction. This will be achieved by the introduction of an integrated model development environment to enable users to undertake this process.

Crucial to this proposal is the integration of bespoke text mining approaches, which will extract relevant passages from publications, and present them to developers as they expand and refine models. In addition to supporting the model development process itself, users will also be able to provide feedback on text mining suggestions, which will be used to improve their relevance. The task of generating large-scale models for any given organism, including the extraction of biochemical knowledge from literature, will thereby become closer to one which we can fully automate.","Although a variety of genome-scale metabolic networks exist, even the most mature are very far from being complete. This project will facilitate the development of genome-scale metabolic reconstructions through the use of advanced text mining approaches. It requires software to support the activity, which will also be created in this project.

Who will benefit?

The beneficiaries of this research are scientists and teams of scientists that use the computational modelling of metabolism as part of their research. This includes academic researchers, research students, and scientists from industries such as pharmaceuticals, biotechnology, agriculture, cosmetics, health, and fermentation and industrial biotechnology generally. The outcome of the work, enhanced metabolic network reconstructions of a host of organisms, will benefit research in a range of fields, including human health and ageing, and biotechnological approaches to the development of biofuels and high-value chemicals.

How will they benefit?

The benefits from the outputs of this research will impact the way in which the beneficiaries carry out modelling of metabolic networks to perform in silico experiments. This is an important part of all systems approaches. The actual reconstruction of human metabolism will benefit pharma, as it will help in identifying targets for new drugs. Since we will have developed an improved map of human metabolism, with specificity to various cell types, this will be an invaluable resource for the replacement, refinement and reduction of research using animals (3Rs), where computational modelling can be carried out relative to human cells rather than laboratory animals - this is extremely important to the cosmetic industry given the EU-wide ban that is now in place. The public at large will also benefit from the enhanced metabolic reconstructions, as they will provide an avenue to develop personalised nutrition, exercise and other aspects of a healthy life.

While many 'chassis' organisms are being developed for various aspects of industrial biotechnology, because of the tools available Saccharomyces cerevisiae and Escherichia coli will continue to play major roles. We thus intend to ensure that we drive developments in these organisms in particular.

As well as these, the software that will be developed will allow researchers to create metabolic reconstructions of any organism of interest. The software will allow for distributed (online) network reconstruction jamborees, and may be used to coordinate community model construction efforts for those organisms. UK industries adopting the metabolic reconstructions will increase their research effectiveness and thus this will contribute to their competitiveness (such as the IPA partner Unilever). We note that this resource will be of special interest to the household products and cosmetic industries since they are now banned from using laboratory animals to develop their products and to test their effectiveness and potential toxicity. Improved accurate metabolic networks of a range of organisms are an invaluable resource for that purpose."
0,535A8EDD-23BA-4B14-BBE6-9591699F7E0D,The Collective of Transform Ensembles (COTE) for Time Series Classification,"Time series classification is the problem of trying to predict an outcome based on a series of ordered data. So, for example, if we take a series of electronic readings from a sample of meat, the classification problem could be to determine whether that sample is pure beef or whether it has been adulterated with some other meat. Alternatively, if we have a series of electricity usage, the classification problem could be to determine which type of device generated those readings. Time series classification problems arise in all areas of science, and we have worked on problems involving ECG and EEG data, chemical concentration readings, astronomical measurements, otolith outlines, electricity usage, food spectrographs, hand and bone radiograph data and mutant worm motion. The algorithm we have developed to do this, The Collective of Transform Ensembles (COTE), is significantly better than any other technique proposed in the literature (when assessed on 80 data sets used in the literature). This project looks to improve COTE further and to apply it to three problem domains of genuine importance to society. In collaboration with Imperial, we will look at classifying Caenorhabditis elegans via motion traces. C. elegans is a nematode worm commonly used as a model organism in the study of genetics. We will help develop an automated classifier for C. elegans mutant types based on their motion, with the objective of identifying genes that regulate appetite. This classifier will automate a task previously done manually at great cost and will uncover conserved regulators of appetite in a model organism in which functional dissection is possible at the level of behaviour, neural circuitry, and fat storage. In the long term, this may give insights into the genetic component of human obesity.
Working closely with the Institute of Food Research (IFR), we will attempt to solve two problems involving classifying food types by their molecular spectra (infrared, IR, and nuclear magnetic resonance, NMR). The first problem involves classifying meat type. The horse meat scandal of 2012/3 has shown that there is an urgent need to increase current authenticity testing regimes for meat. IFR have been working closely with a company called Oxford Instruments to develop a new low-cost, bench-top spectrometer called the Pulsar for rapid screening of meat. We will collaborate with IFR to find the best algorithms for performing this classification. The second problem aims to find non-destructive ways for testing whether the content of intact spirits bottles is genuine or fake. Forged alcohol is commonplace, and in recent years there has been an increasing number of serious injuries and even deaths from the consumption of illegally produced spirits. The development of sensor technology to detect this type of fraud would thus have great societal value, and the collaboration with Oxford Instruments offers the potential for the development of portable scanners for product verification.
Our third case study involves classifying electric devices from smart meter data. Currently 25% of the United Kingdom's greenhouse gasses are accounted for by domestic energy consumption, such as heating, lighting and appliance use. The government has committed to an 80% reduction of CO2 emissions by 2050, and to meet this is requiring the installation of smart energy meters in every household to promote energy saving. The primary output of this investment of billions of pounds in technology will be enormous quantities of data relating to electricity usage. Understanding and intelligently using this data will be crucial if we are to meet the emissions target. We will focus on one part of the analysis, which is the problem of determining whether we can automatically classify the nature of the device(s) currently consuming electricity at any point in time. This is a necessary first step in better understanding household practices, which is essential for reducing usage.",,
1,EAED961A-27E6-42CF-AD0E-9F646C387AEC,The Collective of Transform Ensembles (COTE) for Time Series Classification,"Time series classification is the problem of trying to predict an outcome based on a series of ordered data. So, for example, if we take a series of electronic readings from a sample of meat, the classification problem could be to determine whether that sample is pure beef or whether it has been adulterated with some other meat. Alternatively, if we have a series of electricity usage, the classification problem could be to determine which type of device generated those readings. Time series classification problems arise in all areas of science, and we have worked on problems involving ECG and EEG data, chemical concentration readings, astronomical measurements, otolith outlines, electricity usage, food spectrographs, hand and bone radiograph data and mutant worm motion. The algorithm we have developed to do this, The Collective of Transform Ensembles (COTE), is significantly better than any other technique proposed in the literature (when assessed on 80 data sets used in the literature). This project looks to improve COTE further and to apply it to three problem domains of genuine importance to society. In collaboration with Imperial, we will look at classifying Caenorhabditis elegans via motion traces. C. elegans is a nematode worm commonly used as a model organism in the study of genetics. We will help develop an automated classifier for C. elegans mutant types based on their motion, with the objective of identifying genes that regulate appetite. This classifier will automate a task previously done manually at great cost and will uncover conserved regulators of appetite in a model organism in which functional dissection is possible at the level of behaviour, neural circuitry, and fat storage. In the long term, this may give insights into the genetic component of human obesity.
Working closely with the Institute of Food Research (IFR), we will attempt to solve two problems involving classifying food types by their molecular spectra (infrared, IR, and nuclear magnetic resonance, NMR). The first problem involves classifying meat type. The horse meat scandal of 2012/3 has shown that there is an urgent need to increase current authenticity testing regimes for meat. IFR have been working closely with a company called Oxford Instruments to develop a new low-cost, bench-top spectrometer called the Pulsar for rapid screening of meat. We will collaborate with IFR to find the best algorithms for performing this classification. The second problem aims to find non-destructive ways for testing whether the content of intact spirits bottles is genuine or fake. Forged alcohol is commonplace, and in recent years there has been an increasing number of serious injuries and even deaths from the consumption of illegally produced spirits. The development of sensor technology to detect this type of fraud would thus have great societal value, and the collaboration with Oxford Instruments offers the potential for the development of portable scanners for product verification.
Our third case study involves classifying electric devices from smart meter data. Currently 25% of the United Kingdom's greenhouse gasses are accounted for by domestic energy consumption, such as heating, lighting and appliance use. The government has committed to an 80% reduction of CO2 emissions by 2050, and to meet this is requiring the installation of smart energy meters in every household to promote energy saving. The primary output of this investment of billions of pounds in technology will be enormous quantities of data relating to electricity usage. Understanding and intelligently using this data will be crucial if we are to meet the emissions target. We will focus on one part of the analysis, which is the problem of determining whether we can automatically classify the nature of the device(s) currently consuming electricity at any point in time. This is a necessary first step in better understanding household practices, which is essential for reducing usage.",,"We have chosen our case studies to demonstrate the breadth of domains in which time series classification arises and we hope these will act as a catalyst for other biological, food and climate scientists to work with us and/or our code. The investigators on this project have a strong track record of working with industry, and we aim to exploit our research to have a direct impact.

The work with Institute of Food research has perhaps the greatest potential for immediate impact on society and the economy. The horsemeat scandal shook the public confidence in the sector and the complexity in the international market for meat make it hard to guard against further occurrences. Devices like O.I.s Pulsar offer a cost effective mechanism for screening against contamination. If we can find a better algorithm for classification there is a simple and direct path to implementation within Pulsar. Forged alcohol is commonplace, and cases vary from simple economic crimes through to fraud with serious health implications. In recent years there has been an increasing number of serious injuries and even deaths from the consumption of poor-quality, illegally produced spirits. The development of sensor technology to detect this type of fraud would thus have great societal value, and the collaboration with Oxford Instruments offers the potential for the development of commercial hardware to facilitate the usage of the algorithms our research produces. Improving Pulsar and developing a new product will both have a positive economic and societal impact. Devices like Pulsar help with the public engagement with science, as demonstrated by its appearance on the BBC1 program Ripoff Britain http://youtu.be/t8zWLat8NQ0.

The collaborative research with Imperial is part of the important drive to understand the genetic components of obesity. Model species are useful in this respect as it is possible to directly connect behaviour to genetics in a reproducible way. Hence, if we can automatically detect worms that are exhibiting aberrant behaviour, we can then determine what mutations caused it. Conversely, we can cause mutations in the worm then observe behaviour. Both of these tasks require a laborious, manual identification of mutants. This project will not be involved with performing the experiments. We will instead help look at the best ways of automating this time consuming task. 

Smart meters will soon be in all of our homes collecting detailed data on our electricity usage. This massive investment in technology must yield a significant reduction in our carbon footprint to justify the cost. The key to altering patterns of consumer behaviour is providing useful and relevant information. This in turn requires the ability to extract knowledge from the raw data. We will concentrate on the problem of identifying the nature of devices being used in a household. This offers the potential for constructing more complex models of behaviour based on combined device usage which in turn may lead to more informative advice on how to modify behaviour."
2,C12AA019-4EA5-4122-8222-FA1487A6D2D8,Integrated software solution for the 3-dimensional capture and analysis of footwear evidence,"Footwear impressions provide a source of evidence within a range of criminal investigations including gathering of criminal intelligence. A potential suspect will leave foot or footwear impressions en route to, at and while exiting, a crime scene. This not only allows a sequence of events to be determined but may also link a suspect to a scene if their footwear is distinctive, for example as a consequence of damage or wear. Indoors crime scene officers deal mainly with two-dimensional traces; impressions left by a foot tracking mud, blood or other bodily fluids. Three-dimensional tracks, the proverbial footprint in the flower bed, are common at outdoor scenes. Traditional methods for the collection of three-dimensional traces consist of photography and casting, supported by two-dimensional pattern recognition that can type a footwear sole to a particular make or model of shoe. Three-dimensional imaging is now available as an alternative or complementary option, particularly as algorithms for digital photogrammetry have improved dramatically in recent years allowing easy operational deployment. No expensive three-dimensional scanners are required, only that a crime scene photographer take a few moments to collect additional oblique photographs of footwear impression. Consequently, three-dimensional analyses of footwear impressions are now already possible at a routine operational level, but remain the exception rather than the norm.

As part of previous NERC grant NE/H004246/1 into ancient footprints we developed a range of methods and freeware to facilitate the three-dimensional capture and analysis of footprints. Engaging with police as part of the Impact Plan demonstrated the interest and potential of such methods to enhance the analysis of three-dimensional trace evidence especially by allowing statistical analysis of differences between tracks and/or footwear. Currently comparison is done primarily via visual inspection rather than by quantitative and statistical comparison. Working with our Project Partners we propose to employ a software engineer to draw on this research, practice and existing code to create a single integrated software application for the capture, analysis and presentation of three-dimensional footwear evidence which will allow routine operational deployment by police and forensic agencies both in the UK and overseas. This will change the fundamental cost-benefit ratio associated with the collection of this type of evidence, such that three-dimensional imaging can become the norm rather than the exception. Software of this sort needs to be available to all parties involved in forensic jurisprudence - the defence as well as the prosecution - without handicap of cost. As such our proposed software will be made available as freeware rather than commercialised, which will also assist with user adoption. This knowledge translation has the potential to contribute to criminal investigations and in the safeguarding of society.",,"This project arises directly from the Impact Plan associated with NERC NE/H004246/1 on ancient footprints one of the objectives of which was to raise awareness of three-dimensional imaging of tracks with law enforcement agencies. The information and feedback obtained via this route in particular from the Metropolitan Police led directly to this proposed Innovation Project. The core outcome for law enforcement and forensic agencies will be to change the cost-benefit ratio around the routine operational use of three-dimensional imaging of footwear evidence. This we believe will in turn lead to an increased use of such trace evidence in crime detection, prevention and intelligence gathering thereby benefiting society at large. This will be evidenced by rapid user adoption of the freeware in the first instance (interim impact) and secondly by feedback from the user community to document the actual impact on crime detection and prevention. A clause within the freeware license will require users to provide feedback on request for impact monitoring purposes. In terms of reach our initial target is UK police and forensic agencies, however the reach is not limited geographically and we will subsequently market and target our freeware at overseas crime and forensic agencies. Footwear evidence is currently a relatively small part of the range of trace evidence used in criminal investigations and therefore the significance will initially be modest, however by changing the fundamental cost-benefit ratio around footwear evidence our product will lead to an increased weighting given to footwear evidence in criminal investigations thereby increasing the significance and impact of our freeware over time.

We also identify the general public and school and college students as potential beneficiaries of this project, along with future generations of forensic practitioners. The product will improve education and research around footwear evidence. Additionally, we aim to use a participatory approach to sourcing the data required for the machine learning component of the project, which, although a supplement to the core product, is nonetheless an exciting opportunity to enhance the range of outcomes. The benefit and outcome here is to the wider STEM agenda and given the popular potency of forensics, participatory science and artificial intelligence, we believe that the impact here could be considerable. One of the key requirements of machine-learning algorithms is data, in this case multiple footwear tracks with placed landmarks. We will engage university students on forensic programmes at Bournemouth and Liverpool John Moores universities to collect this data. More importantly, however we will run workshops and 'field-days' at the Bournemouth University outdoor crime facility on the Trigon Estate in Dorset to engage school children in creating, capturing and analysing their own tracks thereby contributing data to the project and participating in a STEM inspiring activity. We will support this activity by a strong 'engagement' element to the projects website, supported by a social media campaign, and coordinate this activity via two Summer Undergraduate Research Assistantships (URAs) funded by Bournemouth University as a direct contribution to this project (6 weeks, Full Time). URA's are paid posts that allow students to work with academics on live research projects gaining experience for their own research, while also fuelling future research aspiration. 

The outcome for those engaged in education/research into vertebrate tracks will be the provision of a community-wide platform for the collection and analysis of track data improving methodological and analytical standards and data comparability. This academic outcome will support research into ancient environments and into the biomechanics both of extant and extinct animals."
3,2B199F77-2AED-4F3B-ABE1-7902F03AC7EE,Planning an Argument,"While in the past machines waited passively for our commands, we are moving to a future where humans and machines work in partnership, where machines are proactive and guide humans' activities (coined Human-Agent Collectives in the ongoing EPSRC project EP/I011587/1). A key challenge is how to allow humans to engage with these machines (which may be software systems or robots) in order to understand and engage with the decisions the machines take. These machines must be able to justify their choices and explain why a particular course of action is appropriate, and humans must be able to challenge these justifications and input into the machine's decision making process. Argument dialogues are an established approach to managing such interactions; they provide a principled way of structuring rational interactions between participants (machines or human) who may, e.g., assert arguments, beliefs and preferences, and question or challenge one another's assertions. A key benefit of argument dialogues is that they provide a familiar mechanism through which a human can engage with a machine's reasoning.

When a machine engages in an argument dialogue it must determine which of the available speech acts to make in order to try and achieve its dialogue goals, this is called its argument dialogue strategy. This proposal addresses the important open problem of how to generate an argument dialogue strategy, by leveraging the results of many years of automated planning research. Automated planning is one of the most well developed sub-fields of artificial intelligence and focuses on developing efficient and general approaches to determine which actions to perform in order to achieve some goal. This is exactly the problem we must solve in order to generate argument dialogue strategies, where the actions are the communicative speech acts and the goals typically refer to social constructs or commitments rather than physical states of the world. 

In order to apply automated planning techniques to generate an argument dialogue strategy, we must represent our argument dialogue strategy generation problem in a language that automated planners can understand; one of the main contributions of this work will be a set of implemented translations that take an argument dialogue strategy generation problem and output a planning problem (represented in a standard planning language), which can then be solved by an existing automated planner. This will provide the first general approach to generating argument dialogue strategies that is not tied to a particular type of dialogue (e.g., persuasion or negotiation) and does not assume the strategiser has knowledge of its interlocutor's dialogue strategy.

Our objectives are: 

O1: Define a general framework for representing an argument dialogue strategy generation problem (DSP). 
O2: Develop a set of benchmark DSPs, represented in the framework of O1, on which to evaluate our approach. This will include DSPs developed using real clinical knowledge.
O3: For each of two specified classes of DSPs (distinguished by the certainty of the beliefs the strategiser has about its interlocutor), formally define and implement translations that map from a DSP of that class to a planning problem defined in a standard planning language.
O4: Evaluate existing planning algorithms for solving the planning problems that result from applying the translations of O3 to the benchmark DSPs of O2, identifying the limitations of existing planning algorithms for solving the benchmark DSPs.

This work will allow humans to engage with and understand a machine's decision making process, which is crucial if we are to trust machines to take decisions for us. In this 14 month project we focus particularly on healthcare applications but the potential applications are wide ranging, including robots that do dangerous tasks and smart homes that manage our domestic life.",,"In the UK, most humans are rarely far from a device (be it laptop, smart phone, tablet, etc.) through which they can access a multitude of services and an unprecedented amount of knowledge and data from diverse sources. Technology is becoming increasingly pervasive, embedded in the cars we drive and the buildings we inhabit, as we move to a future of Smart Cities and the Internet of Things. As technology advances, demands on our time grow, and the available knowledge and services becomes so great that humans can no longer be aware of it all, we are entrusting more, and more important, tasks to our machines, bestowing them with varying degrees of autonomy. Be they robots, e.g., that carry out unmanned missions to hostile environments such as the deep sea or the site of an earthquake, or software systems, e.g., a personal digital healthcare assistant (PDHA) to support elderly people living at home, these machines must be able to interact with humans in order to, e.g., jointly make decisions and reach agreements, explain why a particular course of action is appropriate, and take into account a human's knowledge or preferences. 

The foundational research we propose will allow a machine to determine the arguments or claims to assert, the questions to pose, and the justifications to challenge in order to successfully engage in such interactions. This will benefit society by allowing humans to engage in and understand the decisions these intelligent machines make, crucial if we are to trust and value their decisions. The potential applications of our research are wide ranging, including the robots and PDHAs discussed above, emergency response advice systems, and intelligent home management systems that order our groceries and manage our energy consumption. Such systems will have massive benefits for society, improving the quality of life, making home care more accessible, and avoiding risk to humans. Furthermore, the work carried out in this project will strengthen the UK's position as a global leader in robotics and autonomous system technology, one of eight great technologies that have been identified by the UK government as areas that can boost our exports and drive economic growth.

In this 14 month project, we will focus particularly on healthcare technology applications, considering benchmark examples from the clinical domain and with the support of our project partner, a leading expert in the development of healthcare technologies, identifying particular applications that can benefit from our results in the short term. Current theory and technologies will allow us to use our results to support highly structured human-machine dialogues where what the human can say is restricted; however, we envisage in the longer term applying our approach to allow much more flexible dialogues, where a machine can, e.g., interpret the intended meaning of an utterance from its context or develop a model of a human based on its interactions with them. We will engage with various UK research groups who work on the challenges related to this long term goal and who develop systems that can benefit from such dialogues, in order to produce a roadmap of open problems that must be addressed. This roadmap will benefit the UK by helping to focus research efforts on realising flexible human-machine interactions and has the potential to influence research funding strategy. 

We will engage with the public through social media channels and public engagement events. This is important as we must take account of the public's views on allowing machines to make decisions for us and on engaging with them on these decisions. These activities will allow us to educate the public about the theory that underlies these intelligent machines, mitigating public resistance to autonomous, or semi-autonomous, systems."
4,B4977706-4D3F-4358-81ED-D1B21DE75218,Robustness-as-evolvability: building a dynamic control plane with Software-Defined Networking,"Highly available information networks are an increasingly essential component of the modern society. Targeted attacks are a key threat to the availability of these networks. These attacks exploit weak components in network infrastructure and attack them, triggering side-effects that harm the ultimate victim. Targeted attacks are carried out using highly distributed attacker networks called botnets comprising between thousands and hundreds of thousands of compromised computers. A key feature is that botnets are programmable allowing the attacker to adapt to evolve and adapt to defences developed by infrastructure providers. However current network infrastructure is largely static and hence cannot adapt to a fast evolving attacker.

To design effective responses, a programmable network infrastructure enabling large-scale cooperation is necessary. Our research will create a new form of secure network infrastructure which detects targeted attacks on itself. It then automatically restructures the infrastructure to maximise attack resilience. Finally, it self-verifies whether global properties of safety and correctness can be assured even though each part of the infrastructure only has a local view of the world.

Our research will examine techniques to collect and merge inferences across distributed vantage points within a network whilst minimising risks to user privacy from data-aggregation using novel privacy techniques. We make a start on addressing the risks introduced by programmability itself, by developing smart assurance techniques that can verify evidence of good intention before the infrastructure is reprogrammed.

We set three fundamental design objectives for our design: 
(1) Automated and seamless restructuring of network infrastructure to withstand attacks aimed at strategic targets on the infrastructure.

(2) A measurement system that allows dynamic allocation of resources and fine control over the manner, location, frequency, and intensity of data collected at each monitoring location on the infrastructure.

(3) Assurance of safety and compliance to sound principles of structural resilience when infrastructure is reprogrammed.

Our aim is to develop future network defences based on a smart and evolving network infrastructure.",,"Our joint research program (with industrial and academic partners) will provide new foundations for a resilient network infrastructure. The medium term beneficiaries of this research will be parties who have a stake in the development of future networking infrastructure, and, in the much longer and wider term, everyone who relies on resilience of the global digital infrastructure.

Our technologies will give private sector companies competitive advantage, by making their SDN-based networking tools provide additional functionality, aimed directly at tackling security issues. Our techniques will give greater confidence in network infrastructure; thus enabling utility companies and government to move more critical infrastructure onto standard backbones. 

Software-Defined Networking (SDN) promises massive reduction in costs. The proposed work programme has the potential to demonstrate that not only could SDN switches provide fundamentally better security (by incorporating programmability and hence dynamic algorithms for measurement and response), they might achieve it cheaper than conventional hardware routers."
5,6AAAE17A-F328-4337-8614-484086DB58E3,A Multiobjective Evolutionary Approach to Understanding Parkinson's Disease,"Parkinson's disease (PD) is the second most common neurodegenerative disorder, affecting around 1% of the population over the age of 60. Over the last decade, it has been increasingly recognised that PD is a neuropsychiatric disorder, leading to cognitive dysfunction in addition to motor dysfunction. Cognitive impairments have been detected at an early stage in the disease, making them an important marker for early diagnosis and treatment, and many PD patients go on to develop dementia. However, at present cognitive aspects of the disease are poorly understood, and an accurate prognosis is unlikely. To address this, this work aims to develop computational techniques that can identify and discriminate different groups of PD patients based on the occurrence of both cognitive and motor symptoms. Another problem, which this work also addresses, is the difficulty of performing differential diagnosis between neurodegenerative diseases. These often have overlapping symptoms, meaning that patients can be incorrectly diagnosed and go on to receive inappropriate treatments. For instance, PD can be confused with Alzheimer's, progressive supranuclear palsy, multi-system atrophy and corticobasal degeneration in its early stages. Whilst there is no cure for PD, the development of drugs that halt the progress of the disease is an important area of research. By contributing towards better understanding of the disease and better methods for monitoring disease progress, the proposed research will help address some of the obstacles currently faced by drug development.",,"The proposed research aims to develop computational tools that perform objective accurate diagnosis and prognosis of neurodegenerative diseases using novel yet inexpensive devices. The proposed work targets three application areas: early diagnosis, monitoring and prognosis, and disease understanding.

Early Diagnosis
Due to poor training in primary care, limited provision of expert secondary care and the complexity of presenting symptoms many sufferers of neurodegenerative diseases currently go undiagnosed or are misdiagnosed. Early diagnosis is important for both disease sufferers and their carers, since it enables treatment of symptoms and allows patients to plan their lives and make informed decisions about future treatment and care. With the development of drugs with the potential to halt the progress of neurodegenerative diseases, early diagnosis is likely to become even more important. Furthermore, the combination of high accuracy, the ability to discriminate between different neurological conditions, and the use of inexpensive devices would make our methods well-suited for use in a screening programme for neurodegenerative diseases.

Monitoring and Prognosis
Monitoring is an important part of managing a patient's treatment. However, it is also important for drug development, where there is a need for accurate and objective methods for characterising symptoms and their severity. Current clinical approaches to monitoring rely on human judgment, and display poor consistency both between clinicians and between patients monitored. Hence, the development of objective accurate computational tools for monitoring would have a considerable impact upon both a patient's existing treatment and the development of new treatments. Related to this is the issue of accurate prognosis, which involves predicting the likely course of a disease based upon current measurements of a patient's symptoms. This is particularly important for PD patients, where currently there is no accurate means of determining whether the disease will lead to dementia, or when this will occur.

Disease Understanding 
In general, the symptoms of neurodegenerative diseases such as Parkinson's are not well understood, and this is an important factor underlying the difficulty of diagnosis, prognosis and drug development. In addition to creating tools to perform diagnosis and prognosis, the proposed research also aims to understand the basis of this decision process, both in terms of cognitive and motor symptoms and, significantly, through reference to its neural basis as captured by neuroimaging data. By sharing this information with the medical community, we aim to improve the ability of medical doctors to perform diagnosis and prognosis, increase understanding of the underlying disease processes, and inform drug development.

Economic Impact
Increased early diagnosis rates could lead to considerable economic savings, particularly from delayed admission to hospitals and care homes. This figure has been estimated at around &pound;6000 per person for sufferers of dementia, a condition that affects 1 in 6 people over the age of 80. With the current rate at which the UK population is aging, by 2030 it is estimated that there will be 6 million people over the age of 80, suggesting savings in the region of &pound;6 billion for dementia alone."
6,9F9F9D98-04A9-4720-907B-EB962D6C070D,Matheuristics for multi-criterion data clustering: towards multi-criterion big data analytics,"With rapid increases in data volume in all areas of life, the meaningful analysis of these data is becoming a crucial bottleneck. Whether data are generated by customer transactions, through communications on social media, or as a by-product of manufacturing processes, data are meaningless unless suitable techniques are available to select the most relevant data, analyze these data and turn raw data into tangible information and insight. To some extent, &quot;big data&quot; reverses traditional approaches in data-mining, as data collection now frequently precedes the definition of an actual question or hypothesis. The purported advantage of this approach is that novel, unexpected findings may materialize - a premise that relies, however, on the expert use of suitable approaches for exploratory data analysis. The prominence of &quot;big data&quot; therefore fuels the need and use of scalable and powerful approaches to exploratory data analysis. Data clustering techniques present one of the most fundamental tools in exploratory data analysis, and this project aims to deliver novel techniques that are accurate, flexible and scalable to large data sets.
 
Data clustering techniques present one of the most fundamental tools in exploratory data analysis. Conceptually, data clustering refers to the identification of sub-groups within a data set so that items within the same group are similar and those in different groups are dissimilar; e.g., in the context of insurance data, a &quot;cluster&quot; of people may relate to customers who show similar behaviour in their claim patterns over time, while those in different clusters behave differently. Mathematically, data clustering can be seen as an example of a problem where good solutions are best described using a set of different criteria that account for conflicting properties such as the compactness of clusters and the separation between clusters.

The above observation has recently led to the development of multi-criterion approaches to data clustering, which explicitly consider a number of clustering criteria. This approach has shown a lot of promise, in terms of the accuracy and the robustness of the solutions obtained. However, current techniques for multi-criterion clustering are limited regarding their scalability to very large data sets and also their flexibility with respect to their consideration of different sources of dissimilarity data. This project proposes a novel technique for multi-criterion clustering: the algorithm will combine complementary ideas from two sub-fields of computer science, leading to improved scalability and flexibility of the technique developed. The work will include the development of an interactive user-interface and the application of multi-criterion clustering to problems in finance and marketing. All software produced will be released publicly.",,"Briefly, the non-academic beneficiaries of this project are:
- Users of exploratory data analysis in industry / commerce, including manufacturing as a major source of &quot;big data&quot;.
- Commercial providers of software tools for data-mining. 
- The wider public, as they benefit from improved data analytical techniques through the spin-offs of such knowledge, e.g. through the development of improved recommendation systems (marketing), the identification of groups of co-regulated genes (biology), etc. 

One of the applications considered in this proposal (work package WP4.2) involves measures of customer experience and loyalty and will likely involve industrial partners as the primary source of these data. Further opportunities for collaborations will be sought, e.g. as a part of industrial dissertation projects. The establishment of fruitful collaborations within and outside academia is supported by the third objective of the proposal, which aims to embed multi-criterion clustering techniques into an interactive, easy-to-use interface that is suitable for the rapid training and use by students, fellow academics, industrial partners etc. Similarly, work packages WP4.1 and WP4.2 support exploitation activities directly, as they deliver practical application examples that can facilitate and underpin the dissemination of the approach."
7,03F00ADC-C7E0-49EF-8696-65E9BAFB7323,Data-Driven Surrogate-Assisted Evolutionary Fluid Dynamic Optimisation,"Computational fluid dynamics (CFD) is fundamental to modern engineering design, from aircraft and cars to household appliances. It allows the behaviour of fluids to be computationally simulated and new designs to be evaluated. Finding the best design is nonetheless very challenging because of the vast number of designs that might be explored. Computational optimisation is a crucial technique for modern science, commerce and industry. It allows the parameters of a computational model to be automatically adjusted to maximise some benefit and can reveal truly innovative solutions. For example, the shape of an aircraft might be optimised to maximise the computed lift/drag ratio.

A very successful suite of methods to tackle optimisation problems are known as evolutionary algorithms, so-called because they are inspired by the way evolutionary mechanisms in nature optimise the fitness of organisms. These algorithms work by iteratively proposing new solutions (shapes of the aircraft) for evaluation based upon recombinations and/or variations of previously evaluated solutions and, by retaining good solutions and discarding poorly performing solutions, a population of optimised solutions is evolved.

An obstacle to the use of evolutionary algorithms on very complex problems with many parameters arises if each evaluation of a new solution takes a long time, possibly hours or days as is often the case with complex CFD simulations. The great number of solutions (typically several thousands) that must be evaluated in the course of an evolutionary optimisation renders the whole optimisation infeasible. This research aims to accelerate the optimisation process by substituting computationally simpler, dynamically generated &quot;surrogate&quot; models in place of full CFD evaluation. The challenge is to automatically learn appropriate surrogates from a relatively few well-chosen full evaluations. Our work aims to bridge the gap between the surrogate models that work well when there are only a few design parameters to be optimised, but which fail for large industry-sized problems.

Our approach has several inter-related aspects. An attractive, but challenging, avenue is to speed up the computational model. The key here is that many of these models are iterative, repeating the same process over and over again until an accurate result is obtained. We will investigate exploiting partial information in the early iterations to predict the accurate result and also the use of rough early results in place of the accurate one for the evolutionary search. The other main thrust of this research is to use advanced machine learning methods to learn from the full evaluations how the design parameters relate to the objectives being evaluated. Here we will tackle the computational difficulties associated with many design parameters by investigating new machine learning methods to discover which of the many parameters are the relevant at any stage of the optimisation. Related to this is the development of &quot;active learning&quot; methods in which the surrogate model itself chooses which are the most informative solutions for full evaluation. A synergistic approach to integrate the use of partial information, advanced machine learning and active learning will be created to tackle large-scale optimisations.

An important component of the work is our close collaboration with partners engaged in real-world CFD. We will work with the UK Aerospace Technology Institute and QinetiQ on complex aerodynamic optimisation, with Hydro International on cyclone separation and with Ricardo on diesel particle tracking. This diverse range of collaborations will ensure research is driven by realistic industrial problems and builds on existing industrial experience. The successful outcome of this work will be new surrogate-assisted evolutionary algorithms which are proven to speed up the optimisation of full-scale industrial CFD problems.",,
8,0E35BDFA-B7AE-41C7-B565-C1127611CC22,RePriCo: Resolving Multi-party Privacy Conflicts in Social Media,"Hundreds of billions of items that are uploaded to Social Media are co-owned by multiple users, yet only the user that uploads the item is allowed to set its privacy settings (i.e., who can access the item). This is a massive and serious problem as users' privacy preferences for co-owned items usually conflict, so applying the preferences of only one party leads to privacy violations with severe consequences (e.g., users losing their jobs, being cyberstalked, etc.). A solution to this problem is most timely as it has been highlighted as one of the most important problems to be addressed for an appropriate online privacy management. An example of this problem is a photo in which Alice and Bob are depicted together: how can they agree on the third parties they will share the photo with? Mainstream Social Media would only allow Alice (assuming she uploads the item) to set the privacy settings for the photo, but what if Bob would not like to share with some of Alice's friends? Some initial approaches have been proposed in the past few years, but these have a number of limitations that make them unsuitable in practice. Users are forced to try to resolve such conflicts manually (by e-mail, phone calls, etc.), which is exhausting because social networks are very dynamic and huge in scale. Even more importantly, the process of resolving conflicts manually starts too late, when one user has already posted the item and the privacy violation has occurred.

RePriCo's most exciting, novel, and ground-breaking part will be the automated conflict detection and resolution mechanism based on new automated negotiation technologies. In particular, RePriCo's main challenge will be to automatically suggest solutions to the conflicts in such a way that these solutions are accepted by users most of the time and they do not need to resolve the conflicts manually. This is both ambitious and adventurous, as it requires understanding how users would actually reach an agreement if they were to solve the conflicts themselves manually. RePriCo's hypothesis is that users' negotiation behaviour is significantly influenced by users' relationships and their strength (as existing evidences seem to support), and that a computational negotiation mechanism should be based on this in order to produce acceptable outcomes. One of the main contributions of RePriCo will be a strong empirical base about how users' relationships influence negotiation behaviour. RePriCo will be highly transformative in many aspects: (i) it will empower users to manage their privacy for multi-party co-owned items; (ii) it will consider what is actually acceptable for users and the factors that contribute to that, which no previous approach to solve multi-party privacy conflicts has considered before; and (iii), the empirical base it will develop will not only influence RePriCo's automated conflict detection and resolution mechanisms but also help push research on the topic forward.",,"The project will run a programme with a view to maximising the impact of the project's results to deliver four key impact objectives: 1) sustainable economic impact; 2) influencing policy; 3) raising public awareness and improving quality of life in the cyberspace; 4) developing project team's skills. 

Sustainable economic impact will be delivered by means of a two-staged approach. In the first stage, a series of dissemination activities (magazine article, business videos, business event, attendance to and presentation at Industry conferences) will run to engage with potentially interested businesses. We have already secured interest, support and collaboration from Microsoft, SBL and Egress which will also participate in these events. In the second stage, activities with these already secured partners and possibly other selected partners from stage one will run. The commercial exploitation of the Social Media app to be developed will be explored (following Lancaster University's IP protection policy) with SBL, which will provide consultancies and access to partner companies and customers to facilitate this. We will also explore the applicability of RePriCo's results to other social domains with Egress, which has a secure collaboration tool for online data sharing that induces social networks among its users and faces the problem of multi-party privacy management. Finally, Microsoft has committed to provide a researcher to collaborate in the project to push research in this topic forward and establish a long-term mutually-benefiting collaboration base. RePriCo will utilise internal (IAA, CASE) and external (KTP) grant mechanisms for follow-up collaborations with these businesses. 

The PI will also leverage his previous experience in engaging with policy makers (see Track Record) to raise awareness about multi-party privacy management, to update them with project findings, and explore possible areas of impact in current and prospective regulations. To this aim, the PI will: (i) meet his policy contacts as member of the Policy Fellows Network at Centre for Science and Policy (a network of academics and policy professionals the PI is member of), and Security Lancaster's policy contacts (Academic Centre of Excellence in Cyber Security research the PI is member of); and (ii) contact regulation bodies like ICO,ENISA and EDPS. The information above will also be encapsulated as a whitepaper to share with policy stakeholders and the general public.

Raising Public Awareness and improving quality of life in the cyberspace will be delivered by means of a highly active Social Media and Press Releases strategy to disseminate educational videos and blog entries about the problem of multi-privacy management and hints to improve this as well as to disseminate project results. The project will also leverage Lancaster University's links with schools (South Lakes Teaching School Alliance), the MSc in CyberSecurity (in which the PI teaches) and Security Lancaster's bespoke programmes for national and international organisations, contacts with relevant professional bodies (IAPP,DPF,NADPO), and an invited slot on the IA practitioner's event (organised by SBL), as well as an invited article in SBL's CyberTalk Magazine.

The project's team will enhance their knowledge transfer skills via regular meetings with department's Knowledge Business Centre and Security Lancaster's Partnership Manager, and participation in knowledge transfer activities (Business education event, etc.). The RA will be given the opportunity to pursue research objectives more independently and gain academic network experience by organising together with the PI an academic workshop, and the PhD student (funded by LU) will gain research skills towards her/his PhD by working with the PI and other project researchers. The team will also learn new skills by participating in the activities described above to influence policy."
9,23BCC2EC-8791-411E-B559-168A02CE47BE,Brain-inspired non-stationary learning.,"Computing power and memory storage have doubled approximately every two years, allowing today's computers to memorise essentially everything. In tandem, new machine learning techniques are being developed that harness this wealth of data to extract knowledge, make predictions, and generalize to unseen data; many of these with artificial neural networks at their core. This combination has led to impressive new solutions to numerous real world problems, including image classification and speech processing. 

Despite this progress, computers still lag behind human performance on more general-purpose tasks. In particular, current methods are not well suited to learning in non-stationary settings (where the data is changing over time): a desirable system would learn new things quickly, without forgetting what it knew before. To clarify these ideas, consider an artificial neural network trained to classify clothes from images. This is a non-stationary task, because fashions change and innovate, so the network must continually learn from new examples. However, it must do so without forgetting previous examples (e.g. summer clothes, not seen for all of winter), otherwise it would have to relearn about summer clothes from scratch each spring. In practice, to handle new examples, the network needs to learn at a high rate, but this high learning rate has the side-effect of overwriting old memories; that is, the system is forgetting quickly. Conversely, if the learning rate is low, the network remembers for much longer, but then learning is impractically slow, and no longer agile enough to deal with changing environments.

This research challenge of fast learning on non-stationary tasks without forgetting is therefore a fundamental one, and is recognized as a stumbling block in current approaches to transfer learning, continual learning or life-long learning. But of course, there exists one system that has solved the apparent dilemma: the human brain. We humans live our life in a non-stationary world, and we can both learn quickly and remember for a long time. A classical example from experimental psychology shows that the rate at which a person forgets a series of previously memorised random letters follows a power-law, i.e., the decay is equally large between 1h and 2h as it is between 2h and 4h, or between 1 week and 2 weeks. In contrast, forgetting in artificial systems happens exponentially, i.e., the decay is the same between 1h and 2h as it is between 100h and 101h, and therefore much faster than observed in humans.

In the brain, learning is based on the modification of the connection strength between neurons when a new pattern enters, a process called synaptic plasticity. This change can last for different amounts of time, giving rise to the three timescales: short-term plasticity, long-term plasticity and synaptic consolidation.

The research hypothesis of this proposal is that we can reach human-level performance by building a learning system that takes inspiration from these learning mechanisms of the brain, in particular the different time scales of synaptic plasticity and their interplay. The intuition is the following: an incoming memory is learnt quickly using the fastest learning rate, then this memory is slowly transferred to another component that operates at a slower learning rate, so that it is not overwritten by new incoming memories. 

This proposal therefore addresses two research challenges. I intend to build a unifying learning rule across all three learning timescales, just like I unified long-term and very long-term in past work. I will then investigate the learning and forgetting speed in plastic networks with the unifying learning rule. The network will learn to categorise on non-stationary data, but be tested on all the seen data, currently a very difficult task in machine learning.",,"***Technological impact: This work will benefit current and future developers of smart technologies, since the learning rule developed for this project is likely to inspire new machine learning algorithms (see letter of support from the company Cortexica), new implementations in neuromorphic engineering, and new learning rules for intelligent robotics. In particular, I will use my industry collaboration with Tom Schaul at Google Deepmind, London, to ensure that my learning rule can refine their state-of-the-art deep learning networks, and be another stepping stone toward general-purpose artificial intelligence (see letter of support from Google Deepmind), as well as my industrial contact at Qualcomm - USA, Eugene Izhikevich, to ensure that my work flows into marketable neuromorphic chip design technology. 

***Health impact: On a longer timescale, this work will benefit people with diseases related to learning and memory such as Alzheimer's disease. It will also benefit people with diseases related to connectivity disorders such as autism and schizophrenia. Finally, it will benefit the growing ageing population in the UK and around the world, since this work will set a reference point for the amount of plasticity seen in the adults with natural or biased input statics. It has been shown that the synaptic turnover is increased in the aged brain, therefore my work can be a starting point for studying the behaviour and the impact of synaptic plasticity in an aged brain.

*** Providing tools for the experimental neuroscience community. My computational model will benefit the experimental neuroscience scientific community by providing models that can be used to test scientific hypotheses before performing any experiments; this will therefore reduce animal usage and provide novel tools to speed up science by increasing the number of possibilities that can be tested. To this end, I will publish my code on a standard database in the field (ModelDB) along with an easy-to-use graphical interface.

***Educational impact: I will train a new generation of scientists by training the staff in my laboratory and by teaching at summer schools. But more importantly, I will train a new generation of non-academic workers in the UK by through my computational neuroscience course in the Bioengineering Department at Imperial College London, teaching a solid skillset for working in pharmaceutical, biotechnological, or engineering companies such as high-tech companies using machine learning or robotics, but also in banks and insurances that use artificial neural network techniques.

***Public engagement: I aim to sensibilise the broader public to the discoveries of neuroscience and communicate the great scientific challenges of the future. To this end, I propose to work with the outreach manager of my Department to set up a number of activities, such as press releases after publication, maintenance of a webpage with breaking news, stands at two different outreach activities of Imperial College London (Imperial Festival and Imperial Fringe) as well as giving talks to prospective students during the open days of Imperial."
10,49F70868-3E31-4B5D-9502-EFDF0401DD94,Data-Driven Surrogate-Assisted Evolutionary Fluid Dynamic Optimisation,"Computational fluid dynamics (CFD) is fundamental to modern engineering design, from aircraft and cars to household appliances. It allows the behaviour of fluids to be computationally simulated and new designs to be evaluated. Finding the best design is nonetheless very challenging because of the vast number of designs that might be explored. Computational optimisation is a crucial technique for modern science, commerce and industry. It allows the parameters of a computational model to be automatically adjusted to maximise some benefit and can reveal truly innovative solutions. For example, the shape of an aircraft might be optimised to maximise the computed lift/drag ratio.

A very successful suite of methods to tackle optimisation problems are known as evolutionary algorithms, so-called because they are inspired by the way evolutionary mechanisms in nature optimise the fitness of organisms. These algorithms work by iteratively proposing new solutions (shapes of the aircraft) for evaluation based upon recombinations and/or variations of previously evaluated solutions and, by retaining good solutions and discarding poorly performing solutions, a population of optimised solutions is evolved.

An obstacle to the use of evolutionary algorithms on very complex problems with many parameters arises if each evaluation of a new solution takes a long time, possibly hours or days as is often the case with complex CFD simulations. The great number of solutions (typically several thousands) that must be evaluated in the course of an evolutionary optimisation renders the whole optimisation infeasible. This research aims to accelerate the optimisation process by substituting computationally simpler, dynamically generated &quot;surrogate&quot; models in place of full CFD evaluation. The challenge is to automatically learn appropriate surrogates from a relatively few well-chosen full evaluations. Our work aims to bridge the gap between the surrogate models that work well when there are only a few design parameters to be optimised, but which fail for large industry-sized problems.

Our approach has several inter-related aspects. An attractive, but challenging, avenue is to speed up the computational model. The key here is that many of these models are iterative, repeating the same process over and over again until an accurate result is obtained. We will investigate exploiting partial information in the early iterations to predict the accurate result and also the use of rough early results in place of the accurate one for the evolutionary search. The other main thrust of this research is to use advanced machine learning methods to learn from the full evaluations how the design parameters relate to the objectives being evaluated. Here we will tackle the computational difficulties associated with many design parameters by investigating new machine learning methods to discover which of the many parameters are the relevant at any stage of the optimisation. Related to this is the development of &quot;active learning&quot; methods in which the surrogate model itself chooses which are the most informative solutions for full evaluation. A synergistic approach to integrate the use of partial information, advanced machine learning and active learning will be created to tackle large-scale optimisations.

An important component of the work is our close collaboration with partners engaged in real-world CFD. We will work with the UK Aerospace Technology Institute and QinetiQ on complex aerodynamic optimisation, with Hydro International on cyclone separation and with Ricardo on diesel particle tracking. This diverse range of collaborations will ensure research is driven by realistic industrial problems and builds on existing industrial experience. The successful outcome of this work will be new surrogate-assisted evolutionary algorithms which are proven to speed up the optimisation of full-scale industrial CFD problems.",,"Aerospace and Aviation

The UK has the second largest aerospace industry in the world with significant capabilities in the key areas, including engines, airframe structures, aerodynamics and air traffic. To maintain the UK's world-leading position in this sector, optimisation techniques like evolutionary algorithms will provide a unique opportunity to address the main complex challenges and create innovative designs. This research will remove the obstacles in applying evolutionary techniques in the aerospace and aviation sector. 

One of our project partners, the Aerospace Technology Institute (ATI), was created by the government in 2012 as part of the wider aerospace strategy. As their letter of support attests, the work is &quot;clearly aligned with the key challenges faced in trying to use optimisation for future high-lift aerodynamic design&quot;. We will work closely with QinetiQ, a member of the ATI, on the CFD optimisation in complex, nonlinear flows, work which directly addresses the ATI's national strategy for aerospace technology in the UK.



Industry

In addition to aerospace and aviation, computational fluid dynamics is used across a wide spectrum of UK industry: in civil engineering (e.g., water systems engineering), environmental engineering (e.g., flood prevention; wind turbines; carbon capture systems), mechanical engineering (e.g., automotive design), biomedical engineering (e.g., anaerobic digesters), the food industry and medical devices. In all these arenas optimisation is used to optimise a benefit, such as efficiency, safety, manufacturing or deployment cost, quality of service, energy consumption or health. Being able to effectively optimise these systems therefore clearly has direct and wide socio-economic benefits. The successful outcome of this project will result in new, efficient ways of optimising large, complex systems. These are systems that, because of their complexity, cannot currently be optimised and are therefore operated sub-optimally. The potential impact of this project is therefore very broad. Moreover the UK industrial CFD community is a significant one with industrial research and development of CFD ongoing as well as application through various specialist consultancies.

Optimisation is increasingly used throughout UK industry and commerce, as well as the public sector and throughout science. In addition to the directly CFD-related applications, the breadth of its use is indicated by applications in air traffic safety, radio and mobile telephone networks, drug design, supply chain configuration, renewable energy devices, energy and transport networks, additive layer manufacturing, automotive structures and so on. We therefore anticipate that the methods developed for evolutionary surrogate-assisted optimisation will have impact across a wide range of the UK economy. We will release open source code to facilitate widespread use of the work.


Public impact 

The project will have public impact through Exeter and Surrey's public engagement and outreach programmes, such as F1 in Schools, Future Engineers and Formula Student. It will also impact school workshops interested in computer science and engineering.


Education and Training

This project will train three post-doctoral research associates and an Exeter-funded PhD student to work in at the interface between computational fluid dynamics, evolutionary computing and machine learning. In addition, by spending significant amounts of time working with our industrial partners, they will be able to understand and tackle real-world problems. It will also have an impact on the undergraduate Engineering and Computer Science degree programmes in Surrey and Exeter, through seminars and case studies, involvement in undergraduate teaching and final year projects. The project will therefore have impact by delivering highly skilled people to UK industry and commerce."
11,2B4410FB-73B5-4610-BC5F-7F9DDFC43220,Improving cyber security using realistic synthetic face generation,"The grant application outlines a novel programme of research that questions the uniqueness of facial identity and investigates the use of computer generated face imagery in the area of cyber security.The popularity of the human face as a biometric remains strong despite the introduction of many competing modalities. People are accustomed to being identified by their facial appearance whereas other biometrics such as fingerprints and iris recognition feel more invasive. A programme of research that investigates the concept of identity that is highly relevant to cyber security is proposed. In addition we will develop a novel cyber security application based on facial identity and evaluate its practical security level. Work of this nature has relevance beyond the scope of the project. For example, border control officers routinely verify a person's identity using passport photos but what is the fundamental limit on the ability to achieve this task reliably?",,"The proposed project will provide impact through: its deliverables, potential for future product development, engagement of the general public through interactive online experiments and by advancing scientific understanding in the area of cyber security.
A deliverable of WP1 is a MATLAB toolbox for synthesising face images. The toolbox will comprise a face land marking tool, code for generating the PCA/GMM model and a user interface that allows batch generation of lifelike face images. We anticipate that the toolbox will be highly beneficial to researchers working in the areas of cyber security, biometrics, computer vision and psychology. The toolbox will be made available for download, free of charge, from the MATLAB File Exchange.
The source code for the prototype cyber security application, developed in WP4, will be uploaded to the collaborative software development site www.github.com to encourage the involvement of developers in the wider security community.
The proposed programme of research will result in Gold Standard Open Access publications and be disseminated at prestigious conferences such as ACM Conference on Computer Communications Security (CCS), which is the highest ranked conference in Computer Security in the World."
12,914B2635-F300-42ED-BEF7-442A44EDC87E,An Integrated Vision and Control Architecture for Agile Robotic Exploration,"Autonomous robots, capable of independent and intelligent navigation through unknown environments, have the potential to significantly increase human safety and security. They could replace people in potentially hazardous tasks, for instance search and rescue operations in disaster zones, or surveys of nuclear/chemical installations. Vision is one of the primary senses that can enable this capability, however, visual information processing is notoriously difficult, especially at speeds required for fast moving robots, and in particular where low weight, power dissipation and cost of the system are of concern. Conventional hardware and algorithms are not up to the task. The proposal here is to tightly integrate novel sensing and processing hardware, together with vision, navigation and control algorithms, to enable the next generation of autonomous robots.

At the heart of the system will be a device known as a 'vision chip'. This bespoke integrated circuit differs from a conventional image sensor, including a processor with each pixel. This will offer unprecedented performance. The massively parallel processor array will be programmed to pre-process images, passing higher-level feature information upstream to vision tracking algorithms and the control system. Feature extraction at pixel level results in an extremely efficient and high speed throughput of information. Another feature of the new vision chip will be the measurement of 'time of flight' data in each pixel. This will allow the distance to a feature to be extracted and combined with the image plane data for vision tracking, simplifying and speeding up the real-time state estimation and mapping capabilities. Vision algorithms will be developed to make the most optimal use of this novel hardware technology.

This project will not only develop a unique vision processing system, but will also tightly integrate the control system design. Vision and control systems have been traditionally developed independently, with the downstream flow of information from sensor through to motor control. In our system, information flow will be bidirectional. Control system parameters will be passed to the image sensor itself, guiding computational effort and reducing processing overheads. For example a rotational demand passed into the control system, will not only result in control actuation for vehicle movement, but will also result in optic tracking along the same path. A key component of the project will therefore be the management and control of information across all three layers: sensing, visual perception and control. Information share will occur at multiple rates and may either be scheduled or requested. Shared information and distributed computation will provide a breakthrough in control capabilities for highly agile robotic systems.

Whilst applicable to a very wide range of disciplines, our system will be tested in the demanding field of autonomous aerial robotics. We will integrate the new vision sensors onboard an unmanned air vehicle (UAV), developing a control system that will fully exploit the new tracking capabilities. This will serve as a demonstration platform for the complete vision system, incorporating nonlinear algorithms to control the vehicle through agile manoeuvres and rapidly changing trajectories. Although specific vision tracking and control algorithms will be used for the project, the hardware itself and system architecture will be applicable to a very wide range of tasks. Any application that is currently limited by tracking capabilities, in particular when combined with a rapid, demanding control challenge would benefit from this work. We will demonstrate a step change in agile, vision-based control of UAVs for exploration, and in doing so develop an architecture which will have benefits in fields as diverse as medical robotics and industrial production.",,
13,043FE755-65AF-4415-8529-9DA2B6328A9F,From Human Data to Personal Experience,"Horizon is a multidisciplinary centre for Digital Economy (DE) research and impact. We balance the development of new technologies to capture and analyse human data, with explorations of how these can be used to deliver powerful experiences to people, with an awareness and understanding of the human and social values that must underpin these. We follow a user-centred approach, undertaking research in the wild based on principles of open innovation. 

In its first phase, Horizon has established a core team of over 50 researchers and has reached out to build a wider network of 35 academic and 200 industry, public and third-sector partners. We have established a Centre for Doctoral Training and inaugurated the DE All Hands series of conferences and national DE CDT Summer School. World-class scientific outputs in diverse disciplines have been balanced with economic, cultural and societal impact.

This proposal builds on this critical mass to enable a step-change in Horizon's translational research and impact. We respond to the changing nature of the digital economy as it matures, as the social, physical and digital become blended and as human data becomes an increasingly valuable asset. We offer a vision in which human data enables the creation and delivery of highly personal experiences. We propose to address three major challenges. The first is to establish new technologies that collect and interpret our human data in a more transparent way. The second is to be able to better understand and design new kinds of experiences that employ these technologies to promote the values of personal fulfilment, wellbeing and sustainability. The third is to address key ethical challenges around design for privacy and new models of ownership.

We will work closely with a range of external partners whose interests span: computing and analytics; social policy; and diverse sectors of the DE including creative industries, retail, fast moving consumer goods, finance, energy, transportation and healthcare. We will engage these through a programme of agile translational research projects. These will be integrated into an overarching strategic impact campaign that revolves around three flagships. In turn, these will be supported by two further programmes; one targeted at sustaining the wider DE community and the second at developing the capacity of our researchers to deliver translational research and impact.",,"This primary focus of our proposal is on delivering impact across the maturing Digital Economy. We target business, third-sector organisations and public bodies from a diverse range of sectors including creative industries, retail, consumer goods, finance, energy, transportation and healthcare, as well as companies who are delivering the enabling technologies for interpreting and managing personal data and also the organisations that are shaping public policy around these.

These will benefit in multiple ways, including: 

- Understanding how to better interpret complex and ambiguous human data, including the potential of algorithmic, human and hybrid approaches;

- Understanding how to embed such interpretations into personally meaningfully experiences that meet wider objectives of fulfilment, wellbeing and sustainability;

- Knowing how to do this in an ethical way so as to maintain positive relationships with consumers, customers and citizens;

- Being able to envision and demonstrate future experiences that blend conventional media, services and physical products in new ways;

- Learning about the opportunities and challenges raised by such experiences through public deployments and field trials;

- Expanding the capabilities of the staff through exposure to new concepts, technologies and methods emerging from DE research;

- Gaining access to talented young people (especially CDT students) through internships and agile projects with the potential to recruit them downstream;

- Entering a neutral environment in which they can engage with other partners at different points along the value chain for future human-data experiences and from across the different sectors in which they are being applied.

Our proposal sets out a wide-ranging programme of impact activities to deliver these benefits, including agile translational projects, a strategically coordinated campaign to maximize impact among clusters of partners; and supporting programmes to develop individuals and the wide Digital Economy community."
14,55E78D60-D835-4642-B7E4-DAE26D4B1E51,Computational Creativity Theory,"Computational Creativity is the study of how to build software which takes on some of the creative responsibility in arts and science projects. We are at a stage where software can generate pictures, melodies, jokes and poems, can invent new words and discover new and interesting mathematical theorems, and regularly helps scientists to make important discoveries. This kind software can be used autonomously, or in collaboration with creative people. It is also used in cognitive modelling projects, to shed light on aspects of human and animal creativity. In the last decade, Computational Creativity has come of age, as evidenced by special issues of publications such as the Minds and Machines journal and the AI magazine, and the first International Joint Conference on Computational Creativity, which replaced 10 years of successful workshops at major AI conferences. 

The proposed Leadership Fellow, Simon Colton, is a recognised expert in Computational Creativity, and has been working in the field since 1996. He is unique in having been involved in successful applications of creative software to four different domains, namely mathematical invention, video game design, graphic design and the visual arts. His mathematical theory formation software, HR, has produced theorems and concepts published in the mathematical literature; his visual art software, The Painting Fool, has produced pictures that have been exhibited and attracted much public attention; and research being done in the Computational Creativity group that he leads at Imperial College is helping video games companies to design the next generation of adaptive, personalised games.

A number of authors, such as Boden, Wiggins and Ritchie, have introduced formalisms which help us to be more precise about the creativity of software. However, there is no agreed upon theory which can describe the behaviour of software with sufficient acuity, coverage and formality that enables accurate comparison of implementations. In short, we have no generic way of saying that software B is more creative than software A. This has held back our field, because with no concrete and formal measures of the creativity of the software we build, it has been hard to put forward falsifiable scientific hypotheses that one approach is more creative than another, hence it has been difficult to progress, and to show progress. 

With this Fellowship, we propose to change this situation, by developing Computational Creativity Theory (CCT). This will comprise a series of models, each of which contains some conceptual definitions and some calculations involving those definitions which can be used to compare and contrast the creativity of software. The foundational models will make more precise the notion of a creative act and the impact they can have, and the more acute models will cover aspects of creative behaviour including intentionality, interpretation, imagination, appreciation and affect. To model computer creativity sufficiently well, we generalise past the merely generative and past usual AI notions of value, into new areas where software is expected to invent its own aesthetic and utilitarian measures, and frame its creations by describing its motivations, intentions, methods and innovations and by putting its work into historical and cultural contexts.

The proposed programme of research has the development of CCT at its heart. This is informed by a series of practical projects involving applications to creative language, music, visual arts, mathematics and games, and covering modes of creativity including realtime generation, assistive technologies and creative collaborations. By building and disseminating CCT, we will help to bring Computational Creativity research into a new era, where formal notions of creativity underpin software systems which really enrich our cultural lives.",,"Creativity is a hugely emotive, loaded and often confusing and contradictory term. While the creative industries of a country are as important to its economy as its manufacturing output, we are only beginning to understand the value of innovative practice in the workplace. Scientists, educationalists, engineers, politicians, leaders of industry, philosophers and artists all study creativity for different reasons, whether it is as an intellectual challenge, to increase productivity or to drive through policy changes. The popular conception of human creativity is confused and often steeped in mythology and romantic notions based on ill-defined terms such as imagination and inspiration. Given all these factors, our study of the controversial notion that software can be creative has the potential to seriously impact business, education and culture. In particular, our proposal to impose a concrete formalism able to describe creative behaviour in such a way that numerical comparisons can be used to measure creativity, is likely to be disruptive, and to have a wide impact.

In September 2010, the British Council in Madrid organised a public event to mark its 70th anniversary. The proposed Fellow, Simon Colton, was asked to speak at the event, alongside Ramon Lopez de Mantaras, a high profile Spanish AI researcher with similar interests in creativity studies. The public interest in the notion of software being creative is sufficiently high that Spain's two most popular newspapers, El Pais and El Mundo both covered the event, and ran articles in their weekend editions which covered Colton's work on The Painting Fool project (which aims to build a software system which is one day taken seriously as a creative artist in its own right). The combined print circulation of the newspapers is around 700,000 and they are the most popular Spanish language newspapers on the internet. Hence we hope that the issues of Computational Creativity were firmly planted in the minds of tens or hundreds of thousands of people. Again, due to our covering notions of creativity, our work has similarly been discussed in the New Scientist, More4 TV news and the Metro Newspaper. We will continue to react to press enquiries by being as forthcoming as possible about our work and the creativity issues it raises. Moreover, we will pro-actively write articles for the popular and specialist press, and regularly print and disseminate summary information about our work to governmental, educational and industrial organisations, in addition to releasing iPhone/Android/iPad applications for general consumption.

Software which can act in creative ways is clearly of value to the creative industries and the wider Digital Economy. We have already collaborated on funded projects with the Emote, Introversion, Lionhead, NestorGames and Rebellion games companies, in addition to Universal Music and Sony. Two of the project partners are from industry, and we will work closely with them to embed our research ideas in their working practice. As an example of the kind of industrial impact we expect from the Fellowship, in summer 2010, the proposed Fellow was asked to sit on a steering group looking at the future of digital tools for the creative industries, organised by the Creative Industries Knowledge Transfer Network (CIKTN). This led to the publication of a beacon project report, which was widely circulated to creative industry firms, educational establishments and governmental bodies. On the inside cover was a quote from the proposed Fellow: &quot;We are approaching an exciting time when computers will be powerful enough, and software sophisticated enough, for computers to be our creative partners&quot;. Again, we hope that this led to Computational Creativity issues being raised in creative industry firms and wider organisations, and working with the CIKTN to meet new industry partners and disseminate our work widely form important pathways for impact that we have planned."
15,9688FD27-9E10-4D8E-9D7B-E3E599227BBD,SOCIAM: The Theory and Practice of Social Machines,"SOCIAM - Social Machines - will research into pioneering methods of supporting purposeful human interaction on the World Wide Web, of the kind exemplified by phenomena such as Wikipedia and Galaxy Zoo. These collaborations are empowering, as communities identify and solve their own problems, harnessing their commitment, local knowledge and embedded skills, without having to rely on remote experts or governments.

Such interaction is characterised by a new kind of emergent, collective problem solving, in which we see (i) problems solved by very large scale human participation via the Web, (ii) access to, or the ability to generate, large amounts of relevant data using open data standards, (iii) confidence in the quality of the data and (iv) intuitive interfaces.

&quot;Machines&quot; used to be programmed by programmers and used by users. The Web, and the massive participation in it, has dissolved this boundary: we now see configurations of people interacting with content and each other, typified by social web sites. Rather than dividing between the human and machine parts of the collaboration (as computer science has traditionally done), we should draw a line around them and treat each such assembly as a machine in its own right comprising digital and human components - a Social Machine. This crucial transition in thinking acknowledges the reality of today's sociotechnical systems. This view is of an ecosystem not of humans and computers but of co-evolving Social Machines.

The ambition of SOCIAM is to enable us to build social machines that solve the routine tasks of daily life as well as the emergencies. Its aim is to develop the theory and practice so that we can create the next generation of decentralised, data intensive, social machines. Understanding the attributes of the current generation of successful social machines will help us build the next.

The research undertakes four necessary tasks. First, we need to discover how social computing can emerge given that society has to undertake much of the burden of identifying problems, designing solutions and dealing with the complexity of the problem solving. Online scaleable algorithms need to be put to the service of the users. This leads us to the second task, providing seamless access to a Web of Data including user generated data. Third, we need to understand how to make social machines accountable and to build the trust essential to their operation. Fourth, we need to design the interactions between all elements of social machines: between machine and human, between humans mediated by machines, and between machines, humans and the data they use and generate. SOCIAM's work will be empirically grounded by a Social Machines Observatory to track, monitor and classify existing social machines and new ones as they evolve, and act as an early warning facility for disruptive new social machines.

These lines of interlinked research will initially be tested and evaluated in the context of real-world applications in health, transport, policing and the drive towards open data cities (where all public data across an urban area is linked together) in collaboration with SOCIAM's partners. Putting research ideas into the field to encounter unvarnished reality provides a check as to their utility and durability. For example the Open City application will seek to harness citywide participation in shared problems (e.g. with health, transport and policing) exploiting common open data resources. 

SOCIAM will undertake a breadth of integrated research, engaging with real application contexts, including the use of our observatory for longitudinal studies, to provide cutting edge theory and practice for social computation and social machines. It will support fundamental research; the creation of a multidisciplinary team; collaboration with industry and government in realization of the research; promote growth and innovation - most importantly - impact in changing the direction of ICT.",,"The proposed programme will have beneficial impact on a wide range of stakeholders. Via technology transfer, companies will gain access to new technologues, and also gain the understanding that will allow them to develop new products for communities organising themselves in social machines. Those companies that partner us or support our research will of course have the ability to feed ideas into the research, and frame the problems we are trying to solve; we consider it essential that fundamental research feeds into, and back from, real-world applications.

Smaller-scale entrepreneurs will have new outlets for innovation, and new opportunities to develop radical business models. The public sector and third sector will have available new tools and methods for achieving policy ends. Communities using social machines will also benefit, of course, by the ability to identify and define their own problems, and develop their own solutions. These benefits, in social cohesion and cooperation, will often outlive the immediate issue which drove the development of the social machine.

We should not forget the benefits to the wider academic community of the proposed research. Of course, the development of a community of multi-disciplinary researchers in social machines will benefit the computer science field, but via the observatory and the strong social relevance of the research, we would expect a wide academic community in science and social science benefiting from the deepening of expertise in this area, and the large quantity of data. The 5-year programme would allow a strong multi-disciplinary cohort of researchers to emerge, able to influence a range of fields, spreading expertise in these relatively novel methods of social collaboration. Dissemination will also take place via our programme of Town Meetings, sandpits, hackathons, disruptive skills workshops, etc. Groups associated with the consortium, such as the Web Science Trust, will be able to ensure that SOCIAM's work is widely disseminated and one of our Partners is the world's largest Technical PR Agency.

The impacts will be both economic and non-economic. The economic impacts will be the benefits that come from innovation and cooperation, and from bottom-up solutions to problems. These will include both lowering costs of social problems (e.g. via community policing lowering the costs of crime), and creating opportunities for innovation and commercial exploitation of innovation (as for example with the development of new services based on creative uses of available data). Some of these benefits will fall to entrepreneurs, while others will spill over into the wider community.

Furthermore, the research will enable value to be extracted from the ever-growing quantities of data we see. The social return on investment in data acquisition, particularly public open data, will be dramatically improved as more tools and methods are created for using the data to drive services.

There will be several non-economic impacts too. In policy terms, the impacts will be high, particularly as local solutions for problems - inherently more efficient than centralised problem-solving which cannot always take account of local conditions - will emerge from collaboration in social machines in small communities. Communities will become empowered and self-reliant. The result will be a suite of tools and methods which can be put to work in social contexts by a range of actors - government, to achieve policy goals, groups of people, to achieve social goals, or entrepreneurs, to achieve commercial goals. Indeed, one would expect a social machine to encompass all of these at different times."
16,2734F56F-3DD9-4F4B-B7A3-19CBADB5B60C,Network on the Verification and Validation of Autonomous Systems,"Robots, driverless cars, unmanned air vehicles, etc, can all be built now. However, the main barriers holding back the widespread use of robotics and autonomous systems can be seen as societal: what should the legal framework be for such systems; how can the public come to trust these systems; how can we ensure they are safe; and how do we know such a system will make the decisions we would expect of it? All these are currently impossible to solve with any certainty.

The UK has numerous key research developments concerning the Verification and Validation (V&amp;V) of autonomous systems that can all impact upon this problem. These are clearly of relevance to methods for designing, constructing and deploying autonomous systems but also have importance to Psychology (e.g. social robotics), Philosophy (e.g. machine ethics), and Law (e.g. certification). Constructing autonomous systems without behaviour guarantees can lead to serious outcomes, and may consequently hold back the widespread adoption of these systems. 

This Network will coalesce this activity, drive the research agenda forward, and embed the necessity for V&amp;V firmly within industry, the government, and the public.",,"We aim to engage closely with governmental agencies, professional societies, European and international institutions to contribute to the public debate on issues influenced by the Verification and Validation (V&amp;V) of Autonomous Systems, leveraging and deepening our existing memberships and influence in such bodies. 

Many of the academics involved in the Network, together with their respective universities, have effective mechanisms for dealing with the media, with access to press offices who regularly publicise work nationally and internationally to non-academic audiences. We have already seen how autonomous systems (typically, Robotics) can spark the public's imagination. We will build on this, expand to other forms of autonomous systems (UAVs and driverless cars are natural possibilities), and target public awareness and communication. This is particularly important since V&amp;V provides a viable route towards increased safety, reliability, and public trust.

Our aim, particularly through our Stake-holder Workshops is to engage with a broad range of industries. Although these will initially comprise companies interested in safety, reliability and certification of autonomous systems, we intend to ensure that the message concerning V&amp;V, will be spread and so will aim for publicity across our industrial networks. The Network Portal that will be produced is also important for increased awareness, and showcasing case studies and best practice. It details academic groups within the Network, links to key/new research, case studies in V&amp;V, descriptions of broader application of V&amp;V techniques, material for publicity and opportunities for engagement, and further links to industrial activities. The Portal, together with the Stake-holder Workshops, will provide a focus for material and activity in this area, and will spur further industrial and publicity activities. Finally, we will engage with industrial participants, particularly at these workshops, in order to explore appropriate mechanisms for industrial V&amp;V, ways to stimulate industrial take-up, and sustainable models for the future of the Network."
17,EEF9B859-6CAF-43B4-89D6-51780D131126,ImmunoHopping: Creating New Nature Inspired Cyber Defences,"The Internet of Things has great potential to revolutionise the way in which we deploy networked devices, and to provide networking capability to every-day objects, making them 'smart objects'. Security should be at the core of these newly developed smart objects, but innovation is outstripping the development of security in this context. There is much emphasis on the positive side of this technology without considering the negative implications. It is not too challenging to think of many ways how the Internet of Things can be abused letting outsiders in through a digital ruse. This would include intruders gaining access to a lighting system, to remotely switch off the lights in a property, to assist in home burglary. Its also not too far of stretch to imagine an intruder turning on a cooker remotely, with the potential to cause a form of &quot;digital arson&quot; which we have never before experienced. Yes, it is amazing to be able to text your cooker so that dinner is ready when you get home. However, do we really want these features if it leaves us vulnerable to digital attack on our properties? 

Vast improvements need to be made in the state of the art of cyber defences in order to prepare and protect ourselves for the imminent innovations in digital technology. Novel and effective solutions in computer based security are imperative to research as current techniques may not prove effective in this new context. In order to create the next generation of cyber defence tools we must look to new sources of inspiration. One of these can be in the form of studying how this problem is solved in natural systems, in particular the defence and response mechanisms of the human immune system. 

Artificial immune systems (AIS) are one potential solution which may have significant impact on future cyber defence. They are designed to solve computational problems through studying natural mechanisms in immunology. Current research in AIS for computer security focuses purely on detection of anomalies, leaving the user to respond to the detected threat. Few of these systems actually produce any form of response as a result of detecting a potential intrusion. This is problematic in the Internet of Things as the responsibility would lie with the homeowner who is not a cyber security expert, leaving homes potentially vulnerable to digital intrusions. The novelty of this proposed research is to create a prototype responsive artificial immune system - RAIS, which can both detect intruders and produce appropriate responses in order to mitigate the problem of automatically responding intrusion detection systems. Persistent engagement with a cyber defence stakeholder will ensure that the prototype system is useful in cyber defence applications. 

Our approach to this is to perform a deep interdisciplinary study of the translation of detection to response within the human immune system by modelling immune responses. A mechanism in immunology termed the 'immunological synapse' will be studied form the basis of a model used to create a novel blueprint for the responsive artificial immune system. This will occur through constructing agent-based models of the natural system from which these necessary properties can be abstracted by looking at how two cell types, Dendritic Cells and T-helper cells interact to produce immune responses to pathogens. We will model this interaction using knowledge already amassed by the host group, and aim to extend the research through performing further experiments to refine these models. The discipline hop is to be hosted within an immunology lab, whose research aims to understand immune mechanisms of response in order to create immunotherapies for treating cancers, by turning the immune system against detected tumour cells. Understanding of natural immune responses is key for both the future developments of artificial immune systems and also in how to use the immune system therapeutically in the fight against cancer.",,"The direct impact of this research has two distinct areas: in the cyber security domain and in the clinical immunology domain. Cyber attacks on UK businesses and homes are caused by imperfections in software and infrastructures, which are exploited by a mixture of those involved in organised criminal activity and less so by curious individuals causing attacks for the prestige of exploiting vulnerabilities. Either way, this is a very costly business, impacting severely on the UK economy. Any tool or technique which can potentially prevent the exploitation of computer systems will be useful in mitigating such attacks. Current countermeasures of anti-virus software, network firewalls and intrusion detection systems are no longer adequate to protect against powerful attacks, not only on traditional but on emerging networks. Attacks to mobile 'smartphones' are becoming more prevalent, but security techniques have not adapted to cope with demand. Bring your own device networks are becoming increasingly common within the commercial sector, and traditional countermeasures cannot cope with the diversity of devices and services required by users. Similarly, military networks are also required to be connected to the internet in addition to managing highly sensitive data and services. This is difficult to achieve without compromising the required functionality. As we see the emergence of the Internet of Things, we may become extremely vulnerable to new types of attack which will impact upon us in a very private and personal setting. Again, traditional countermeasures are not directly applicable in this scenario. The onus is on the user to monitor the Internet of Things domestic network and to manually respond to a given threat. This is unrealistic to expect domestic users to become system administrators, and thus the development of an intrusion detection system which can automatically respond to threats would be ideal. This grant aims to make a significant impact in the way in which we develop and deploy such systems for this new and emerging technology. 

The secondary impact is through the development of novel models of immune responses. The way in which the immune system responds to threats is a vital piece in the puzzle for understanding how to fight of infectious diseases, this being particularly pertinent given the current problems with diseases such as Ebola. Learning how the responses work and more importantly how to modulate these responses is key in providing future immunotherapies. Additionally, the immune system is integral in the human body's response to cancer. By understanding how to manipulate immune responses, it is possible to to derive therapies which can limit the effects of and destroy cancerous growths. This forms a main part of the research performed at the Academic Unit of Clinical Oncology, which is the partner institution for this proposal. The models generated as a result of this proposed research will assist in clarifying our understanding of how the immune system responds and thus how it may be manipulated for clinical purposes. This is particularly important bearing in mind the ageing population. 

Impact will be achieved not only through traditional dissemination routes of publication of academic papers and conference demonstrations, but through using the extensive outreach network already in place within Julie's current role as Outreach Officer for the School of Computer Science. The work will be presented at workshops for children, on university open days, public lectures and in particular at Women In Technology events within the UK. In addition, additional outreach is performed in conjunction with a popular YouTube channel termed computerphile, which aims to educate the general public. Julie has already provided material for three videos on artificial immune systems, and the intention is to work with the makers of computerphile in order to gain recognition of the research presented in this proposal"
18,DA643CB0-D437-48F2-83F5-B8EC1F448988,SimplifAI,"This project seeks to investigate the feasibility of the integration of a range of data sets about an urban area to enable transport operators to manage traffic flows more effectively, with particular application to the control of transport-determined air quality levels. The data sets include time varying data about traffic (average speed, flow rate), weather (wind speed, direction, temperature) and air quality (NOx concentrations), as well as static data about route topology, geography and infrastructural assets. 

These data sets will be enriched to enable the use of an automated approach to derive regional strategies for urban transport operators, so that traffic flows can be influenced strategically, and in real time, through an urban region. Currently the creation of regional strategies by traffic operators is a time consuming manual process requiring a high level of experience and expertise, and is aimed specifically at minimising delay for traffic during exceptional events (such as road closures or large concerts). 

The project will study the feasibility of a novel, two stage approach: 

(i) exploring a new way of enhancing and enriching transport and environmental data feeds by adding semantics in the form of meta data and ontological context, to improve the clarity and usability of the data; 

(ii) enabled by (i), provide automated support for the real-time creation of regional transport plans (strategies) that urban transport operators can enact. 

The success of this study, and consequent exploitation of the technological advances, would lead to UTMCs being empowered to address the urban challenge of ensuring satisfactory air quality within urban areas, and will be able to inform specific road user groups (e.g. cyclists) of air quality conditions on their route choices. More generally, UTMCs will have access to technology to support the creation of regional strategies to address other challenges, such as dealing with road closures, multiple resource effecting scenarios, or balancing flows on the network, within real-time.",,"Using a wide range of data about current and impending traffic flows, weather, the built environment and air quality readings, it is possible to predict an air quality problem within an urban region during a particular time period. What is not currently possible is to generate, in real time, specific strategies specifying changes in combinations of UTC assets (traffic lights, information signs, variable speed limits etc) that if carried out immediately are likely to avoid the air quality problem. This is the problem that we are attempting to solve, and if successful the feasibility phase has the potential to lead to widespread impacts. As well as the benefit on efficiency of strategy production, automation can lead to reduced errors in the process, and less reliance on in-house expertise.

Transport for Greater Manchester (TfGM) will benefit as they will gain a greater understanding of the sources and content of available data assets, and how they can enhance their service using this data. They will also increase their expertise in the construction of strategies for regional control of road traffic, and at the end of the project be able to utilise any tools delivered in helping to create the strategies for influencing flows of traffic through their region. In particular, they will be able to explore a means of influencing regional traffic flows in order to avoid compromising air quality standards. Partners BT, InfoHub and KAMfutures will have a greater understanding of the added value of heterogeneous data sets, their integration and enrichment, and the potential of centralised control strategies, to help in complex system management. Such is the ubiquitous nature of the problem of influencing regional flows of urban traffic, the products in terms of software tools and data sets which come about from the research have the potential to be sold commercially on a global basis. 
 
UTMCs generally will benefit as they will have an examples of tools and methods that, taking advantage of a range of data, can be used in regional control. Starting with TfGM, the fusing of the data sets and development of the proposed technology will empower UTMCs to expand the service they provide to the community, and where that is used for air quality management, improve the quality of life for anyone using the Urban area. This would particularly help UTMCs where operator expertise is sparse. Drivers and the general public will eventually benefit as the use of the data and consequent more informed regional controls will better inform and control vehicles, better balance the network resource, and help to avoid pollution hot spots. Hence the research has the potential to improve the nation's health, by avoiding pollution build-up, and the nation's wealth, by commercialisation of any tools produced and data sets used to be sold in UTMCs worldwide. The timescale for impacts to be made will be approximately 3 years after the end of the feasibility, as the project will need to move through the demonstrator and full prototype stages before operational use."
19,4B5BE37F-4453-4794-92AB-CDAD52241A23,MACACO: Mobile context-Adaptive CAching for COntent-centric networking,"Finding new ways to manage the increased data usage and to improve the level of service required by the new wave of smartphones applications is an essential issue. The MACACO project proposes an innovative solution to this problem by focusing on data offloading mechanisms that take advantage of context and content information. Our intuition is that if it is possible to extract and forecast the behaviour of mobile network users in the three dimensional space of time, location and interest (i.e. 'what', 'when' and 'where' users are pulling data from the network), it is possible to derive efficient data offloading protocols. Such protocols would pre-fetch the identified data and cache them at the network edge at an earlier time, preferably when the mobile network is less congested, or offers better quality of service. Caching can be done directly at the mobile terminals, as well as at the edge nodes of the network (e.g., femtocells or wireless access points). 

Building on previous research efforts in the fields of social wireless networking, opportunistic communications and content networking, MACACO will address several issues in this space. The first one is to derive appropriate models for the correlation between user interests and their mobility. Lots of studies have characterised mobile nodes mobility based on real world data traces, but knowledge about the interactions with user interests in this context is still missing. To fill this gap, MACACO proposes to acquire real world data sets to model mobile node behaviour in the aforementioned three-dimensional space. The second issue addressed is the derivation of efficient data-offloading algorithms leveraging the large-scale data traces and corresponding models. Firstly, simple and efficient prediction algorithms will be derived to forecast the node's mobility and interests. Then, MACACO will provide data pre-fetching mechanisms that both improves the perceived quality of service of the mobile user and
noticeably offloads peak bandwidth demands at the cellular network. A proof of concept will be exhibited though a federated testbed located in France, Switzerland and in the UK.",,"The MACACO project is totally aligned with the expected types of impact of the second topic of the CHIST-ERA 2012 call. If successful, MACACO will foster new &quot;services enabling the emergence of innovative network technologies&quot; by providing the required context- and content-aware models and protocols to manage the increased data usage required by the new wave of smartphones applications. By doing so, MACACO aims to reinforce the European scientific excellence in the mobile service provision and helping European carriers to offload their cellular traffic.

Additionally, by detecting and modelling the correlations between user mobility and the traffic demand he/she generates, MACACO aims to strengthen the research field of mobile networks and human behaviour prediction and &quot;develop a deeper fundamental and comprehensive understanding of new enhanced communication network architectures&quot;. Moreover, by facilitating development of new context- and content-aware applications, the project expects to foster significant innovation for industry. Novel and improved services will also have a significant for final users, given the fundamental role played by mobile Internet nowadays. In fact, several companies (from small start-ups to corporations) can benefit from improved wireless broadband services and from innovative content delivery mechanisms.

As also required by the call goals, MACACO &quot;brings together researchers and research communities working on distinct network layers and on content and context extraction in the broader framework of a content- and context-adaptive communication networks&quot;. As stated before, the consortium was carefully constituted to gather partners that are pretty complementary and qualified to address the context-content correlation and related data offloading challenge. This constitutes one of the strengths of the project, which could not be conducted with the participation of only one or few of the involved partners. Thus, in addition to the technological impact, MACACO will have a significant impact in terms of competence building. The partners will combine research and experience in a wide set of areas to gain unique competence, which will be brought forward to other European partners through the dissemination and exploitation activities of the consortium."
20,5367219A-264F-4CAD-95C5-098E713DA9A8,An Integrated Vision and Control Architecture for Agile Robotic Exploration,"Autonomous robots, capable of independent and intelligent navigation through unknown environments, have the potential to significantly increase human safety and security. They could replace people in potentially hazardous tasks, for instance search and rescue operations in disaster zones, or surveys of nuclear/chemical installations. Vision is one of the primary senses that can enable this capability, however, visual information processing is notoriously difficult, especially at speeds required for fast moving robots, and in particular where low weight, power dissipation and cost of the system are of concern. Conventional hardware and algorithms are not up to the task. The proposal here is to tightly integrate novel sensing and processing hardware, together with vision, navigation and control algorithms, to enable the next generation of autonomous robots.

At the heart of the system will be a device known as a 'vision chip'. This bespoke integrated circuit differs from a conventional image sensor, including a processor with each pixel. This will offer unprecedented performance. The massively parallel processor array will be programmed to pre-process images, passing higher-level feature information upstream to vision tracking algorithms and the control system. Feature extraction at pixel level results in an extremely efficient and high speed throughput of information. Another feature of the new vision chip will be the measurement of 'time of flight' data in each pixel. This will allow the distance to a feature to be extracted and combined with the image plane data for vision tracking, simplifying and speeding up the real-time state estimation and mapping capabilities. Vision algorithms will be developed to make the most optimal use of this novel hardware technology.

This project will not only develop a unique vision processing system, but will also tightly integrate the control system design. Vision and control systems have been traditionally developed independently, with the downstream flow of information from sensor through to motor control. In our system, information flow will be bidirectional. Control system parameters will be passed to the image sensor itself, guiding computational effort and reducing processing overheads. For example a rotational demand passed into the control system, will not only result in control actuation for vehicle movement, but will also result in optic tracking along the same path. A key component of the project will therefore be the management and control of information across all three layers: sensing, visual perception and control. Information share will occur at multiple rates and may either be scheduled or requested. Shared information and distributed computation will provide a breakthrough in control capabilities for highly agile robotic systems.

Whilst applicable to a very wide range of disciplines, our system will be tested in the demanding field of autonomous aerial robotics. We will integrate the new vision sensors onboard an unmanned air vehicle (UAV), developing a control system that will fully exploit the new tracking capabilities. This will serve as a demonstration platform for the complete vision system, incorporating nonlinear algorithms to control the vehicle through agile manoeuvres and rapidly changing trajectories. Although specific vision tracking and control algorithms will be used for the project, the hardware itself and system architecture will be applicable to a very wide range of tasks. Any application that is currently limited by tracking capabilities, in particular when combined with a rapid, demanding control challenge would benefit from this work. We will demonstrate a step change in agile, vision-based control of UAVs for exploration, and in doing so develop an architecture which will have benefits in fields as diverse as medical robotics and industrial production.",,"High technology industries have been identified as a key sector in the UK economy. Robotic and autonomous systems have in turn been singled out as technologies of particular importance to maintain economic competitiveness in high technology applications. We intend to develop the next generation of active vision sensors for autonomous robots, focusing on delivering systems with real-time high-speed functionality at low-power, low-weight, and low-cost. These will have potential applications across a range of industries, including security, transportation, manufacturing, agriculture, nuclear and healthcare. Just some of these applications are highlighted below.

Agile micro air-vehicles, and more generally, advanced vision-based navigation systems for autonomous robots will find both civilian and military applications in reconnaissance and search and rescue operations. The system being developed will be applicable in unmanned vehicles (air, land and water based), either as a primary navigation and control system, or alternatively to enhance safety and provide additional features. Further applications include inspection (e.g. nuclear decommissioning, inspection of overhead power lines, monitoring of oil rigs or power sub-stations, agricultural surveys, etc) and space exploration. In all these fields, robots are currently used, but our research will offer significant performance benefits, expanding the scope of applications. Applications of autonomous robots, from self-driving automobiles, to drone-based goods delivery and robotic companions, have been recently attracting both large amount of public interest and significant industry investment. While some of these still remain long-term aspirations, the companies we collaborate with on this project see a more immediate (on a 5-10 year horizon) use of our technologies in their application domains.

There is widespread expectation of autonomous robots entering everyday life - but for many applications not just the performance but also the system cost, size and power consumption are currently prohibitive. The combination of performance and small-size/low-cost of our proposed system will make it suitable for cost-sensitive applications, from consumer robotics to toys. Similar properties are also needed for automated video surveillance (especially distributed smart cameras, e.g. crowd and traffic monitoring, early forest-fire detection, or fall monitoring for the elderly) and in vehicular applications (e.g. parking assistance, collision avoidance and driver alertness). Low-power intelligent sensing is a prerequisite for the plethora of much talked-about &quot;internet-of-things&quot;, &quot;ambient intelligence&quot; and &quot;cyber-physical systems&quot; applications, and in portable and wearable systems.

The unparalleled high-speed potential of our near-sensor vision processing approach will be a key advantage in the field of manufacturing process control, where an advanced machine vision system can provide the opportunity to respond to and to control high-speed events. These might include component manufacture and assembly, laser welding, control of industrial robots, and high-speed metrology for sorting or visual inspection for quality assurance. 

Further afield, and on longer time-scales, potential applications of the developed technologies can be identified in fields ranging from nuclear research to healthcare. For example, high-speed sensor-level tracking systems, based on the technologies we will develop could be used for beam control in particle accelerators, or to reduce the highly-redundant data typically collected by these systems, or to locate and track radiation instability within nuclear fusion research facilities. In healthcare, ultra high-speed vision systems could be used to improve the accuracy and reduce time-to-diagnosis of cancer screening - identifying circulating tumour cells from blood samples of several million cells."
21,DDF817C9-E1D7-4FD6-99C8-D3750073D114,CRITiCaL - Combatting cRiminals In The CLoud,"The Cloud is an emerging technology that offers democratic access to computing power, data storage, software and services often for a small pay-per-use cost. Like any new technology the Cloud has potential for great good, but in the wrong hands can facilitate criminal activity. Within this project we seek to understand the different types of crime that can happen in the Cloud, build systems that will allow the detection of this criminal behaviour and enable the use of digital evidence to lead to successful prosecution of Cloud crime perpetrators.

In order to achieve this goal we are forming a truly inter-disciplinary research centre leveraging the strengths of both Durham and Newcastle Universities. Bringing together the strengths of Durham in criminology, law and ethics along with the strengths of Newcastle in the areas of (computer) systems security, artificial intelligence, data mining and psychology. We are convinced that Cloud crime can only be detected and tackled by such a truly inter-disciplinary centre. Such a centre will actively create the research foundations for successful computational methods in crime detection combined with good user engagement, generating research that can cross disciplines and directly inform public policy, police and prosecution practices and transform public understanding of Cloud crime. 

This will involve development of a true understanding of what crime can be conducted on the Cloud. Facilitated through the development of cloud crime scripts, defining the activities of a criminal act, which will aid discussion between the different disciplines and must be presentable in a format understandable by our key stakeholders: Cloud providers/users/developers, law enforcement agencies and the criminal justice system.

The detection of criminal activity in the cloud requires the integration of heterogeneous sensors, aggregation and analysis techniques, where we draw upon existing expertise in cloud security assurance (Gross, IBM), host monitoring and anomaly detection Ben-ware (McGough, Wall, DSTL), and fuzzy search on unstructured data, intrusion detection and analysis (Nifty, Yan). We propose combining the systems expertise with complementary techniques in artificial intelligence, including data mining (McGough), behaviour machine learning, anomaly detection (Ploetz) and hierarchical machine learning and knowledge extraction (Bacardit).

This portfolio gives raise to multiple means to derive and combine intelligence, present bespoke visualizations, situational awareness, grammar or language generation for the cloud crime scripts. Thus allowing the centre to tailor the intelligence, and its presentation, to a given stakeholders needs. We propose using additional human computation and crowd sourcing techniques to reduce the number of situations where the system incorrectly identifies a criminal act. The use of human computation and crowd sourcing will also allow us to hone the machine learning system, developing a suite of hybrid techniques that, together, will improve cloud crime detection but will frame the results in such a way as to support subsequent crown prosecution processes. This latter achievement will require expertise in the disciplines of criminology, forensic sciences, law and ethics and will require collaboration with police forces throughout the UK and Action Fraud. 

In addition we will bring in relevant work around (i) forensic psychology (Oxburgh) that will deliver case-sensitive interview and investigative procedures for witnesses, victims and investigators; (ii) prosecution procedures that will ensure that evidence going to court is not compromised by intelligence gathering methodologies and (iii) prevention of underreporting of Cloud crime and improvement of public understanding and confidence.",,"It is accepted that the use of the Cloud is expanding at an exceptional rate and the technology is developing very rapidly. The Centre is committed to not just keeping pace with the technical development but also to enable understanding of the issues behind the new areas of criminality in the cloud. The Centre will maximise the impact (in knowledge, societally and economically) of its activities for the UK and the region, following Newcastle University's dedication as civic university and the Centre for Cybercrime and Computer Security (CCCS) theme 'Protecting Society's Fabric'. Impact from the activities will fall in the broad areas below:

1. Knowledge on cloud crime and cybercrime. 
Outputs from Work Packages (WP) 1 and 5 will enable greater understanding of the environment and criminal use of the Cloud. Through our stakeholder network this will expand knowledge, assets and capacities for all stakeholders involved, be it Law Enforcement Agencies (LEA) - police forces etc., cloud providers, victim organizations, or government. This includes:

- Methodological knowledge in terms of guidelines and best practices affecting policy 
- Action plans to exercise upon such as investigative interview scripts
- Recommendations for policing and governance for policy and operational development.

Output from WP 2 will enable the use of novel system architectures to gain real time situational awareness, including real-time detection and simulation thereby delivering intelligence on the criminal activity. This will also offer the opportunity of use by LEAs for better approaches to digital forensics and detection. 

2. People and UK Cyber Security Experts.
Expansion of the knowledge base understanding and expertise by the Centre are critical aspects of WP 2, 4 and 6. By the research of the centre and through the engagement strategy there will be effective knowledge transfer to stakeholders in addition all researchers will have the environment to become experts of the fields of cloud computing and cloud crime. The Centre Draws together experts from Durham and Newcastle forming a hub of cybercrime and security research in the North East that is second to none. Additional impacts include:

- PhD students, in particular with the EPSRC DTCs Cloud Computing for Big Data and Digital Civics in collaboration and supervised by investigators, additional PhDs contributed by the institutions, and GCHQ/ACE-CSR PhDs aligned with the centre topics.
LEAs, police forces and SME cloud developers will both have access to experts and will themselves have the opportunity to become experts in cloud and cyber crime understanding for policing and governance.

3. Economics.
The north East already has a significant ICT and digital economy (e.g. Sunderland Software city, Dyanamo NE, Cobalt Data park) the centre will provide an additional growth impulse to these initiatives. Drawing on a fabric of SMEs already present and connected to the universities and centres. (WP6)
Amplifying government and council investments from Newcastle and Sunderland council, as well as the universities, e.g. Science Central in Newcastle, or the council/government co-funded Newcastle cloud centre.
WPs 2-6 will create a critical mass for cloud security research aligned with the G-cloud vision exercised in the Northeast.

4. Society.
Use of the Cloud will increase dramatically in the future; the development of policies, detection methods, understanding can be used a source of new policy development and discussion. In particular the Centre will improve public confidence as pursued in the user engagement strategy (WP5, 6) and drive down under-reporting as discussed with Northumbria police.
Feedback with end-users from both virtual real communications how cloud crime and their harm is taken care of and a better understanding of the criminal environment will lead to higher detection rates and confidence in the LEAs."
22,5A592DE6-D2A9-443F-A87C-13E18E56B026,"Understanding scenes and events through joint parsing, cognitive reasoning and lifelong learning","The goal of this MURI team is to develop machines that have the following capabilities:
i) Represent visual knowledge in probabilistic compositional models in spatial, temporal, and causal hierarchies augmented with rich attributes and relations, use task-oriented representations for efficient task-dependent inference from an agent's perspective, and preserve uncertainties;
ii) Acquire massive visual commonsense via web scale continuous lifelong learning from large and small data in weakly supervised HCI, and maintain consistence via dialogue with humans;
iii) Achieve deep understanding of scenes and events through joint parsing and cognitive reasoning about appearance, geometry, functions, physics, causality, intents and belief of agents, and use joint and long-range reasoning to fill the performance gap with human vision;
iv) Understand human needs and values, interact with humans effectively, and answer human queries about what, who, where, when, why and how in storylines through Turing tests.

Collaboration with US:
Principal Investigator: Dr. Song-Chun Zhu
Tel. 310-206-8693, Fax. 310-206-5658, email: sczhu@stat.ucla.edu
Institution: University of California, Los Angeles
Statistics and Computer Science
8125 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095
Institution proposal no. 20153924

Other universities in the US
CMU: Martial Hebert Computer Vision, Robotics &amp; AI
Abhinav Gupta Computer Vision, Lifelong Learning
MIT: Joshua Tenenbaum Cognitive Modeling and Learning
Nancy Kanwisher Cognitive Neuroscience
Stanford: Fei-Fei Li Computer Vision, Psychology &amp; AI
UIUC Derek Hoiem Computer Vision, Machine Learning
Yale Brian Scholl Psychology, Cognitive Science",,Not required
23,C4A23B1D-9CAE-44E6-9D7D-DA33CC16C8BD,Self-repairing hardware paradigms based on astrocyte-neuron models,"The human brain is remarkable in its ability to self-repair, for example following stroke or injury. Such self-repair results from a range of distributed and fine-grained mechanisms which act in tandem to ensure that the neurones (the basic building blocks in the brain) continue to function in as close to a normal state as possible.

In contrast modern electronic systems design typically relies on a single controller or processor, which has very limited self-repair capabilities. There is a pressing need to progress beyond current approaches and look for inspiration from biology to inform electronic systems design.

Recent studies have highlighted that interactions between astrocytes (a type of glial cell) and neurones in the brain provide a distributed cellular level repair capability where faults that impede or stop neuronal firing can be repaired by a re-adjustment of the local weights of connections between neurones in the brain.

This project aims to exploit these recent findings and develop a new generation of self-repairing algorithms by taking inspiration from these results to design a new generation of &quot;astro-centric&quot; algorithms. To achieve this we will include components representing both neurones and astrocytes in our electronic systems and model the interactions between these in such a way as to capture the distributed repair capabilities seen in the biological system.",,
24,559D161F-3F88-460D-8783-C20D420321EC,"Interdisciplinary Centre for Finding, Understanding and Countering Crime in the Cloud","The Cambridge Interdisciplinary Centre for Crime in the Cloud (CICCC) will combine the diverse range of skills available in the Institute of Criminology, the Faculty of Law and the Computer Laboratory at the University of Cambridge. Our approach will be multidisciplinary, including researchers with expertise in computer science, criminology, cybersecurity, economics, psychology, forensics and law.
Our approach will be data driven. We have negotiated access to some very substantial datasets including large feeds of data such as spam email messages and technical information about the operation of cloud services from several major cloud providers and public bodies. Together, they will constitute the largest data resource available anywhere outside of classified systems on abuse online; we will have more, and more diverse, data than almost all service firms or law enforcement agencies, creating a unique opportunity for research to develop new tools for cloud crime detection and forensics. 
We will mine and correlate these datasets to extract information about criminal activity. Our analysis will enhance our understanding of crime in the cloud, enable us to devise identifiers of such criminality, allow us to build systems to detect crime when it occurs, and ensure we collect evidence of wrongdoing to a high standard. We will work closely with law enforcement to ensure appropriate interventions can be undertaken.
Our overall objective is to create a sustainable and internationally competitive centre for academic research into cybercrime. The primary aim of the centre is to improve the security of users of cloud services, and to improve outcomes for those who would be affected by their misuse. We will develop a strong legal framework to operate in, and maintain high ethical standards in everything we do. We will incorporate this into APIs for abuse data sharing that support appropriate authentication, nonrepudiation and privacy mechanisms, and feed these back to the industry.
We aim to provide the police with an enhanced ability to search large amounts of data related to cybercrime in the cloud, improved forensics, better chain-of-custody mechanisms for evidence, additional training, and meaningful statistics on cybercrime. We have strong relationships with industry, and we will provide them with important data and insights on how, when and why criminality in the cloud occurs, thus enabling cloud providers to improve security for the users of cloud services by cracking down swiftly on abuse. We will also work closely with other academics, providing sanitised datasets to researchers generally, and enable trustworthy researchers access to our full dataset on the same basis as ourselves. This will solve the main problem faced by most academics who want to do research on cybercrime, namely the difficulty of getting access to real data on actual abuse.",,"Our project will have impact through four channels: law enforcement, industry, academia and to the public and policymakers generally.
We aim to develop better ways for academics and law enforcement to work together. We already have established relationships with UK and US law enforcement, having worked with SOCA in the past on tracking phishing, and with the Met on forensic tools to recover video from deleted files on mobile phones. We have an active collaboration with the US National Cyber-Forensics Training Alliance (which includes the FBI, Secret Service and various police forces).
One of the co-investigators directs the Cambridge Police Executive Programme, which has trained many chief constables and senior officers from forces worldwide. It attracts over 300 alumni and other participants to the annual Cambridge conference on Evidence-Based Policing and we will use this venue for impact. We will also disseminate our results through ACPO, whose cyber lead also heads up our regional cybercrime squad. From year three of the project, one of our research associates will focus on user engagement and will help the NCA, the Met and other police forces take on board not just our data but also the forensic tools and methodologies that we develop.
Industrial impact will be at least as important, and the service firms' big problems include how to share abuse data. At present some data is shared between key people on a basis of personal trust but this will not scale indefinitely. Our centre will start to fill that gap by providing a curated feed for sharing, rather than raw data; and by starting to develop the electronic infrastructure required. We will build on data-sharing APIs already under development by industry to provide, first, electronic signature mechanisms to assure the evidential value of data, and second, sanitized data based on a careful treatment of privacy issues, from the UK/EU, US and international viewpoints. We anticipate that this will be of sufficient value to the industry that by the end of year two we can start to attract serious industry financing and make the centre sustainable (independently of RCUK funding) by the end of year five. We will continue to work with industry security teams and the ad-hoc 'trust groups' as we have done for many years. We will continue to contribute data, code and intelligence to industry partners.
Cybercrime is moving rapidly up the political and news agenda with growing public concern that falling real-world crime is offset by rising crime online. This raises significant policy issues around the Home Office narrative of falling crime. It has also led to many media opportunities for the investigators, not just on UK TV and print media, but worldwide. Our public outreach and education activities will continue. If the grant is awarded to Cambridge we will have more results to talk about, and our media activities and expertise will enable us to maximise their impact.
Our earlier work on the costs of cybercrime demonstrated that most of these costs are indirect and stem from business foregone because of consumer reluctance to shop, bank or engage with government online, and because of defensive and clean-up costs, not all of which are appropriate. Better public appreciation of the real risks and costs is a public good, and it is also valuable to educate businesses generally about this topic. For example, our study showed that some 4% of offered transactions at UK e-commerce websites were declined because of alerts from fraud engines. Access by retailers to better fraud detection systems and methodologies can make a real contribution to the economy, not just by cutting rime, but by increasing sales.
Finally, we are active members of CSaP (Cambridge Science and Policy) which brings many senior civil servants to Cambridge on short visits. This will give us a channel to disseminate results to policy teams across Whitehall and in Brussels including to ministers."
0,60B1D4A1-9774-4E46-8D3F-14B9F2626275,Mathematical models and algorithms for allocating scarce airport resources (OR-MASTER),"Congestion at major airports in the UK and across Europe and the rest of the world is a serious and growing problem. Already Heathrow faces problems occasioned by serious congestion for a major part of the day while at Gatwick demand is expected to exceed capacity for 17 hours per day by 2025. According to a Eurocontrol study, planned capacity at the 138 Eurocontrol Statistical Reference Area (ESRA) airports is expected to increase by 41% in total by 2030, with demand exceeding airport capacity by as much as 2.3 million flights (or 11%) in the most-likely forecast growth scenario. 

The development and deployment of airport capacity is a major societal issue engendering intense public debate in the UK and around the world.
Capacity at congested airports is expressed in slots. A slot identifies a time interval on a specific date during which a carrier is permitted to use the airport infrastructure for landing or take-off. Current slot allocation procedures suffer (inter alia) from the following limitations:

1)Simplistic modelling of the objectives and operational/regulatory constraints bearing on the multiple stakeholders involved in (and affected by) the slot allocation process.

2)Insufficient capture of the interactions encountered in airport networks.

3)The use of empirical or ad hoc processes for determining (rather than computing) declared capacity which address neither the uncertainties involved in airport capacity assessment nor the complexity and size of the real-world problem, even at the single-airport level.

Consequently, existing approaches to the allocation of airport capacity fail in a number of critical ways to reflect the complexities presented by the real world. This creates allocation inefficiencies which, in turn, result in poor airport capacity utilisation with significant negative impacts on airport revenues, airline operating costs, the level of service offered to passengers and the environment.

There is thus a pressing need to meet the major scientific challenge of developing novel mathematical models and solution approaches to transform the airport slot allocation process and its associated outcomes. The programme grant aims to do just that for a single airport and for a network of airports. Mathematical models will be developed and analysed which consider the objectives and requirements of all stakeholders and which take account of a wide range of operational and regulatory constraints. The intrinsic complexity of the proposed programme and its large scale (especially for the case of the network-wide slot allocation) will mean that it will provide an excellent test-bed for the development of new heuristics and hyper heuristics for large scale complex scheduling problems more widely. Algorithms that will be developed and tested by this project will provide essential support for the complex large scale capacity allocation problems that arise in other types of transportation networks, including rail networks. In addition, it could extend to other types of networks that share similar problem structures, such as those in energy and telecommunications.

The models and solution techniques developed will underpin the development of novel decision support systems which have the potential to make a major impact on airport operations. The research team has an internationally leading profile in the areas of mathematical modelling, heuristic development, stochastic optimization, airport slot allocation, airport management and performance assessment. It has an excellent track record of research cooperation with all categories of stakeholders. It will cooperate closely with an impressive array of leading industry stakeholders in order to make sure that the work is as cutting edge industrially as it is scientifically.",,"This proposal is addressing an issue of great societal and scientific concern. It will have significant impact on i) the air transport industry, ii) policy makers and government, iii) associated scientific communities, iv) research training and v) the general public. 

In what follows, we provide an overview of some of the key potential impacts of the proposed research programme on each of these stake-holding groups.

(i) Air Transport Industry: The optimization of slot allocation will bring major benefit to airports, the airline industry, and providers of air traffic and ground handling services. All of these industry sectors will benefit from the improved utilization of airport capacity which will result from improved slot allocation. The models and algorithms that will be developed will help air transport industry decision makers to reduce delays, increase schedule reliability, decrease operating costs, and increase revenues.

(ii) Policy Makers and Government: The results of the proposed programme will support decision makers to manage efficiently the available airport infrastructure and to improve the allocation of financial and other resources for developing additional capacity. The proposed programme will thus impact on issues of airport capacity management and development which are of central importance to the UK Government and across Europe.

(iii) Associated Scientific Communities: The proposed programme will have significant impact on the advancement of knowledge in a field that cuts across several scientific communities including operations research, mathematics, management science, computer science, air transport and engineering. It addresses challenges that lie at the interfaces of these fields. This programme will contribute substantially to our mathematical understanding of the modelling of complex large scale systems. It will also contribute to the development and theoretical understanding of innovative adaptive search methods for solving scheduling problems under uncertainty.

 (iv) Research Training: This Programme Grant will provide a unique and valuable research training environment for our research associates and PhD students. They will have the opportunity to engage closely with international research centres and air transport industry organizations through short term visits and secondments. This international exchange and industrial exposure will underpin their inter-disciplinary research training experience.

(v) General Public: This programme has the potential to generate significant social, economic, and environmental impact. The development of models and systems for improved airport capacity management will substantially decrease congestion and reduce delays for the travelling public. Further, airport congestion generates a negative impact on air pollution, aircraft noise and safety, while delays cause significant economic damage. For example, related studies in the USA have estimated that the total economic impact of air transportation delays on the US economy for 2007 was $28.9 billion."
1,D2C85CEF-7602-4A7B-9012-6057DBF0F561,FAIME: A Feature based Framework to Automatically Integrate and Improve Metaheuristics via Examples.,"Given a number of example problems, FAIME will design metaheuristics tailored by modifying their source code. Compared to human designed metaheuristics, these will be better, faster and stronger. Pilot studies indicate they will perform better than human designed metaheuristics, as we are using them as a starting point. They will be designed faster than via the manual design process. We will able to make stronger statistical statements about their properties. In short, we will employ a machine learning approach to design metaheuristics, explore how features can be used to make predictions, and how effectively we can design metaheuristics given a problem's features. 
 
There are three broad types of computational problems; easy (tractable), hard (intractable), and impossible (incomputable). These problems are challenging purely because of the astronomical number of possible solutions and trying every possibility would require an infeasible amount of time. Metaheuristics tackle the hard problems, mitigating this issue by sampling a tiny subset of the possibilities. One drawback is that there is no guarantee of performance. FAIME will examine the efficacy of automatically designing metaheuristics, along with key performance indicators of the metaheuristics, such as scalability, tolerance, robustness, convergence.

There are issues with current practices in metaheuristic research which we plan to tackle directly. These include the following:
-There are few principles to assist with the design of metaheuristics, making it a trial and error process. A feature based approach will provide a framework against which to address this. 
-There is an explosion in the number of published metaheuristics, in particular metaphorically inspired ones, with few guidelines to select among them. A feature based approach can provide a basis offering some guidance. 
-Metaheuristics are often designed in isolation from the problems they will be applied to. FAIME will use feedback from the example problems to inform the design process. 
Also, manually designing metaheuristics has its limitations. While it does produce better performing metaheuristics, it does not contribute to an understanding of how to produce better metaheuristics. FAIME drives deeper by automatically generating metaheuristics and then using features to provide a framework for classification, providing a basis to understand their design. 
 
This FAIME project will
-automatically designs metaheuristics for example problems using a factory consisting of two levels, a generating-level and a test-level which generates and tests metaheuristics. The metaheuristics will be generated by modifying their source code, and then tested on a set of problems. We will 
-investigate the properties of the metaheuristics (scalability, tolerance, robustness, convergence).
-employ a recent technique which takes existing source code and improves it.
-be implemented on parallel machines, which are ideally suited to the design process.
-draw on computability theory, probability theory, and other recent mathematical results to inform the design process and feature based approach.

We do not yet have an effective classification scheme for metaheuristics and problems. A classification scheme which reflects the relationships of entities being classified brings clarity and allows predictions to be made. Using the feature based classification scheme, we will investigate the accuracy of predictions we can make about metaheuristics and problems. We aim to be able to make statistical statements about the performance of metaheuristics such as, metaheuristic H will deliver a solution of quality X units, with tolerance +/- Y units, within Z time units, thus offering some indication of performance and which metaheuristic to choose for which problem.",,"ECONOMY
FAIME will appeal to start-up companies and SMEs which rely on metaheuristics at the centre of their products or services. The results of FAIME can feed into their procedures or used directly to deliver high-quality low-cost metaheuristics, designed at the push of a button. FAIME will support the activities of fledgling initiatives that need to rapidly develop prototype metaheuristics to meet changing demands. Any activity which propels new ideas at a faster pace will contribute to wealth creation. This allows tighter gaps in the market to be populated more efficiently and reduces the time to market for any product or service relying on metaheuristics. Today's SMEs become tomorrow's large organizations and therefore any infrastructure that supports this transition will have merits. 

PEOPLE
We are supplying a tool which will assist metaheuristic designers. FAIME can be used to improve or test existing metaheuristics. FAIME will deliver a new methodology with which researchers can be more creative and productive. This places us in a position to be able to deliver higher quality metaheuristic to a wider audience. If a set of problems change, reflecting say changes in the market, FAIME can deliver a new metaheuristics in response, thus supporting people within the organization by providing the necessary infrastructure to rapidly design specific metaheuristics.

As an example, some mathematical software packages have built-in libraries of genetic algorithms. These packages are used widely by, for example engineers, who model a problem and then optimize it. The short term vision, within the scope of this project, is to develop a software library which is freely available. A longer term vision of FAIME, beyond its current scope, is to have this library integrated into third party software.

KNOWLEDGE
FAIME represents a step-change in the approach to designing metaheuristics, offering a new technique (a way to generate metaheuristics), and a method to understand the technique (a way to classify and make predictions about metaheuristics and problems). FAIME will provide a new platform on which research questions can be addressed offering inroads to a number of theoretical and practical issues such as scalability. FAIME offers an integrated framework for metaheuristics and problems, deepening our knowledge, laying the foundations for the next generation of metaheuristic researchers. 

The field of metaheuristics is largely driven by researchers comparing metaheuristics. While this process itself can be automated, FAIME goes further. We propose a feature based approach to describe and classify metaheuristics and problems allowing predictions to be made about them. Thus we are making a contribution to knowledge at a different level to the way progress is usually perceived in the field.

SOCIETY
Metaheuristics are used in a number of branches concerning health care, from nurse rostering and surgery scheduling to image recognition. As an example of how FAIME can be employed, beyond that of our current industrial partner, consider scheduling surgeries at different hospitals. Each hospital is individual having its own staff, number of operating theatres and patients. An off-the-shelf metaheuristic could be employed to schedule the surgeries, but FAIME offers the ability to build tailored scheduling metaheuristics specific to each hospital, each one having different resources. FAIME will be able to take into account differences between hospitals which a generic metaheuristic cannot. This will allow us to make better use of existing resources."
2,557222BF-D47A-459D-AF98-562F3D8A2E38,The Digital Creativity Hub,"The creative industries are crucial to UK social and cultural life and one of the largest and fastest-growing sectors of the economy. Games and media are key pillars for growth in the creative industries, with UK turnovers of &pound;3.5bn and &pound;12.9bn respectively. Research in digital creativity has started to be well supported by governmental funds. To achieve full impact from these investments, translational and audience-facing research activities are needed to turn ideas into commercial practice and societal good. We propose a &quot;Digital Creativity&quot; Hub for such next-step research, which will produce impact from a huge amount of research activity in direct collaboration with a large group of highly engaged stakeholders, delivering impact in the Digital Economy challenge areas of Sustainable Society, Communities and Culture and New Economic Models.

York is the perfect location for the DC Hub, with a fast-growing Digital Creativity industry (which grew 18.4% from 2011 to 2012), and 4800 creative digital companies within a 40-mile radius of the city. The DC Hub will be housed in the Ron Cooke Hub, alongside the IGGI centre for doctoral training, world-class researchers, and numerous small hi-tech companies.

The DC Hub brings:

- A wealth of research outcomes from Digital Economy projects funded by &pound;90m of grants, &pound;40m of which was managed directly by the investigators named in the proposal. The majority of these projects are interdisciplinary collaborations which involved co-creation of research questions and approaches with creative industry partners, and all of them produced results which are ripe for translational impact.

- Substantial cash and in-kind support amounting to pledges of &pound;9m from 80 partner organisations. These include key organisations in the Digital Economy, such as the KTN, Creative England and the BBC, major companies such as BT, Sony and IBM, and a large number of SMEs working in games and interactive media. The host Universities have also pledged &pound;3.3m in matched funding, with the University of York agreeing to hire four &quot;transitional&quot; research fellows on permanent contracts from the outset leading to academic positions as a Professor, a Reader and two Lecturers.

- Strong overlap with current projects run by the investigators which have complementary goals. These include the NEMOG project to study new economic models and opportunities for games, the Intelligent Games and Game Intelligence (IGGI) centre for doctoral training, with 55+ PhDs, and the Falmouth ERA Chair project, which will contribute an extra 5 five-year research fellowships to the DC Hub, leveraging &pound;2m of EC funding for translational research in digital games technologies.

- A diverse and highly active base of 16 investigators and 4 named PDRAs across four universities, who have much experience of working together on funded research projects delivering high-impact results. The links between these investigators are many and varied, and interdisciplinarity is ensured by a group of investigators working across Computer Science, Theatre Film and TV, Electronics, Art, Audio Production, Sociology, Education, Psychology, and Business.

- Huge potential for step-change impact in the creative industries, with particular emphasis on video game technologies, interactive media, and the convergence of games and media for science and society. Projects in these areas will be supported by and feed into basic research in underpinning themes of data analytics, business models, human-computer interaction and social science. The projects will range over impact themes comprising impact projects which will be specified throughout the life of the Hub in close collaboration with our industry partners, who will help shape the research, thus increasing the potential for major impact.

- A management team, with substantial experience of working together on large projects for research and impact in collaboration with the digital creative industries.",,"The Digital Creativity (DC) Hub will work within one of the fastest growing sectors of the UK economy - the creative industries - which in 2012 contributed &pound;71bn to the UK economy, as well as having extraordinary social and cultural impact in our focus areas of games and interactive media. This, together with our very large research portfolio and 80+ committed partners offers the perfect landscape for translational research to generate significant economic, cultural and social impact. The DC Hub aims to become a long-term self-sustaining world centre of excellence in impact-driven digital creativity research, working with a wide range of external organisations (such as with CDEC to understand and realise the potential of massive quantities of behaviour and preference data from interacting with games and media). 

The Creative Digital Industries are underpinned by hi-tech SMEs. Increasing skill levels and injecting research advances in such a community is best achieved through long-term engagement of interdisciplinary researchers, who can be seconded into organisations to work with them and understand their business needs. The critical mass and longevity of the Hub will allow effective engagement these partners, who can only engage at appropriate points of their business cycle, spawning growth and innovation. 

We will engage our growing base of partners (80+ at the bid stage) through workshops, sandpits, secondments, continuing professional development (CPD) and other events, and particularly through co-creation of research directions, questions and approaches. 

The DC Hub offers the opportunity for a step change, yielding increased profits through internationally distinctive UK games and interactive media industries which are technologically advanced and research-aware. 

The impact on society will be particularly through projects such as &quot;Sim York&quot; where we will create games and interactive media which allow the person in the street to understand the impact of policy decisions.

Cultural impact will come through games and interactive media outputs, and also through working with our gallery and museum partners. For example we have the agreement of the National Media Museum that the DC Hub will have a permanent curated space to showcase the cutting edge translational technologies. Digital approaches to heritage allow much broader access to cultural artefacts and we will explore this in the DC Hub.

We will present our work, with our industry partners, in schools (particularly via our education alliance partners) to spread excitement about careers in Digital Economy areas, to raise the aspirations of children, with a particular view to encouraging women to enter Digital Economy professions. Education researchers in the Hub will work directly with those developing games and interactive media technologies to maximise social impact.
 
Entrepreneurial DC Hub investigators and researchers will be trained in business models, commercialisation of IP and develop significant depth in sector understanding (through secondments, working closely with other disciplines, and through sector-specific training delivered by our partners). They will be well-connected into industry and user organisations and become well versed in the issues and techniques of the creative and digital industries, developing a long-term understanding which will, we believe, result in a wealth of fascinating new research questions, and real benefits for wider society through the now-ubiquitous medium of digital games and interactive media. They will be supported to create spin-outs and develop games and apps - with encouragement to get their ideas out &quot;in the wild&quot; for testing and feedback with the multitude of user groups accessed through our partners. We will enable further development of ideas by providing support to partners in gaining knowledge transfer funding, from KTP, SBRI, CDEC, Innovate UK, Creative England and other calls."
3,9E5D5541-4082-4069-A705-D56B06C8C275,Self-repairing Hardware Paradigms based on Astrocyte-neuron Models,"The human brain is remarkable in its ability to self-repair, for example following stroke or injury. Such self-repair results from a range of distributed and fine-grained mechanisms which act in tandem to ensure that the neurones (the basic building blocks in the brain) continue to function in as close to a normal state as possible.

In contrast modern electronic systems design typically relies on a single controller or processor, which has very limited self-repair capabilities. There is a pressing need to progress beyond current approaches and look for inspiration from biology to inform electronic systems design.

Recent studies have highlighted that interactions between astrocytes (a type of glial cell) and neurones in the brain provide a distributed cellular level repair capability where faults that impede or stop neuronal firing can be repaired by a re-adjustment of the local weights of connections between neurones in the brain.

This project aims to exploit these recent findings and develop a new generation of self-repairing algorithms by taking inspiration from these results to design a new generation of &quot;astro-centric&quot; algorithms. To achieve this we will include components representing both neurones and astrocytes in our electronic systems and model the interactions between these in such a way as to capture the distributed repair capabilities seen in the biological system.",,"Our project is &quot;blue-skies research&quot; at the cutting edge of bio-inspired design for next generation electronic systems. The project aims to develop computational tools, algorithms and robotic demonstrators to implement distributed, fine-grained repair in spiking neural networks that take inspiration from recent findings regarding biological brain repair mechanisms. 

Economic impact. 
We envisage that our project will have longer term economic impact. Uptake of our ideas by the UK electronics design industry has the potential to increase economic activity through additional exploitation routes for novel design principles. The UK is well placed to benefit from this research as 40% of European design houses are based in the UK. Exploitation routes are discussed in the pathways to impact document.

Societal impact
The research targets application areas in robotics as demonstrators. This choice was motivated in part to increase potential impact beyond the scientific community. Demonstration of robotic systems where a brain inspired electronic control system is able to respond to a range of fault scenarios has the potential to generate considerable publicity and associated impact to highlight bio-inspired Engineering research in the UK. Additionally, such reliable robots will be of benefit in the domain of assisted living. The research at Ulster will assist in the understanding of brain function/dysfunction and therefore impact in general mental health and well-being."
4,65D2C5B9-B15E-4411-88A4-850B788BA098,Inferring the Purpose of Network Activities,"The sophistication of attacks targeting computer networks is constantly increasing. Recently, we have witnessed multiple sophisticated targeted attacks against governments and companies. Such attacks are much different than traditional network attacks, because attackers have virtually unlimited resources and can tailor their operation to the victim's network, making these attacks very difficult to detect. In fact, current state of the art detection techniques are inadequate to protect computer networks against targeted attacks.

In this proposal, we aim to make some fundamental steps towards being able to reliably detect targeted attacks on computer networks. To this end, we plan to abstract the observation from the actual manifestation of an attack, and focus on the purpose behind network activities instead. We believe that modern machine learning techniques such as deep belief networks can be used to automatically learn high-level features from network data. Such features are indicative of the purpose for which the network activity is performed, rather than of the specific techniques and tools used to accomplish that purpose. These high-level features can then be used in traditional supervised machine learning to detect whether a network activity is being performed with a malicious intention or a benign one.",,"Being able to accurately detect stealthy network attacks by motivated adversaries if of fundamental importance for both the UK government and industry. Recent events such as the Regin and Stuxnet malware have demonstrated that the way in which attacks are currently detected is not effective. This proposal aims at setting the foundation to overcoming this problem, by changing the way in which network attacks are detected.

The main goal of this proposal is to change the way in which the academic community is mitigating network attacks. Instead of looking at the actual manifestation of an attack, we aim at understanding the purpose for which a network activity is conducted. If successful, this proposal will inspire a wealth of research both from the computer security academic community as a whole. To make sure that the academic community is aware of the techniques proposed in this project and of its results, Dr Stringhini will publish two papers (one for WP1 and one for WP2) in top computer security conferences such as the IEEE Symposium on Security and Privacy or the ACM Conference on Computer and Communications Security. He will also engage the community by describing the developed techniques at invited talks and seminars worldwide, to which he is often invited.

As we mention in the proposal, and as our letters of support show, Dr Stringhini already has an established partnership with Lastline inc., and the researchers at this company are keen on applying the techniques developed for this project to the network defence systems sold by the company. This partnership has the potential of fostering a long-term collaboration that could bring important results that go beyond the duration of this proposal. Dr Stringhini will also actively look for additional industry partners, and build a industry network that will be able to effectively bring the techniques developer for this project to the real world, with a consequent benefit for Internet users.

Internet users could have additional benefits from the developments of this project. Being able to accurately infer the purpose behind network activities could dramatically reduce the need for invasive security mechanisms such as CAPTCHAS and security questions.

Finally, the techniques developed in this proposal will have an important benefit for the UK critical infrastructure. To make an impact on how the operators of the critical infrastructure deal with attacks, Dr Stringhini will reach out to partners at National Grid, who have had long term collaboration with the Computer Science Department at UCL, in particular through Professor David Pym."
5,FF57F6A2-B935-4B2B-8AB3-7A350C8A8C8F,I-DRESS,"The main objective of the project is to develop a system that will provide proactive assistance with dressing to disabled users or users such as high-risk health-care workers, whose physical contact with the garments must be limited during dressing to avoid contamination. The proposed robotic system consists of two highly dexterous robotic arms, sensors for multi-modal human-robot interaction and safety features.
The system will comprise three major components, each of radical impact to the field of assistive service robotics: (a) intelligent algorithms for user and garment detection and tracking, specifically designed for close and physical human-robot interaction, (b) cognitive functions based on the multi-modal user input, environment modelling and safety, allowing the robot to decide when and how to assist the user, and (c) advanced user interface that facilitates intuitive and safe physical and cognitive interaction for support in dressing. The consortium consisting of three partners provides the expertise for the main lines of research required by the project: CSIC-UPC will work on perception and human-robot interaction, IDIAP will contribute to robot learning, and UWE-BRL will provide the expertise in safety and interface design. 
The developed interactive system will be integrated on commercial WAM robotic arms and validated through experimentation with users and human factor analysis in two assistive-dressing scenarios. Additionally, developed robot safety features and the learning by demonstration algorithms will be implemented on a Baxter robot, thus ensuring general applicability and easier acceptance of the project results by both industry and scientific community.",,"Robot safety is the key issue that must be solved if robots are to leave the research labs and be allowed to enter all areas of society. A lack of widely adopted standards for personal care and medical robots is currently being considered by special working groups within the ISO - ISO TC 184/SC 2 and IEC SC62A. A new ISO 13482 safety standard for personal care robots was published in 2013; these types of robots involve close robot-human interaction for providing personal services. Dr Dogramadzi is actively involved in these working groups and the results of I-DRESS will contribute to further developments and considerations of the standard committees. Leaving the robot system in the homes with users is beyond the scope of this project, due to health and safety governance procedures. However, we will attempt to test the system with healthy adults in the laboratory environment. 
The use of robot learning from demonstration techniques will allow the developed system to automatically adapt to the specific needs of each user (which can be changing with time). This enlarges the scope of possible applications as well as the potential markets, since the dressing assistant can learn rapidly what it is expected to do, without requiring the manual reprogramming of the robot through a computer language. This versatility is expected to be crucial for such challenging physical human-robot interaction.
The technological and HRI advancements made as part of the proposed scenarios can be extended to use of similar robot configurations to support other activities of daily living. The ensuing independence for individuals, previously reliant on carers and others, will mean greater autonomy to participate in society and the workplace. Even in cases where the number of carers is reduced from two to one for personal care assistance, there will be a significant healthcare saving and more effective use of precious human resources. 
The impact of the human factors and interface design research proposed here is of crucial importance in facilitating understanding of mixed initiative human-robot interaction multi-modal dialogue. Together with extending application to people with accessibility needs where one or more communication modalities might be impaired, this research will produce pragmatic solutions for the use of robotics in the assistive care sector.

Final project developments will be done under the Open Source policies, and the use of the ROS tools will ensure their reusability. The aim of the project is to produce transferable skills, that once developed can be used as a module by other research groups or in industry. The dissemination of project results and strategies for contacting with various stakeholders is described in the section 3.2."
6,104CB8DE-0728-471C-9355-24B04B278B0A,Science of Sensor System Software,"Sensors are everywhere, facilitating real-time decision making and actuation, and informing policy choices. But extracting information from sensor data is far from straightforward: sensors are noisy, prone to decalibrate, and may be misplaced, moved, compromised, and generally degraded over time. We understand very little about the issues of programming in the face of pervasive uncertainty, yet sensor-driven systems essentially present the designer with uncertainty that cannot be engineered away. Moreover uncertainty is a multi-level phenomenon in which errors in deployment can propagate through to incorrectly-positioned readings and then to poor decisions; system layering breaks down when exposed to
uncertainty. 

How can we be assured a sensor system does what we intend, in a range of dynamic environments, and how can we make a system ``smarter'' ? Currently we cannot answer these questions because we are missing a science of sensor system software. We will develop the missing science that will allow us to engineer for the uncertainty inherent in real-world systems. We will deliver new principles and techniques for the development and deployment of verifiable, reliable, autonomous sensor systems that operate in uncertain, multiple and multi-scale environments. The science will be driven and validated by end-user and experimental applications.",,"Developing a science of sensor system software will address directly the critical barrier to impact from sensor systems: can the information we are collecting be trusted to meet the mission goals? We will advance academic boundaries and have industrial and agency end user impact, including in other academic disciplines (e.g. environmental engineering). We will maintain our strong publication record in premier journals and conferences, and develop a new community of researchers through workshops (at least one with international participants) for example, a specialist Royal Society Scientific Meeting. We will ensure we make a direct impact on developers and end users by working closely with a wide selection of companies and agencies that will provide us with representative case studies. Our partners range from SMEs, to global companies and government agencies, working across many sectors, from transportation to environmental services. We will deliver new results and insights to them, as they arise, stimulating further development of their sensor-based systems, and helping them answer questions they could not previously answer (for example, quantifying the extent and effect of sensor failures). Through this we will gain critical experience and feedback about the applicability of our end-to-end approach. The innovation centre CENSIS (http://censis.org.uk ) will also help disseminate our results through our participation in their regular events and their representation of our programme to other sectors and organisations. We have detailed engagement and impact development plans and an experienced business development manager for the programme."
7,7379503B-00F0-4D6E-84DC-E3FE4CD55904,"Understanding scenes and events through joint parsing, cognitive reasoning and lifelong learning","The goal of this MURI team is to develop machines that have the following capabilities:
i) Represent visual knowledge in probabilistic compositional models in spatial, temporal, and causal hierarchies augmented with rich attributes and relations, use task-oriented representations for efficient task-dependent inference from an agent's perspective, and preserve uncertainties;
ii) Acquire massive visual commonsense via web scale continuous lifelong learning from large and small data in weakly supervised HCI, and maintain consistence via dialogue with humans;
iii) Achieve deep understanding of scenes and events through joint parsing and cognitive reasoning about appearance, geometry, functions, physics, causality, intents and belief of agents, and use joint and long-range reasoning to fill the performance gap with human vision;
iv) Understand human needs and values, interact with humans effectively, and answer human queries about what, who, where, when, why and how in storylines through Turing tests.

Collaboration with US:
Principal Investigator: Dr. Song-Chun Zhu
Tel. 310-206-8693, Fax. 310-206-5658, email: sczhu@stat.ucla.edu
Institution: University of California, Los Angeles
Statistics and Computer Science
8125 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095
Institution proposal no. 20153924

Other universities in the US
CMU: Martial Hebert Computer Vision, Robotics &amp; AI
Abhinav Gupta Computer Vision, Lifelong Learning
MIT: Joshua Tenenbaum Cognitive Modeling and Learning
Nancy Kanwisher Cognitive Neuroscience
Stanford: Fei-Fei Li Computer Vision, Psychology &amp; AI
UIUC Derek Hoiem Computer Vision, Machine Learning
Yale Brian Scholl Psychology, Cognitive Science",,
8,6CCED8B0-6DD9-4A07-994E-7B9C485353EB,Fast Generalised Rule Induction,"The proposed research will significantly advance the state of the art in the field of data stream mining, in particular by providing adaptable rules that are understandable by humans but can be implemented in rule based expert systems. 

Data stream mining is growing in importance and packages such as MOA are known to be used by widely in the data stream mining community. However, none of these environments provide techniques for the extraction of descriptive rules that can express patterns and changes of the patterns encoded in data streams. Descriptive data stream mining techniques exist such as cluster analysis, but none that identifies the patterns in the form of expressive and meaningful rules. This is important because it allows domain experts to look for potentially interesting but unknown patterns and changes of the pattern over time. The fact that the patterns are described in the form of rules allows to draw conclusions on why the pattern exist and how it could be influenced. Creating these rule sets is challenging as they need to be adapted automatically, as patterns encoded in a stream may change over time (known as concept drift); and also because they need to be created in a single pass through as data streams are potentially never ending. 

The research will be integrated into a popular open source environment, with the two contenders being the MOA or the KNIME data mining workbenches. Especially the integration of the methodology in MOA will accelerate the adoption of expressive descriptive data stream mining techniques in both academic and commercial communities. In particular UK telecommunication and chemical companies are amongst the categories of companies that gain significant advantages from the extraction of descriptive rules in real-time from streaming data. The method will enable the telecommunication industry to improve the efficiency of detecting interesting event patterns in national telecommunication networks on the fly, and thus help forecasting performance bottlenecks and faults (personal communication with British Telecom). Chemical companies can employ this technology for monitoring sensors in chemical plants to identify plant stages on the fly without the need for time consuming analyses in the laboratory. This will trigger R&amp;D investment of UK companies for making use of this research, which is reflected by the fact that British Telecom has already contributed &pound;30,000 towards a PhD studentship to the PI's research in stream data mining techniques. Such exploitation of this research will lead to growth in the performance of technology companies, new jobs and an increase in revenue, and thus will give the UK an economical competitive advantage. 


Society will indirectly advance from scientific areas that advance through the results of this research. And wider applications of the results of this research will be explored by considering the analysis of electroencephalogram (EEG) data fast in real-time. It is likely that new insights could have a direct impact on the public health aiding diagnosis and understanding of the brain. Also advances of the industry through this research will have an indirect impact on society. For example the forecasting of performance bottlenecks in national telecommunication network will lead to more reliable telecommunication applications such as telemedicine.

The wider academic community will also benefit from the results of this research through the open source implementation of this project's methodology, as this will allow researchers to find and express interesting patterns on the fly in a wide variety of fast scientific data streams. For example expressing and adapting complex patterns from meteorological sensors, detecting and/or expressing changes of patterns of brain activity through life functional Magnetic Resonance Imaging (fMRI), etc. Our unique position at the University of Reading provides ready access to these scientific data sources.",,"The outcome of this project will have considerable implications for the design of data mining algorithms for commercial systems that enable the exploitation of fast data streams to tackle large-scale scientific and industrial problems. Data mining companies range from service oriented to software development organisations. Examples of software environments produced by such organisations are Weka, RapidMinder, KNIME and MOA. These specialist software environments incorporate a collection of algorithms and facilitate creation and execution of data mining tasks. These environments typically arose from academic research projects and some have even become commercial products (such as KNIME &amp; RapidMiner). These software environments are adopted by large companies because they allow to automate the data mining processes, for example KNIME is adopted by Actian Ltd and Weka by BT Research. However, none of these environments provide techniques for the extraction of descriptive rules that can express patterns and changes of the patterns encoded in data streams. Currently open source MOA is the only of these environments that provides algorithms to analyse streaming data in real-time, however, expressive and descriptive algorithms do not yet exist for data streams.

The proposed research will significantly advance the state of the art in the field and be integrated in the popular MOA environment for mining data streams. This will accelerate the adoption of expressive descriptive data stream mining techniques in both, academic and commercial communities. In particular UK telecommunication and chemical companies are amongst the categories of companies that gain significant advantages from the extraction of descriptive rules in real-time from streaming data. Regarding the telecommunication industry this will improve the efficiency of detecting interesting event patterns in national telecommunication networks on the fly; and thus help forecasting performance bottlenecks/faults (personal communication with BT). Chemical companies can employ this technology for monitoring sensors in chemical plants to identify plant stages on the fly without the need for time consuming analyses in the laboratory. This will trigger R&amp;D investment of UK companies for making use of this research, which is reflected by the fact that British telecom already contributes &pound;30,000 towards a PhD studentship to the PI's research in this area. Such exploitation of this research will lead to growth in the performance of technology companies, new jobs and an increase in revenue, and thus will give the UK an economical competitive advantage.
 
The academic community will greatly advance from the results of this research through the open source implementation of this project's methodology, as this will allow researchers to find and express interesting patterns on the fly in fast data streams and thus gain new insights in their field of research. For example expressing and adapting complex patterns from meteorological sensors, detecting and/or expressing changes of patterns of brain activity through life functional Magnetic Resonance Imaging (fMRI), etc.

The results of this research can be used for expressing rules describing sentiment changes in social media, which can be used to predict social unrest and thus enhances national security, government policy and general public. The general public will indirectly advance from scientific areas that advance through the results of this research. For example the results of this research can be used to analyse electro encephalogram (EEG) data fast in real-time and thus gain new insights, which indirectly has an impact on the public health. Also advances of the industry through this research will have an indirect impact on the general public. For example the forecasting of performance bottlenecks in national telecommunication network will lead to more reliable telecommunication applications such as telemedicine."
9,B9137D72-BC4A-4D3E-B849-7C6D930BF110,BetterCrowd: Human Computation for Big Data,"In the last few years we have seen a rapid increase of available data. Digitization has become endemic. This has lead to a data deluge that left many unable to cope with such large amounts of messy data. Also because of the large number of content producers and different formats, data is not always easy to process by machines due to its its diverse quality and the presence of bias. Thus, in the current data-driven economy, if organizations can effectively analyze data at scale and use it as decision-support infrastructure at the executive level, data will lead to a key competitive advantage. To deal with the current data deluge, in the BetterCrowd project I will define and evaluate Human Computation methods to improve both the effectiveness and efficiency of currently available hybrid Human-Machine systems.

Human Computation (HC) is a game-changing paradigm that systematically exploits human intelligence at scale to improve purely machine-based data management systems (see, for example, CrowdDB [13]). This is often obtained by means of Crowdsourcing, that is, outsourcing certain tasks from the machine to a crowd of human individuals who perform short tasks (also known as Human Intelligence Tasks or HITs) that are simple for humans but still difficult for machines (e.g., understanding the content of a picture or sarcasm in text). Involving humans in the computation process is a fundamental scientific challenge that requires obtaining the best from human abilities and effectively embedding them into traditional computational systems. The challenges involved with the use of HC are both its efficiency (i.e., humans are naturally slower than machines in terms of information processing) and effectiveness (i.e., while machines deterministically compute, humans behavior may be unpredictable and possibly malicious).

The project is composed of two main parts. We will first look at how to improve crowdsourcing effectiveness by proposing novel techniques to detect malicious workers in crowdsourcing platforms. In the second part, we will make HC techniques scale so that they can be applied to larger volume of data focusing on scheduling tasks to the crowd (WP2).",,"Other than academic beneficiaries, this research will impact commercial organizations and people involved in crowdsourcing activities.

Big Data Analytics Market.
The scale of data currently being produced by large organizations requires novel ways of managing and, most importantly, analyzing such enormous amounts of data in order to produce value for data consumers (e.g., company customers, employees, or governmental organization clients). In this context, high data quality is critical. The techniques developed within the scope of this project for efficient and effective human computation can be used to create better Big Data solutions and products in coordination with analytics platforms used within large-scale organizations. An example of industry use of hybrid human-machine techniques by means of crowdsourcing is already in place at Twitter where new trending topics and acronyms are detected in real-time by a mix of machine-based stream processing approaches and crowdsourcing on Amazon Mechanical Turk. The outcome of the BetterCrowd project has potential for impact on Big Data crowd-based solutions by optimizing requests to the crowd and the overall output quality produced.

Enterprise crowdsourcing.
In the enterprise domain, large companies (e.g., IBM, Microsoft, VeriSign) have already started to run in-house crowdsourcing: Employees of the company are the crowd to which data quality HITs are sent. Crowdsourcing in this context is different in aspects such as worker reputation and incentives. However, research findings on scalability aspects will be directly applicable to this way of leveraging knowledge workers within companies. Scheduling HITs to a crowd for in-house crowdsourcing is extremely important as this can be considered as a single multi-tenant system with jobs run with different priorities.

Crowdsourcing as career path.
The crowdsourcing market has seen an exponential growth over the last few years with a doubling of the market over the last two years. That said, crowdsourcing is still a highly unregulated market. In the longer term, the more efficient and effective use of crowdsourcing that will result from this project will support the creation of better conditions for the work of the crowd not excluding the definition of crowd work as a profession which is already a reality in developing countries such as India."
10,8A961665-5A1C-4F34-8EFA-6BD98CF0B29E,Argument Mining,"Argument and debate form cornerstones of civilised society and of intellectual life. Processes of argumentation run our governments, structure scientific endeavour and frame religious belief. Recognising and understanding argument are central to decision-making and professional activity in all walks of life, which is why we place them at the centre of academic pedagogy and practice; it's why such a premium is placed upon these skills; and it's why rationality is one of the very defining notions of what it is to be human. 

Our theories of how argument is structured go back to Ancient Greece. In the past thirty years or so, the computational sciences have started to build models and engineer software based on these theories: this is the field of argument technology, and the recent surge in activity is testament to the vitality and broad applicability of the field. Though argument technology, in which the UK is a world leader, has had applications in domains as diverse as healthcare, public policy, government and the media, the focus has been squarely upon Artificial Intelligence technologies for supporting human argumentation and subsequent automated reasoning with the results. Arguments made outside such software walled gardens have been off the agenda simply because automatic machine understanding of unfettered naturally occurring reasoning has been too hard to tackle. 

Before 2014, that task -- argument mining -- had been tackled only speculatively and only in specific domains by a very small number of groups such as those at Toronto, Leuven and Dundee. By the end of 2014, more than twenty research labs across the US and EU were gearing up to tackle the problem, there were several international meetings including a regular workshop series at the largest computational linguistics conference, and dozens of results being reported. The reason for this huge upswing in activity lies in maturing technology and the returns available. Opinion mining has transformed the way that market research and PR is carried out, deploying big data analysis techniques to understand the attitudes people hold towards products and brands. Sentiment analysis has had an even greater impact in predicting financial markets by analysing broad moods and perspectives that are expressed in the press. Argument mining is the natural evolution of these technologies, providing a step change in the level of detail available -- moving from not just analysing what opinions people hold, but why they hold the opinions they do. This is why major organisations such as IBM, with whom we are partnering in this project, are so interested in the technology. 

The Centre for Argument Technology now curates the largest publicly accessible corpus of analysed argument in the world, and has a well known and widely used tool stack for managing datasets, conducting analyses, and visualising the results. This provides a unique platform from which we can both extend existing techniques for argument mining, but also, much more ambitiously, use insights from the philosophy of argumentation and from rhetoric to transform the reliability and applicability of argument mining technology. In particular, we will use the theory of argumentation schemes that characterises stereotypical patterns of reasoning to guide the process of searching for argument components, and the theory of rhetorical figures and tropes as the basis for developing a new class of algorithms for argument recognition. We will thus be transforming bare statistically-driven approaches with detailed theories of structure which can act to define expectations in a way that constrains the machine learning task thereby improving accuracy and applicability. By partnering with IBM and J&amp;L Techology (a domain-specific SME), the project aims not just to radically improve performance of these techniques, establishing the UK's position at the cutting edge, but also to deliver those performance gains to end users.",,"The Centre for Argument Technology has a sustained, fifteen-year track record of delivering transformative research results that have enabled both engineered infrastructure (see aifdb.org) and practical applications (arg-tech.org). Argument Mining represents an area in which the UK has the potential to be the world leader if it capitalises upon its early successes in this high potential global market (estimated to be worth $10bn by 2020). This project aims to deliver foundational results in its first three years which are then scaled up and deployed in the fourth year.

One of the challenges in developing a transformative programme of research is that, on the one hand, working with too many partners quickly exhausts the effort available to manage the relationships effectively. On the other hand, working with too few risks over-specialising techniques to a particular user and, in the worst case, introducing a single point of failure. Our approach to this dilemma is to invite a number of companies (such as Amazon and the BBC with both of whom we already have a relationship) to engage in a series of meetings in year four, but more importantly to work closely throughout the project with two commercial partners -- companies that have substantially different profiles. 

IBM is re-aligning much of its service provision around Watson, a cloud-based tool-suite to which Argument Mining is a natural companion. IBM thus represents a 'channel partner': a way for the results of the project to delivered to users of IBM's services. We will also work with J&amp;L Technology, an SME working on a single product in a single domain, but a product that is likely to be highly disruptive in a domain that is traditionally woefully under-served by technology. For J&amp;L's offering in the market, Argument Mining introduces functionality that represents an important unique selling point. J&amp;L is currently piloting its software in one of the top ten organisations in its domain in the world (an organisation with an annual IT spend in excess of $25m). J&amp;L thus represents an 'end user' of Argument Mining technology. 

Despite their differences, both IBM and J&amp;L also act as 'thought leaders' in their respective markets: they are seen by their customers to be establishing the trends that others will follow. If the techniques developed in this project were to be advocated by our two commercial partners, there is potential for very widespread impact as a result of this thought-leadership. Our programme of collaboration involves both formal structures (including quarterly meetings of the project's Commercial Board with representatives from both companies), and practical work practices, resting heavily on a series of bilateral exchanges of staff including over six months of residencies for University project staff with the partner organisations. The Commercial Board ensures that ad hoc, responsive communication is complemented by regular, scheduled interaction, whilst the residencies are a key way of enabling staff in the University to gain deep insight into the challenges and opportunities presented by the domains of the commercial partners, and at the same time for staff at the partners to better understand the problems and solutions investigated in the project. To ensure that the project's work is disseminated more broadly in the commercial partners (which is particularly important for such a large organisation as IBM), we will also run Demo Days on site at the partners, with open invitations to staff from across the organisation to see project results and explore opportunities.

For both IBM and J&amp;L Technology, the project represents an area of strategic importance, which is why the two companies are together committing contribution in kind estimated to be worth over &pound;272k, thereby significantly increasing the value for money for the public purse."
11,29D984FF-A8CB-4EC2-9389-AD464BFBD742,"Understanding Scenes and Events through Joint Parsing, Cognitive Reasoning and Lifelong Learning","The goal of this MURI team is to develop machines that have the following capabilities:
i) Represent visual knowledge in probabilistic compositional models in spatial, temporal, and causal hierarchies augmented with rich attributes and relations, use task-oriented representations for efficient task-dependent inference from an agent's perspective, and preserve uncertainties;
ii) Acquire massive visual commonsense via web scale continuous lifelong learning from large and small data in weakly supervised HCI, and maintain consistence via dialogue with humans;
iii) Achieve deep understanding of scenes and events through joint parsing and cognitive reasoning about appearance, geometry, functions, physics, causality, intents and belief of agents, and use joint and long-range reasoning to fill the performance gap with human vision;
iv) Understand human needs and values, interact with humans effectively, and answer human queries about what, who, where, when, why and how in storylines through Turing tests.

Collaboration with US:
Principal Investigator: Dr. Song-Chun Zhu
Tel. 310-206-8693, Fax. 310-206-5658, email: sczhu@stat.ucla.edu
Institution: University of California, Los Angeles
Statistics and Computer Science
8125 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095
Institution proposal no. 20153924

Other universities in the US
CMU: Martial Hebert Computer Vision, Robotics &amp; AI
Abhinav Gupta Computer Vision, Lifelong Learning
MIT: Joshua Tenenbaum Cognitive Modeling and Learning
Nancy Kanwisher Cognitive Neuroscience
Stanford: Fei-Fei Li Computer Vision, Psychology &amp; AI
UIUC Derek Hoiem Computer Vision, Machine Learning
Yale Brian Scholl Psychology, Cognitive Science",,
12,AEFD1A3F-F06C-4A42-9933-AB06900DB738,ACE-LP: Augmenting Communication using Environmental Data to drive Language Prediction.,"Communication is the essence of life. We communicate in many ways, but it is our ability to speak which enables us to chat in every-day situations. An estimated quarter of a million people in the UK alone are unable to speak and are at risk of isolation. They depend on Voice Output Communication Aids (VOCAs) to compensate for their disability. However, the current state of the art VOCAs are only able to produce computerised speech at an insufficient rate of 8 to 10 words per minute (wpm). For some users who are unable to use a keyboard, rates are even slower. For example, Professor Stephen Hawking recently doubled his spoken communication rate to 2 wpm by incorporating a more efficient word prediction system and common shortcuts into his VOCA software. Despite three decades of developing VOCAs, face-to-face communication rates remain prohibitively slow. Users seldom go beyond basic needs based utterances as rates remain, at best, 10 times slower than natural speech. Compared to the average of 150-190 wpm for typical speech, aided communication rates make conversation almost impossible.

ACE-LP brings together research expertise in Augmentative and Alternative Communication (AAC) (University of Dundee), Intelligent Interactive Systems (University of Cambridge), and Computer Vision and Image Processing (University of Dundee) to develop a predictive AAC system that will address these prohibitively slow communication rates by introducing the use of multimodal sensor data to inform state of the art language prediction. For the first time a VOCA system will not only predict words and phrases; we aim to provide access to extended conversation by predicting narrative text elements tailored to an ongoing conversation.

In current systems users sometimes pre-store monologue 'talks', but sharing personal experiences (stories) interactively using VOCAs is rare. Being able to relate experience enables us to engage with others and allows us to participate in society. In fact, the bulk of our interaction with others is through the medium of conversational narrative, i.e. sharing personal stories. Several research projects have prototyped ways in which automatically gathered data and language processing can support disabled users to communicate easily and at higher rates. However, none have succeeded in harnessing the potential of such technology to design an integrated communication system which automatically extracts meaningful data from different sources, transforms this into conversational text elements and presents results in such a way that people with severe physical disabilities can manipulate and select conversational items for output through a speech synthesiser quickly and with minimal physical and cognitive effort. 

This project will develop technology which will leverage contextual data (e.g. information about location, conversational partners and past conversations) to support language prediction within an onscreen user interface which will adapt depending on the conversational topic, the conversational partner, the conversational setting and the physical ability of the nonspeaking person. Our aim is to improve the communication experience of nonspeaking people by enabling them to tell their stories easily, at more acceptable speeds.",,"Communication is fundamental to quality of life. Having a voice enables disabled individuals to direct their lives; it impacts on their mental health and helps in finding employment and remaining in work for longer. Slow communication rates mean that many disabled people are lonely which results in poor quality of life. &quot;The Right to Speak&quot;, a 2012 report by the Scottish Government warns that poor communication affects delivery of care and puts people's lives at risk. Although estimates suggest that 0.05% of the UK population could benefit from current VOCAs, recent research by Communication Matters (CM), the largest charity for people who use AAC, has recognised that with changing demographics and improved technology, future generations of VOCAs could be used by an order of magnitude more people (0.5% of the UK population). 

This project will target nonspeaking literate people including people with cerebral palsy; locked-in syndrome, motor neuron disease (MND), head and neck cancer, and Parkinson's disease (PD). According to the CM research, the target groups would account for around 25% of AAC users, approximately 315,000 people in the UK. The severity of disability and the progressive nature of some diseases impacts on families, carers, healthcare professionals, friends and all who interact with the AAC user, increasing the beneficiaries significantly.
This is a participatory research project and as such, at least 20 end users, their families, friends and caregivers will have direct benefit from the project. From experience, we know that participants benefit socially and their self-esteem and communication skills improve when involved in a research project.

We will extend our existing AAC Research Group online community to engage interested parties throughout the project to reduce the chasm between innovation and adoption. In addition, we will present workshops at Communication Matters Symposia throughout the project to present our results to nonspeaking people, their families, speech and language therapists and other professionals interested in AAC.

We have a strong track record working with industry to transfer research into commercial products. We will carry out a professionally-run industrial workshop in the form of a technology roadmapping session in year 3 to identify routes to impact. We have identified two possible routes (an open source non-profit organisation and a technology start-up) which will be the starting point for our discussions with University advisors, our Advisory Group and our Industrial partners. Our three industrial partners anticipate that they will benefit from the outcomes of the research, both in terms of AAC technology and within mainstream surveillance data analysis.

Publicity features in newspapers, radio and television will be used to raise the public awareness of the abilities and needs of people who use AAC; building on our success in obtaining media publicity for our previous projects. We will also collaborate with the Scottish National Museum to create an installation as part of their new Communications exhibition."
13,1B2525D5-7622-4D3C-8018-EC0359E70FD7,Numeric Domain Model Acquisition from Action Traces,"Many industrial and commercial applications of planning technology have to reason about numbers. For example, in the area of autonomous systems and robotics, an autonomous robot often has to reason about its position in space, power levels and storage capacities. We call the internal representation that the robot has of its environment its model of the world. It is essential for these models to be easy to construct and ideally, they should be automatically constructed.

This project concerns the subject of learning formal models of state-transition systems from observation of those systems operating. Consider an observer unfamiliar with the game of chess: by observing a sequence of moves, much can be learnt about the rules of the game. Watching a bishop for long enough demonstrates that only diagonal moves are possible for this piece.

Domain model acquisition, or automated modelling, is the problem of allowing a computer to learn its own world model by observing actions. We will develop new methods of automated modelling for state transition systems with numeric state variables. And when we refer to domain model acquisition, we refer to the learning of any state transition system from example data that includes sequences of state transitions, whether that is in the context of automated planning, general game playing, interactive narrative, workplace rostering, or any other type of underlying problem.

We first plan to learn models of domain with a common restriction of numeric variables in planning; the restriction to action costs. This restriction means that each ground action has a constant cost, and that the only numeric variable accumulates the sum of these individual action costs over the length of the plan. This accumulated value is the optimisation variable. Board games with action costs are those in which a score is accumulated throughout the play of the game.

Successful completion of the first stage means that we have a domain model acquisition algorithm to learn models that include action costs. This class of planning domains is an important subset of numeric planning domains. However, many planning domains contain more complex numeric properties and, in particular, arbitrary numeric variables and constraints. The second stage, therefore, will concentrate on developing algorithms to learn these constraints. 

Completion of the project will allow models of many different problems to be learnt simply from observation. Examples include such things as capacity limits for certain resources, dimensional constraints for positioning items and strength of friendship level requirement to enable certain actions within a social-network aware interactive narrative setting.",,"Who will benefit from this research?

Healthcare Providers
To demonstrate impact of the current proposal, we propose the creation of a prototype software system for learning transition constraints in real-world instances of nurse rostering problems. We will coordinate with Teesside University's School of Health and external stakeholders, to acquire real-world rosters from hospital wards. 

Healthcare Informatics
Healthcare rostering is part of the wider field of Healthcare Informatics. Academic output relevant to the impact case will be disseminated at relevant Healthcare Informatics venues, such as AIME, AMIA and IEEE ICHI.

How will they benefit from my research?

Healthcare providers have to operate in a rapidly changing regulatory and legislative environment. As an example, the National Institute for Health and Care Excellence (NICE) has recently issued detailed new requirements for the safe staffing of hospital wards following problems with staffing levels at Mid Staffordshire NHS Trust. Providers operate in an environment where they must provide adequate staffing levels, on limited budgets, and within legislative boundaries (for example, the European Working Time Directive).

We will release all software that contributes to the academic work packages as open-source software. In addition to this, by the end of the work programme, and through discussion with our stakeholder group, we will be in a position to further develop the prototype employee allocation system into a commercial product.

Two ways of commercialising the output from the impact packages are relevant to different types of end-user. For larger organisations, a product to be used off-the-shelf that would compete eventually with systems such as Allocate Software's HealthRoster could be developed. Learning the constraints, rather than programming them explicitly, provides a competitive advantage in the speed to deploy software when new constraints are added to the system. In addition, constraints will be learnt automatically in real-time, from the vast data accumulated by healthcare organisations.

For smaller healthcare providers with less resources to invest in large systems, a web service will be set up where users can manually input their current rosters, and the constraints on their staffing will be inferred. This could be provided at a lower cost, or potentially free of cost, and still provides significant societal impact. 

Successful deployment of these software systems can lead to cost savings along with higher staff and patient satisfaction."
14,B7CB4B4B-BFB7-4FA5-938E-C14F2EAFE64F,Visual Commensence for Scene Understanding,"The goal of this MURI team is to develop machines that have the following capabilities:
i) Represent visual knowledge in probabilistic compositional models in spatial, temporal, and causal hierarchies augmented with rich attributes and relations, use task-oriented representations for efficient task-dependent inference from an agent's perspective, and preserve uncertainties;
ii) Acquire massive visual commonsense via web scale continuous lifelong learning from large and small data in weakly supervised HCI, and maintain consistence via dialogue with humans;
iii) Achieve deep understanding of scenes and events through joint parsing and cognitive reasoning about appearance, geometry, functions, physics, causality, intents and belief of agents, and use joint and long-range reasoning to fill the performance gap with human vision;
iv) Understand human needs and values, interact with humans effectively, and answer human queries about what, who, where, when, why and how in storylines through Turing tests.

Collaboration with US:
Principal Investigator: Dr. Song-Chun Zhu
Tel. 310-206-8693, Fax. 310-206-5658, email: sczhu@stat.ucla.edu
Institution: University of California, Los Angeles
Statistics and Computer Science
8125 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095
Institution proposal no. 20153924

Other universities in the US
CMU: Martial Hebert Computer Vision, Robotics &amp; AI
Abhinav Gupta Computer Vision, Lifelong Learning
MIT: Joshua Tenenbaum Cognitive Modeling and Learning
Nancy Kanwisher Cognitive Neuroscience
Stanford: Fei-Fei Li Computer Vision, Psychology &amp; AI
UIUC Derek Hoiem Computer Vision, Machine Learning
Yale Brian Scholl Psychology, Cognitive Science",,
15,EAB3B6F2-3390-41B4-9640-C740258D7193,"Machine Learning, Robust Optimisation, and Verification: Creating Synergistic Capabilities in Cybersecurity Research","The need for better support to deal with the threats of cybersecurity is undisputed. Organisations are faced with an ever growing number of malware and integrated malware attack tools, attempted attacks on infrastructure and services, an increasing number of insider attacks, and advanced persistent threats for high-priced assets. Dealing with such threats requires that organisations have ICT staff that is at least familiar with cybersecurity issues and preferably has actual skills in cybersecurity regardless of the role of such staff. Likewise, management and decision makers need to be aware of cybersecurity issues and reflect these in their actions. Large organisations often have a Chief Information Security Officer (CISO) who deals with the operational and strategic issues of cybersecurity for his or her organisation. But SMEs typically cannot afford a role with such oversight on cybersecurity, which makes them especially vulnerable.

The scale and diversity of cybersecurity issues that an organisation faces means it cannot possibly consider each single vulnerability of its systems against each credible or potential adversary whose presence would turn a vulnerability into an actual threat. A CISO or decision maker, though, needs to have a fairly abstract view of all this complexity where the choice of abstraction is not driven by technical aspects but by modalities such as risk, compliance, availability of service, and strategy. This view often has to take into account the cybersecurity of external or partner organisations, which is problematic as organisations are reluctant to share such sensitive information. Therefore, a CISO or decision maker needs a representation of relevant internal or external systems and services that allows him or her to make decisions of either operational or strategic nature. 

The uncertainty expressed in such abstractions is typically probabilistic or strict in nature. For example, a bank may have a good idea of the probability that a given teller machine has a corrupted external interface that clones inserted bank cards, based on past history, location of the machine and so forth. Strict uncertainty often relates to threats for which no (or insufficient) historical information is available to estimate probability distributions, or it is used to express the combinatorial nature of a problem, for example the different orderings in which one may schedule critical tasks.

This project brings together research leaders in machine learning, robust optimisation, verification and cybersecurity to explore new modelling and analysis capabilities for needs in cybersecurity. The project will investigate new approaches for modelling and optimisation by which cybersecurity of systems, processes, and infrastructures can be more robustly assessed, monitored, and controlled in the face of stochastic and strict uncertainty. Particular attention will be paid to privacy: new forms of privacy-preserving data analytics will be created and approaches to decision support that respect privacy considerations; for corporate confidentiality, we will invent foundations that enable different organisations to model and analyse cross-organisational cybersecurity aspects whilst respecting the type of privacy inherent in organisations' confidential information by establishing appropriate information barriers.",,"Each project site will host a workshop. These workshops will be open to any interested parties, but we will also send out targeted invitations to PIs and co-Is of other projects of this call, people from industry and governmental agencies, and other academics at all career stages (including PhD students). The aim of these workshops is to promote the objectives of the project, to disseminate project outcomes, to encourage external researchers (including PhD students) to share problems and collaborate with us, and to engage industrial problem owners. 

We believe it is vital to create tool prototypes to increase impact of our foundational research. We therefore have an increased staff density in the final 12-16 months of the project to ensure that we can create powerful and convincing tool demonstrators. 

Our research outputs will be made available in open-access form. This applies to research papers, software, research data (unless the latter is confidential due to its origin) or other project artefacts. We plan to use tools such as Zenodo (https://zenodo.org) to host such data and software with supporting DOIs. 

We feel that this work is quite important and relevant to the concerns of the general public. Security and especially privacy of individual data is on most people's mind. Therefore, we aim to engage with the media to communicate to a general audience the research problems that we are addressing as well as our research outputs. We will choose such venues in a flexible and opportunistic manner. Let us mention the Imperial College London Festival, Science Events at the Darwin Centre in London, events hosted by the BBC, radio interviews with UK and Singapore stations, and also media outlets in the wider EU. 

 We broaden academic impact through further activities. We mean to develop course material (lecture notes, exercises, software) that other academics can use to teach material at the junction of machine learning, cybersecurity, privacy, and verification. Furthermore, we plan to write overviews of the research problems and our solutions addressed to a general scientific audience - aiming at appropriate venues such as the Communications of the ACM but also targeting venues beyond Computing, for example the New Scientist.

We plan to schedule a Hackathon around the second workshop. Here, we mean to provide a somewhat more sophisticated 
gamification of the learning material developed for outreach to schools, to engage students (undergraduates, postgraduates or PhD students) and early career researchers with these research issues in the hope of attracting them to these research topics.

Both PIs and the co-I will explore during this project how research outcomes can be leveraged to generate research proposals for more applied or impact oriented research. We also plan to propose a seminar at the Leibniz-Centre for Informatics that would happen around the end of this project. These meetings gather international research leaders and decision makers in academia, industry, and to some degree from government to explore a research challenge. We would use this event in part to promote our research findings but also to form new collaborations and networks, and to provide stimulus for creating new or better research solutions.

To foster more longterm impact in academia, we will propose a workshop satellite event for a large security conference, for example for the ACM Computer and Communications Security conference series. We would expect this event to be repeated annually to provide a forum for exchange of research ideas and outputs at the junction of areas at which this proposal is situated."
16,4617A6F3-97AF-4091-A34F-F2B368865BBD,Framework for Computational Persuasion,"Persuasion is an activity that involves one party trying to induce another party to believe something or to do something. It is an important and multifaceted human facility. Obviously, sales and marketing is heavily dependent on persuasion. But many other activities involve persuasion such as a doctor persuading a patient to drink less alcohol, a road safety expert persuading drivers to not text while driving, or an online safety expert persuading users of social media sites to not reveal too much personal information online. As computing becomes involved in every sphere of life, so too is persuasion a target for applying computer-based solutions.

Many of the current persuasion technologies for behaviour change (e.g. for encouraging healthier life styles) are based on some combination of questionnaires for finding out information from users, provision of information for directing the users to better behaviour, computer games to enable users to explore different scenario concerning their behaviour, provision of diaries for getting users to record ongoing behaviour, and messages to remind the user to continue with the better behaviour.

Interestingly, argumentation is not central to the current manifestations of persuasion technologies. The arguments for good behaviour seem either to be assumed before the user accesses the persuasion technology (e.g. when using diaries, or receiving email reminders), or arguments are provided implicitly in the persuasion technology (e.g. through provision of information, or through game playing). 

So explicit consideration of arguments and counterarguments are not supported with existing persuasion technologies. Yet in real-world persuasion, in particular in applications such as behaviour change, presenting convincing arguments, and presenting counterarguments to the user's arguments, is critically important. For example, for a doctor to persuade a patient to drink less alcohol, the doctor has to give good arguments why it is better for the patient to drink less, and for how it is possible. 

In this project, we intend to bring argumentation into a new generation of persuasion technologies. An automated persuasion system (APS) is a system that can engage in a dialogue with a user (the persuadee) in order to persuade the persuadee to do (or not do) some action or to believe (or not believe) something. To do this, an APS aims to use convincing arguments in order to persuade the persuadee. 

The dialogue may involve moves including queries, claims, and importantly, arguments that are presented according to some protocol. The dialogue may be asymmetric since the kinds of moves that the APS can present may be different to the moves that the persuadee may make. For instance, the persuadee might be restricted to only making arguments by selecting them from a menu (in order to obviate the need for natural language processing of arguments being entered). In the extreme, it may be that only the APS can make moves. Whether an argument is convincing depends on the context and on the characteristics of the persuadee. An APS maintains a model of the persuadee, and this is harnessed by the strategy of the APS in order to choose good moves to make in the dialogue.

Computational persuasion is the study of formal models of dialogues involving arguments and
counterarguments, of user models, and strategies, for APSs. The overall goal of this project is to develop a formal framework for computational persuasion. This framework will extend recent developments in computational models of argument. The emphasis will be on APSs that will help users in changing behaviour (e.g. to persuade the user to drink less, or to not text while driving).",,"Immediate impacts from the project (i.e. during and in the year or two after the project) will come from the development of the theoretical framework for computational persuasion, and by the case studies where we will be undertaking the first trial of argumentation technology in persuasion with users. Both the theoretical and empirical studies should be of substantial interest to the artificial intelligence community (in particular the subfield developing computational models of argument). 

Further short-term impacts in academic research (i.e. in the first three years after the end of the project) should come from uptake by researchers developing persuasion technology for behavioural change, and for researchers evaluating technology for behavioural change in domains such as healthcare and administration (e.g. encouraging healthy lifestyles, encouraging citizenship, encouraging safe driving, etc).

We will aim for medium impacts (i.e. within 5 years after the end of the project) from the research by further developing and evaluating our technology for computational persuasion in diverse applications for behaviour change. This will come through promoting our work during the project, and by seeking funds to evaluate the technology in specific domains after the end of the project (in conjunction with our collaborators in behaviour change at UCL). 

Potential areas where the project technology could be applied for behaviour (with substantial benefits to individuals and society) change include the following:
 
- healthy life styles (e.g. eating more fruit and veg, taking exercise, decreasing drinking)

- addiction management (e.g. gambling, smoking, drugs)

- weight management (e.g. addressing overweight, bulimia, anorexia) 

- treatment compliance (e.g. self-management of diabetes)

- vaccinations (e.g. encouraging uptake)

- personal finance (e.g. borrowing less, saving more)

- education (e.g. starting or continuing with a course, studying properly)

- energy efficiency (e.g. reducing domestic electricity consumption, installing home insulation)

- citizenship (e.g. voting, recycling, contributing to charities, decreasing food waste); 

- safe driving (e.g. not exceeding speed limits, not texting while driving); 

- unacceptable online behaviour (e.g. addressing racism, sexism, trolling, etc).

- antisocial behavior (e.g. addressing aggressive behaviour, vandalism)

Finally, we anticipate that there will be a medium term impact on technologies for ecommerce. Obviously persuasion is an important aspect of commerce. This is not necessary negative. Consider how a helpful shop assistant can direct a customer to a product that s/he believes is appropriate for the customer. However, computational persuasion could be abused. Therefore, we believe that an impact of this research project will be a better understanding of the potential of computational persuasion, and therefore an important starting point for researchers and policy makes wanting to develop guidelines and regulations for the appropriate use of computational persuasion in ecommerce."
17,C4AEF7A4-0309-4B25-9CCA-C510D87399BB,Making Sense of Sounds,"In this project we will investigate how to make sense from sound data, focussing on how to convert these recordings into understandable and actionable information: specifically how to allow people to search, browse and interact with sounds.

Increasing quantities of sound data are now being gathered in archives such as sound and audiovisual archives, through sound sensors such as city soundscape monitoring and as soundtracks on user-generated content. For example, the British Library (BL) Sound Archive has over a million discs and thousands of tapes; the BBC has some 1 million hours of digitized content; smart cities such as Santander (Spain) and Assen (Netherlands) are beginning to wire themselves up with a large number of distributed sensors; and 100 hours of video (with sound) are uploaded you YouTube every minute.

However, the ability to understand and interact with all this sound data is hampered by a lack of tools allowing people to &quot;make sense of sounds&quot; based on the audio content. For example, in a sound map, users may be able to search for sound clips by geographical location, but not by &quot;similar sounds&quot;. In broadcast archives, users must typically know which programme to look for, and listen through to find the section they need. Manually-entered textual metadata may allow text-based searching, but these typically only refer to the entire clip or programme, can often be ambiguous, and are hard to scale to large datasets. In addition, browsing sound data collections is a time-consuming process: without the help of e.g. key frame images available from video clips, each sound clip has to be &quot;auditioned&quot; (listened to) to find what is needed, and where the point of interest can be found. Radio programme producers currently have to train themselves to listen to audio clips at up to double speed to save time in the production process. Clearly better tools are needed.

To do this, we will investigate and develop new signal processing methods to analyse sound and audiovisual files, new interaction methods to search and browse through sets of sound files, and new methods to explore and understand the criteria searchers use when searching, selecting and interacting with sounds. The perceptual aspect will also investigate people's emotional response to sounds and soundscapes, assisting sound designers or producers to find audio samples with the effect they want to create, and informing the development of public policy on urban soundscapes and their impact on people.

There are a wide range of potential beneficiaries for the research and tools that will be produced in this project, including both professional users and the general public. Archivists who are digitizing content into sound and audiovisual archives will benefit from new ways to visualize and tag archive material. Radio or television programme makers will benefit from new ways to search through recorded programme material and databases of sound effects to reuse, and new tools to visualize and repurpose archive material once identified. Sound artists and musicians will benefit from new ways to find interesting sound objects, or collections of sounds, for them to use as part of compositions or installations. Educators will benefit from new ways to find material on particular topics (machines, wildlife) based on their sound properties rather than metadata. Urban planners and policy makers will benefit from new tools to understand the urban sound environment, and people living in those urban environments will benefit through improved city sound policies and better designed soundscapes, making the urban environment more pleasant. For the general public, many people are now building their own archives of recordings, in the form of videos with soundtracks, and may in future include photographs with associated sounds (audiophotographs). This research will help people make sense of the sounds that surround us, and the associations and memories that they bring.",,"Potential beneficiaries of this project outside of the academic research community include anyone who could benefit from new ways to explore sound and audiovisual data, or could benefit from access to the sounds that would be enabled by the research. Examples from different sectors are given below.

Commercial private sector:
* Commercial companies designing audio equipment, through easier access to new audio research;
* Musicians, composers and sound artists, through ways to find and explore new sounds as part of their creative output;
* Computer games companies, through new ways to reuse sound datasets creatively for new game sounds;
* Audio archiving companies, through access to the latest algorithms and methods for annotating and exploring sound archives;
* Television and radio companies, through ability to use sound data exploration technologies in the creation, editing and re-use of audio and audiovisual programmes.
* Acoustic consultants, through access to new ways of mapping and understanding soundscapes, which will help drive new design possibilities for the built environment.
* Internet-of-things companies who supply smart cities with networked sensor systems, through access to novel acoustic algorithms for more sophisticated mapping.
Policy-makers and others in government and government agencies:
* Urban planning authorities, through new insights into the impact of sounds and how to visualize and understand these impacts;
* Research funders, through establishment of a network of researchers in sound data research, opening up new opportunities for valuable research, and new demonstrators showing the value of research.

Public sector, third sector and others:
* Museums and other organizations with sound archives, through new software methods to allow people to explore and use their archives;
* Smart cities, through better ways to make sense of acoustic data from urban microphone arrays;
* Science promotion organizations, in particular through outputs from the projects on how people perceive and navigate sounds.

Wider public:
* People interested in exploring audio recordings at home, school, college or university, either for educational or general interest purposes;
* People recording sounds on mobiles and other portable devices, including those capturing audio as soundtracks to videos;
* Teachers in schools, colleges or universities who want to use sound examples for teaching audio or music;
* People living in urban environments, through improved city sound policies and better designed soundscapes, making the urban environment more pleasant;
* Audiences of creative output involving audio and music, through availability of new creative outputs facilitated by creative access to new sounds.

Researchers employed on the project:
* Improved skills in research methodologies, which may be transferred into e.g. the commercial private sector on completion of the project."
18,59E168A1-90FE-47C3-AA01-124031AE08A7,Cyber security solutions for smart traffic control systems,"We aim to develop a solution framework that can efficiently tackle the cyber security vulnerabilities of smart traffic control systems. Such solutions would be very beneficial for both the industry and society, as current Internet of Things (IoT) based networks, which enjoy significant interest as a key technology towards the development of smart societies, are typically very vulnerable against cyber attacks. This is especially true in the case of traffic systems, as they are among the key national strategic infrastructures that have to be protected at the highest security level.

This proposal is the first study that aims to propose a solution to these cyber security challenges. In particular, we believe that such efficient solutions need to meet the following criteria:
1. Human-agent heterogeneity: The system design has to be capable of dealing with the heterogeneity of interactions and information sharing between different participating devices (i.e., agents) and humans within the traffic control system. 
2. Robustness: the system has to be resistant against different major attack scenarios that may significantly vary in both motives and execution. Adversaries could be very strategic in planning sequential attack actions.
3. Adaptivity: The defence mechanisms should react in real time, in an online manner, and adaptive to the concrete actions of the attackers.
4. Limited resources: The defence strategies have to take into account that there are typically limited resources available to execute each step, or decision made during the process.

To address these criteria, we propose a solution that relies on three fundamental components: (i) human-agent collectives based framework; (ii) game theory; and (iii) resource-constrained online machine learning. In particular, the human-agent collectives based perspective allows us to build a unified framework for smart traffic control systems that can efficiently deal with the heterogeneity between different participating agents and humans. We then use security game theory to discover and analyse major attack scenarios, in order to make our system design robust against them. We then apply resource-constrained online machine learning algorithms to develop efficient adaptive and real-time defence mechanisms. Additionally, these mechanisms also consist of further game theoretic approaches that can be used to predict the strategic behaviour of the attackers, and thus, can make more efficient decisions. Finally, we will build a real testbed as a proof-of-concept of our proposal.

There are many advantages of this solution, namely:
1. The unified human-agent collectives based framework will enable us to define formal descriptions and categorisations of interactions within the system in a principled manner. This will provide a strong basis to develop methodologies to identify and analyse suspicious behaviours.
2. With the game theoretic framework, we will be able to investigate the worst case scenarios of different attack types. This will provide us a full understanding of what should and should not be taken into account during the design of the system.
3. The game theoretic framework also provides an efficient tool to analyse the strategic behaviour of the attackers. This knowledge will play an essential role in predicting the possible future actions of the attackers.
4. The online defence mechanism combines the extracted knowledge, provided by the strategic behaviour analysis within the human-agent collectives and game theoretic frameworks, with efficient machine learning techniques to quickly and efficiently detect malicious behaviours.
5. Given the available resources, the defence mechanism then can decide what is the best response actions that can either prevent the malicious user to cause any harm (in case of sufficient resources are available), or to minimise the damage the attacker can cause (in case of having restricted resources).",,"Knowledge: As our proposed research is the first to tackle the cyber security challenges of smart traffic control systems, it will enable significant benefits to the traffic control systems research and development community, as well as to wider communities such as Internet of Things (IoT), cyber security, and smart societies. In particular, our findings will provide a generic framework that will allow us to design cyber security solutions for IoT systems in a principled manner, making the dream of building smart societies in a safer way. 

Economy and Society: The proposed research will be of significant benefit to our industrial partners. In particular, Sairui-Tech has explicitly identified its benefits from our findings in the corresponding Letter of Support. In addition, while other partners such as MRG Effitas and Intellisense.IO have not provided explicit evidences of direct benefits, they have expressed their interest to integrate our findings into their products. On the other hand, the society will also benefit from our findings, as in the presence of secure solutions, people can trust IoT based system better, and thus, increasing the speed of transition to smart societies. 

People: Throughout the project, the research assistants (RA) and the software engineering technician will gain significant expertise in designing efficient cyber security solutions for IoT systems. This expertise is highly sought after in both industry and academia, since tendencies show that IoT systems will be one of the main technologies of the future, which will need high quality cyber security solutions."
19,5F9441E4-06B2-4008-B92D-148E766407F8,Deep Probabilistic Models for Making Sense of Unstructured Data,"The future information infrastructure will be characterized by massive streaming sets of distributed data-sources. These data will challenge classical statistical and machine learning methodologies both from a computational and a theoretical perspective. This proposal investigates a flexible class of models for learning and inference in the context of these challenges. We will develop learning infrastructures that are powerful, flexible and 'privacy
aware' with a user-centric focus. These learning infrastructures will be developed in the context of particular application challenges, including mental health, the developing world and personal information management. These applications are inspired by collaborations with citizenme, the NewMind Network for Mental Health Technology Research and Makerere University in Kampala, Uganda.",,"We will follow the principles of open data science to ensure the impact of our work is felt as strongly as possible in industry, health and for the developing world. These principles are as follows:

- Make new analysis methodologies available as widely and rapidly as possible with as few conditions on their use as possible.
- Educate our commercial, scientific and medical partners in the use of these latest methodologies.
- Act to achieve a balance between data sharing for societal benefit and the right of an individual to own their data. 

In practice we will carry out these ideas by making our software freely available under BSD licenses (see e.g. Sheffield's GPy software, 65 watchers and 159 stars on GitHub), deploying a program of focussed summer schools, for example the the Gaussian Process Summer Schools (http://gpss.cc) with editions across Europe, Australia, South America and Africa and participating in other summer schools and tutorials.

Our entire project is a close collaboration with industrial partners, clinical partners and researchers in developing countries. These collaborators have representatives on our advisory group which will steer our research to ensure it feeds directly into our users' needs."
20,2C3512EE-4635-4A8D-B68B-BF9F49C163F5,Future Everyday Interaction with the Autonomous Internet of Things,"This project seeks to investigate the design of interaction mechanisms and user interfaces for a future Autonomous Internet of Things (A-IoT): a system of interconnected devices that reaches beyond most current incarnations of the IoT to include aspects of autonomy or automation as a key feature. Nascent instantiations of the A-IoT range from smart thermostats that learn to autonomously control central heating systems based on the presence of users and their routine, to washing machines that order detergent for delivery when it runs out. In other words, this A-IoT can proactively respond to sensed environmental changes, effectively doing work on behalf of users, with the promise of a more efficient use of resources (e.g. to use less energy for heating) or increased convenience (e.g. to always have detergent available). 
The wealth (or &quot;deluge&quot;) of data produced by the IoT is likely to keep growing beyond human capacity to turn it into meaningful information that can be acted on. Therefore, it will require future interactive systems to increasingly support the delegation of granular decision making over large and complex data to autonomous computational agents, allowing users to make informed choices about their general needs and comfort. In an Autonomous IoT; data and decisions will be, in part, 'actively' managed by the devices and their software, drawing upon machine learning techniques and optimization algorithms.
However, recent studies examining the real-world acceptance of a commercial smart thermostat highlighted how errors, limited legibility of the system operation, and excessive user expectations caused frustration and led to some users abandoning the technology. Our own prior work revealed people distrust a potential smart energy infrastructure due to lack of accountability of the ownership, intent, and permitted activities of the autonomous technology. These results suggest that the design of A-IoT systems needs to address several challenges to be made accountable; including, on the system side, designing autonomous decision-making to take into account the uncertain nature of contingent human behaviour; and on the user side, the need to make these systems legible and usable in everyday life. Indeed there is an inherent tension between making a system's operation legible and not overwhelming users with the technical complexity of artificial intelligence algorithms. To date, the methodologies to design such systems are rather sparse and not specific to A-IoT systems (spanning HCI, AI, and Ubicomp) and hence a more focused approach is required to determine the core design principles and methods for the implementation of A-IoT systems.
Our goal is thus to establish the scientific underpinnings of user interactions with A-IoT systems, in a domestic everyday context, with the aim of elucidate the following research questions: to what extent may users be willing to delegate agency to A-IoT systems in everyday contexts? How should interactions with A-IoT systems be engineered to support rather than hinder users' daily activities? What capabilities are essential for intelligent agents to manage such A-IoT systems? How can we design such systems so that they allow users to delegate control, yet easily regain it? Unless such questions are fully addressed, A-IOT systems are likely to frustrate users, resulting in significant waste of time and resources.
Hence, we will address these challenges through a combination of techniques, including the study of existing practices, the iterative development of novel A-IoT prototypes and their evaluation in-the-wild. Such a multidisciplinary approach is made possible by a team that brings together internationally-leading researchers in human-computer interaction, artificial intelligence and design ethnography.",,"The Internet of Things has been identified by the UK government as a key area for investment, recognizing its high potential for impact on the national economy and, more in general, upon society (www.gov.uk/government/publications/internet-of-things-blackett-review). Equally, autonomous systems have been recognized by the EPSRC as a priority area and &quot;part of [their] response to national challenges&quot;. This project addresses both these recognized innovation opportunities through its aim to combine IoT and autonomous intelligent systems into the A-IoT and release its potential for applications in domestic everyday settings.

The proposed research cuts across three of the six priority research areas set out in the roadmap for interdisciplinary research on the Internet of Things; namely People, Trust, and Data [IoTSIG 2013].

Our focus on domestic practices relates to a broad range of activities such as supply (production and distribution), storage, food preparation, eating and waste reduction, a central societal concern in the UK. These cut across a range of key societal sectors including agriculture, manufacturing, transportation, gastronomy and energy. The results of the proposed research will be relevant to stakeholders from these sectors, informing how their activities might be supported by the A-IoT.

End-users engagement
The approach through which the project aims are achieved is an inclusive, user-centred design process, involving end-users at all stages of the design process. Participatory design and envisionment workshops, and field deployments of prototypes will take place throughout the project. These activities will involve our project partners, specific food consumer groups and commercial food venues (reached through our partners), as well as members of the general public.

Industry engagement
Industry engagement is undertaken with and through our project partners: Wireless Things PLC and Senseye, as IoT technology providers, Sutton Community Farm and Homemade Cafe Ltd. as potential IoT technology beneficiaries. The partners will shape the design of our prototypes and trials, in order to make our results relevant to their needs. We will also showcase achievements and outputs, including new IoT application and services, at a suitable industry events (e.g. Innovate UK conferences), to engage other key industry players in the electronic technology, UX design and food supply sectors. We will allocate the role of managing and furthering interactions with industrial partners to Ramchurn, given his experience running a number of successful Knowledge Transfer Secondment activities with Hampshire County Council and BAE systems as well as running the Industrial Placements programme for the Electronics and Computer Science department at Southampton. 

Communication &amp; Press activity
An advisory panel of stakeholders will be convened, consisting of industry, the third sector, governing bodies, and external academics especially from fields not included in the project team. We will hold annual all-hand meetings with the panel to present and reflect research progress, and to seek strategic research guidance from stakeholders, specifically in relation to impact performance, and further opportunities to disseminate findings. 

Project output public release
In addition to the normal academic dissemination routes the research findings will be made available through summaries and briefing papers, including a final report detailing the project aims and key headline descriptive results. These documents will be made available on a dedicated project website. To maximise impact we will host a dissemination event at the end of the programme. 
This approach will follow a similar format to the 'outcomes' section of other EPSRC projects from the applicants (e.g. ORCHID, HORIZON), which resulted in national and international media coverage (e.g. BBC, Guardian, Independent, New Scientist, C4)."
21,9BEFF095-25CA-475E-A559-15A027FAFE47,COMMANDO-HUMANS: COMputational Modelling and Automatic Non-intrusive Detection Of HUMan behAviour based iNSecurity,"This project addresses mainly the Human Factors challenge of the joint Singapore-UK call, and it has an interdisciplinary team with expertise in cyber security, cognitive psychology, and human-computer interface (HCI). It aims at producing direct evidence that human behaviour related insecurity can be detected automatically by applying human cognitive models to model and simulate humans involved in security systems. A key outcome of the project will be a working software system that can be used for this purpose by researchers and practitioners. The project will focus on human user authentication systems as a representative use case and will produce new knowledge on the role of human behaviours in such systems and security systems in general. Both the software framework and new knowledge on human behaviours can also help address other challenges of the call (e.g., detection of intruders/extremists requires knowledge on how they behave; protection of user privacy require knowledge on how human users handle personal data; policy makers need to understand behaviours of their organisations' employees and human attackers targeting their organisations to make more informed decisions).

It has been well known that human factors are a very important aspect of cyber security, as recognised by governments all over the world e.g., in the UK Cyber Security Strategy (2011), in Singapore's National Cyber Security Masterplan 2018 (2013), and in the US Federal Cybersecurity Research and Development Strategic Plan (2011). Human related insecurity is often related to intended or unintentional (maybe subconscious) insecure human behaviours. To conduct research on human behaviours (in cyber security, HCI, psychology and other related fields), researchers normally depend on involvement of real human users via surveys, interviews, simulated scenarios, observations of real cases, interactive games, or other specially designed user studies. Such approaches are often time-consuming and costly, and suffer from other issues like limited and/or biased samples, questionable ecological validity, difficulties in reproducing results, and impossibility of running some studies due to ethical/privacy/legal concerns.

This project aims at developing the first (to the best our knowledge) general-purpose computational framework and supporting software tools that will enable automatic detection of human behaviour related insecurity at the HCI level without the need to involve real human users. The framework will be built on computational models of human cognitive processes, HCIs, human behaviour related attacks and (in)security measures. The framework will be non-intrusive: instead of evaluating the running system itself, the framework will evaluate an abstract executable model of the system and humans involved. Removing real human users from the process allows faster and more objective inspection of potential insecurity of a given security system. The automated process can still be combined with traditional user studies to make better use of limited resources in automatically detecting potential insecurity problems deserving further manual analysis.

The framework and software tools developed will be of great value for cyber security researchers, security system designers/developers and security industry to deliver securer systems to end users. As a natural byproduct, they will also allow easier evaluation of usability of security and non-security related computer systems with an HCI. As we mentioned above in this summary, people having concerns on other challenges of the call can benefit from the project's outcomes as well.

In this project we will focus mainly on HCI-level (&quot;micro&quot;) human behaviours, but possible extensions to higher-level (&quot;macro&quot;) behaviours (e.g., how human users adapt their behaviours over time via rehearsals and learning) will be looked at as well to pave the way for our future research.",,"The &quot;Academic Beneficiaries&quot; field of the Je-S form explains the expected academic impact in detail, so here we focus on economic and societal impact.

While the project is targeting mainly researchers, we will make the software framework accessible to non-researchers as well so it can help security system designers and developers, and security industry in general to check human behaviour related insecurity problems at the HCI level in the design stage of their security products and services. Even when user studies are still needed to evaluate their products and services' performance, the software framework can help identify key areas they need to pay more attention to and thus making a better use of the limited resources. This, on one hand, can help enhance the research capacity, knowledge and skills and efficiency of security industry to deliver securer security products and services, and on the other hand can improve the overall experience and quality of life of end users by reducing security incidents that can be avoided before such products and services are introduced into the real world. If it is possible to collect more realistic (and anonymous) information about human users using a deployed security product or service, the vendor/provider can also identify more potential insecurity problems that exist for a particular group of users only and find ways to serve them better. 

We also expect that the software framework developed will help organisations' policy makers and IT managers to get more information about behaviours of their employee's and human attackers targeting their organisations, and the usability-security trade-off of their security systems (deployed and those under consideration for purchase), which will allow them to make more informed decisions on things like what security systems to use, how to use them, what security policies should be enforced, and if any training or educational programmes are needed for their staff and customers. We understand policy makers and IT managers will have more interests in macro human behaviours and more systems beyond human user authentication, so they can be potential users of the planned extensions of our research in future.

Like most IT systems, there are two types of end users of security products and services: 1) non-security service providers using such products and services developed by other companies to serve their customers (e.g., banks); 2) end human users who are actually using the products and services. In addition to indirectly benefiting from the software framework we will develop, both groups of end users can actually use the software framework to conduct independent evaluation of security products and services they use, which can help increase transparency of the security industry and eventually benefit security industry by giving more credits to better products and services. This may also foster a new service on independent security and usability evaluation of IT systems (e.g., like what Virus Bulletin Ltd is currently doing on anti-malware products). We will exploit the possible commercialisation of the software framework developed towards this direction.

As can be expected, our proposed research on human behaviours at the HCI level will create new knowledge on how human users and attackers behave and interact with computer systems. Such knowledge is not only useful for researchers, but equally so for practitioners and end users. This is particularly important for security education and training purposes, e.g., in designing and implementing cyber security awareness campaigns for the general public. The focused cyber security systems, human user authentication systems, are also a very good use case here as passwords are widely used in security education and training.

It deserves mentioning that the human and HCI modelling parts of our software framework are independent of security, so can be used for evaluating usability of any IT systems."
22,05934601-CAA0-42D0-8D46-E89E0126386F,Aerial Additive Building Manufacturing: Distributed Unmanned Aerial Systems for in-situ manufacturing of the built environment,"Additive Building Manufacturing (ABM) is transforming the construction industry through the 3D printing of buildings and building components. A number of countries are now demonstrating ABM can substantially reduce construction time, material and transport costs, improve worker safety standards and alleviate construction's impact on urban traffic congestion and the environment. ABM also provides geometrical variety at no additional cost. In contrast to most manufacturing sectors, variety is a necessity within construction to satisfy different client requirements and adapt to unique terrain, boundary and laws governing each physical site. 
However, current ABM systems are difficult to deploy on construction sites due to their large size and fixed 3D Print build volumes that are not sufficiently flexible to deal with the complexities of most building scenarios, or provide adequate measures for human safety. These ABM technologies are unable to undertake maintenance and repair work, or construct buildings in many urban or elevated sites. They are also not able to be utilised for post-disaster reconstruction activities where their manufacturing speed would be of great assistance.
To address this limitation, this research proposal aims to develop the world's first Aerial Additive Building Manufacturing (Aerial ABM) System consisting of a swarm of aerial robots (Unmanned Aerial Systems (UAS)) that can autonomously assess and manufacture building structures. Aerial ABM offers major improvements to human safety, speed, flexibility, and manufacturing efficiency compared to existing ABM and standard building construction technologies. We have already developed and demonstrated pilot results using UAS that can extrude 3D Print material during flight and we have developed simulation environments that allow for autonomous planning and execution of manufacturing with swarms of UAS working in collaboratively. 
Using the resources of the EPSRC grant, we will co-develop and demonstrate a working Aerial ABM system that will manufacture structural elements such as walls and a freeform building pavilion. This will require innovation and major technical contributions in Hardware, Autonomy as well as in Materials and Structures. Building on the consortium's world-leading expertise in these areas and support from industrial partners (Skanska, Ultimaker, BuroHappold, Dyson and BRE), we aim at delivering the following main research contributions through this grant:

Aerial ABM Hardware
- A novel Aerial ABM robot design with autonomous vision based stabilisation, navigation and mapping of a dynamically changing environment that is optimised for flight and 3D Printing tasks.

Aerial ABM Autonomy
- A framework for autonomous manufacturing that utilises swarm intelligence for collaborative robot-to-robot operations, dynamic task sharing/allocation, adaptive response to context and dynamic environment content involving functions such as new methods of collision avoidance.
- Develop new modes of communication and control that enable the safe co-existence and cooperation of human workers, other robots and Aerial ABM robots on construction sites. Novel research in human-robot interaction, feedback and haptic interface functionalities will enable manufacturing flexibility suitable for construction sites that are always unique in size, shape and contextual complexity.
- An integrated design and real-time structural analysis software that delivers optimal structural integrity from minimal material weight within building design strategies that leverage this free-form manufacturing process to create innovative building design possibilities.

Aerial ABM Materials and Structures
- Development of new high-performance 3D-printable composite material and deposition procedures for the additive manufacture (3D Printing) of free-form light-weight building structures utilising autonomous UAS.",,"The three main impact beneficiaries are the construction sector, society at large and the environment. The Aerial Additive Building Manufacturing (Aerial ABM) system proposed in this application can provide substantial benefits by enabling autonomous capabilities of collaborative working with human workers, situational awareness in unknown environments and low-risk manipulation in dangerous settings. These capabilities will benefit the three beneficiaries as follows:

CONSTRUCTION
Construction companies such as Skanska are economically penalised for time over-runs. Our technology has the potential to reduce manufacture time and complexity while expanding capabilities by providing autonomous behaviours and sensing to predict, measure, qualify and expedite construction. This can provide major cost savings and increased profits. 

Transportation accounts for 20% of construction costs [1]. Aerial ABM can significantly reduce transportation volumes as only raw material is delivered to site.

UK Government Construction Strategy mandates all construction projects to utilise 3D Building Information Models (BIM) technology by 2016 [2]. Aerial ABM is able to increase building efficiency by integrating BIM more effectively with the construction on site.

SOCIETY
Health and Safety: 
&quot;The construction industry is the most dangerous sector in Britain....448 British soldiers have been killed in Afghanistan since 2001. Over the same period, more than 760 construction workers have been killed on British sites.&quot; notes the UK Health and Safety Executive in 2014 after seeing an additional 3200 deaths from other construction related matters [3]. An Aerial ABM system can significantly reduce construction's 31% share of all UK worker fatalities [4] by reducing risk to construction workers in hazardous and labour-intensive tasks while allowing continued human participation in safer construction activities.

Increasing Housing Opportunities: 
The UK is heading for a housing crisis where people cannot afford to buy or rent as a result of a shortage in housing and high costs of building. A RICS report forecasts house prices to rise 4.5%/yr for the next 5yrs [1]. Charities such as Shelter [5] advocate that the UK must reduce the cost and time of construction in order to make housing and other building services more attainable. Aerial ABM can help reduce the cost and time of urban construction, through a scalable and digital manufacturing process that saves construction time and resources.

Humanitarian: 
A fast, effective means to undertake post-disaster re-construction and repair can save human lives directly and indirectly. Aerial ABM robots can commence work well before human workers can operate safely and they can do so in areas that are hard to access by ground. The technology can offer Governments and NGOs versatile, rapidly deployable, and low-cost solutions for emergency response and transitional shelter construction. 

ENVIRONMENT
Aerial ABM enables almost zero percent waste construction while its on-site operation reduces transportation volumes. The insights from this project on novel structural and material engineering can also inform building designs requiring less material and lower logistic mass flows. These benefits reduce the embodied energy of buildings and enable reductions in construction's 47% share of the UK's carbon emissions.

[1] BIM Taskgroup, HM Government BIS, 2014
[2] BRE, Construction Site Transport, The Next Big Thing: Why Transport Is Important What You Can Do About It, 2003. 
[3] http://www.thisismoney.co.uk/money/mortgageshome/article-3040143/Housing-shortage-triggering-worrying-upsurge-property-prices-expectations-2-5-rise-2015.html
[4] Daniel Boffey, The needless death of Richard Laco and what it tells us about Britain's perilous building sites, The Observer, 2014
[5] http://england.shelter.org.uk/campaigns/why_we_campaign/the_housing_crisis/building_more_homes/building_more_homes"
23,6098EC5F-7935-4684-B452-600C87F8EDF9,Synthetic Portabolomics: Leading the way at the crossroads of the Digital and the Bio Economies,"Synthetic biology involves the design and development of novel, useful biological systems, or the redesign of those systems that exist already. This approach promises to be of major value to society. Potential applications include the production of high-value materials, such as fine chemicals and pharmaceuticals, bio-remediation, sustainable energy, medical diagnostics, and agriculture. 

In Synthetic Biology novel biological genetic circuits are developed using engineering principles in order to add the new properties to a given organism - called a host or chassis. The type of chassis used will vary according to the application and the circuit. For example, for food and agriculture it is highly desirable to use organisms that have been shown to be safe for human consumption. However, currently, most circuits are designed for, and tested in, a single organism such as the commonly used bacterium Escherichia coli. Moving these circuits to another organism requires the circuit to be re-engineered and retested in the new organism, a process which is very time consuming and costly. This process of 'refactoring' slows down research and costs industry a huge amount of time, effort and money. 

A major problem is that the connections between the designed genetic circuit and the chassis organism are specific to a given species of chassis. So the genetic circuit ends up being redesigned to meet the new connections required for a different species. In our project we will standardise the connection between a given genetic circuit and the chassis organism. We will develop a set of academically and industrially useful organisms where the plug-in points for the genetic circuit will be the same for each of our organisms, allowing the genetic circuit to be moved from one organism to another with changes. We refer to this standardised plug-in system as a 'bio-adaptor'.

This programme grant will initiate a new field in Synthetic Biology, called 'Portabolomics'. This is a highly novel approach that has not been achieved by any other groups to-date. The key to the success of the project is to understand the networks of molecular processes that occur in a cell, since it is these networks that will need to be modified to make the bio-adaptor. We will apply a range of the state-of-the-art computing approaches to this task including many techniques from Computing Science, including network analysis, formal methods and data mining, for which our group has a wide range of world-leading expertise.

The results of the Portabolomics project will not only be a new system of major value to UK synthetic biology research and industry, but will enhance the field of computing science as new computational techniques will need to be developed to achieve our goals.",,"Synthetic Biology promises to substantially contribute to the UK economy and society. The Chancellor of the Exchequer G. Osborne estimated that the &quot;global synthetic biology market is predicted to grow to &pound;11 billion by 2016&quot;. The potential benefits of this rapidly growing field are detailed in a number of key documents including the UK Synthetic Biology Roadmap and the Royal Society Engineering report. 

The Portabolomics project, once in place, will try to ensure the widest possible economic and societal impact from the research and resources it will generate. Since we are generating a new field for the synthetic biology community, a large amount of impact will fall into the domain of academic beneficiaries and is discussed above. 

However, the key outputs emerging from the project will impact on industry directly. The new Portabolomics enabled chassis, our Portabolomics design specifications and standards, and our software systems will have the most immediate impact on industry. The translation of these outputs will be carried out initially with our industrial partners but will be widened through interactions with other organisations such as SynCiTE and CPI (see pathways to impact section). The biological outputs of the project will be of direct value to these partners as described in the main document (see Strand 2 of Pathways to Impact).

The novel computational approaches to the analysis of complex systems we develop will result in novel approaches for the computational design of synthetic biological systems that will be of clear commercial value in related fields such as neuroscience. Furthermore, this project will develop a wide range of novel research outputs in the fields of computing science. These outputs will be generically applicable, not only in the computing industry but in any other fields requiring advanced data mining strategies for Big Data and network approaches, such as distributed computing and network technology. 

More widely, the work of this project in closing key gaps in our knowledge of bacterial processes at the molecular level will impact directly on the process of Synthetic Biology. In turn, this impact on how Synthetic Biology is done will open up new avenues for product development and the application of Synthetic Biology in a wide range of industrial domains. Moreover, the outputs of the project will enhance the capacity of Synthetic Biology to meet the next generation of challenges in a much wider range of fields such as those outlined in the UK Synthetic Biology roadmap. 

The project will also have a strong impact on the general public. Our proposed SynBioSmith Sandbox (see Pathways to Impact, strand 5) will allow the general public to carry out synthetic biology experiments without needing to be present in a laboratory. This system will provide enthusiasts such as the DIY Bio movement with an enhanced understanding of the methodology behind synthetic biology and provide access to equipment in a safe and controlled environment. Our vision is to allow the general public to carry out Synthetic Biology over the Internet in a controlled and well-regulated manner making simple Synthetic Biology accessible to everyone. Further impact on the general public will be mediated by the Responsible Innovation section of our project (see strand 1 of the Pathways to Impact), and will allow social scientists will be able to reach out to a broad range of stake holders (scientists, industrialist, NGO, Government and the public)."
24,946A43B0-35C9-4F36-ADAE-EF8D6A15376C,Human-Like Computing Machine Intelligence Workshop,"Recent prominent advances in statistical AI, such as the success of
driverless cars and Alpha Go's recent defeat of
the 9-dan ranked South Korean Go player Lee Se-dol have considerably
increased public awareness of the potential for computers to achieve
human-level competence in settings which were previously the exclusive
realm of human judgement and skill. However, in human terms
statistical AI techniques still act as skilled zombies, having
unconscious skills detached from the ability to express goals or intentions.
The lack of ability to communicate in a co-operative fashion with
human beings limits the long-term potential of these approaches
in many commercially and socially valuable settings.

The workshop will address issues related to interaction in which human co-operative and communicative skills
can be studied and modelled computationally. While many of these issues have
been prominent for some time in symbolic AI, we are yet
to see successful integration of statistical and symbolic AI approaches
which achieves the broad range of phenomena present in human behaviour.
This workshop aims to contribute to the objectives of the EPSRC's Human-Like
Computing priority area by crystalising the key issues which need to
be prioritised in this area.",,"It became clear from discussions at the EPSRC Bristol workshop on Human-Like Computing that
different communities of researchers were out of touch with latest
research and trends within areas far from their own. This led
to the suggestion of the value in holding an inter-disciplinary
workshop in which leading experts in these fields provided overviews
and descriptions of latest research findings to those from other
fields. It was agreed that the format and UK tradition of the Machine
Intelligence series would provide an ideal forum for holding such a workshop,
and that the EPSRC should be approached for funding.
As a consequence the proposed workshop will
have the theme of ``Human-Like Computing''. It was further agreed that
invited papers from the workshop will be published in a special
issue of the journal Topics in Cognitive Science. Ulrike Hahn,
a member of the editorial board of the journal, has been tasked with
organising this special issue.

For the limited cost such a workshop is expected to have considerable
long-term impact in helping form a community of researchers working on some
of the hardest unsolved problems in bot Artificial Intelligence and Cognitive
Science."
0,B8C647AE-DDA1-4CAF-B299-2491C4C7049B,"Machine Learning for Patient-Specific, Predictive Healthcare Technologies via Intelligent Electronic Health Records","Healthcare systems world-wide are struggling to cope with the demands of ever-increasing populations in the 21st-century, where the effects of increased life expectancy and the demands of modern lifestyles have created an unsustainable social and financial burden. However, healthcare is also entering a new, exciting phase that promises the change required to meet these challenges: ever-increasing quantities of complex data concerning all aspects of healthcare are being stored, throughout the life of a patient. These include electronic health records (EHRs) now active in many hospitals, and large volumes of data being collected by patient-worn sensors. 

The resulting rapid growth in the amount of data that is stored far outpaces the capability of clinical experts to cope. There is huge potential for using advances in computer science to use these huge datasets. This promises to improve healthcare outcomes significantly by allowing the development of new technologies for healthcare using the data - this is an area that promises to develop into a major new field in medicine. Making sense of the complex data is one of the key challenges for exploiting these massive datasets. 
 
This programme aims to establish a new centre focussed on developing the next generation of predictive healthcare technologies, exploiting the EHR using new methods in computer science. We describe a number of healthcare themes which demonstrate the potential to improve patient outcomes. This will be achieved in collaboration with a consortium of leading clinicians and healthcare companies. The primary aim is to develop the &quot;Intelligent EHR&quot;, which will have applications in creating &quot;early warning systems&quot; to predict patient problems (such as heart failure), and to help doctors know which drug or treatment would best be used for each individual patient - by interpreting the vast quantities of data available in the EHR.",,"The proposed programme has the potential for very significant impact for patients in the hospital or in the home, by optimising patient management and improving outcomes. Patients who are deteriorating will be identified early, which will allow preventative action to be taken, avoiding serious escalation (such as unplanned admission to an ICU), and which will reduce the incidence of preventable morbidity, cardiac arrests, and death. Those hospital patients recovering faster than expected, and who are deemed to be sufficiently stable, can be discharged home earlier. Patient-specific treatments will be enabled in both themes of the proposed work, improving patient outcomes by improving the efficacy of care provided. The translation of such systems into the home environment will provide benefit to patients with long-term conditions, such as chronic obstructive pulmonary disease (COPD) and heart failure - this will be achieved via predictive systems that allow clinicians to track patient condition without the false-alarm rate associated with existing systems, and which prevents existing systems providing benefit to existing patients.

The NHS as an organisation will benefit from the research because improved patient outcomes are associated with lower healthcare costs, as a result of shorter stays in hospital and fewer unplanned admission to (expensive) higher levels of care. Additionally, patients will be stratified according to risk of severity / deterioration, allowing improved use of clinical staffing resources. Clinicians will benefit by being able to interpret, for the first time, the very large and heterogeneous datasets that are available for their patients - enabled by robust, probabilistic tools created during the programme.

Companies in the commercial private sector will benefit from the research, where existing links with SMEs (university spin-outs) and large IT companies (Microsoft and Philips) will allow rapid implementation of the techniques developed during the programme. Such companies have an interest in &quot;intelligent healthcare&quot; algorithms that can be integrated into existing healthcare IT products, which will add significant value and market differentiation. Additionally, the rapidly-growing market for wearable devices is currently focused only on consumers - the proposed centre will extend this market to healthcare technologies, by exploiting the opportunities for large-scale innovation and clinical validation that exist in the programme. The UK economy will benefit by the likely creation of new spin-out activity based around the activity of the proposed centre.

The scientific community, and the UK research base in particular, will benefit from developing capacity in an emerging field of global importance, and where the proposed centre will train the next generation of researchers in computational health informatics - the use of machine learning for healthcare technologies. The methodology developed within the proposed programme will be of translational benefit to other scientific disciplines, including other computational and mathematical sciences. Results from the proposed work will feed into the Alan Turing Institute, where researchers involved across the UK will benefit from the development of large-scale methodologies for data science.

The public will benefit via public-engagement activities run in collaboration with the Institution for Engineering &amp; Technology (the world's largest multidisciplinary engineering professional institution), the George Institute for Global Health (which holds public-engagement conferences at its bases in Oxford, Sydney, Beijing, and New Delhi), the Royal Academy of Engineering (which supports Clifton's work via a Research Fellowship), and the NIHR Oxford Biomedical Research Centre (which holds regular public outreach events)."
1,A0547DB5-5871-4911-B5F5-20892E9AD7A3,MaDrIgAL: MultiDimensional Interaction management and Adaptive Learning,"As tech giants like Google, Facebook, Apple and Microsoft continue to invest in speech technology, the global voice recognition market is projected to reach a value of $133 billion by 2017 (companiesandmarkets.com, 2015). Speech-enabled interactive systems in particular, such as Apple's Siri and Microsoft's Cortana, are starting to show significant economic impact, with the virtual personal assistant (VPA) market estimated to grow from $352 million in 2012 to over $3 billion in 2020 (Grand View Research, 2014).

Although such commercial systems allow consumers to use their voice in interacting with their devices and services, the user experience is still limited due to the lack of naturalness of the conversations and limited social intelligence of the VPA. Moreover, the quality of these user interfaces relies on large, carefully crafted rule sets, making development labour-intensive and not scalable to new application domains. With the emergence of the Internet of Things and voice control in the smart home, there is a huge demand for scalable development of natural conversational interfaces across task domains.

MaDrIgAL will develop a radically new approach to building interactive spoken language interfaces by exploiting the multi-dimensional nature of natural language conversation: in addition to carrying out the underlying task or activity, participants in a dialogue simultaneously address several other aspects of communication, such as giving and eliciting feedback and adhering to social conventions. In analogy to the singing voices in a madrigal, simultaneous processes for each dimension operate in harmony to produce multifunctional, natural utterances. Consider the two alternative responses S2a and S2b in the following example:

U1: Hello, I would like to book a flight to London.
S2a: Which date did you have in mind?
S2b: Ok, flying to London on what date?

Whereas S2a only asks for the next piece of information to book the flight (uni-dimensional), S2b also gives feedback about the arrival city, allowing the user to correct any recognition errors (multi-dimensional). We aim to develop a principled multidimensional modelling and learning framework that covers a wide range of different phenomena, including the implicit confirmation in S2b.

This multi-dimensional approach will not only allow us to build systems that support more natural and effective interactions with users, but also enables cost-effective development of such interfaces for a variety of domains by learning transferable conversational skills (e.g., selecting actions in domain independent dimensions). We will therefore demonstrate our approach by building interactive spoken language interfaces for multiple application domains in a home automation scenario, allowing users to interact with for example their Smart TV or heating control system. We will closely collaborate with the industrial partner SemVox to explore this scenario.

The project will bring together expertise in statistical machine learning approaches to state-of-the-art spoken dialogue systems and natural language generation, as well as linguistic theories of multi-dimensional dialogue modelling (collaborating in particular with academic partner Prof. Bunt). MaDrIgAL will develop Next Generation Interaction Technologies relevant to Health Technology and Assisted Living, as well as tackle the question of a common user interface to the Internet of Things and Big Data.",,"From an end-user importance point of view, this work will be of interest to a wide range of businesses and companies in the UK, operating in areas such as speech technology, AI, home automation, etcetera. Specifically, we will develop and release a new spoken dialogue system (SDS) architecture compliant with the recently developed ISO standard for dialogue act annotation. This will promote interoperability, enabling the developer community to collaborate and reuse each other's components more easily, and ultimately help industry in developing products with spoken language interfaces more efficiently. The proposed research will also help to strengthen the impact of statistical SDS. Development costs for statistical techniques seem currently too high for manufacturers, due to the requirement of sufficient domain-specific data. The proposed new modular architecture indirectly addresses the economic feasibility of developing products with interactive interfaces for a variety of application domains.

This will ultimately help the UK to push past the US, which is still the main competitor in this area, despite the emergence of new language technology companies such as VocalIQ and Arria. For example, a senior speech scientist at Apple has said publicly that statistical methods will form the algorithmic basis for Siri in the near future. These increased capabilities will also enable us to tackle more complex interaction scenarios, such as social robotics, situated multimodal dialogue with smart devices, and eventually controlling and managing the Internet of Things.

From a societal importance point of view, this project will create a unified user interface which allows every-day users to access and control the Internet of Things in an intuitive way using natural language, and as such lowers the barrier to access and benefit from new technology. This is especially relevant for elderly and/or disabled users in a home automation scenario for example."
2,256246F0-B378-4FDC-9F3E-531F70E3956C,The Parameter Optimisation Problem: Addressing a Key Challenge in Computational Systems Biology,"In recent years, advances in both the physical and life sciences have increasingly come from the collaborations of researchers across disciplines, and the development and use of tools from a range of areas. A prototypical example of this interdisciplinary approach to science is systems biology, the field concerned with quantifying how the interaction of individual system components control biological function and behaviour. Systems biology has become increasingly quantitative, with a shift from diagrammatic representations of interaction networks to sets of mathematical equations that model (i.e. simulate) how the concentrations of molecular species vary with time. A key advantage of such models is that they can be used to predict how the networks they represent will respond to specific perturbations, such as changes in environmental conditions (e.g. temperature) or the addition of pharmacological agents. The ability to easily generate such predictions reduces the need for large numbers of expensive and time-consuming experiments. 

However, the more complex a biological network is, the more complex the corresponding model needs to be, and the greater the range of possible biological behaviours that can be exhibited. This means that extensive computer simulations are needed to adjust the parameters controlling the model so as to accurately reproduce (i.e. fit) the experimental behaviour observed. For biologically realistic models which can involve hundreds of different molecular species, the number of simulations required to adjust the parameters of a given model to achieve the optimal fit to data can be prohibitively large, far exceeding that which is possible on practical timescales. Thus, for the predictive power of mathematical models to be fully realised in the systems biology domain, methods are required that allow this parameter optimisation procedure to be carried out in a computationally efficient manner.

The proposed project will address this need by bringing state-of-the-art methods from computer science to bear on the problem, which have been successfully applied previously to highly parametrised problems like aircraft conflict alert systems, design optimisation of lightweight materials and routing of mesh sensor networks (amongst others). In addition, we propose to develop new methods specifically engineered for the systems biology domain that can provide insight into model behaviour, beyond simply returning a single estimate of the best fit parametrisation (e.g. methods for identifying parameters yielding equally good fits to data, and also parameters which simultaneously fit the model to data generated in diverse experimental conditions). As part of this, we will develop a package of open source software tools that will be embedded within a software infrastructure designed for systems biologists, enabling the methods developed in this work to be readily applied to problems in the field that are currently computationally intractable. 

To test and refine the algorithms developed, they will be applied to the gene network that generates circadian oscillations (the circadian clock) in the key plant species Arabidopsis thaliana, for which high-quality experimental data recorded in a range of genetic and environmental backgrounds is available, together with a suite of mathematical models of varying complexity. As part of this work, biochemically detailed models of the clock will be directly fitted to multiple experimental datasets for the first time, yielding models with greater predictive power. Many processes critical for plant growth and reproduction are regulated by the clock (e.g. photosynthesis and flowering time). In the long term, the ability to optimise plant models of increasing complexity with the class of methods we will develop here may thus help predict how the viability of economically important crop species will be affected by future temperature shifts resulting from climate change.",,
3,AC491B7D-BF59-4EEB-9E39-6F49DD944496,Nodes from the Underground: Causal and Probabilistic Approaches for Complex Transportation Networks,"An efficient transportation system is vital to the economic and social well-being of large cities. The transport demand implied by economic growth, however, requires transport networks to become more and more complex, making their management difficult. Fortunately, modern systems such as the London Underground generate vast amounts of data that can be analysed to better understand passenger behaviour and needs. Besides understanding the typical daily patterns that we can observe on a regular basis, Data Science methods allows us to look into in the less usual events such as unplanned disruptions that are still important to any user, and to also model individualised behaviour instead of only aggregates. 

In a large system such as the London Underground, signal failures and disruptive events eventually take place, requiring passengers to change plans in a variety of ways. This research provides advanced statistical modelling and machine learning approaches to learn from past events to examine how passengers adapt themselves when a disruption occurs. When a disruption takes place, the model will provide information of likely changes, such as increased number of passengers leaving a station because they could not reach their destination. These models are important for transport authorities to understand the resilience of the system, different combinations of location and time of a disruption, and unusual responses from passengers that may motivate different communication strategies to inform users of better travel adjustments. This research also opens up conceptual ideas to be exploited in the future using new technologies to monitor and adaptively respond to passenger needs in a more optimised and time-effective way.",,"Among the potential non-academic beneficiaries, we list:

1. Transport authorities such as Transport for London. Although most of the transport authorities have their own analytics teams and vast expertise, they are welcoming of academic research, as made explicit in the Letter of Support provided to us by TfL. In particular, research motivated by basic methodology is important to them, as it focuses on complex models which are not straightforward to integrate with current systems, but which in the long run offers offer potentially important contributions. In addition, by the completion of our research, we will provide these beneficiaries with methodologies and tools that will enhance the operational management of public urban railway systems.

2. Users of large transport systems. Millions of daily users of large transport systems such as London Underground have the potential to benefit directly from a system that reacts quickly and adaptively to disruptions, saving time and reducing the stress of commuting in a big city. This includes secondary effects on productivity gained by more efficient transportation in the society as a whole. In January 2015, Dr Silva and Dr Kang presented preliminary work related to this proposal at the Science Museum &quot;Lates&quot; event, in South Kensington. There was an overwhelmingly positive response from the museum attendees to the type of work we intend to continue in depth with this grant. The general public, and Underground users among them, appreciates the fact that scientific community engages in rigorous research to improve their quality of life.

3. City administrators. Good urban infrastructure has long-term economic benefits. The confidence given by a state-of-the-art transport system is a major factor boosting the profile of cities that expect to be labelled as &quot;world-class&quot; environments for businesses and quality of life. Much of London's reputation, for instance, can be traced back to reliance on its public transport network, including the oldest underground network in the world. Knowing the limitations of its transport system as facilitated by new technologies provides crucial information for the city management.

4. Companies working on complex systems and smart city solutions. Understanding behaviour under disruption is relevant to organisations such as taxi companies, which may benefit for the outcomes of this research as knowing excess demand generated from passengers interrupting their journey can also be used to allocate more vehicles to serve customers that would otherwise arrive late at their destinations. Companies who provide general apps of interest for city dwellers may potentially benefit from APIs streaming forecasts of reactions to disruptions in the transportation network, as some users may be interested in avoiding overcrowded locations. We intend to work closely with these beneficiaries, for example, through UCL's Knowledge Transfer resources.

5. Other companies/researchers working on transport problems outside the domain of passenger movement. The ideas developed in this project are also relevant to other transportation problems in networks, such as computer networks and other applications discussed in research areas such as network tomography. Part of the technology on computational methods developed here can be transferred to these domains mainly through general multi-disciplinary journals as well as journals and conferences in targeted fields."
4,C0462D42-1645-4653-A285-274DFCDAAFC5,TRANSIT: Towards a Robust Airport Decision Support System for Intelligent Taxiing,"There is an imminent need to make better use of existing aviation infrastructure as air traffic is predicted to increase 1.5 times by 2035. Many airports operate at near maximum capacity, and the European Commission recognises the necessity to increase capacity to satisfy demand. In addition, inefficient operations lead to delays, congestion, and increased fuel costs and noise levels inconveniencing all stakeholders, including airports, airlines, passengers and local residents. 

A critical issue is routing and scheduling the ground movements of aircraft. Although ground movement is only a small fraction of the overall flight, the inefficient operation of aircraft engines at taxiing speed can account for a significant fuel burn. This applies particularly at larger airports, where ground manoeuvres are more complex, but also for short-haul operations, where taxiing represents a larger fraction of an overall flight. It is estimated that fuel burnt during taxiing alone represents up to 6% of fuel consumption for short-haul flights resulting in 5m tonnes of fuel burnt per year globally. This project aims to investigate a decision support system which considers multiple factors to provide more robust taxiing routes. 

Current decision support systems for routing and scheduling taxiing aircraft suffer from several limitations:

1) The only objective they consider is minimising taxi time, ignoring other important factors. These other factors include taking into account engine performance which is linked to fuel consumption, environmental impact and cost. Routes and schedules, which are efficient in terms of fuel and cost, are therefore compromised as a result of considering a one dimensional objective.

2) Airframe dynamics are not taken into account during planning of routes and schedules. Consequently, the taxing instructions issued may be hard to follow, making compliance with the allocated routes unrealistic.

3) Taxi time is typically based on average speeds of aircraft. This is an over-simplification meaning that any taxiing manoeuvre which falls outside the expected duration can affect the taxiing of other aircraft. Furthermore, if the approach of including overly conservative time buffers to absorb uncertainty is adopted, the resulting overall airport operating efficiency will be degraded.

4) It is difficult to specify taxiing speeds and heuristic rules for routing and scheduling systems as: they depend on airport layout and operational requirements, which can vary throughout the day according to the volume of air traffic. Consequently, routing and scheduling systems have to be reconfigured for specific airports and operational constraints.

5) Due to variability in taxi speed and over-simplistic models of aircraft, there is lack of understanding as to how much benefit can be achieved by automated routing and scheduling in real-world settings. 

TRANSIT will directly address these limitations of current systems, to make better use of existing airport infrastructure and lessen the impact of the growing aviation sector on the environment. Multi-objective optimisation algorithms will be integrated with models of aircraft to balance the reduction of taxi time, cost and emissions. We aim to make the routing and scheduling system easily reconfigurable to any airport. The uncertainty will be directly incorporated in planning, resulting in robust taxiing, verified by pilot-in-the-loop trials.

TRANSIT aims to investigate such a system and its associated benefits in collaboration across a broad range of disciplines and fields (Engineering, Operational Research, and Computer Science) needed to tackle such challenging problem. Cooperation with leading industrial stakeholders, and consultation with established academics, ensure that the work is cutting edge while reflecting needs of the industrial partners.",,
5,C9CE37DA-F999-432D-B8DB-017D565BCC4E,TRANSIT: Towards a Robust Airport Decision Support System for Intelligent Taxiing,"There is an imminent need to make better use of existing aviation infrastructure as air traffic is predicted to increase 1.5 times by 2035. Many airports operate at near maximum capacity, and the European Commission recognises the necessity to increase capacity to satisfy demand. In addition, inefficient operations lead to delays, congestion, and increased fuel costs and noise levels inconveniencing all stakeholders, including airports, airlines, passengers and local residents. 

A critical issue is routing and scheduling the ground movements of aircraft. Although ground movement is only a small fraction of the overall flight, the inefficient operation of aircraft engines at taxiing speed can account for a significant fuel burn. This applies particularly at larger airports, where ground manoeuvres are more complex, but also for short-haul operations, where taxiing represents a larger fraction of an overall flight. It is estimated that fuel burnt during taxiing alone represents up to 6% of fuel consumption for short-haul flights resulting in 5m tonnes of fuel burnt per year globally. This project aims to investigate a decision support system which considers multiple factors to provide more robust taxiing routes. 

Current decision support systems for routing and scheduling taxiing aircraft suffer from several limitations:

1) The only objective they consider is minimising taxi time, ignoring other important factors. These other factors include taking into account engine performance which is linked to fuel consumption, environmental impact and cost. Routes and schedules, which are efficient in terms of fuel and cost, are therefore compromised as a result of considering a one dimensional objective.

2) Airframe dynamics are not taken into account during planning of routes and schedules. Consequently, the taxing instructions issued may be hard to follow, making compliance with the allocated routes unrealistic.

3) Taxi time is typically based on average speeds of aircraft. This is an over-simplification meaning that any taxiing manoeuvre which falls outside the expected duration can affect the taxiing of other aircraft. Furthermore, if the approach of including overly conservative time buffers to absorb uncertainty is adopted, the resulting overall airport operating efficiency will be degraded.

4) It is difficult to specify taxiing speeds and heuristic rules for routing and scheduling systems as: they depend on airport layout and operational requirements, which can vary throughout the day according to the volume of air traffic. Consequently, routing and scheduling systems have to be reconfigured for specific airports and operational constraints.

5) Due to variability in taxi speed and over-simplistic models of aircraft, there is lack of understanding as to how much benefit can be achieved by automated routing and scheduling in real-world settings. 

TRANSIT will directly address these limitations of current systems, to make better use of existing airport infrastructure and lessen the impact of the growing aviation sector on the environment. Multi-objective optimisation algorithms will be integrated with models of aircraft to balance the reduction of taxi time, cost and emissions. We aim to make the routing and scheduling system easily reconfigurable to any airport. The uncertainty will be directly incorporated in planning, resulting in robust taxiing, verified by pilot-in-the-loop trials.

TRANSIT aims to investigate such a system and its associated benefits in collaboration across a broad range of disciplines and fields (Engineering, Operational Research, and Computer Science) needed to tackle such challenging problem. Cooperation with leading industrial stakeholders, and consultation with established academics, ensure that the work is cutting edge while reflecting needs of the industrial partners.",,
6,CB19031D-2938-4363-A3CD-E74EF1CF9D76,TRANSIT: Towards a Robust Airport Decision Support System for Intelligent Taxiing,"There is an imminent need to make better use of existing aviation infrastructure as air traffic is predicted to increase 1.5 times by 2035. Many airports operate at near maximum capacity, and the European Commission recognises the necessity to increase capacity to satisfy demand. In addition, inefficient operations lead to delays, congestion, and increased fuel costs and noise levels inconveniencing all stakeholders, including airports, airlines, passengers and local residents. 

A critical issue is routing and scheduling the ground movements of aircraft. Although ground movement is only a small fraction of the overall flight, the inefficient operation of aircraft engines at taxiing speed can account for a significant fuel burn. This applies particularly at larger airports, where ground manoeuvres are more complex, but also for short-haul operations, where taxiing represents a larger fraction of an overall flight. It is estimated that fuel burnt during taxiing alone represents up to 6% of fuel consumption for short-haul flights resulting in 5m tonnes of fuel burnt per year globally. This project aims to investigate a decision support system which considers multiple factors to provide more robust taxiing routes. 

Current decision support systems for routing and scheduling taxiing aircraft suffer from several limitations:

1) The only objective they consider is minimising taxi time, ignoring other important factors. These other factors include taking into account engine performance which is linked to fuel consumption, environmental impact and cost. Routes and schedules, which are efficient in terms of fuel and cost, are therefore compromised as a result of considering a one dimensional objective.

2) Airframe dynamics are not taken into account during planning of routes and schedules. Consequently, the taxing instructions issued may be hard to follow, making compliance with the allocated routes unrealistic.

3) Taxi time is typically based on average speeds of aircraft. This is an over-simplification meaning that any taxiing manoeuvre which falls outside the expected duration can affect the taxiing of other aircraft. Furthermore, if the approach of including overly conservative time buffers to absorb uncertainty is adopted, the resulting overall airport operating efficiency will be degraded.

4) It is difficult to specify taxiing speeds and heuristic rules for routing and scheduling systems as: they depend on airport layout and operational requirements, which can vary throughout the day according to the volume of air traffic. Consequently, routing and scheduling systems have to be reconfigured for specific airports and operational constraints.

5) Due to variability in taxi speed and over-simplistic models of aircraft, there is lack of understanding as to how much benefit can be achieved by automated routing and scheduling in real-world settings. 

TRANSIT will directly address these limitations of current systems, to make better use of existing airport infrastructure and lessen the impact of the growing aviation sector on the environment. Multi-objective optimisation algorithms will be integrated with models of aircraft to balance the reduction of taxi time, cost and emissions. We aim to make the routing and scheduling system easily reconfigurable to any airport. The uncertainty will be directly incorporated in planning, resulting in robust taxiing, verified by pilot-in-the-loop trials.

TRANSIT aims to investigate such a system and its associated benefits in collaboration across a broad range of disciplines and fields (Engineering, Operational Research, and Computer Science) needed to tackle such challenging problem. Cooperation with leading industrial stakeholders, and consultation with established academics, ensure that the work is cutting edge while reflecting needs of the industrial partners.",,"The immediate impact of TRANSIT includes better understanding of causes, behaviour and consequences of uncertainties, and the dynamic nature of ground movement obtained by the analysis of real-world data and simulation. Such knowledge can be used in short term (1-5 years) by airports/airlines in their day-to-day planning. However, the vision of TRANSIT, through the investigation of modelling techniques and optimisation methods, is to develop a basis for future decision support and flight deck automation systems for ground movement. Such a system, developed and implemented in the long term (5-15 years) will be of benefit to industry, environment and society. Pathways to impact are designed to deliver impacts by exchanging knowledge between academics and industry, educating the next generation of researchers, exploring future research directions, delivering public awareness, and in particular fostering economy performance and improving society in the following areas:

Quality of life: Taxi time will be optimised considering uncertainty in times, detailed taxi speeds and other operating conditions producing two benefits: 1) more precise and robust taxi schedules, which will reduce the chances of congestion and therefore subsequent delays; 2) Reducing the time spent on taxiing. Both of these benefits will contribute to the quality of life of passengers as they pass through the airport. 

Environment: Optimising airport ground movement with regard to fuel consumption will decrease the amount of fuel burnt during taxiing, resulting in lower emission of greenhouse gases and associated pollutants in the immediate vicinity of airports. This is an important consideration as, while taxiing is only a small portion of the overall journey, jet engines burn very inefficiently at low speed and therefore make a substantial contribution to the total emissions.

Health: Reduced taxi time and optimised aircraft engine performance, means aircraft engines are running for a shorter period of time with lower fuel consumption, decreasing noise and pollutants, benefiting residents in the immediate vicinity of airports.

Policy: The environmental impact of the proposed research directly helps the UK to fulfil its national and international commitments. Decrease in the emission of greenhouse gases aligns with the Climate Change Act 2008, which aims for the net UK carbon account for 2050 to be 80% lower than 1990. Furthermore, the European White paper 'Roadmap to a Single European Transport Area' calls to reduce greenhouse gas emissions to 20% of 2008 levels, and Flightpath 2050 envisions emission-free taxiing by 2050. 

Cost reduction for passengers, airlines and airports: Decreasing the number and length of delays and dynamic decision making for different airport operational scenarios will have a direct impact on reducing costs, in terms of wasted time or missed connections for passengers, and in terms of costs of using airport infrastructure, aircraft and crew costs for airlines. Preliminary studies on Active Routing framework indicate a reduction of up to 50% in both taxi time/fuel consumption. 

Competitiveness of air transport industry: Minimising transit time, fuel consumption and costs are key factors in an already highly competitive industry. Not only is it important for the aviation industry to remain competitive with alternative modes of transport, but also it should provide a reliable service, as a flight is part of a passenger's overall journey. Therefore, reducing delays at an airport is an important milestone in effectively transporting passengers from door to door, over an ever more interconnected transportation network.

Safety: While conflicts between taxiing aircraft usually do not pose a serious safety hazard, they result in costly damage and interrupted operation. By providing largely conflict-free taxi routes, generated by the proposed optimisation framework based on full-4DTs, this risk can be substantially reduced."
7,20D278B7-9954-41DB-95CC-9B98BCB90DE0,The Parameter Optimisation Problem: Addressing a Key Challenge in Computational Systems Biology,"In recent years, advances in both the physical and life sciences have increasingly come from the collaborations of researchers across disciplines, and the development and use of tools from a range of areas. A prototypical example of this interdisciplinary approach to science is systems biology, the field concerned with quantifying how the interaction of individual system components control biological function and behaviour. Systems biology has become increasingly quantitative, with a shift from diagrammatic representations of interaction networks to sets of mathematical equations that model (i.e. simulate) how the concentrations of molecular species vary with time. A key advantage of such models is that they can be used to predict how the networks they represent will respond to specific perturbations, such as changes in environmental conditions (e.g. temperature) or the addition of pharmacological agents. The ability to easily generate such predictions reduces the need for large numbers of expensive and time-consuming experiments. 

However, the more complex a biological network is, the more complex the corresponding model needs to be, and the greater the range of possible biological behaviours that can be exhibited. This means that extensive computer simulations are needed to adjust the parameters controlling the model so as to accurately reproduce (i.e. fit) the experimental behaviour observed. For biologically realistic models which can involve hundreds of different molecular species, the number of simulations required to adjust the parameters of a given model to achieve the optimal fit to data can be prohibitively large, far exceeding that which is possible on practical timescales. Thus, for the predictive power of mathematical models to be fully realised in the systems biology domain, methods are required that allow this parameter optimisation procedure to be carried out in a computationally efficient manner.

The proposed project will address this need by bringing state-of-the-art methods from computer science to bear on the problem, which have been successfully applied previously to highly parametrised problems like aircraft conflict alert systems, design optimisation of lightweight materials and routing of mesh sensor networks (amongst others). In addition, we propose to develop new methods specifically engineered for the systems biology domain that can provide insight into model behaviour, beyond simply returning a single estimate of the best fit parametrisation (e.g. methods for identifying parameters yielding equally good fits to data, and also parameters which simultaneously fit the model to data generated in diverse experimental conditions). As part of this, we will develop a package of open source software tools that will be embedded within a software infrastructure designed for systems biologists, enabling the methods developed in this work to be readily applied to problems in the field that are currently computationally intractable. 

To test and refine the algorithms developed, they will be applied to the gene network that generates circadian oscillations (the circadian clock) in the key plant species Arabidopsis thaliana, for which high-quality experimental data recorded in a range of genetic and environmental backgrounds is available, together with a suite of mathematical models of varying complexity. As part of this work, biochemically detailed models of the clock will be directly fitted to multiple experimental datasets for the first time, yielding models with greater predictive power. Many processes critical for plant growth and reproduction are regulated by the clock (e.g. photosynthesis and flowering time). In the long term, the ability to optimise plant models of increasing complexity with the class of methods we will develop here may thus help predict how the viability of economically important crop species will be affected by future temperature shifts resulting from climate change.",,"Economic Impact

Systems biology is one of the UK government's core research themes, with significant potential in economically critical areas such as medicine, biofuels and crop breeding. The Royal Academy of Engineering and the Academy of Medical Sciences have described systems biology as a vehicle for &quot;advancing knowledge and building the nation's wealth&quot;, highlighting the construction of predictive mathematical models of complex dynamical networks as a fundamental objective. For the field to be successful in exploiting mathematical modelling approaches to understand and design biological systems, it is critical to develop model-fitting techniques that can: (i) cope with highly parametrised systems (i.e. large numbers of biochemical species); and (ii) combine the information provided by multiple experimental datasets. This project addresses this fundamental need, and although the empirical evaluation of the project outputs focuses on circadian clock networks, the algorithms developed and software released can be broadly applied across the systems biology domain. The workshops delivered as part of the project will explicitly encourage the uptake of the developed tools by industrial and academic researchers from diverse application areas.

The outcomes of the proposed work will therefore impact on the ability of systems biology researchers in academia and industry to produce technologies that deliver a more sustainable and healthy future. As a specific example of this potential impact, recent results show that plant breeders have unwittingly modified several clock genes in major crops, including wheat and barley, over hundreds to thousands of years. This process progressively adapted flowering and harvest times to more northerly latitudes, allowing the cereal crops to spread across Europe from the Near East, and by extension facilitated the population expansion in these areas. The clock genes may well be needed again, as climate change brings new combinations of temperature and photoperiod, affecting phonological timing in both agricultural and natural ecosystems. It is anticipated that the optimisation methods developed during the project will be subsequently employed to construct large-scale temperature-dependent models of the plant clock that will be able to simulate such genetic and environmental changes. These models may thus help predict how future climate shifts will affect the ability of crops to survive, grow and reproduce, and the corresponding optimal genetic modifications for sustaining viable yields.

There are therefore many potential economic beneficiaries of this research. Companies (and universities) working in the medical, energy and agricultural sectors can exploit the outputs of this project to fit and interrogate their models, as part of their product development. Governments and the public will benefit in areas such as food security and energy security (through e.g. improvements in crop yield/resistance and biofuels). Consumers will benefit in a range of areas, from cheaper food and fuel to advances in health.

Social Impact

To engage groups outside academia and industry with the research, public engagement activities will be organised to highlight the societal and economic implications of the project outputs, and, more broadly, the key role played by mathematics and computer science in addressing 21st century challenges. In addition, project results will be incorporated into taught undergraduate and postgraduate courses at Exeter and Edinburgh, highlighting the extent to which computational techniques are crucial to cutting-edge research in systems biology. Students on these modules will also benefit from being exposed to the various job opportunities that exist in the biomedical and biotechnology sectors for graduates from the physical sciences. Outreach activities will also be organised for local schoolchildren in Exeter, showcasing how mathematics is used to model natural phenomena."
8,BB30EFC6-D76B-46C0-B9F5-5A4C700C4364,LOCATE: LOcation adaptive Constrained Activity recognition using Transfer learning,"It is estimated that there are six million surveillance cameras in the UK, with only 17% of them publicly operated. Increasingly, people are installing CCTV cameras in their homes for security or remote monitoring of elderly, infants or pets. Despite this increase, the use of the overwhelming majority of these cameras is limited to evidence gathering or live viewing. These sensors are currently incapable of providing smart monitoring - identifying an infant in danger or a dehydrated elderly. Similarly, CCTV in public places is mostly used for evidence gathering. 

Following years of research, methods capable of automatically recognising activities of interest, such as a person departing a service station without making a payment for refueling the car, or one tampering with a fuel dispenser, are now available, achieving acceptable levels of success and low false alarms. Though automatic after installation, the installation process not only requires putting the hardware in place but also involves an expert studying the footage and designing a model suitable for the monitored location. At each new location, e.g. each new service station, a new model is needed, requiring the effort and time of an expert. This is expensive, difficult to scale and at times implausible such as for home monitoring for example. This requirement to build location-specific models is currently limiting the adoption of automatic recognition of activities, despite the potential benefits.

This project, LOCATE, proposes an algorithmic solution that is capable of using a pre-built model in a different location and adapting it by simply observing the new scene for a few days. The solution is inspired by the human ability to intelligently apply previously-acquired knowledge to solve new challenges. The researchers will work with senior scientists from two leading UK video analytics industrial partners; QinetiQ and Thales. Using these partners' expertise, the project will provide practical and valuable insight that can further boost the strong UK industry of video analytics. The United Kingdom is currently a global player in the video analytics market, and the leading country in the Europe, Middle East and Africa (EMEA) region. 

The method will be applicable to various domains, including for home monitoring and CCTV in public places. To evaluate the proposed approach for home monitoring, LOCATE will work alongside the EPSRC-funded project SPHERE, which aims to develop and deploy a sensor-based platform for residential healthcare in and around Bristol. The findings of LOCATE will be integrated within the SPHERE platform, towards automatic monitoring of activities of daily living in a new home, such as preparing a meal, eating or taking medication. 

The targeted plug-and-play approach will enable a non-expert user to setup a camera and automatically detect whether an elderly in the home had had their meal and medication, for example. A shop owner can similarly detect pickpocketing attempts in their store. The community can thus make better use of the already in place network of visual sensors.",,"A) Economic Impact:
The LOCATE framework attempts to enable plug-and-play automatic activity recognition using visual sensors. This is central to the already strong and growing UK industry in video analytics. Current approaches require hand-crafted location-specific activity representation models, increasing costs and at times limiting the applicability of automatic monitoring. A location-adaptive solution would (i) decrease installation costs and enable wider adoption of automatic activity recognition, (ii) extend solutions to domains where location-specific models are difficult to obtain such as homes and highly-sensitive security environments, and (iii) encourage other established companies and start-ups to enter the video analytics market, as a result of the decrease in cost and increase in applicability. 

Recently, a number of UK and international SMEs have focused on developing mobile applications that are capable of achieving basic image processing such as motion detection towards alarms for intrusions in residential environments. The LOCATE framework could encourage these SMEs to expand their approaches to more advanced computer vision methods that are capable of detecting activities such as an infant in danger or a pet unable to access food or water. This could result in further boosting the customer base of these SMEs. During the follow-up phase of the project, links will be established to these SMEs and start-ups.

LOCATE primarily aims to make the most of the already installed and functioning network of wired and wireless cameras in the UK. Empowering this infrastructure to detect and prevent, rather than to be used for evidence gathering or only scarcely for live viewing, is a better utilisation of available resources.

B) Societal Impact:
Following from the economic impact, wider adoption of automatic monitoring and its extension to novel domains is one step closer to a healthier and safer society. 
When applied to healthcare monitoring, Activities of Daily Living (ADL) have been established as a measure of one's functional status and quality of life. Automatic monitoring of ADLs would allow better assessment of one's health as well as intervention when needed. 
When used for surveillance, automatic detection of activities of interest will enable intervention towards saving belongings as well as lives.
Automatic understanding of a person's activities can also encourage developing approaches to human computer interaction as well as robot computer interaction that are smarter with agile responses.

C) Academic Impact:
The project contributes to two research areas: visual activity recognition and relational-knowledge transfer learning, establishing a novel area of research in relational-knowledge transductive transfer learning for visual activity recognition. A challenge will be released to encourage other researchers to pose solutions to this problem.
The LOCATE project aims to establish the PI as a leading researcher in this novel area, continuing what is already a successful career in video analysis and activity recognition. The project will establish working collaborations between the PI and the current project partners as well as new extended collaborations.
At least one postdoctoral researcher and PhD candidate will become proficient in transfer learning approaches - a skill in high demand in research laboratories world wide and of profound effects for applications in Computer Vision and beyond."
9,368EFF37-EE61-4ECC-87EF-A38BE0560667,DAPM: Detecting and Preventing Mass-Marketing Fraud (MMF),"Fraud can be broadly defined as trickery used to gain a dishonest advantage, usually financial, over another person or organisation. Mass-marketing fraud (MMF) is a type of fraud that exploits mass communication techniques (e.g., email, Instant Messenger, bulk mailing, social networking sites, telemarketing) to con people out of money. It is a scam that targets victims in most countries. The 419 or 'Nigerian' scam is one example. In this scam the fraudster requests upfront fees with the promise that the victim will recover large sums of money in return for little effort. However, not all mass-marketing frauds con the victim with the promise of making large sums of money. In the romance scam, for example, the criminal pretends to develop a romantic relationship with the victims and later requests money to help them, especially in a crisis. In the charity scam, victims believe they are giving money to a genuine charity.

The proposed project will develop novel techniques to detect and prevent MMF. Through its multi-disciplinary approach and close focus on co-designing the solutions with its range of project partners and testing them in-the-wild during live MMF-detection settings, the project will lead not only to new scientific understanding of the anatomy of MMF but also to tools and techniques that can form the basis of practical interventions in tackling such fraud. 

Working with partners outside of academic is crucial to the success of this project. Over the years various types of organisations have worked hard to detect and prevent MMF (often in silos) - with some methods appearing to be somewhat effective. Nonetheless, the numbers of victims do not appear to be dissipating. Awareness campaigns have succeeded in alerting the public to this particular crime; however, it is difficult to know if they have reduced the potential number of victims (especially, given that many victims are aware of the crime prior to becoming victims; see Whitty, 2013, in press). Prosecution for this particular crime is very resource intensive, and its effects on crime reduction are unknown. We have chosen partners (national and international) who have specialities in different fields including: law enforcement, intelligence, third sector, and industry. They have different knowledge to share and also can potentially tackle the problem using different methods (e.g., industry can screen out and detect fraudsters, law enforcement can trace criminals and raise awareness, third sector can implement methods to protect citizens and to make them more resilient).

From an academic perspective, a multi-disciplinary approach increases our chances of detection and prevention of this crime. Understanding the types of people susceptible, the situational conditions that make a person more vulnerable, and the methods and materials (e.g., online profiles, messages, communication methods) used to convince the target that the interaction is authentic and persuade them into giving up their money to a fraudster is crucial in the development of methods to combat this problem. Combining this knowledge with more technical knowledge provides us with a much greater capability to detect and prevent. For example, technical indicators, such as phone numbers, IP addresses, links to stolen identity material, stylistic patterns of persuasive messaging provide a much richer understanding of the crime and provides a greater number of variables to assist in detection. In addition, any tool developed in detection of MMF needs to convince the end-user of the likelihood that they are being scammed (which is especially difficult when the criminal is attempting to persuade the victim to believe otherwise). Given this an HCI approach is crucial when developing usable approaches and messages that persuade the potential victim that they are interacting with a criminal. Finally, we need to consider the ethics of the type of personal data we might utilise to detect and prevent MMF.",,"This project has been designed to have maximum societal and economic impact, both nationally and internationally. Notably, we have 16 partners involved, all of whom would benefit from the programme; however, we envisage much further reach beyond our partners (many of which are willing to disseminate beyond their organisations). The beneficiaries and how they will benefit are listed below:

Law enforcement/intelligence: Law enforcement and intelligence agencies included in this project span various countries including: UK (City of London Police, Action Fraud, National Fraud Intelligence Bureau), USA (Federal Trade Commission), Canada (Canadian Anti-Fraud Centre, Royal Canadian Mounted Police), and Australia (Western Australian Police). The work co-created here will help them with tracing criminals and collecting appropriate forensic evidence to help increase chances of successful prosecution and social legitimacy that arise as a consequence. Law enforcement agencies are also keen to support 'disruption strategies'; that is, find ways to prevent individuals from becoming scammed in the first place and to reduce the scale of those frauds that occur. The work here contributes to this agenda.

Regulatory bodies: Regulatory bodies are public authority or government agencies responsible for exercising autonomous authority over some area of human activity in a regulatory or supervisory capacity - some also work as prosecuting bodies. In the main they codify and enforce rules and regulations and impose supervision for the benefit of citizens. Some sectors set up their own bodies to regulate that sector, setting up codes of practices. We have a number of such bodies as partners on our project (national and international), including: Trading Standards, National Trading Standards Board, Online Dating Association, Federal Trade Commission and Australian Competition &amp; Consumer Commission. The work conducted in this project will help develop guidelines and practices that the organisations can enforce or promote via self-regulation.

Industry: Many different types of industry are affected by MMF, given that scammers inhabit their sites and/or target their clients. This ultimately can lead to a loss of earnings as well as a loss of trust to the industry partner (leading to greater financially losses). We have some industry partners (Mymateyourdate, Barclays) as well as partners who can introduce us to many more (Online Dating Association, Women's Fraud Network). Moreover, we anticipate that our findings will be of use to many industry organisations both nationally and abroad. Industry can use the work conducted in this project to prevent scammers from inhabiting their sites and victimising their clients - ultimately leading to a greater trust in their product. Moreover, one of our industry partners (Scamalytics) can use the science developed here to improve their product (as with any other organisation who develops software to detect scammers).

Third sector: We have a number of third sector organisations partners on the project, including those who work to protect industry (CIFAS, Women's Fraud Network, Fraud help desk). These partners and others (e.g., charities, community groups etc.) we envisage will be able to draw from our findings to help prevent MMF and repeat victimisation to industry as well as individuals.

UK citizens and citizens abroad: Most importantly, the work we propose here should go some way into preventing MMF - a crime with financial and often psychological impacts, which affect not only the person who has been defrauded but also those in their social networks (family, friends). Moreover, the methods to protect these individuals will consider their rights and attitudes to privacy when choosing personal data for use in detection and prevention tools. Citizens will be included in studies throughout the project."
10,6FD37F9D-21B3-4D8E-A5A5-619245B34B53,Exploration of machine learning for pre-emptive scheduling in single-instruction multiple-data mega-kernel designs,"This project is about exploring the question if we can use machine learning to make parallel computing units like graphics processing units (GPUs) more efficient. GPUs have become powerful parallel processors, but using this power is often difficult. Currently, GPUs are mostly used for algorithms that are easy to parallelize. However, I believe that it is possible for more algorithms to benefit from the power of GPUs, if we offer new ways for GPU programming and more intelligent task scheduling strategies. 

In the past I have been researching novel frameworks to allow the parallelisation of computational methods that are usually hard to translate for an execution of parallel hardware like GPUs. For that I used a parallel programming concept called Mega-Kernel. The essence of this concept is that a few computing units out of several thousand are responsible for scheduling computational tasks with different priorities. The Mega-Kernel concept has been demonstrated to provide a powerful extension of conventional kernel based program execution management that can deliver significant performance enhancement from single instruction multiple data (SIMD) approaches. However, to date the queue optimisation capabilities that are at the core of the approach use static rule based decision processes and in particular do not provide optimal hardware utilization or automatic intelligent preemptive scheduling. There are many real world applications for which performance could potentially be transformed by more dynamically adaptive scheduling capabilities and the SIMD architecture itself provides an opportunity to realise this. In this project we will explore statistical machine learning at the parallel hardware level to automatically predict the priorities of tasks in complex real world data analysis tasks such as the reconstruction and motion correction of n-dimensional motion corrupted medical image data. In particular, we will explore the real-time capabilities of machine learning supported preemptively scheduled reconstruction for the direct integration of motion correction into the scan process of fetal magnetic resonance data. Motion correction is the only way to provide comprehensive investigation of all fetal organs at high resolution. However, the currently used algorithmic pipeline is unidirectional, slow and consists of error-prone post-scan-processing steps. The lack of interactivity makes manual corrections or an integration into the scan process impossible at the moment. Automatically prioritised preemptive scheduling of the algorithm's tasks on commodity hardware like GPUs will likely provide a way to introduce real-time capabilities for such extremely complex computing methods. 

It will be feasible during the 14 months of this project to explore if machine learning could be an option to predict the hardware utilisation of the tasks of a complex example algorithm like motion correction with potential corrective user input. If successful, the results of this project are likely to introduce a new paradigm in high-performance computing and will contribute to a paradigm-shift in medical image acquisition of moving objects.",,"This project focuses on developing new software technologies for high-performance computing (HPC) with applications to medical image analysis. 
The most immediate impact is expected through uptake of the developed fundamental methods by the HPC community and by researchers in medical image analysis. The PI will advance this uptake through direct collaboration with experts in HPC (Imperial College, MPI Saarbr&uuml;cken) and medical image acquisition (King's College London) and is currently exploring the potential for collaboration with other research groups (academic visits for example to Vienna University of Technology, Austria, the HPC facility in Nottingham, UK and the University Hospital Leipzig, Germany).

Dynamic scheduling for GPUs provides a great number of possibilities. Exploring the integration of machine learning into these techniques can pave the way for new parallel programming models, following the idea that algorithms can be written more easily by defining work entities and their relations rather than execution steps. With the possibility to control execution states at runtime, we can provide an intelligent task manager for GPUs. We will also explore the application areas that can be tackled with advanced GPU scheduling strategies, including motion correction in n-dimensional magnetic resonance imaging (MRI), which includes research on computational intelligence and real-time scheduling for time critical processes.

At the same time we will work on the remaining features that hamper the straight forward mapping of complex algorithms for GPU execution. We will enable learned task priorities that completely hide task prioritisation from the programmer, allowing an arbitrary small granularity of priority levels. With the combination of all these efforts, we will make GPUs a more accessible device for the development of complex parallel algorithms.

The chosen example application will contribute significantly towards the development of better and more objective imaging biomarkers in medical imaging. Such markers are crucial for early or differential diagnosis. In addition they allow clinicians to reach informed decisions and viewpoints on potential treatments and therapies. Therefore, clinicians in medical imaging and fetal medicine will be also direct beneficiaries of this research. 

Our novel fast and interactive motion correction methods will likely lead to novel work flows for screening, diagnostics and image guided intervention in the prenatal clinical practice. Together with our clinical partners we will work out new work flows that base on the unleashed power of intelligently scheduled parallel algorithms. In the long run our methods are generalizable and can be transferred to other hard to parallelise algorithms that require interactive prioritised input. 

It is also very likely that the medical imaging technology industry and GPU technology industry will be highly interested in the proposed research. I will therefore keep my existing connections to these industries alive and discuss research results with them and potential new contacts permanently during the project. IP arising from this will be protected for commercialization (e.g. via patents) and exploited together with Imperial Innovations (which coordinates the activities of technology transfer, company incubation and investment for Imperial College London).

This project will also have an indirect impact on the next generation of computer scientists. Students at Imperial will indirectly benefit from the project by being close to most recent state of the art in HPC and parallel algorithm development. The PI is a dedicated lecturer and is keen to pass on his knowledge about these techniques to the next generation. Expertise about GPU programming and HPC is an important feature of this project. Parallel algorithm development requires special training and most Computer Science curricula take this into account by now."
11,165CAEC8-8C43-4C4C-AD1B-6F8BF394D56D,DAPM: Detecting and Preventing Mass-Marketing Fraud (MMF),"Fraud can be broadly defined as trickery used to gain a dishonest advantage, usually financial, over another person or organisation. Mass-marketing fraud (MMF) is a type of fraud that exploits mass communication techniques (e.g., email, Instant Messenger, bulk mailing, social networking sites, telemarketing) to con people out of money. It is a scam that targets victims in most countries. The 419 or 'Nigerian' scam is one example. In this scam the fraudster requests upfront fees with the promise that the victim will recover large sums of money in return for little effort. However, not all mass-marketing frauds con the victim with the promise of making large sums of money. In the romance scam, for example, the criminal pretends to develop a romantic relationship with the victims and later requests money to help them, especially in a crisis. In the charity scam, victims believe they are giving money to a genuine charity.

The proposed project will develop novel techniques to detect and prevent MMF. Through its multi-disciplinary approach and close focus on co-designing the solutions with its range of project partners and testing them in-the-wild during live MMF-detection settings, the project will lead not only to new scientific understanding of the anatomy of MMF but also to tools and techniques that can form the basis of practical interventions in tackling such fraud. 

Working with partners outside of academic is crucial to the success of this project. Over the years various types of organisations have worked hard to detect and prevent MMF (often in silos) - with some methods appearing to be somewhat effective. Nonetheless, the numbers of victims do not appear to be dissipating. Awareness campaigns have succeeded in alerting the public to this particular crime; however, it is difficult to know if they have reduced the potential number of victims (especially, given that many victims are aware of the crime prior to becoming victims; see Whitty, 2013, in press). Prosecution for this particular crime is very resource intensive, and its effects on crime reduction are unknown. We have chosen partners (national and international) who have specialities in different fields including: law enforcement, intelligence, third sector, and industry. They have different knowledge to share and also can potentially tackle the problem using different methods (e.g., industry can screen out and detect fraudsters, law enforcement can trace criminals and raise awareness, third sector can implement methods to protect citizens and to make them more resilient).

From an academic perspective, a multi-disciplinary approach increases our chances of detection and prevention of this crime. Understanding the types of people susceptible, the situational conditions that make a person more vulnerable, and the methods and materials (e.g., online profiles, messages, communication methods) used to convince the target that the interaction is authentic and persuade them into giving up their money to a fraudster is crucial in the development of methods to combat this problem. Combining this knowledge with more technical knowledge provides us with a much greater capability to detect and prevent. For example, technical indicators, such as phone numbers, IP addresses, links to stolen identity material, stylistic patterns of persuasive messaging provide a much richer understanding of the crime and provides a greater number of variables to assist in detection. In addition, any tool developed in detection of MMF needs to convince the end-user of the likelihood that they are being scammed (which is especially difficult when the criminal is attempting to persuade the victim to believe otherwise). Given this an HCI approach is crucial when developing usable approaches and messages that persuade the potential victim that they are interacting with a criminal. Finally, we need to consider the ethics of the type of personal data we might utilise to detect and prevent MMF.",,"This project has been designed to have maximum societal and economic impact, both nationally and internationally. Notably, we have 16 partners involved, all of whom would benefit from the programme; however, we envisage much further reach beyond our partners (many of which are willing to disseminate beyond their organisations). The beneficiaries and how they will benefit are listed below:

Law enforcement/intelligence: Law enforcement and intelligence agencies included in this project span various countries including: UK (City of London Police, Action Fraud, National Fraud Intelligence Bureau), USA (Federal Trade Commission), Canada (Canadian Anti-Fraud Centre, Royal Canadian Mounted Police), and Australia (Western Australian Police). The work co-created here will help them with tracing criminals and collecting appropriate forensic evidence to help increase chances of successful prosecution and social legitimacy that arise as a consequence. Law enforcement agencies are also keen to support 'disruption strategies'; that is, find ways to prevent individuals from becoming scammed in the first place and to reduce the scale of those frauds that occur. The work here contributes to this agenda.

Regulatory bodies: Regulatory bodies are public authority or government agencies responsible for exercising autonomous authority over some area of human activity in a regulatory or supervisory capacity - some also work as prosecuting bodies. In the main they codify and enforce rules and regulations and impose supervision for the benefit of citizens. Some sectors set up their own bodies to regulate that sector, setting up codes of practices. We have a number of such bodies as partners on our project (national and international), including: Trading Standards, National Trading Standards Board, Online Dating Association, Federal Trade Commission and Australian Competition &amp; Consumer Commission. The work conducted in this project will help develop guidelines and practices that the organisations can enforce or promote via self-regulation.

Industry: Many different types of industry are affected by MMF, given that scammers inhabit their sites and/or target their clients. This ultimately can lead to a loss of earnings as well as a loss of trust to the industry partner (leading to greater financially losses). We have some industry partners (Mymateyourdate, Barclays) as well as partners who can introduce us to many more (Online Dating Association, Women's Fraud Network). Moreover, we anticipate that our findings will be of use to many industry organisations both nationally and abroad. Industry can use the work conducted in this project to prevent scammers from inhabiting their sites and victimising their clients - ultimately leading to a greater trust in their product. Moreover, one of our industry partners (Scamalytics) can use the science developed here to improve their product (as with any other organisation who develops software to detect scammers).

Third sector: We have a number of third sector organisations partners on the project, including those who work to protect industry (CIFAS, Women's Fraud Network, Fraud help desk). These partners and others (e.g., charities, community groups etc.) we envisage will be able to draw from our findings to help prevent MMF and repeat victimisation to industry as well as individuals.

UK citizens and citizens abroad: Most importantly, the work we propose here should go some way into preventing MMF - a crime with financial and often psychological impacts, which affect not only the person who has been defrauded but also those in their social networks (family, friends). Moreover, the methods to protect these individuals will consider their rights and attitudes to privacy when choosing personal data for use in detection and prevention tools. Citizens will be included in studies throughout the project."
12,F2EFEAF0-BC68-491F-A056-8D061E9A1D16,Personalized Exploration of Imagery Database,"&quot;I want to see jackets which are stylish, but not too fancy. Say, 70% stylish.&quot;

This project aims to develop new techniques which can significantly improve data browsing experience in online shopping, dating, media recommendations, and many other applications. 

Two very common ways to explore large collections of imagery items, for instance, in online shopping, are to browse a hierarchy of items and to search with textual keywords. The returned results are browsed in lists, typically ordered by popularity. However, popularity is defined across all users as one homogeneous peoples, and users cannot sort by their own subjective criteria, e.g., by their own personal `style' for clothes; What is `stylish' to one person will be passe to another. Furthermore, there is no way to place items on a continuous scale, where the criteria amount for each item is known, e.g., how stylish a particular piece of clothing is to a user. 

Our goal is to develop new techniques which enable users to organize and explore imagery data based on their own subjective criteria at a high semantic level. This is a challenging problem: Many criteria are hard to quantify and a user may not even be able to articulate the criteria. 
We face this challenge by observing that even though users may not be able to specify their criteria quantitatively, or even fully describe them, they are still able to communicate their own notions by providing examples, e.g., &quot;this shoe is cooler than that one&quot;. Our goal is to build an algorithm that arranges a large corpus of visual data according to these examples. Once built, the arranged data can be browsed with an interface that exploits the learned criteria to navigate the continuous scale.

The key contributions of the proposed research will include 1) exploring different modes of user interaction and elaborate on reflecting the resulting knowledge to 2) a new algorithm that, by breaking the limitations of existing approaches, effectively and efficiently learns from user-provided examples and thereby makes personalized data exploration realistic.",,"This project aims to develop new techniques which can significantly improve data browsing experience by enabling users to organize data collections based on their own subjective, semantic-level criteria. If successful, these techniques can be directly used in many applications that use/require data exploration. In particular, online shopping will be the biggest beneficiary of this research. The UK is one of the largest and ever growing markets in online shopping: As of November 2013, online shopping increased 10% over the year 2012 and revenues reached a monthly record of &pound;10.1 billion. Specific application scenarios include
1) Finding the perfect chair for a user's room from thousands of possibilities across different styles, by ranking a small subset of chairs by preference.
2) Finding a tasty wine (in terms of personal preference) by trying a small number of different wines: Even novice users could easily establish their shopping portfolio, without having to gain knowledge of domain-specific keywords such as `Tannin' and `Tartaric Acid'.
This research will therefore, contribute to qualitative and quantitative growth of the online shopping market in the UK by attracting users with a significantly improved experience.

Online shopping is only an example of many data browsing applications. Additional application examples are
- Online dating (&pound;170 million market in the UK): Attractiveness is personal-- ranking a small subset of people would help to tailor the personal matches you received by a personal appearance attractiveness scale.
- Media recommendation (e.g., Netflix, iTunes, Kindle): Ranking a few films, albums, or books in an online library to quickly organize the entire collection by your preference.

Our strategy for realizing such a browsing system is to make advances in machine learning, computer vision, and HCI. In particular, one of key technical contributions of this project will be an improved algorithm for semi-supervised learning. Since semi-supervised learning is nowadays extensively used in diverse areas including data mining, social networks analysis, robotics, and genetics, in the long-term, this project will impact on a much broader range of economic and academic activities which may benefit from these techniques.

Furthermore, our techniques will have a societal impact by helping people save time: if successful, users would no longer have to spend hours hunting for just the right item. Users could sort by their particular criteria, and have a good chance of finding it within a small amount of time. Collectively, this saves people a lot of time, and makes the shopping experience or more generally, the data browsing experience, much more pleasant."
13,D39D6983-C800-46D5-9DAA-871D1664C7CC,Closed-Loop Multisensory Brain-Computer Interface for Enhanced Decision Accuracy,"The goals of our interdisciplinary effort are to develop new methodologies for modeling multimodal neural activity underlying multisensory processing and decision making, and to use those methodologies to design closed-loop adaptive algorithms for optimized exploitation of multisensory data for brain-computer communication. We are motivated by the observation that a dismounted soldier or a tank driver routinely makes decisions in time-pressured and stressful conditions based on a multiplicity of multisensory information presented in cluttered and distracting environments. We envision a closed-loop brain-computer interface (BCI) architecture for enhancing decision accuracy. The architecture will collect multimodal neural, physiological, and behavioral data, decode mental states such as attention orientation and situational awareness, and use the decoded states as feedback to adaptively change the multisensory cues provided to the subject, thus closing the loop. To realize such an architecture we will make fundamental advances on four fronts, constituting our research Thrusts: (1) modeling multisensory integration, attention, and decision making, and the associated neural mechanisms; (2) machine-learning algorithms for high-dimensional multimodal data fusion; (3) adaptive tracking of the neural and behavioral models during online operation of the BCI; and (4) adaptive BCI control of multisensory cues for optimized performance. We have assembled a multidisciplinary team with expertise spanning engineering, computer science, and neuroscience. We will take a fully integrated approach to address these challenges by combining rare state-of-the-art experimental capabilities with novel computational modeling. Complementary experiments in rodents, monkeys, and humans will collect multimodal data to study and model multisensory integration, attention, and decision making, and to prototype a BCI for enhanced decision accuracy. Our modeling efforts will span Bayesian inference, stochastic control, adaptive signal processing, and machine learning to develop: novel Bayesian and control-theoretic models of the brain mechanisms; new stochastic models of multimodal data and adaptive inference algorithms for this data; and novel adaptive stochastic controllers of multisensory cues based on the feedback of users' cognitive state.",,
14,ED51F8DC-FD5E-4B32-980F-EB3CB16B4465,Precision Soil Mapping,"In order to produce the amount of food required for an ever expanding population, farmers need to maximumise yield while minimising input costs. Precision farming - a concept based on responding to spatial and temporal differences within fields - has been adopted as a key tool by approximately one quarter of all arable farmers in the UK. It is thought that another 25% utilise some precision techniques but are not currently managing the differences within their fields, but rather just recording data. Studies have shown that variably applying seed and fertiliser, based on different soil properties, consistently give positive results. A two year experiment of variable rate nitrogen fertilisation at Cranfield University proved that detailed maps of soil fertility collected with an on-line soil sensor can result in &pound;50 per ha. net profit to the farmer when integrated with data on crop growth. Similarly, research from the Intelligent Precision Farming (IPF), one of AgSpace's software customers, has shown variably applying seed can produce 34% more yield. However, implementation of precision farming has traditionally been a labour intensive process thus making it cost prohibitive for many growers. The current practice of precision agriculture lacks high resolution, low cost soil maps that would enable farmers to manage in-field variations. With current advances in proximal and remote sensing technology and modelling of multi-data resources, it is possible to produce these maps to enable the implementation of advanced precision farming products on a wider scale, making them affordable to a larger market. To increase yields to the levels being discussed by the Royal Society (60% by 2050) requires a collaborative approach.

This large scale collaborative project aims to integrate satellite data with the UK's most comprehensive soil datasets to produce a 'precision soil map'. The map so produces would present an economically viable alternative to current soil survey methods, while making use of these existing datasets. The level of data and technology available presents a very exciting proposal for arable and vegetable farming to embrace precision farming, enabling them to increase production efficiency, with reduced environmental impacts and at lower input costs. The reduction of environmental impacts will be largely achieved by reducing the amount of agrochemicals applied into the soil, so benefiting ground waters and surface runoff.

AgSpace will provide the raw satellite data for this project, and also ground-truthed data gathered from experimental farms. Cranfield University and the James Hutton Institute will provide access to England and Wales and the Scottish soils datasets, respectively. The project final product is a 'precision soil map' accessible to farmers at a resolution unrivaled in the UK. The new soil management zone map will enhance farmers' understanding of their own soil and will provide an economical route into the implementation of precision farming, which this consortium contends is one of the most important issues in combating yield shortages.",,"The project will have impact on sustainable intensive crop production. The main purpose of this project is to increase farm's competitiveness by providing precision soil maps that support improvements in spatially targeted variable rate applications of farm inputs. The integration of data on soil brightness with soil properties (England, Wales and Scottish datasets) will furnish the end users with two new advanced products that will enable exploration of spatial and temporal variation in soil properties, and will allow for precise management of farm inputs, as well as assisting in better Agricultural Land Classifications (ALCs) and other land use assessments. In summary, the approach will offer farmers an innovative and unrivaled level of detail from which to plan their operations. 

There is a broad range of stakeholders, with varying and often unexpected levels of potential interest in the results of the current project. The main end-users will be farmers, farming groups and environmental agencies. In addition, service providers (i.e. contractors, precision farming service providers and agronomists) and land managers are expected to comprise the major project stakeholders. Homogeneous application of farm inputs are adopted currently by the majority of farmers (&gt;75%) worldwide. Variable rate applications are expected to result in increasing crop yield, while concurrently reducing environmental impacts by reducing the amount of agrochemicals applied into the environment. The overall economic impact will be an increase in farming efficiency that will affect UK farmers and ultimately farmers worldwide. More profitable agriculture would advantage farmers as well as the wider national economy. It is foreseen that the economic saving due to the lowered amount of inputs applied, together with the increased yields, will overcome demonstrably the cost of adopting the envisaged new precision soil maps. 

The research is expected to have a significant positive environmental impact by providing new commercial soil maps, which will enable farmers to reduce the amount of fertilisers they use, by only targeting those areas of the field where there is a need for nutrients. A reduction in fertiliser use would lead to reduced soil and ground water contamination and, therefore, this research will be of significant interest to those concerned with the presence of agrochemicals in the environment. The reduced amount of fertilisers will also reduce greenhouse gas emissions (GHG) and global warming potential (GWP), which will have a positive impact on the environment, with reference to the EU framework directive for &quot;A thematic strategy on the sustainable use of pesticides&quot; (COM(2006)372, COM(2006)778). In this context, water utilities, deemed to be a highly influential stakeholder group are likely to be extremely important, given their high levels of interest and influence. Since this project is expected to lead to reduction of water contamination with agrochemicals, it will reduce investment in extracting these chemicals from drinking water. Hence, commercial water companies and the Environment Agency, who are responsible for protecting the ecological status of bodies of water, will also be interested. Other potential beneficiaries from the public sector include Defra who are interested in minimising environmental impact, whilst improving food security. Fertiliser manufacturers will benefit through improving product stewardship and reduction in environmental impacts.

This project is expected to improve the reliability of management zones delineated for variable rate applications of farm input. In addition, it is expected to reduce the cost of site-specific applications by reducing the cost associated with traditional soil sampling and laboratory analyses. Therefore, manufacturers of precision farming hardware and software will benefit because it will improve their product performance and so make these products more attractive to farmers."
15,1D40B500-36C8-48BC-ACE2-DE70FD7221C9,LAMBDA: Learning Algorithms for Modularity in Broad and Deep Architectures,"Do you know how Facebook recognises faces in images? Do you know how your iPhone understands your speech? The secret behind each of these is a technology called &quot;Deep Learning&quot;, which uses biologically-inspired algorithms called &quot;neural networks&quot;. 

Over the next decade, society will become more reliant on this technology. But... these algorithms require an IMMENSE amount of computing power, and therefore electricity, and for example can take many weeks to learn a given task.

The LAMBDA project explores an approach to deep learning which is not just deep, but also broad - hence &quot;Learning Algorithms for Broad and Deep Architectures&quot;. We aim to (1) make &quot;broad&quot; models that are faster/easier to learn, and as a consequence (2) reduce the energy consumption. Our approach builds upon previous award winning research by the PI, in exactly this area. If successful, we will be able to reproduce the same abilities as current deep neural networks, but with a significantly reduced energy consumption, and whilst learning such architectures a significantly easier task for scientists.",,"Who might benefit from this research and how?
===

Social Impact
=========
Machine Learning is an &quot;enabling&quot; technology --- it underlies and glues together numerous services that the public use every day. ML is used in search engines, in social networking sites, in mobile phones, in banks and more. As these systems become more ubiquitous, they become more a part of our everyday existence. This project is aimed at fundamental enhancements to the underlying technology, making it easier for these services to be deployed in every day life, most especially on small mobile devices such as phones, smart-watches or even tiny embedded devices in clothing. We will develop new types of computational machine learning methods which both learn faster, and consume less electricity during the process. Further details of our strategies for this can be found in our Pathways to Impact document.


Economic Impact
==========
Large companies such as Google/Facebook are known to deploy their compute services in northern Alaska, or anywhere very very cold or near a hydroelectric dam, in order to reduce their overall electricity bill. If we are to have this sort of technology available to smaller companies, not large conglomerates who can afford this, we need to find a way to reduce energy by modifying the algorithms themselves. LAMBDA moves us in this direction. The results of LAMBDA will be new algorithms, frameworks and experiments showing how much more efficient and effective training of learning systems can be when we consider modular (broad) architectures. Our potential impact is enhanced by partnership with the SpiNNaker project (part of the 1 billion Euro Human Brain Project), and with the world's leading low-energy chip designer, ARM Ltd. Further details of our strategies for this can be found in our Pathways to Impact document."
16,E7E52D16-8BD5-454A-9205-EAEC116EE5BF,Understanding infants' curiosity-based exploration,"As parents know, babies are curious learners. The vast majority of infants' time is spent freely exploring (Oudeyer &amp; Smith, in press), at home, at nursery or at playgroup. By sampling their learning environment based on their own curiosity infants quickly acquire two fundamental components of cognition without which they would never engage effectively with the world: categories and words. Understanding early category learning and how it interacts with word learning is therefore critical to teasing apart the complex processes by which infant cognition develops into an adult-like understanding of the world. However, although curiosity-driven exploration accounts for almost all of infants' experience, our understanding of category and word learning comes almost entirely from tightly-controlled, highly structured experiments. Decades of elegantly-designed studies show that these interacting phenomena are exquisitely sensitive to features of the environment (e.g., Younger, 1983; Plunkett, Hu &amp; Cohen, 2008; Quinn, Eimas &amp; Rosenkrantz, 1993). Because these experiments typically take place in artificial laboratory conditions, however, we do not know how babies themselves choose to learn, and as a consequence, we do not know the best way to help them do so.

Here I propose the first studies of infants' curiosity-based exploration and word learning. Using cutting-edge head-mounted eyetrackers I will record typically-developing infants interacting freely with objects, generating the first detailed description of curiosity-based exploration and laying the foundation for future research in atypical exploration. Objects will be custom-designed to vary systematically (e.g., in shape), allowing me to record exploratory sequences to reveal what level of complexity infants prefer to learn from. Half the infants will hear labels for the objects and be tested on their word learning, revealing how categorisation and word learning interact in an infant-centred, rather than adult-designed, environment. 

For a complete understanding of infants' exploration, however, we not only need to know what infants do, but also how they do it: what mechanisms drive curiosity? Computational models can clarify the cognitive processes underlying a behaviour (McClelland et al., 2010). However, as yet we have no model of infants' curiosity-driven exploration and word learning. Based on my existing modelling work (Twomey &amp; Westermann, 2015), I will develop the first testable, mechanistic theory of curiosity-based exploration and word learning in infants.

As the first investigation of human infants' curiosity-driven category and word learning this research has clear academic impact (papers, conferences, future studies of atypical exploration, collaborations). It also has societal impact: my findings will inform policymakers' understanding of development, help designers create evidence-based books and toys that facilitate learning, and equip parents and early years practitioners with the knowledge they need to support babies' cognitive development. There will be numerous opportunities to build networks and develop my knowledge exchange skills (e.g., data sharing, publications, conferences, talks) and public engagement experience (e.g., writing news articles, organising public engagement events). In parallel I will undertake a programme of researcher and Principal Investigator development at my RO. To cement my position as an innovator in curiosity research I will visit the INRIA Research Institute to learn to implement curiosity-based learning in the Poppy humanoid robot, becoming the first UK researcher to use Poppy and providing scope for designing future studies far beyond the current work. Finally, recruiting and supervising a research assistant will develop my leadership and mentorship skills. Overall, this project will not only prepare me for founding my own group, but will also secure my unique position as an international leader in curiosity research.",,"Who will benefit from the proposed project?

Understanding infants' exploration and its relationship to language acquisition is critical to our understanding of development more broadly, yet infants' curiosity has not been studied. By examining how infants drive their own learning the proposed work will be of substantial benefit to parents, policymakers, early years educators, industry and academia.

How will these communities benefit?

- Parents. The findings will alert parents to the fact that infants are self-driven learners and that the environment in which this learning takes place is important. This take-home message, as well as more detailed findings during the course of the research, will be communicated to parents visiting the Babylab to take part in the empirical studies, via post-study debriefing and printed/online materials, providing them with tools - and confidence - they need to successfully and independently support their babies' development and language learning.

- Policymakers and early years educators. &quot;Planned, purposeful play&quot; is at the core of the Department for Education's Statutory Framework For The Early Years (2014, p. 9), despite a lack of understanding of the mechanisms underlying this exploration. The planned work defines a pathway to developing principled, evidence-based guidelines for supporting learning through play.

- Members of the public. I will disseminate this work in a range of online and face-to-face venues. Online articles (e.g., The Conversation) and blog posts will discuss my findings and their implications for our understanding of development. Parents' days, science exploration days (with Science From The Start) drop-in sessions and nursery visits in the local Lancaster community will increase the visibility of developmental research and provide opportunities for the end-users of this research (i.e., parents) to speak directly to researchers in an informal setting. Overall, this research will bring about an increase in public understanding of infant development and language acquisition.

- Industry. A wealth of toys, books and apps marketed as &quot;educational&quot; are targeted at the parents of babies and toddlers, but as yet none are designed based on scientific evidence. Findings from this project will enable the design of books, apps and toys which provide optimal learning opportunities for a given age group.

- Academia. The proposed work will result in a substantial theoretical advance in developmental psychology as well generating the first knowledge base on infant's curiosity-driven exploration and word learning. It will define a new interdisciplinary field and as such will generate strong interest not only in psychology, but also in computational modelling and developmental robotics. Cross-disciplinary collaborations with roboticists at Aberystwyth University, UK, and INRIA, France, will drive forward our understanding of how artificial intelligence agents can benefit from incorporating insights from developmental psychology. These links will be strengthened and new collaborations inspired by an interdisciplinary curiosity symposium to be presented at a prestigious international conference. The research will also provide a baseline measure of curiosity-driven exploration in typically-developing infants, providing a vital comparison sample for researchers in atypical development."
17,E55B4879-8FE7-40AD-8E09-E5A6D326EA59,ONR-15-FOA-0011 MURI Topic #3 - Closed-Loop Multisensory Brain-Computer Interface for Enhanced Decision Accuracy,"The goals of our interdisciplinary effort are to develop new methodologies for modeling multimodal neural activity underlying multisensory processing and decision making, and to use those methodologies to design closed-loop adaptive algorithms for optimized exploitation of multisensory data for brain-computer communication. We are motivated by the observation that a dismounted soldier or a tank driver routinely makes decisions in time-pressured and stressful conditions based on a multiplicity of multisensory information presented in cluttered and distracting environments. We envision a closed-loop brain-computer interface (BCI) architecture for enhancing decision accuracy. The architecture will collect multimodal neural, physiological, and behavioral data, decode mental states such as attention orientation and situational awareness, and use the decoded states as feedback to adaptively change the multisensory cues provided to the subject, thus closing the loop. To realize such an architecture we will make fundamental advances on four fronts, constituting our research Thrusts: (1) modeling multisensory integration, attention, and decision making, and the associated neural mechanisms; (2) machine-learning algorithms for high-dimensional multimodal data fusion; (3) adaptive tracking of the neural and behavioral models during online operation of the BCI; and (4) adaptive BCI control of multisensory cues for optimized performance. We have assembled a multidisciplinary team with expertise spanning engineering, computer science, and neuroscience. We will take a fully integrated approach to address these challenges by combining rare state-of-the-art experimental capabilities with novel computational modeling. Complementary experiments in rodents, monkeys, and humans will collect multimodal data to study and model multisensory integration, attention, and decision making, and to prototype a BCI for enhanced decision accuracy. Our modeling efforts will span Bayesian inference, stochastic control, adaptive signal processing, and machine learning to develop: novel Bayesian and control-theoretic models of the brain mechanisms; new stochastic models of multimodal data and adaptive inference algorithms for this data; and novel adaptive stochastic controllers of multisensory cues based on the feedback of users' cognitive state.",,"DoD/MoD

A soldier makes decisions in time-pressured and stressful conditions, based on a cluttered visual and auditory scene containing moving objects and flashes, exposed to different lighting conditions, and various auditory cues. By discovering the fundamental mechanisms underlying multisensory integration, attention, and decision making, and by developing new machine learning and control-theoretic methods to model, decode, and control the neural mechanisms underlying key mental states, we will develop a closed-loop BCI to enhance decision accuracy under such adverse scenarios. This will significantly advance DoD/MoD efforts to increase situational awareness and national security.

Civilian Impacts

It will also serve civilian and commercial needs, such as pilot, vehicular, and control command interfaces. Given the truly interdisciplinary nature of this work, which spans neuroscience, engineering, and computer science, this proposal will generate new programs of study to train graduate students in these emerging interdisciplinary areas. We will train 14 graduate students and postdocs per year. These highly qualified trainees will have a positive impact on the US/UK economy, science and engineering."
18,06421D7D-F25B-44C0-9C2A-3884A0F865E5,LUCID: Clearer Software by Integrating Natural Language Analysis into Software Engineering,"Developers spend most of their time maintaining code, with little tool support.
To maintain code, one must understand it. Clear code is easier to read and
understand, and therefore less expensive and risky to evolve and maintain; it
is also notoriously difficult to write. We will help developers write clearer
code to speed maintenance, and increase developer productivity. Source code
unites two channels - the programming language and natural language - to
describe algorithms. LUCID will advance the state of the art in software
engineering by developing new analyses that exploit the interconnections
between these channels to find uninformative names, stale comments, and bugs
that manifest as discrepancies between the two channels.",,"LUCID attacks a core software engineering concern; it will build tools that
help developers to maintain software more quickly, with less risk and less
cost. Thus, the work in this proposal has the potential for enormous economic
benefits in the long term. The UK has one of the strongest software sectors in
Europe. For example, in 2008 the UK accounted for 25% of European software
companies. By making software maintenance cheaper, this project will benefit
companies that sell software by lowering the costs of evolving their code and
releasing new versions. These tools will also benefit the many companies that
evolve and maintain custom software systems for their own in house use, by
lowering the cost of these infrastructural projects."
19,6821087B-3835-43D7-8296-33ED1183A6C9,Closed-Loop Multisensory Brain-Computer Interface for Enhanced Decision Accuracy,"The goals of our interdisciplinary effort are to develop new methodologies for modeling multimodal neural activity underlying multisensory processing and decision making, and to use those methodologies to design closed-loop adaptive algorithms for optimized exploitation of multisensory data for brain-computer communication. We are motivated by the observation that a dismounted soldier or a tank driver routinely makes decisions in time-pressured and stressful conditions based on a multiplicity of multisensory information presented in cluttered and distracting environments. We envision a closed-loop brain-computer interface (BCI) architecture for enhancing decision accuracy. The architecture will collect multimodal neural, physiological, and behavioral data, decode mental states such as attention orientation and situational awareness, and use the decoded states as feedback to adaptively change the multisensory cues provided to the subject, thus closing the loop. To realize such an architecture we will make fundamental advances on four fronts, constituting our research Thrusts: (1) modeling multisensory integration, attention, and decision making, and the associated neural mechanisms; (2) machine-learning algorithms for high-dimensional multimodal data fusion; (3) adaptive tracking of the neural and behavioral models during online operation of the BCI; and (4) adaptive BCI control of multisensory cues for optimized performance. We have assembled a multidisciplinary team with expertise spanning engineering, computer science, and neuroscience. We will take a fully integrated approach to address these challenges by combining rare state-of-the-art experimental capabilities with novel computational modeling. Complementary experiments in rodents, monkeys, and humans will collect multimodal data to study and model multisensory integration, attention, and decision making, and to prototype a BCI for enhanced decision accuracy. Our modeling efforts will span Bayesian inference, stochastic control, adaptive signal processing, and machine learning to develop: novel Bayesian and control-theoretic models of the brain mechanisms; new stochastic models of multimodal data and adaptive inference algorithms for this data; and novel adaptive stochastic controllers of multisensory cues based on the feedback of users' cognitive state.",,
20,15A84428-6FD8-4260-B24D-E82FB3938C57,Future Everyday Interaction with the Autonomous Internet of Things,"This project seeks to investigate the design of interaction mechanisms and user interfaces for a future Autonomous Internet of Things (A-IoT): a system of interconnected devices that reaches beyond most current incarnations of the IoT to include aspects of autonomy or automation as a key feature. Nascent instantiations of the A-IoT range from smart thermostats that learn to autonomously control central heating systems based on the presence of users and their routine, to washing machines that order detergent for delivery when it runs out. In other words, this A-IoT can proactively respond to sensed environmental changes, effectively doing work on behalf of users, with the promise of a more efficient use of resources (e.g. to use less energy for heating) or increased convenience (e.g. to always have detergent available). 
The wealth (or &quot;deluge&quot;) of data produced by the IoT is likely to keep growing beyond human capacity to turn it into meaningful information that can be acted on. Therefore, it will require future interactive systems to increasingly support the delegation of granular decision making over large and complex data to autonomous computational agents, allowing users to make informed choices about their general needs and comfort. In an Autonomous IoT; data and decisions will be, in part, 'actively' managed by the devices and their software, drawing upon machine learning techniques and optimization algorithms.
However, recent studies examining the real-world acceptance of a commercial smart thermostat highlighted how errors, limited legibility of the system operation, and excessive user expectations caused frustration and led to some users abandoning the technology. Our own prior work revealed people distrust a potential smart energy infrastructure due to lack of accountability of the ownership, intent, and permitted activities of the autonomous technology. These results suggest that the design of A-IoT systems needs to address several challenges to be made accountable; including, on the system side, designing autonomous decision-making to take into account the uncertain nature of contingent human behaviour; and on the user side, the need to make these systems legible and usable in everyday life. Indeed there is an inherent tension between making a system's operation legible and not overwhelming users with the technical complexity of artificial intelligence algorithms. To date, the methodologies to design such systems are rather sparse and not specific to A-IoT systems (spanning HCI, AI, and Ubicomp) and hence a more focused approach is required to determine the core design principles and methods for the implementation of A-IoT systems.
Our goal is thus to establish the scientific underpinnings of user interactions with A-IoT systems, in a domestic everyday context, with the aim of elucidate the following research questions: to what extent may users be willing to delegate agency to A-IoT systems in everyday contexts? How should interactions with A-IoT systems be engineered to support rather than hinder users' daily activities? What capabilities are essential for intelligent agents to manage such A-IoT systems? How can we design such systems so that they allow users to delegate control, yet easily regain it? Unless such questions are fully addressed, A-IOT systems are likely to frustrate users, resulting in significant waste of time and resources.
Hence, we will address these challenges through a combination of techniques, including the study of existing practices, the iterative development of novel A-IoT prototypes and their evaluation in-the-wild. Such a multidisciplinary approach is made possible by a team that brings together internationally-leading researchers in human-computer interaction, artificial intelligence and design ethnography.",,"The Internet of Things has been identified by the UK government as a key area for investment, recognizing its high potential for impact on the national economy and, more in general, upon society (www.gov.uk/government/publications/internet-of-things-blackett-review). Equally, autonomous systems have been recognized by the EPSRC as a priority area and &quot;part of [their] response to national challenges&quot;. This project addresses both these recognized innovation opportunities through its aim to combine IoT and autonomous intelligent systems into the A-IoT and release its potential for applications in domestic everyday settings.

The proposed research cuts across three of the six priority research areas set out in the roadmap for interdisciplinary research on the Internet of Things; namely People, Trust, and Data [IoTSIG 2013].

Our focus on domestic practices relates to a broad range of activities such as supply (production and distribution), storage, food preparation, eating and waste reduction, a central societal concern in the UK. These cut across a range of key societal sectors including agriculture, manufacturing, transportation, gastronomy and energy. The results of the proposed research will be relevant to stakeholders from these sectors, informing how their activities might be supported by the A-IoT.

End-users engagement
The approach through which the project aims are achieved is an inclusive, user-centred design process, involving end-users at all stages of the design process. Participatory design and envisionment workshops, and field deployments of prototypes will take place throughout the project. These activities will involve our project partners, specific food consumer groups and commercial food venues (reached through our partners), as well as members of the general public.

Industry engagement
Industry engagement is undertaken with and through our project partners: Wireless Things PLC and Senseye, as IoT technology providers, Sutton Community Farm and Homemade Cafe Ltd. as potential IoT technology beneficiaries. The partners will shape the design of our prototypes and trials, in order to make our results relevant to their needs. We will also showcase achievements and outputs, including new IoT application and services, at a suitable industry events (e.g. Innovate UK conferences), to engage other key industry players in the electronic technology, UX design and food supply sectors. We will allocate the role of managing and furthering interactions with industrial partners to Ramchurn, given his experience running a number of successful Knowledge Transfer Secondment activities with Hampshire County Council and BAE systems as well as running the Industrial Placements programme for the Electronics and Computer Science department at Southampton. 

Communication &amp; Press activity
An advisory panel of stakeholders will be convened, consisting of industry, the third sector, governing bodies, and external academics especially from fields not included in the project team. We will hold annual all-hand meetings with the panel to present and reflect research progress, and to seek strategic research guidance from stakeholders, specifically in relation to impact performance, and further opportunities to disseminate findings. 

Project output public release
In addition to the normal academic dissemination routes the research findings will be made available through summaries and briefing papers, including a final report detailing the project aims and key headline descriptive results. These documents will be made available on a dedicated project website. To maximise impact we will host a dissemination event at the end of the programme. 
This approach will follow a similar format to the 'outcomes' section of other EPSRC projects from the applicants (e.g. ORCHID, HORIZON), which resulted in national and international media coverage (e.g. BBC, Guardian, Independent, New Scientist, C4)."
21,0BFCDA38-B539-4E7F-A2CE-0C29D77B07EE,Brains on Board: Neuromorphic Control of Flying Robots,"What if we could design an autonomous flying robot with the navigational and learning abilities of a honeybee? Such a computationally and energy-efficient autonomous robot would represent a step-change in robotics technology, and is precisely what the 'Brains on Board' project aims to achieve. Autonomous control of mobile robots requires robustness to environmental and sensory uncertainty, and the flexibility to deal with novel environments and scenarios. Animals solve these problems through having flexible brains capable of unsupervised pattern detection and learning. Even 'small'-brained animals like bees exhibit sophisticated learning and navigation abilities using very efficient brains of only up to 1 million neurons, 100,000 times fewer than in a human brain. Crucially, these mini-brains nevertheless support high levels of multi-tasking and they are adaptable, within the lifetime of an individual, to completely novel scenarios; this is in marked contrast to typical control engineering solutions. This project will fuse computational and experimental neuroscience to develop a ground-breaking new class of highly efficient 'brain on board' robot controllers, able to exhibit adaptive behaviour while running on powerful yet lightweight General-Purpose Graphics Processing Unit hardware, now emerging for the mobile devices market. This will be demonstrated via autonomous and adaptive control of a flying robot, using an on-board computational simulation of the bee's neural circuits; an unprecedented achievement representing a step-change in robotics technology.",,"Primary beneficiaries of this research will be the UK science and technology research base, and the UK robotics and mobile computing industries, where rapid technological impact will be facilitated by close collaboration with project partners NVidia and Movidius. Both companies are leaders in the development of low energy and low weight, high-performance computing devices and are both currently partnering with drone manufacturers. 

We also anticipate impacts in areas where energy and weight constraints exist, in particular in both space and agricultural robotics where we have existing collaborations. Other relevant areas are mobile technology, wearables, and pervasive computation.

We will further pursue industrial impact through foundation of a spin-out company specialising in software solutions for AI controllers, and application for follow-on funding (e.g. with Innovate UK).

Academic impact will be substantial, continuing the applicants' track record of publishing in the highest profile general science and engineering journals; foundational results in the areas of cognitive, behavioural and computational neuroscience are also anticipated, as the team takes an integrated experimental, computational and robotic approach to understanding the neural bases of complex behaviours including flight control, sensory integration, and decision-making.

There will be direct impacts through the training of the researchers on the grant. They will acquire a unique portfolio of expertise including key technologies of aerial robotics, bio-mimetic AI, and GPU accelerated computing. Their expertise will be much sought-after in the future. We will broaden this impact by offering training sessions in these key technologies to students and researchers in the involved institutions in conjunction with our regular project meetings.

The applicants have an excellent track record in public outreach both through media (e.g. lead PI Marshall interviewed on Discovery Science, BBC tv, BBC Radio 4) and outreach to schools locally, and nationally. Other activities include science festivals and public events. The team already has a long track record of such engagement with the public on all levels from school children to adults (e.g. http://greenbrain.group.shef.ac.uk/outreach/, http:// users.sussex.ac.uk/~tn41/outreach.php). Outreach activities will increase the impact of the project on the general public, which is relevant for public understanding of science and the technological potential offered by biomimetic control and robotics, as well as increasing interest in further study in STEM subjects among early secondary-age school children. Engaging strongly with the public will also be instrumental in avoiding misconceptions about the risks related to autonomous robots, something which otherwise could become a serious problem for this rapidly developing technology."
22,86935946-25FF-4111-BDC9-DAA56E219311,AI Planning with Continuous Non-Linear Change,"Intelligent autonomous systems have a significant role to play in meeting the increasing needs of a growing modern society. These systems can take many forms. Autonomous robots can assist humans in performing tasks, work in manufacturing, and play an important role in cleaning up or exploring environments too hostile for humans. Autonomous large scale software systems can control our over-subscribed transport and power networks, allowing us to operate them as efficiently as possible to serve the growing population.

In order for an autonomous system to act intelligently and achieve its goals, it needs to be able to plan, that is decide what actions to take and when to take them: this is the problem of Artificial Intelligence (AI) planning. The major research goal for AI planning is to create systems that are domain-independent, that is they are not human-programmed to solve one specific problem; but rather are general purpose and capable of planning in scenarios encountered across a wide range of applications.

Decades of research has produced increasingly capable AI planners, and there have been successes in using these in a diverse range of applications, including the planning of global ocean liner movements and security penetration testing. There are, however, still major challenges to be met in creating systems that are scalable and expressive enough to form the core of the AI systems needed to meet future societal challenges. One major challenge, and the focus of this project, is equipping planners with the expressive capability to reason about a complex and dynamic world. Many interesting target problems, such as nuclear clean up or traffic control, require not only conventional reasoning based on facts that are true and false; but also reasoning about non-linear continuous dynamics: radiation exposure or traffic flow.

This project addresses the challenge of creating a planner capable of reasoning with non-linear continuous dynamics alongside all the existing state-of-the-art capabilities of the most expressive modern planners (time, deadlines, soft constraints and cost optimisation) without significantly compromising scalability. Such a system will be an invaluable asset in controlling the autonomous systems of the future.

The research challenges that need to be addressed to achieve this are significant, as present techniques for reasoning with these problems make compromises in one way or another. Some techniques are limited in scalability due to a requirement to 'discretise' time, splitting it into small chunks, and reasoning about whether to do something every fraction of a second. Others are incompatible with other expressive features; or rely on technologies that support only linear change. In this project we build on OPTIC, a planner that supports only linear change, due to its support of other expressive features and good potential for scalability. 

We will address the challenges of reasoning with non-linear change in a linear framework by reasoning with piecewise-linear approximations of the continuous change. The main challenges here are determining how to integrate reasoning about these with existing techniques for expressive reasoning; and generating sufficiently accurate approximations automatically. The finer we make the approximation, the more points we have to reason about and the harder it is to solve efficiently; yet approximations that are not fine enough will not permit us to solve the problem. 

Throughout the project, alongside development of the planner, we will focus on creating models of target problems, guided by our contacts with organisations in the relevant fields. These will allow us to ensure our work remains focussed on addressing the challenges that will most benefit application as well as providing us with benchmarks against which we can evaluate the project. Our target applications include nuclear clean up; medical dose scheduling and traffic flow management.",,"The domain-independent nature of AI planners leads to great scope for application in a wide range of areas. The UK is already a world leading centre for AI Planning research, this project will consolidate this and develop even more capable technology, to move towards addressing major future economic and societal challenges. 

Within this proposal we have identified three key areas of national interest to which the project can contribute, and have a dedicated domain acquisition work package to explore the requirements of these problems through visits to relevant organisations:

-Traffic Control Planning: the UK road networks are increasingly overcrowded leading to high levels of congestion and pollution. AI planning has real potential in this domain, as evidenced by recent successes; however, in order to do better and to help to address this challenge fully more accurate reasoning about the non-linear continuous dynamics of traffic flow is needed.

-Nuclear Decommissioning: management and clean-up of radioactive waste is a huge challenge at sites such as Sellafield, due to the UK's nuclear legacy. This is carried out by humans and in order to enforce strict radiation exposure limits very specific plans are made for their activities at at the radioactive site. Such plans are currently generated by hand, but this could be automated by a system capable of reasoning with non-linear exposure levels. This is also an important step towards effective robotic clean up in areas with higher radiation levels, such as Fukushima.

-Control of Power Networks: the challenges faced in the National Grid are planning effective use of generation facilities to deal with the fluctuating output of renewable energy production, which is often dependent on the weather; and maintaining a stable and safe state in the network in response to changes in demand or generation. There has been much research into applying AI technologies in this area, including some of the PI's own work, but there are still open challenges. The inherently non-linear dynamics of power systems means that they are a good target application for this project.

Planning also has much to contribute to the economy. Businesses often have to manage large fleets of vehicles (e.g. logistics firms) or plan efficient use of limited resources (e.g. airport management or train tracks). The PI has already been involved in a successful project using AI planning to determine efficient repositioning of ocean liners, in collaboration with ITU Copenhagen and MAERSK. Efficient operations planning is a challenge for humans, and inefficiency costs thousands of pounds. Allowing reasoning about non-linear dynamics, and even non-linear cost functions, will extend the scope for application of AI planners in large-scale operations planning. 

For small businesses in need of effective problem solving and planning solutions the cost of hiring expert software developers is prohibitive; an off-the-shelf domain-independent planner can help here. During this project the founder of a new tech start up 'Medic' will be working as an MSc student in the Planning group at King's. The start up aims to provide effective guidance on dose timings for combined drug therapies, based on plasma concentration levels. The MSc project will focus on making AI planning models of this problem. These will provide useful benchmarks, a route to impact in healthcare, and valuable insights into the use of planning in a small business.

Finally, physical robotic systems will play a major role in future society and, in order to act intelligently and autonomously, they need to be able to plan their activities. These systems are particularly valuable in situations that are too remote or dangerous for humans, for example, in space, deep ocean exploration and search and rescue scenarios. They also have an important role to play in supporting human activity, for example, the potential for robotic assistants to support the elderly."
23,63331C58-2A94-46EB-A665-89B6D9EA0609,FORGING: Fortuitous Geometries and Compressive Learning,"Statistical machine learning has been instrumental in providing algorithms that enable us to draw valid conclusions from empirical data. Its successes rely crucially on a rigorous mathematical theory. 
Unfortunately, as the modern data sets are increasingly high dimensional, new challenges gathered under the term `curse of dimensionality' render many of the existing data analysis methods inadequate, questionable, or inefficient, and much of the existing theory becomes uninformative. Mitigating the curse of dimensionality receives a lot of research attention currently. However, many fundamental questions remain unresolved. The aim of this project is to provide answers to two of these:

Q1: What kinds of data distributions make a given high dimensional learning problem easier or harder to be solved?

Q2: What kinds of learning problems can be approximately solved compressively, on a low dimensional subspace?

We propose a stance complementary to efforts that look for ways to counter the various observed detrimental effects of the dimensionality curse: We shall exploit some very generic properties of high dimensional probability spaces to develop a unified theory, and its algorithmic implications, to unearth some precise conditions that enable us to solve high dimensional problems in low dimensions. These conditions will depend on the geometry of the problem. We will use a new notion of problem-dependent compressive distortion that we have started developing, and which will build on a so far unexploited connection between random projections and empirical process theory. 

The expected outcome will be applicable across a range of different machine learning and data mining problems, and we validate this in case studies.",,"This project is expected to provide answers to two fundamental open questions in high dimensional machine learning and data mining, along with a generic methodology for resolving these in various learning settings. As such, it will provide a new way of thinking about high dimensional data problems that shift the focus away from case by case solutions to the observed detrimental effects of the curse of dimensionality, and instead will be based on exploiting some very general properties of high dimensional data spaces. Without being able to make this qualitative shift, the unprecedented increase in the dimensionality of data sets in many areas of science and engineering, we risk to lose the performance guarantees that theory can provide for practice. 

Researchers and practitioners in data mining and machine learning will benefit from this research, and the generality of our approach. Much research effort is currently spent on mitigating the detrimental effects of the curse of dimensionality in the recent years. The time has come when it is feasible to build up the theoretical foundations and eliminate inefficient case-by-case trial-and-error strategies. Fundamental research is essential to achieve this, and this is what we propose to do.

We expect to make societal impact: The project will assign a research student to work at developing the PI's ideas into algorithms for the medical domain in a collaboration envisaged with Prof. Tom Marshall and colleagues in the School of Health and Population Sciences (College of Medical and Dental Sciences) at the University of Birmingham. By applying our results to the UK national primary care database we expect to contribute to improving early diagnosis of patients, which is an important problem in public health medicine. 

High dimensional data problems are ubiquitous on many areas of science, engineering and businesses, and machine learning is an enabling technology in many of these. Therefore this project will have an impact indirectly on all of these, and here are some concrete examples: (i) In genomics and proteomics, where high dimensional measurements are made routinely and inexpensively while the number of subjects of a specific condition is limited; (ii) In biomedical imaging, and computational neuroscience, where the resolution of measuring devices is ever increasing, and different modalities of measurements are possible and available; (iii) In web, multimedia and market basket analysis, for example for customer preference prediction from purchase logs, where more and more sophisticated customer profiles are feasible, with many descriptors, giving rise to higher and higher data dimensionality.

Our approach is necessarily cross-disciplinary as it will integrate together results and techniques from several areas of mathematics -- high dimensional probability theory and concentration of measure, empirical process theory, functional analysis, theoretical computer science, computational geometry, information theory, and random matrix theory -- to produce a new analytic strategy able to resolve fundamental issues in machine learning and data mining. Our results will naturally feed back upstream to researchers whose mathematical results we will use, and this may stimulate new research at the boundaries between disciplines."
24,CC7272D1-2FED-41A9-B061-489759E0F6E5,A Neuromorphic Control System for Agile Biped Walking,"Rush-hour in a London mainline railway station: a passenger effortlessly walks swiftly through the swirling crowd, looking at a large screen 20 meters away, talking on a mobile phone in his left hand, holding a cup of coffee in his right hand, avoiding collision with anyone, and making his way to platform 14. 

This seems effortless. But from the robotics view, this is almost miraculous because all these tasks are controlled by a single brain, efficiently in parallel. To behave like this human, today's robot would have to use a large million-dollar super-computer or several connected computers using Kilowatts of energy, and the performance would still not comparable to that of a human brain. To build a robot brain by reverse engineering the human or animal brain has been the ultimate goal of many large inter-disciplinary projects in recent years. Today, the most promising technology to physically and structurally emulate the brain is neuromorphic engineering, which uses electronic circuits to mimic neuro-biological architectures. Compared with standard computer-based controllers, neuromorphic controllers are naturally parallel, more compact and more energy efficient. It is widely thought that a neuromorphic brain will be the centre of the next generation of intelligent autonomous robots. 

Many studies in neuromorphic engineering have developed neuromorphic systems to realize specific functional modules of the brain, e.g., hearing, vision, olfaction, cognition, and action learning. The proposed project is targeting another fundamental control function of the human brain -- bipedal (two-legged) walking. Just like humans and animals, a robot must be able to move agilely in order to execute its tasks in the natural environment. But, compared with traditional counterparts, the performance of the neuromorphically controlled legged robots (especially biped robots) is very poor in terms of versatile and agile locomotion. This is mainly because their neuromorphic circuits emulated only the basic function module of the spinal neural network, which could only realize propulsion control. In animals, propulsion control and body posture control are fully integrated, which is fundamental for their agile locomotion in a complex natural environment. Particularly, in humans, to meet the functional requirements of agile bipedal walking, the spinal neural network is heavily modulated by the supraspinal levels. However, it is still not fully understood in biology how the neuronal modules at the spinal level and supraspinal level interact with and modulate each other in the control of human bipedal locomotion. 

Building on the team's track record in biped robotics, neuromorphic circuit design, neuromorphic simulation, and computational neuroscience, the proposed project aims to fill this gap via developing a multi-module and multi-level (i.e., spinal level and supraspinal level) neuromorphic system. In the neuromorphic system in this project, we will implement the functions of three neuronal modules that have been known to play important roles in human locomotion control. By coupling such a neuromorphic system with a purposely designed biped robot using a new method (model-driven concurrent integration), we will be able to explore the unknown interaction/modulation mechanisms between these modules that could lead to agile biped walking.

At the heart of our proposal is the ambition to make a notable step forward in the area of neuromorphic robotics. This project will, for the first time, demonstrate an agile 3D biped robot that has human-like walking patterns and a neuromorphic control mechanism.",,"Our project could have academic and societal impacts in the following three key areas. 

1. Shedding new insights on the neuronal control mechanism of human locomotion

The rapidly advancing robotics technologies are providing new tools for testing biological hypotheses or uncovering complex mechanisms in biological systems, which would be difficult or impossible using conventional methods. For example, biologists have long marvelled over the complex social behaviours seen among insects such as bees and ants, where different groups of individuals specialize in different tasks and cooperate perfectly. Now, using swarm robotics and evolutionary robotics, a team of roboticists has given a new explanation of how the specialization and cooperation mechanism may have evolved from simple behaviours. Likewise, there are puzzling questions regarding the control mechanism of humans' bipedal locomotion. It is thought that human walking may be controlled by three neural modules: Central Pattern Generators (CPGs), reflexes, and an internal model. But we don't know how these modules connect/interact with each other to generate versatile and agile biped walking. This is a critical issue in neuroscience and evolutionary biology, because bipedalism is the most fundamental human characteristic that separated the first hominids from the rest of the four-legged apes. The lack of knowledge in detailed circuits of the brain should not prevent us from probing this issue using robotics. One central task of this project is to explore this issue using a multi-module-multi-level neuromorphic system coupled with a sophisticated biped robot. At the end of the project, if the neuromorphic system involving those three neural modules successfully generates agile biped walking in our robot, the interaction/modulation mechanism developed in this project will suggest new testable hypothesis on the interaction mechanism of these modules in human locomotion control. These new insights will be particularly useful for developing smart next-generation neuro-prosthesis. 

2. Developing sophisticated neuromorphic robots using a systematic integration approach.

The neural circuits and the body mechanics of animal locomotor system have evolved together for millions of years. This has led to a fully integrated hierarchical neuro-mechanical system, where the neural circuits are closely coupled with the muscular system at various levels and time scales. Similarly, a future neuromorphic robot may have to have an integrated multi-module-multi-level architecture (like the system in this project), if it is to be capable of sophisticated dynamic motion, such as the agile biped walking in this project. Therefore, system integration will be a central issue in the design of such a robot, which can't be solved by the ad hoc or heuristic integration approaches widely used in the design of today's neuromorphic robots and biomimetic robots. The &quot;model-driven concurrent integration&quot; approach proposed in this project is aiming to systematically solve this critical issue in the emerging area of neuromorphic robotics. This will inspire researchers to develop new systematic integration approaches for efficiently developing sophisticated neuromorphic robots.

3. Inspiring future roboticists and furthering public understanding of advanced robotics

As our project is to explore exciting potentials at the interface of two publicly appealing research areas (biped robots and neuromorphic brain), it has great potential for engaging with young scientists, from school age to early career graduates. 
Our previous work on a record-breaking fast biped robot has attracted media interests (e.g., BBC news). We expect the proposed project to similarly engage public interest given the public's appetite for humanoid/biped robots. Such media coverage would provide an opportunity to reveal to a wide public audience how biologically inspired research is undertaken in robotics."
0,8CAF3171-3335-4A2F-8C08-7E749CFCBE19,Learn to Play: Computational Assessment of Musical Playability for Users' Practice,"This Follow-On Funding for Impact and Engagement proposal is based on research from the AHRC Digital Transformations project, 'Transforming Musicology' (AH/L006820/1), and the Electronic Corpus of Lute Music project, most recently as 'Lute Music in the Open (ECOLM III)', AH/H037829/1. It explores the concept of 'playability' of music. By developing a system to assess the difficulty of a displayed piece, and then using this system to create on demand a set of practice exercises based on passages within the music judged to be tricky by the system, it will help students learning to play an instrument (flute, guitar, or renaissance lute). 

The guitar is the most widespread instrument in the world today, and the internet provides a literally bewildering number of 'tabs' (scores notated in the format known as tablature) requiring no formal knowledge of music notation. Tablature provides instructions about the placement of fingers to form chords or melodies and the sequence in which they should be played. It is a system that has stood the test of time, and has been used for hundreds of years, at least since the 15th century, and is particularly useful for instrumental teaching, especially in the early stages.

There is a vast amount of music available online and the system we create will help musicians find music to suit their skill level. The system will analyse the playability of tablature versions of pieces of music for guitar (classical and other styles) and for renaissance lute (we already have a corpus of c10,000 pieces in ECOLM). Using measures based on hand-stretches and position-shifts indicated in the tablature we'll compute indexes of playability of individual chords and transitions between them.

The flute is another very popular instrument among self-learners and young people, especially in schools; based on figures from the Hackney Music Service, we estimate that over 3,000 non-beginner flute students take lessons in London schools alone. We'll build on earlier work carried out by co-I Fiebrink on the modelling of difficulty in flute music, a very useful starting point, since the simpler texture of the music allows us to focus on its melodic aspects rather than on chords (as on guitar or lute). 

We'll then use standard machine-learning techniques to build models of playability to identify difficult passages in unknown flute, guitar and lute pieces. They will also be used to grade pieces (based on the difficulty of the most technically-challenging passages) and the results compared with the grades listed by music publishers in their catalogues. The proof-of-concept demonstrator forming the main output of the project will then use simple algorithms to generate entirely new exercises derived from these passages for practising by a student.

All the above will be evaluated by our user community - players at various levels and flute, guitar and lute teachers.

The music will be presented within a high-quality graphical user interface provided by our music-industry partner, Tido Music. Currently used for a number of educational packages, mostly aimed at amateur pianists, it will be adapted to communicate remotely with the playability estimation and exercise generation back-end developed and maintained by Goldsmiths. This way our models can be tested from the outset with a professional user-interface, and use musical scores from the Tido music library (access restrictions entirely under Tido's control), or from elsewhere, without compromising rights ownership.

The lessons learned will be applied directly in two ways. We shall hold a workshop for professional and amateur musicians, including those involved as beta-testers, to discuss their assessment of the system with its designers and developers. This feedback will then be used as material for a full proposal to Innovate UK for funds to carry out further research and development to take this work beyond proof of concept to a commercially viable product.",,"The guitar is the most popular instrument in the world today, and hundreds of thousands of players at every level learn and exchange music online. Millions of pieces can be found notated in tablature, an ideal system for learners (e.g., a single site, www.ultimate-guitar.com, has tens of thousands of regular users and offers 800,000 songs in tab), but finding those within a player's capability is a difficult and tedious task. This project aims to make that easier and to provide automatically-generated exercises for tricky passages in chosen pieces.

In musicological research carried out within the Transforming Musicology project and elsewhere, we have found that an important feature of historical repertories not hitherto studied extensively is the 'playability' of instrumental pieces (especially in arrangements), and we developed proof-of-concept measures to locate hard passages within pieces for renaissance lute notated in tablature. These can be applied to the vast modern repertory of music for guitar in tablature found on the internet. Co-I Fiebrink has also developed a method for grading pieces for flute (in normal staff-notation) according to their technical difficulty. 

This proposal builds on that work in a system to help self-learning musicians, and their teachers, find music to suit their level of attainment from online resources, and - a feature unprecedented as far as we are aware - which generates a set of original practice exercises based on hard passages identified by the system.

Generating exercises from difficult passages of music will have a definite medium-term impact on the teaching and learning of the flute and the guitar (our principal focuses). Practising short exercises is far more enjoyable than struggling with complete pieces. Automated generation of original exercises based on a piece being studied (as are often written out by a teacher) provides a new and enjoyable experience for beginners and intermediate learners, and will help advanced students (up to conservatory level) make best use of their practice time. 

Our music industry partner, Tido Music, have expressed a need to respond to a demand from their users for automated playability measurements of this sort. With them we shall be able to trial the method at an early stage of development, providing user feedback into the design process. We'll develop a demonstrator to show the potential for this approach which will be submitted to Tido's team of beta-testers. This is a basis for rapid development of a working system that will be offered as a service in conjunction with their educational materials. The 'grading' methods we use will enhance the experience of searching for music suitable for their level of ability for online customers of companies distributing such music. Once the proof-of-concept demonstrator produced here is developed to industry standards, and its capabilities extended to other instruments (especially the piano) it is likely to have a wide uptake among instrumental teachers as well as self-learning online customers.

We'll publish academic papers in the normal way throughout the project to ensure immediate academic impact for the project, further promoted by the use of posters and demos at music/computing, music education and musicology conferences. Our project workshop will report on activities, inviting interested communities and stakeholders to try the system and hear about our vision of how it offers a transformative enhancement for instrumental teaching yet preserves traditional standards and values. 

This project seizes an opportunity presented by the application of a novel musicological concept developed in T-Mus (that of 'playability' as a determinant of historical repertoire) in a public-facing role with potential as a commercial venture. It thus further contributes to the stated belief of the T-Mus project that &quot;Musicology need not - and should not - be an 'ivory-tower' discipline.&quot;"
1,CDB2D881-C213-4F1C-893A-31F3E160C9EE,Generating Descriptive Sentence Labels for Multinomial Sentiment-bearing Topics (GenSent),"Sentiment-topic models are a suite of algorithms whose aim is mine and uncover rich opinion structures from text. The utility of sentiment topic models stems from the fact that the inferred hidden sentiment-bearing topics, represented as a multinomial distribution over words, resemble the opinion information of a collection, which can be used as a lens for exploring and understanding opinions from large archives of unstructured text. However, a major challenge in applying sentiment-topic models for exploratory purposes is to interpret the meaning of the discovered sentiment-bearing topics, which, so far, relied entirely on manual interpretation. In addition, current sentiment-bearing models are not able to facilitate accurate opinion and sentiment understanding. For example, by examining the sentiment-bearing topic &quot;amazon order return ship receive refund damaged disappointed policy unhappy&quot;, one can interpret that this topic captures opinions relating to &quot;unsatisfactory online shopping experience&quot;. But it is impossible to gain deep insight of the opinion, i.e., whether the sentiment unhappy is only targeted to the product being ordered, or it is also related to Amazon's policy.

A solution to automatic interpretation and labelling of sentiment-bearing topics is most timely because: (i) when applying sentiment-topic models for data exploration, users are forced to interpret the inferred sentiment-bearing topics manually, which is slow and impractical when analysing highly dynamic or large scale data; and (ii) automated tools facilitating accurate opinion understanding is crucial for many practical applications (e.g. cybersecurity and business intelligence), as it allows one to derive knowledge from large amounts of text data and to formulate decisions, converting data into actionable knowledge.

The project aims to push the frontier of sentiment-topic modelling through the development of a novel framework for automated generation of sentence labels that can accurately describe the opinions of multinomial sentiment-bearing topics and are optimally suitable for humans in terms of clarity, brevity and information-richness. The main challenges will be the accurate interpretation of opinions encoded in sentiment-bearing topics and the generation of concise sentence labels which convey the essences of sentiment-bearing topics as much as possible. This is both ambitious and adventurous because: (i) it has already been demonstrated to be a challenging task to automatically labelling standard topics concerning topical information alone (as existing evidence seems to support). Labelling sentiment-bearing topics involves capturing and interpreting semantics from both sentiment and topic dimensions and the dependencies between them, thus adding an additional dimension of complexity for the labelling task; (ii) the two requirements for sentence label generation, i.e., maximal opinion coverage and high conciseness, naturally conflict with each other. How to optimise the trade-off between these two orthogonal objectives for generating a most suitable sentence label is an important scientific question.",,"The project will run a programme with a view to maximising the impact of the projects' results to deliver three key impact objectives: (i) sustainable economic impact; (ii) Increasing public awareness and understanding; and (iii) developing project team's research and knowledge transfer skills.

Sustainable economic impact will be delivered by means of a two-staged approach. In the first stage, a series of dissemination activities (magazine article, business event, attendance to and presentation at industry conference) will run to engage with potentially interested businesses. We have already secured interest, support and collaboration from the BBC and Lincedo which will also participate in these events. In the second stage, activities with these already secured partners and possibly other selected partners from stage one will run. The BBC, apart from providing a real-world dataset for the project's use, has committed to provide a researcher to collaborate in the project to push research in this topic forward and establish a long-term mutually-benefiting collaboration base. The commercial exploitation of the framework to be developed in this project will be explored (following Aberdeen University's IP protection policy) with Lincedo. GenSent will also utilise other grant mechanisms (e.g. KTP, TSB) for follow-up collaborations with these businesses. 

Raising public awareness and understanding of computational tools for data analytics will be delivered by means of a highly active Social Media strategy to disseminate educational videos and blog entries about how these tools can better support data exploration and understanding as well as to disseminate project results. We will present and discuss our research in an informal setting, e.g. using the the established formats of Cafe Scientifique (cafescientifique.org). In addition, to reach a younger audience, we will participate national science festivals, e.g. the Edinburgh International Science Festival or the British Science Festival. We also want to inspire young people to pursue careers in computing science and data science.

The team will enhance their knowledge transfer skills via regular meetings with the project partners, i.e., the BBC and Lincedo, and will participate in the knowledge transfer activities (business education event, etc.). The RF will be working towards the completion of the specified objectives and will be given opportunities to gain academic network experience by attending leading international conferences, and the PhD student (funded by AU) will gain research skills towards his PhD by working with the PI and other project researchers. The team will also learn new skills by participating in activities described above to promote computational tools for data analytics."
2,E532A30F-1893-4528-83AC-69C86D1B5B84,CONSULT: Collaborative Mobile Decision Support for Managing Multiple Morbidities,"The provision of healthcare to people with long-term conditions is a growing challenge, which is particularly acute for the growing proportion of the UK population that suffers from multiple morbidities.

Research has established that involving patients in the management of their own disease has long-term health benefits. Advances in wireless sensor technology means that it is practical for patients to monitor a wide range of health and wellness data at home, including blood pressure, heart function and glucose levels, without direct supervision by medical personnel. The advent of smart phone technologies, appearing widely throughout the nation's population, enables the exciting possibility of putting state-of-the-art intelligent decision-support systems into the hands of the general public.

However, such sensor data is currently disconnected both from the patient context, provided by the Electronic Health Record, and from the treatment plan, based on current best-evidence guidelines and customised by the patient's GP. In cases of multi-morbidities, there is no clear strategy for combining multiple guidelines into a coherent whole. Furthermore, personalised treatment plans are rigid and do not dynamically adapt to changes in a patient's circumstances. Finally, the record of patient condition and decisions made is not routinely captured in a standardised way, preventing learning from feedback about treatment effectiveness. 

To address these problems, CONSULT will combine wireless &quot;wellness&quot; sensors with intelligent software running on mobile devices, to support patient decision making, and thus actively engage patients in managing their healthcare. Our software will use computational argumentation to help patients follow treatment guidelines and will learn details specific to individuals, personalising treatment advice within medically sound limits. Critically, the software will detect conflicts in treatment guidelines that frequently arise in the management of multiple morbidities. The software will provide advice regarding which treatment options to follow, when the conflicts can be resolved by the patient and when a resolution requires an intervention from a clinician. The software will thus help patients handle routine maintenance of their conditions, while ensuring that medical professionals are consulted when appropriate. This will enable patients to take charge of their own conditions, while being fully supported in both traditional and new ways. By routinely capturing the data provenance of the recommendations made, actions taken and the resulting patient progress, the software will provide valuable insights into the effectiveness of treatments and underlying guidelines in multi-morbidity scenarios.

The technology will be evaluated across multiple dimensions in a proof-of-concept study, engaging stroke patients, their carers and medical professionals, while capitalising on King's College London's world-leading position in stroke research and its established patient groups, particularly those connected to the South London Stroke Register programme.

Helping patients to govern their own care will reduce the demands made on medical professionals, while reaping the health benefits of self-management. Integrating live information from monitoring devices will make it possible to distinguish between situations that need attention from medical professionals, and those that do not, reducing the number of extra appointments that patients and doctors need to schedule. Using live information will also make it possible to detect changes in the course of a disease, allowing pre-emptive actions to be taken, and thus reducing the amount of time that someone suffering from a long-term condition may have to spend in hospital. Overall, our approach will not only provide more efficient care, but also allow care to be better tailored to the needs of each individual.",,"Chronic health conditions are widespread in the UK. NHS England estimates that around 15 million people in England (30% of the population) suffer from chronic health conditions. In Scotland and Wales, the proportion of the population affected is even higher. Such conditions require constant management, and they account for 50% of all GP appointments and 64% of all outpatient appointments. With careful monitoring, it is possible for those who are chronically ill to lead high-functioning lives and to have their care managed at home. However, many chronic conditions can easily lead to hospital stays, with the result that 70% of all inpatient bed days are required to treat the chronically ill. The prevalence of chronic conditions is closely correlated with age. For example, 14% of those younger than 40 report a long-term condition, compared with 58% of those over 60. With an ageing population nationwide, dealing with chronic conditions will consume even more resources in the future.

The number of people with more than one chronic disease is also growing. It is predicted that there will be 2.9 million people with two or more long-term conditions by 2018, an increase of one million since 2008. Such multiple morbidities are difficult to navigate because traditionally each disease has been managed separately, so drug regimes and treatment plans are developed in isolation and may conflict with each other. This growth in multiple morbidities presents a further challenge to our healthcare system: by 2018, dealing with them will cost GBP 5 billion more than in 2011.

A key feature of our proposed CONSULT approach is that it goes far beyond what is possible with medical advice web sites. By adopting a collaborative approach based on integrating wellness sensor data with a patient's electronic health record (EHR), it is possible to provide personalised care in home settings, reducing the amount of hospital and GP time required, and improving treatment outcomes. By specifically targeting the issues that arise in handling multiple morbidities, our approach aims to help the most vulnerable members of the chronically ill population.

Specifically, our research will impact several categories of stakeholders:

* Patients will be assisted in the management of their conditions. Our work will literally put up-to-date information and support at their fingertips. Our goal is to help patients sort through what is relevant to them, understand their options, avoid information overload and make the best decisions, even when treatment guidelines conflict. By engaging patients in this way, we aim to help them obtain the health benefits that have been shown for the chronically ill who self-manage their treatment. Overall, our approach offers both economic benefits, reducing the cost of long-term care, and social benefits in terms of increased quality of life.

* Carers will be empowered in the management of patients with chronic conditions. Our work will provide live support and connection to their patients, and the added security of knowing that medical professionals will be informed of any relevant changes in patients' conditions.

* Clinicians' efforts will be better apportioned. Since wellness sensor data will be integrated into the EHR, clinicians will be able to monitor patients' conditions without bringing them into clinic, and receive alarms when there are situations that require immediate attention.

In addition, medical researchers will be able to obtain integrated data from sensors and EHRs to conduct observational studies on efficiency of treatments and accuracy of measurements in the home setting. The provenance of data collected, backing decision support, will enable commissioning bodies to gain unique insight into the efficiency and cost-effectiveness of treatments. Technology will also impact commercial decision-support providers looking to deliver collaborative home care solutions."
3,726BD3BB-BA3B-4091-BDD6-D88E76415F13,"The Wearable Clinic: Connecting Health, Self and Care","An increasing number of people live with long term physical and mental health conditions, such as diabetes, heart disease or depression. Many of these people find that their symptoms fluctuate in severity over time, including periods of relative calm and episodes during which symptoms become much worse. However, patients with long term conditions typically see their doctor during pre-arranged visits at fixed intervals, rather than on the basis of their current symptoms. For instance, people with chronic kidney disease commonly have appointments every 3 months. These visits are often felt unnecessary during stable periods, during which patients could probably manage well by themselves, but irregular enough to spot worsening symptoms early enough and prevent more severe episodes of illness - what we call 'fall back episodes'. 

We propose to develop a set of software tools for smartphones and tablets, called the &quot;Wearable Clinic&quot;. This will help patients with long term conditions, together with their carers and doctors, to better manage their health in daily life, respond more quickly to changes in symptoms and prevent fall back episodes. This could prevent unplanned admissions to hospital, which are not only distressing and disruptive for patients and their families, but expensive for the NHS. Furthermore, it could make it easier to integrate care for patients with multiple long term conditions (e.g. both diabetes and chronic kidney disease), who are often treated by different doctors, at different places, and at different times. 

For patients, using the Wearable Clinic starts with measuring symptoms in daily life using wearables. These data are then automatically combined with data held in NHS records on their diagnoses, lab results, and treatments in order to predict the likely future course of symptoms, and whether there is a risk of a fall back episode. Finally, the software will propose a modifiable care plan that takes account of the patient's range of existing conditions, current and predicted health status, availability of local care resources, and the patient's own preferences. Where it is possible and safe to do so, care plans will remove clinically unnecessary and unwanted appointments, saving time and money for both the patient and the NHS. 

To achieve this vision, we propose to apply data science techniques to analyse data collected from a) medical records and b) wristband wearables and smartphone technologies ('wearables') worn by patients with long term conditions. While the Wearable Clinic concept could potentially be useful for managing a range of long term conditions, we will first test it out in two different conditions, where symptoms are known to fluctuate over time: schizophrenia and chronic kidney disease. Statistical techniques will be applied to see if data collected from patients using wearables can be used to a) predict changes in symptoms and b) produce tailored care plans for individual patients. We will trial methods that collect and use data in ways that take into account individual risk factors (e.g. age, ethnicity) and conserve the battery life of devices. 

While the project primarily aims to develop new computer algorithms, statistical models and computer software, we will trial the technical aspects of the Wearable Clinic with a small number of healthy volunteers, people with schizophrenia and people with chronic kidney disease. We will also investigate costs, benefits, and potential risks of the Wearable Clinic in its earliest stages of development and, where necessary and feasible, integrate solutions during the lifetime of the project. A series of workshops open to the public will be held to explore cross-cutting issues such as trustworthy data use and privacy. This will pave the way for future studies and maximise the chances that the Wearable Clinic actually makes it into practice - thus improving the quality of care for patients with long term conditions.",,"Economic: To quantify the potential value of the Wearable Clinic for the healthcare system and for patients, we will conduct an early economic evaluation and value of information analysis. This will assess the economic feasibility of the Wearable Clinic concept and inform future investment decisions as well as decisions to conduct further research to reduce the key (developmental and post-market) uncertainty. 

The market size for wearable sensors is estimated to be &pound;70B by 2025, giving substantial scope for commercialisation work. It is highly likely that patentable IP will be produced as part of the Wearable Clinic and the research team will work with UMIP (University of Manchester Intellectual Property) and Trustech (the North West's NHS innovation service) who have a strong track record in bringing innovative products to market. This make take the form of creating spin-out companies or licensing technologies as deemed most appropriate. To de-risk this process industrial support has been secured from a number of partners (see Letters of Support), including Cerner as major provider of healthcare IT systems and Withings as an SME in wearable health technology. This will be complemented by support from the Greater Manchester Connected Health Ecosystem, a partnership of &gt;70 organisations, including NHS, social care, patient groups and industry.

Societal: Long-term conditions conditions place a major burden on health and social care services, coupled with significant reduction in quality of life for affected individuals. Half of GP appointments, and &pound;7 in every &pound;10 spent on health and social care, is spent on treating and caring for people with LTCs. The number of people affected, especially those with multiple coexisting conditions, increases steadily as a result of population ageing and lifestyle factors. The NHS faces the challenge of implementing &pound;22bn of efficiency savings by 2020, and it is expected that important savings will have to come from the intelligent use of IT for patients with long-term conditions. 

The Wearable Clinic has the potential to improve patient activation and stimulate self-management in patients with LTCs. Improved patient activation and better self-management are associated with improved health outcomes and lower care consumption, and generally lead to higher satisfaction with health services. The research team has established collaborations with the Manchester Academic Health Sciences Centre (MAHSC) partnership between the University of Manchester and the NHS, the Greater Manchester Academic Health Science Network and Health Innovation Manchester to ensure future translation of the results of the project into the NHS."
4,776B7758-FA15-4FAC-BFCE-3A213A077E89,Supporting Security Policy with Effective Digital Intervention (SSPEDI),"The behaviour of people is known to be critical to the security of organizations across all sectors of the economy. As users of IT systems, their action, or inaction, can create cyber security vulnerabilities. For example, users can be tempted to give away their authentication credentials (by phishing), to install malign software (malware), choose weak or inadequate passwords, or they may fail to install security patches, to scan computers for viruses, or to make secure backups of critical data. 

Organizations design security policies which users are supposed to follow, for example, instructing them not to give away their authentication (login) credentials, or not to open certain kinds of attachments sent in unsolicited emails. However, in practice, managers find it very difficult to encourage users to follow policy. 

This project will investigate effective ways to improve security communications with users, to enable them to understand security risks, and to persuade them to comply with policy. Our hypothesis is that to be most effective, communications and policy implementations must take into account individual personalities and motivations. Technological support is therefore required to support security communications and security persuasion so that it can scale up to large organizations. 

We propose to transfer ideas and knowledge from the existing academic field of persuasive technologies and digital behaviour interventions, and apply them to the user security compliance problem. We will build, and trial, real technologies that implement persuasive strategies in real user security scenarios. These scenarios will be selected in partnership with industrial security practitioners. 

The project takes a broad, interdisciplinary view of the roots of the user compliance challenge, and draws additionally on expert knowledge from the fields of psychology, behavioural decision, security, sentiment analysis and argumentation in search of solutions.",,"User compliance with IT security policy is a difficult challenge faced by managers of most organisations, across all sectors of the economy. Security risks could often be mitigated if users were to comprehend and follow policies. Even very technical attacks often require a failing on the human side of cyber defences. For example, phishing is often used to steal authentication (login) credentials of users, which can then be exploited in a complex attack, and users can also be tricked into authorising actions by malware sent as email attachments. Encouraging appropriate user behaviour within organisations is thus key to securing organisations, and thus to building a resilient nation. 

The proposed research aims to deepen knowledge of how to support the implementation of IT security policies using digital behaviour interventions. It will investigate strategies for security communication with users of IT in organisations, and trial effective, personalised computer technologies that can persuade users to comply with policy. Worldwide research in this area is in its infancy, although attempts to persuade, of one form or another, are used throughout security. This project will move the UK to a world leading position in digital behaviour intervention and persuasion in security.

The impact will be of three principal kinds. Firstly, it will generate empirical knowledge of which digital intervention strategies are likely to be successful for which kinds of security policies in which organisational contexts. This knowledge will be of interest to IT professionals and managers as well as those interested in the scientific outcomes. Secondly, this knowledge, and technologies similar to those trialled, will be applicable within organisations by IT security practitioners. The results may translate to broader IT security communication situations, for example, between governments and individuals (although this is not a goal for the project). Thirdly, the research will contribute to the nation's cyber-security capacity by building and strengthening relationships between academics and industry professionals, and by developing skills and human capital around security and persuasion.

The research will be conducted alongside industry experts to ensure that it tackles human factors security policy scenarios of the greatest relevance, and so that pathways to translate the research knowledge into practice are clearly identified."
5,365FF502-B977-45C0-B321-C18408CEB1E8,Pothole Identification and Management Autonomous System,"Context:
A 2016 survey for KwikFit by ICM Research estimated the annual total cost of pothole damage to UK motorists was &pound;684m. Research by the Asphalt Industries Alliance (ALARM 2016) identified that the total cost of compensation claims against local authorities in England and Wales for 2015/2016 was &pound;28.4m - 76% of which was directly attributable to potholes. 
Whilst local authorities have a legal responsibility for dealing with damage to roads, they are heavily dependent on issues being reported to them by the general public to enable assessment and repair. The reactive nature of pothole reporting, assessment and repair is inefficient and largely ineffective.

Aims and Objectives:
The aim of the project is to develop technology improving the way local authorities identify and manage potholes, and is designed to enable them to improve roads and reduce costs. The key objectives are:
1. To develop affordable sensor technology to enable a vehicle mounted sensor travelling at speeds of up to 65Km/hr to identify and capture road damage between road kerb and centre line.
2. To classify the road damage for reporting purposes.
3. To provide an image of the damage.
4. To provide an accurate location of the road damage.
5. To create a &quot;learning database&quot; of typical damage parameters.
6. To report the road damage on a store-and-forward or real-time basis.
7. To capture and store data on a cloud based service.
8. To map data so that it is easily understood.
9. To provide web-based access to map based products.

The project will initially focus on delivering a prototype for identifying, classifying, reporting and sharing information on potholes. Real-time applications will be developed following successful demonstration on the initial capability.

Applications and benefits:
The project will provide vehicle mounted sensors which can identify and classify potholes and other road damage, provide an accurate position for the damage to the road, report the occurrence by forwarding to cloud storage and enable the data to be accessed through a web browser showing the data on a map. 
We believe the most effective mechanism to deliver comprehensive mapping of local authority roads will be by mounting the sensors on refuge and recycling waste collection vehicles. This could be supplemented by standalone survey vehicles. Combining data from local authorities would provide comprehensive mapping of a region and potentially the UK.
Benefits include:
1. The creation of a detailed web-based mapping source of potholes in roads for highways agencies, local authorities and private subscribers. The data would include classification of damage, imaging and accurate positioning. This would enable highways authorities and local authorities to make informed decisions on the prioritisation of road repairs and provide private subscribers with a tool to avoid road damage to private vehicles.
2. Real-time data capture and reporting of ground disturbances for military users in operational environments with a risk of improvised explosive device (IED) attack; enabling potential threats to be mitigated.
3. Real-time data capture and reporting of natural and man-made hazards for unmanned precision farming vehicles; enabling avoidance action to be taken before impact.",,"The impact of our work would be to reduce the burden to the UK economy; to the local authority, to the motorist, and to insurers of pothole damage to vehicles by providing a value-added service.

From our research proposal the following ones could benefit:
1. Local authority highways departments
2. Highways agencies 
3. Road hauliers and logistics companies
4. Consumers (i.e. private drivers)
5. Insurance companies
6. Providers of on-line road navigation data

The primary market are the UK local authorities which have a responsibility for road repairs together with the Highways Agency. Authorities and agencies would benefit from an improved understanding of the scale and scope of damage and the capability to prioritise and co-ordinate repair activities. Currently local authorities rely on potholes being reported to them by drivers or residents on an ad-hoc basis, together with periodic inspections of A and B roads. They send out an assessor who measures the width and depth of the pothole. If the assessor's measurements show that the pothole is above a threshold, the local authority will send out a maintenance crew to repair the road surface. A maintenance crew consists of several people, a lorry, tarmac and machinery and several crews may be employed across a local authority. In contrast to that, our automated pothole detection, recognition and classification procedure will remove the need for a separate visit by authority assessors allowing potholes to be automatically prioritised by severity for repair in real time. The proposition is to provide accurate information to local authorities daily, identifying potholes in a way which enables them to prioritise and spend their budget in the most efficient way possible. 

In the UK in 2015, damage to vehicles caused by driving over potholes in roads cost motorists &pound;684m in vehicle repairs. Therefore, commercial vehicle operators and consumers would benefit from improved planning of routes to avoid unnecessary damage to vehicles.

Our automated Pothole Identification and Management system will benefit insurance companies as they could adjust their monthly fee depending on the road damage in the area of each client.

It is also to provide this information to motorists through real-time satnav updates which alert drivers with pothole warnings and allow them to reduce their speed and to take evasive measures."
6,3A8770EE-893F-445E-AE3C-1F12F37AF472,ASPIRE: Automated Sensing &amp; Predictive Inference for Respiratory Exacerbation,"There is an urgent, unmet need for reliable, intelligent systems that can monitor patient condition in the home, and which can help patients manage long-term conditions. Delays in recognition of the changes in physiological state worsen outcomes and increase healthcare costs. The ASPIRE programme uses chronic obstructive pulmonary disorder (COPD) as an exemplar, which affects over 210 million people globally. This condition costs the National Health Service over &pound;800 million each year, over half of which is spent treating patients in hospital, rather than caring for them in their homes.

Intelligent monitoring systems are required to address the needs of patients with long-term conditions in their homes. However, no wearable systems have penetrated into clinical practice at scale, due to: (i) poor tolerance of existing wearable devices for monitoring; (ii) a lack of robustness in the estimates of the vital signs that wearable sensors produce; (iii) very limited battery life that requires batteries to be re-charged at a rate that prevents their use on a large scale; and (iv) limited subsequent use of the data for helping the patient understand and manage their condition.

We propose to develop an &quot;intelligent&quot; home-based system, with smart algorithms embedded within lightweight healthcare sensors, to overcome these limitations. Our novel work will incorporate next-generation machine learning algorithms to combine information from healthcare sensors with information from GP and hospital visits. This will enable the system to learn &quot;normal&quot; health condition for individual patients, with knowledge of other conditions from which they may be suffering, and which can then make recommendations to the patient concerning self-management of their condition. This work will include close working with world-leading clinicians to ensure that the recommendations provided by the system are correct for the individual patient.",,"The proposed programme has the potential for very significant impact for patients in the home who are suffering from long-term conditions, by optimising self-management and improving outcomes. Patients who are deteriorating will be identified early, which will allow preventative action to be taken, avoiding serious escalation (such as unplanned presentation to a hospital Emergency Department), and which will reduce the incidence of preventable morbidity and death. Patient-specific care will be enabled using an intelligent system that learns patient characteristics in real-time, thus improving patient outcomes by improving the efficacy of care provided, and the recommendations made to patients to assist in (i) understanding their own condition and (ii) taking steps to maintain stability. 

The translation of such systems into the home-care environment will provide benefit to patients with long-term conditions, such as chronic obstructive pulmonary disease (COPD, our exemplar) - this will be achieved via predictive systems that allow patients to track view their condition, and robust forecasts of their health status, without the false-alarm rate associated with existing systems, and which prevents existing systems providing benefit to existing patients. Healthcare workers, including community nurses and GPs, will benefit from the outputs of the programming by being given a quantitative assessment of cohorts of their home-based patients, and records of both their physiological data and associated predictions / recommendations made by the ASPIRE system.

The NHS as an organisation will benefit from the research because improved patient outcomes are associated with lower healthcare costs, as a result of fewer stays in hospital and fewer unplanned admission to (expensive) higher levels of care. Additionally, patients can be stratified according to risk of severity / deterioration, allowing improved use of community staffing resources. Clinicians will benefit by being able to interpret, for the first time, the very large and heterogeneous datasets that are available for their patients - enabled by robust, probabilistic tools created during the programme.

Companies in the commercial private sector will benefit from the research, where involvement of the industrial partners will allow rapid implementation of the techniques developed during the programme. Such companies have an interest in &quot;intelligent healthcare&quot; algorithms that can be integrated into existing healthcare IT products, which will add significant value and market differentiation. Additionally, the rapidly-growing market for wearable devices is currently focused only on consumers - the proposed work will extend this market to healthcare technologies, by exploiting the opportunities for large-scale innovation and clinical validation that exist in the programme. The UK economy will benefit by the possible creation of new spin-out activity based around the activity of the proposed work, in addition to the &quot;first to market&quot; advantage conferred on the industrial partners.

The scientific community, and the UK research base in particular, will benefit from developing capacity in an emerging field of global importance, and where the proposed project will train the next generation of researchers in intelligent community care, and integrated home/hospital provision. The methodology developed within the proposed programme will be of translational benefit to other scientific disciplines, including other computational and mathematical sciences. Results from the research will feed into the Alan Turing Institute, where researchers involved across the UK will benefit from the development of large-scale sensing methodologies for data science.

The public will benefit via public-engagement activities run in collaboration with charities described in &quot;Pathways to Impact&quot;."
7,B3CA833C-0636-42E0-8028-A4E82BEB8335,Data Awareness for Sending Help (DASH),"This project explores integration of new and emerging data sources for potential impact on emergency response. In an emergency medical situation, ambulances must get to those in need as quickly as possible in order to provide care and, ultimately, to save lives. Decisions about which ambulance should respond to each incident need to be made rapidly. However, making such decisions is complicated: incidents can occur simultaneously or in short succession over a wide area; the locations of ambulances are constantly changing; and there are many environmental factors that can affect response times, such as traffic and weather conditions. In such a complex and dynamic environment, a form of automated decision support, known as computer assisted dispatch (CAD), is often installed to help staff make these decisions.

In recent years, there has been a sharp expansion in the volume and types of data sources that might potentially be linked to CAD, presenting new possibilities for improving current decision-support systems. Doing so could enable ambulances to respond faster as emergency situations develop, improving emergency care, lowering costs, increasing efficiency and improving health outcomes for patients. Potentially useful data might come from any of the following sources: the general population (via social media and other mobile Apps); specific user segments (via in-home/wearable sensors, particularly for high-risk patient groups); urban infrastructure (via public transport monitors, embedded road sensors or weather stations); and other public sector actors (such as collaborating emergency response agencies or healthcare providers).

This proposed Policy Demonstrator Project, entitled &quot;Data Awareness for Sending Help&quot; (DASH), aims to explore the potential of these &quot;new data&quot; sources for improving ambulance response times. The project builds on a new research collaboration between King's College London (KCL) and the London Ambulance Service (LAS), which is evaluating novel methods for ambulance dispatch by simulating ambulance call-outs based on historical LAS system logs. DASH will lay the groundwork for extending this preliminary study in important new directions, by predicting changes in response times due to integration of additional data sources.

There are a number of challenges that DASH aims to address, both technical, in terms of how feasible it is to access and use new data sources reliably, and social or ethical, in terms of how acceptable and appropriate it is to use data in this way, particularly data about individuals. DASH asks three specific research questions:

(1) What are the benefits and risks for emergency service agencies, healthcare providers and the public related to linking new and emerging data sources to emergency response? A comprehensive literature review and targeted focus groups will highlight which new data sources could be tapped and will weigh benefits, costs and risks associated with data-enhanced emergency response, across various sectors.

(2) What are the technical challenges involved in linking new and emerging data sources to CAD technologies to provide the most important benefits? A technical investigation will consider practical aspects of linking new data sources to CAD and will explore innovative modelling methods that could be applied in a low-cost but high-impact and secure manner.

(3) How can practitioners and policymakers learn from our study of linking new and emerging data sources to the London Ambulance Service? DASH will produce a set of outputs designed to inform practitioners and policymakers, including: a Policy Brief outlining our findings; a Case Study that assesses linking new data and associated methodologies to LAS's CAD system; a Software Prototype, built on previous work, that demonstrates how a data-enhanced CAD system might work; and a report on the broader applicability of the findings to other emergency response agencies in the UK.",,"Timely and efficient ambulance response is a major challenge for the UK. The BBC News recently reported that only 1 in 13 ambulance services are meeting their targets for response times (http://www.bbc.co.uk/news/health-38077409). One of the reasons for slowdowns occurs in A&amp;Es, where ambulance crews have to wait before handing over patients to under-staffed hospital personnel. Wales is the only region not lagging behind, and that is because they changed how incident calls are classified so that fewer are required to be completed in the shortest response time of 8 minutes. With an ageing population nationwide and over-stressed National Health Service (NHS), the demands on ambulances and crews will only increase.

This proposed &quot;Data Awareness for Sending Help&quot; (DASH) project investigates how new and emerging forms of data could help improve efficiency of emergency response services. The interdisciplinary nature of the project means that it has the potential to benefit a wide-ranging set of stakeholders:

* the general public, who are the ultimate beneficiaries of the research proposed here, the people who will experience first-hand more efficient operational models of emergency response services;

* the providers of emergency services and health care, and partner organisations such as Clinical Commissioning Groups (CCGs), as well as government officials and general policy makers; and

* researchers who study policy, healthcare and technology, from emergency response and medical specialists to general policy researchers, to computer science, artificial intelligence and operational research experts.

DASH will produce a Case Study around the London Ambulance Service (LAS), which will not only benefit LAS directly, but also its working partners and peers: other emergency services agencies and healthcare providers. DASH will make a valuable contribution to future strategic and operational development of LAS by identifying and assessing specific benefits and risks of linking new and emerging sources of data to LAS computer assisted dispatch (CAD), quantified through operational efficiency indicators. Findings will be extrapolated to other emergency services agencies nationwide. The impact of producing the Case Study will be measured in terms of stakeholder feedback in the short term and changes in practice in the long term.

DASH will build a Software Prototype, extending prior work of project investigators, to simulate dispatch conditions and allow researchers to visualise historical scenarios and ask &quot;what if&quot; questions about future scenarios. Researchers from operational research, computer science and artificial intelligence, as well as applied researchers in policy and public health, will benefit from the opportunity to consider the effects of linking new and emerging data and modelling methodologies to CAD, within a specific real-world context (LAS), as well as more broadly across the UK. The impact of producing the Software Prototype will be measured directly in terms of usage statistics, and indirectly in terms of citations.

DASH will publish a Policy Brief describing the potential for new and emerging forms of data to have a transformative effect on the effectiveness and efficiency of emergency services delivery. Thus, specialist practitioners, general policymakers and the general public will benefit from insights into the policy potential of new and emerging forms of data. The impact of the Policy Brief will be measured through direct feedback from those who read the brief, as well as other, indirect measures such as social media and press monitoring."
8,FC08DFD5-0D11-4600-90D8-13BA665FAFC7,Wearable and Autonomous Computing for Future Smart Cities: A Platform Grant,"The focus of this Platform Grant is the combination of wearable systems networked with smart city and building management systems, and the processing of the collected data. The Platform will cover infrastructure and devices and will require innovation in hardware and software in order to realise the goal of a people centred smart city. The topic of the Platform and the underpinning research themes require a multidisciplinary approach that can be provided by the unique expertise of the research group in the Department of Electronics and Computer Science at the University of Southampton.

Applications of the technologies will enable effective collection, communication, and processing of this data that, in turn, will enable applications such as crowdsensing activities or allow, for example, the provisioning of ultra-personalised services for users to enrich their experience as they navigate their environment, and engage in work and leisure. Such a capability would allow them to purchase personalised services (e.g. healthcare, entertainment, fashion), enable participatory sensing initiatives to support smart city applications (e.g. real-time traffic updates, pollution monitoring), or help coordinate evacuations during major disasters. Combining wearable sensors with intelligent building management systems can provide distributed sensing of the environment within the building as well as monitoring user activity and wellbeing in order to improve the effectiveness and efficiency of building services (e.g. heating, and ventilation). Such a capability will also become an important research tool to aid in our understanding of building occupant behaviour.

Key research challenges exist in developing user-friendly ubiquitous energy-constrained wearable systems and interfacing these reliably and securely with external networks. Wearable sensors and devices will place individuals at the centre of the smart city and enable a step change in the level of interaction possible. It is essential to develop robust, agile algorithms and mechanisms that can cope with potential failures that may arise in the sensors and networks. Combining AI with sensors enables intelligent interacting agents that can form multi-agent systems exceptionally capable of solving problems and interpreting information. Such developments will underpin autonomous systems, benefit the burgeoning Internet of Things (IoT) and enable the next generation of smart city applications. A flexible funding Platform underpinning the group in these crucial areas of expertise will enable pioneering work and the pursuit of emerging opportunities.",,"The development of robust and reliable networked wearable sensors across a smart city environment will benefit individuals, society and the economy in a number of ways. For example, the development of new wearable sensors will facilitate improved health that when combined with a smart city infrastructure could facilitate real-time health monitoring beyond the normally defined ambient assisted living environment (i.e. the home) and into the city. This will enable data to be gathered under a much wider range of scenarios, further improving the provision of healthcare services. Wearable sensors for monitoring the environment and networking these within a smart city infrastructure will enable a step change the amount and variety of data captured as well as facilitating increasingly popular new methods for gathering data such as crowd sourced sensing activities (e.g. in pollution monitoring). The impact in both of these examples will be further enhanced by the combination with AI to form multi-agent systems that can interpret the data and, where applicable, recommend actions. It will facilitate the interaction of the individual with their surroundings, enabling the provision of ultra-personalised services. The ability to act on the gathered data and provide, or highlight, highly informed opportunities for individuals to purchase goods and services will lead to new commercial opportunities.

The work will benefit both software and hardware designers and developers by defining and informing the next generation of smart city and wearable systems. There is the potential to expand the use and capability of existing smart city networks and infrastructure but also to drive improvements in underpinning electronic and systems design in both wearable and smart city domains. Applying low energy based solutions to devices, circuits, sensors and communications will both improve existing products and enable new opportunities. This is a particular requirement for wearable devices and the wider Internet of Things where the supply of energy is a practical constraint in the majority of potential applications. Improved energy efficiency in computation, sensing and communication between the wearable and smart city infrastructure is central to our vision. Specific industrial beneficiaries include the project partners ARM who are heavily involved in designed electronic systems for IoT applications. For example, Southampton has a strong relationship with ARM through its ARM-ECS Research Centre (Director: BAH, Technical Manager: GM), as recognised by the award of &quot;University Research Group of the Year&quot; in the National Microelectronics Institute (NMI) Industry Awards 2015. We have discussed this proposal with senior colleagues at ARM, and they have shown strong interest in supporting the project through staff time for technical discussions and membership of the project's advisory board, and internships for researchers. Other relevant companies include Phillips, Texas instruments, Intel and Microsoft all of whom are known to be researching low energy systems and applications such as ICT based healthcare solutions. Project partner Mayflower have developed a control system for streetlights based upon a wireless network providing an example infrastructure for the smart city environment. This research will enable improved utilisation of such assets and the company are supporting the research with hardware modules to expedite advances and ensure relevance. NquiringMinds Ltd will find new applications and case studies for their Trusted Data Exchange and InterliNQ: IOT Hub products."
9,24101E41-4E40-4A42-8906-4413306BEAF2,iNVERTOX: Rapid intelligent in silico prediction of sub-lethal ecotoxicological effects in invertebrates following pharmaceutical exposure,"The iNVERTOX project will be the first of its kind to discover and predict both phenotypic and molecular level effects of trace pharmaceutical residues on small, but ecologically critical invertebrate organisms living in UK and international freshwaters which are now impacted by human activity. Pharmaceuticals are widely recognised as bioactive contaminants in our environment having been measured globally at very low concentrations. They enter the environment predominantly following excretion of consumed human and animal medicines and have been shown to be resistant to wastewater treatment. This leads to their consistent and prolonged infusion into receiving water catchments. Recently, three pharmaceuticals were placed on a &quot;watch-list&quot; of emerging priority pollutants following extensive studies of their toxicity to biota. However, the occurrence and diversity of pharmaceuticals contamination in the environment extends much further, with significantly more compounds detected in river water, sediments, soils and recently even in environmental species at any one time. Therefore given the scale of this problem, measurement of their effects on our environment is far too slow and laborious. More innovative and rapid approaches are required to understand and mitigate any effects these may have on our environment. Realistically, this must now involve some form of advanced computational modelling to use the limited information we have to predict the effects of additional pharmaceuticals. Moreover, traditional ecotoxicity testing for micro-pollutants use lethal doses and in the case of pharmaceuticals, these are often much higher than measured environmental concentrations. This suggests that more subtle effects need to be researched instead as a more accurate assessment of risk. In some cases, such small changes have resulted in a significant ecosystem imbalance which has indirect effects on wildlife, our environment and potentially also on human health. These so called, &quot;sub-lethal phenotypic effects&quot; are often more difficult to determine and establishing defined links to a pharmaceutical exposure is extremely challenging. The aim of this project is to study and model four sub-lethal phenotypic effects on a model freshwater benthic invertebrate species (Gammarus pulex) including growth rate, feeding rate, ventilation and locomotion following controlled exposure to low doses of over 60 pharmaceuticals typically found in the aquatic environment. In addition to this, changes in the organism at a molecular level will form a novel, central focus and enable knowledge discovery of how biota respond to such exposures at a fundamental level. This will be achieved via metabolomics, which is the measurement of thousands of small molecules present in a biological system following exposure to environmental contaminants. Lastly, and most importantly, this information will be used to build an set of advanced computational models using new machine learning tools to rapidly allow a user to screen potential phenotypic and molecular level effects of a pharmaceutical on biota in silico and minimise or remove the need for extended use of animals in ecotoxicity testing for this purpose. This project will therefore be pioneering in its approach and draw together the best academic and industry expertise from King's College London, The Francis Crick Institute, London and a global leader in pharmaceuticals, AstraZeneca, to rapidly and responsibly understand the effects of pharmaceuticals on environmental organisms.","This project will generate groundbreaking knowledge on the subtle effects of pharmaceuticals in the environment on a model freshwater benthic invertebrate, Gammarus pulex. As excellent indicators of surface water quality, these species are consistently impacted by pharmaceuticals and their metabolites at the ng-ug/L level mainly via sewage treatment plant effluents. Non-lethal phenotype-level effects, metabolomics studies and analytical measurements of &gt;60 pharmaceuticals in G. pulex will be combined to generate biologically-inspired artificial neural networks and/or support vector machine models for rapid prediction of ecotoxicity from molecular level changes. In particular, models will be used to (1) predict growth rate, feeding rate, ventilation and locomotion effects; (2) identify metabolic pathways affected by pharmaceuticals; and (3) reduce the number of animals required for ecotoxicity testing in the future. The project will house five work packages (WPs): (1) Bioanalytical methods for G. pulex; (2) Pharmaceutical exposures and non-lethal effect measurement; (3) Metabolomics of exposed G. pulex and pharmaceutical residue measurement in biota; (4) Machine learning methods to model metabolomics/chemical measurement datasets to predict sub-lethal effects and/or affected pathways; and (5) Bioevaluation of novel biomarkers of exposure to pharmaceuticals. Metabolite/chemical analysis will be performed using gas and liquid chromatography coupled to (high resolution) mass spectrometry. Correlations with phenotypic effects will be identified using, for example, principal component analysis, Volcano plots and Z-transformation to rapidly identify dependent biomarkers. Linkage to pharmaceutical exposure will be built-in to models via internal pharmaceutical concentrations. Lastly, and in reverse, the prediction of molecular level changes will be investigated from quantitative structure-activity relationships and phenotype data for biomarker discovery and read-across.","This project will make a groundbreaking and timely contribution to environmental protection efforts by rapidly advancing the understanding of the effects of emerging contaminants such as pharmaceuticals and their metabolites on biota. Therefore, economic, academic and societal impact will lie in a novel and rapid approach to animal health assessment in UK and international rivers. The main beneficiaries are the pharmaceutical, chemical and environmental protection industries, policy-makers &amp; regulators, healthcare (including third sector research institutions), the general public, early career researchers/students and academia in environmental and analytical science related fields. Academically, this is a timely issue to address now as the analytical technology and existing knowledge of this issue has advanced to a level which enables this groundbreaking work to be done. Combining two world-leading research institutions (King's and the Crick) with AstraZeneca as an IPA partner who prioritises and leads environmental health activity in this industry, our track record and expertise in this field will be focussed on delivering impact mainly across the pharmaceutical sector to inform practice and eventually policy. Firstly, the use of a robust in silico effects predictor will result in benefits to industry given that experimental determination is likely to be impractically long, complicated and at significant cost. Eventually, and if transferable across species, this may also enable better prioritisation of risk assessment strategies for both new and existing compounds as well as horizon scanning in drug discovery to ensure environmental effects are considered from the start. Long term, in silico predictors may enable environmental regulators to proactively rather than retroactively manage protection strategies in parallel with the pharmaceutical industry. As pharmaceuticals are organic molecules, in silico predictors may then also be transferable to other organic contaminant classes which will benefit the chemical industry as a whole. Dissemination of the findings during the project lifetime will be achieved by publicly accessible peer-reviewed publications, participation in world-leading environmental, metabolomics and computational modelling conferences, seminars at local and national level and via media engagement activity. The latter will be used to engage the public to raise the awareness of the proper use and disposal of unused pharmaceuticals and how they may affect organisms living in our rivers. The optimised in silico models themselves will be made freely and publicly accessible via the KCL website accompanied by the raw data generated. Furthermore, datasets will be uploaded to recognised scientific databases (e.g. MassBank, MetaboLights and MetaCyc) to ensure they can be effectively used by other researchers. Training and career progression of the researchers will form a central point of impact. Staff will be trained in state-of-the-art facilities at world-leading institutions in advanced technical skills for metabolomics, ecology, analytical chemistry and computational modelling as well as highlighting and supporting career development opportunities. In particular, working in such a collaboration ensures development of highly desirable professional knowledge, skills and attributes under both academic and industrial guidance and support will be offered to research staff to plan and advance in their careers afterwards. Additionally, both existing and prospective students will have involvement across the project via local outreach, internship and Masters research project activity. Therefore the next generation of scientists will avail of the industrial partnership activity with support for career mapping. Ultimately, this project will benefit the environment and society as well as the UK economy."
10,269B0EE0-534E-43FF-8F50-E3EBD007BFE6,Leveraging the Multi-Stakeholder Nature of Cyber Security,"Cyber Security (CyS) is a challenging, distributed, multi-stakeholder problem. It is distributed in the sense that the expertise to comprehensively assess the level of security of a given IT system is commonly not all available in one location; e.g. detail on the IT components within a company is available within that company, while detail on operating system software vulnerability may be available to the OS manufacturer and further expert insight may be available to public security agencies, such as CESG. It is a multi-stakeholder problem because a number of human stakeholders, from IT designers to users with varying levels of expertise, need to effectively communicate and work together in order to deliver systems with an appropriate level of CyS assurance.

This interdisciplinary project brings together leading academic experts from the University of Nottingham, UK and Carnegie Mellon University, USA, with a strongly integrated project partner: CESG - the UK's National Technical Authority for Information Assurance. The project is designed to leverage the distributed, multiple human stakeholder nature of CyS by developing a novel framework with the necessary scientific underpinning to improve user access to user-tailored CyS information, operationalised as a cutting-edge, data-driven Online CYber Security decision support System (OCYSS). This approach id designed to directly address an acute shortage of availability and access to highly qualified CyS experts by both small-to-large scale users from government to industry. 

The role of OCYSS is to effectively and efficiently integrate expert and user inputs, capturing commonly uncertain vulnerability levels of individual components as well as vulnerabilities arising from the interaction/combination of these components, to efficiently deliver appropriate, balanced, informed and up-to-date threat analysis and CyS decision support to users. 

Importantly, the OCYSS framework:
- Addresses the limited availability of CyS experts by comprehensively capturing and aggregating their insight and expertise to assess the vulnerability, including associated levels of uncertainty, of individual system components (e.g. intrusion detection, encryption) and their interactions (e.g. SSL 3.0 and weak password). This information is captured centrally by OCYSS and updated regularly. 
- Avoids delays in threat analysis and potential mitigation by providing a direct pathway for newly discovered component vulnerabilities &amp; component interaction vulnerabilities (and associated uncertainty) to be rapidly put forward, incl. by manufacturers such as Oracle and third party organisations such as Symantec.
- Is designed to deliver user-tailored, comprehensive and up-to-date threat analysis and decision support which is continuously updated as new information becomes available. OCYSS two-stage outputs capture uncertainty in A) the threat analysis inputs (e.g. uncertainty around a component vulnerability over time and by different experts) and B) in intuitive benefit-cost analysis on threat mitigation in response to asset ranking by users (e.g. a low value asset may not warrant a high investment to address a low threat).

Going beyond the scope of a standard research project, this project is designed to not only deliver cutting-edge science, developing key advances in data science and HCI, but to also deliver a real-world, open source prototype of the OCYSS framework. This enables the project to conduct an exceptional level of evaluation and tailoring to real-world CyS challenges, including the deployment of OCYSS in real-world contexts such as government departments advised by CESG. Further, through this approach, the project is able to deliver both open source algorithms and a substantial open-source software platform prototype, facilitating the academic reproduction of results, as well as substantially boosting the potential of commercial up-take of the project outcomes.",,"The proposed research will directly benefit UK stakeholders in the public and commercial sectors which are dependent on the UK's ability to perform informed, timely and accurate cyber security (CyS) assessments and decisions. Through enabling user-specific, comprehensive, up-to-date and actionable decision support, incl. dynamically updated cost-benefit analysis highlighting priority areas of investment to improve security, the project is designed to deliver clear benefits in CyS and thus UK IT system resilience. The latter is of direct benefits to the UK's economy and wider population. At an international level, insights gained will be applicable to supporting CyS efforts beyond the UK, in the US and world-wide.

As highlighted by CESG, the UK Government's National Technical Authority for Information Assurance, the work proposed addresses urgent challenges in CyS around the lack of availability and access to highly qualified CyS experts by large, medium and small-size users. This prevents the timely cyber-security assessments of users' IT systems, exposing them, their respective users and the wider UK public to cyber-attack incl. the theft of both commercial and personal data. The project is designed to leverage the distributed multi-stakeholder nature of CyS by introducing a framework (and required scientific underpinnings) which combines the expertise of CyS experts and trusted third parties (captured regularly) with the user-provided information on user IT systems, in order to deliver up-to-date, comprehensively informed, user-tailored CyS assessments and decision support. The project goes beyond the creation of an academic framework - producing a real world, open-source software prototype which will enable real world deployment, evaluation and facilitate both commercial up-take and academic replication. 

Through working closely with the international partner - Carnegie Mellon University (CMU), and CESG, the project is in a unique position to deliver real impact that goes beyond academic output and real world tools - but that will lay the foundations for a novel sector in the UK CyS industry which provides CyS decision support by aggregating multiple, incl. human and digital sources.

Specifically:
- By working closely with and leveraging strong support from CESG, the project is designed to deliver the foundations for a new comprehensive approach to CyS assessment which combines the knowledge of the UK's world-leading CyS experts with cutting-edge interdisciplinary data, HCI, and Human Factors science for data elicitation, fusion and communication techniques in order to provide a step-change in the availability and quality of CyS decision support.
- Through strong collaboration with CMU, a world leader in CyS, the project provides a pathway for cross-learning between the UK and the US as well as creating a platform for extended networking both for the project partners but also for the wider UK CyS sector. For example, the project includes two outward facing workshops designed specifically for this purpose.

Beyond impact related directly to CyS assessments, the methods created during this research will also deliver impact through informing analysis in other contexts where the systematic fusion of generally qualitative human insight with often uncertain quantitative data is vital, such as environmental planning and market research. 
The proposed research directly addresses the Partnership for Conflict, Crime and Security Research (PaCCS) RCUK priority area of CyS. No funding in EPSRC's portfolio addresses the essential research at the interface of data-science, modelling of uncertain data and HCI / Human Factors as the proposed project. Finally, as an underpinning aspect of the Digital Economy (DE), the combination of human insight &amp; expertise with smart data processing and representation is directly relevant to the DE and ICT areas in HCI, AI Technologies, Graphics and Visualisation and Information Systems."
11,F6499ED5-5AA5-4878-AB6A-6BF0240B1D2D,LUCID: Clearer Software by Integrating Natural Language Analysis into Software Engineering,"Developers spend most of their time maintaining code, with little tool support.
To maintain code, one must understand it. Clear code is easier to read and
understand, and therefore less expensive and risky to evolve and maintain; it
is also notoriously difficult to write. We will help developers write clearer
code to speed maintenance, and increase developer productivity. Source code
unites two channels - the programming language and natural language - to
describe algorithms. LUCID will advance the state of the art in software
engineering by developing new analyses that exploit the interconnections
between these channels to find uninformative names, stale comments, and bugs
that manifest as discrepancies between the two channels.",,
12,6BB833F2-2EE9-4682-8744-971126920559,Coarse Approximator Compilation,"Computers have revolutionised our lives, from mobile phones that exceed the computational power of early supercomputers by orders of magnitudes, to today's supercomputers that help discovery of new drugs to cure serious diseases and to design more energy efficient vehicles and buildings. All this progress has been made possible by continuously increasing computational power. However, there are two threats to this trend. First, harnessing this resource has become increasingly difficult. Imagine a car that provides direct control of fuel mix, 20 gears and adjustable valve timing. This car will provide excellent performance, but requires a driver with an engineering degree to make the optimal adjustments. Second, similar to improved car fuel efficiency, there is increasing demand for improved computational energy efficiency. We cannot attach larger batteries to a mobile phone, or build a nuclear power station next to each data centre. While there are ongoing discoveries that improve efficiency, these solutions intensify the first problem: they increase the difficulty. For a solution to be truly practical it needs to be usable by non-experts! This project aims to address this in case of Approximate Computing -- a recently proposed technology aiming to increase energy efficiency by orders of magnitude.

The basic insight of approximate computing is that, traditionally, computers always provide a precise and exact solution instead of a good enough solution. This obsession with precision is very energy wasteful. Imagine that you quickly look into your wallet to check how much cash you carry, you wonder if it is a 1-2 GBP, about 20 GBP or more than 50 GBP. One usually does not really care if it is 17.42 GBP or 17.43 GBP. In such a situation, it would be a waste of time to count the cash precisely. Research has shown that a vast body of problems can take advantage of this kind of imprecision.

This research project aims to make approximate computing technology available to non-expert programmers. In particular, the main obstacle for widespread adaptation is that current state of the art in approximate computing burdens the application programmer with providing a suitable approximate alternative. This is comparable to burdening the driver of a car with sophisticated mechanical tasks such as changing a timing belt. The premise of this project is that it is possible to derive an approximation automatically and that this process should be integrated with the tool that every programmer already uses -- the compiler.",,"A successful outcome of this project will benefit all users of compute intensive tasks. Such tasks included mobile devices that provide augmented reality to high performance computing simulation in drug discovery or engineering. It will allow these tasks to be completed faster, while requiring less energy. The latter directly relates to improved life and reduced global energy consumption with all associated benefits to the environment and quality of life. Additionally, it will enable computations order of magnitudes more complex than today. These enable additional breakthroughs in medical and engineering research, as well as providing new functionalities to mobile devices.

For these benefits to be realised it is important to make the technology developed in this project available as widely as possible. We will pursue multiple routes:

-) Our project partner Luis Ceze is well established in the approximate computing community and works as a consultant for Microsoft Research. He will disseminate results to the wider approximate computing community as well as to one of the largest software developers in the world. In particular, Microsoft produces also one of the most widely used compilers: Visual Studio.
-) Our project partner Codeplay customises compiler technology to their customer's needs. Codeplay's customers range from local SMEs to multinational companies such as Sony. Codeplay is very interested to commercialise results of this project and integrate them with their and their customer's technologies.
-) By building this project on the LLVM infrastructure (widely used in industry and other academic groups) and making the outcomes available as open source, we provide a pathway for easy adaptation."
13,4E5F6D54-AC3B-43FA-B569-BD5C4EE1B477,FuSe: Technologies For Bioacoustic Sensing,"Biodiversity is facing an unprecedented decline whilst the pressure on the earth's ecosystems continues to grow. Recognising the status of biodiversity and its benefit to human wellbeing, the world's governments committed in 2010 to take effective and urgent action to halt biodiversity loss through the Convention on Biological Diversity's targets. These targets require monitoring to assess progress towards specific goals. Such large-scale biodiversity assessment calls for methods which are able to provide an understanding of large-scale patterns in species' distributions, abundances and changes over time. This relies on surveys to collect data that are representative at a regional to national scale, and robust analysis that is able to provide an informed understanding of species' populations.

As a group, bats (Chiroptera) are particularly challenging to monitor because most are nocturnal, wide-ranging and difficult to identify. Historically the monitoring of bats in temperate regions has focused on intensive site-based visual counts or capture surveys. There is considerable value in these approaches, but it is difficult to confidently infer from these what is happening at a wider population level. Acoustic surveys have been used in the UK to monitor bats for the past few decades (e.g. Bat Conservation Trust's National Bat Monitoring Programme), but it is only very recently that advances in sensor technology and analytical acoustic tools have made it possible to identify and monitor more than a handful of easy to identify species. 

Many of the first open-source tools for automatically detecting and identifying bat species from sound recordings were developed from our previous NERC funded research. We developed acoustic reference libraries for European bat species and the first automatic machine learning classifier. We have since built on this work through our recent EPSRC and Zooniverse funded projects, to make use of the latest machine-learning technologies (Convoluted Neural Networks, CNNs) and through this developed an open-source pipeline for the detection of search-phase echolocation calls and species identification.

Despite recent and exciting developments in acoustic species identification, there remain substantial challenges for their cost-effective use within a scalable monitoring tool. A major barrier for deployment at scale is the expense of acoustic sensors, which typically cost up to &pound;1000. As part of this consortium, research at Oxford University has focused on the development of low-cost sensors (http://soundtrap.io), which record uncompressed audio to an SD card. The current sensors are not designed to record at high frequency for bats, but prototype versions, which would cost less than &pound;50 have already been modified and deployed in trials to record bats. 

The FuSe (Technologies For Bioacoustic Sensing) research consortium brings together this expertise in bat acoustic analysis (University College London), computer science and open source sensor hardware (Oxford University), with large-scale citizen science (British Trust for Ornithology), and national-level bat population monitoring (Bat Conservation Trust). Integrating cutting edge analytical tools for the species identification of bats with the development of new low cost sensors, and expertise in the interpretation of data collected through large-scale volunteer-based acoustic surveys and bat monitoring, we will create an end-to-end open-source system for the large-scale acoustic monitoring of bat populations. This is a catalytic proposal, which has huge implications for the future of bat and bioacoustic monitoring.",,"Through careful working with stakeholder groups and our project partner the Bat Conservation Trust (see Case for Support, Outcomes, Beneficiaries and Impacts) we will co-produce the requirements for data outputs and interactions of the FuSe system. The project will produce substantial benefits for the Bat Conservation Trust, stakeholders and wider public by:

-- Enabling rigorous and sustainable monitoring of bats at national and international scales --

There is a clear need for recent advances in sensor technology and analytical acoustic tools to be implemented into national and international bat monitoring programmes. In the short and medium term by working in partnership with the Bat Conservation Trust the innovations described in this proposal will benefit bat monitoring in the UK by enabling: (1) the monitoring of a greater range of bat species than is currently possible, increasing the number of species for which it is possible to produce robust metrics of status, change and distribution; (2) the collection and processing of much larger amounts of data for a similar level of volunteer effort. This will improve our ability to collect data over longer survey periods and to monitor the rarer bat species. As a longer-term benefit, innovation made possible through this project is likely to adopted by other countries, leading to a step improvement in global bat monitoring and understanding of bats.

-- Providing robust data for wider use and decision making by stakeholders --

Implementing such innovation into national bat monitoring would provide data that will be of immense value to a wide range of stakeholders, including government, statutory nature conservation bodies, local planning authorities, developers, land managers and conservation bodies. In the short and medium term, outputs from the project will be used for a wide range of purposes including for (1) strategic planning; (2) the assessment of favourable conservation status; (3) the condition of protected sites; (4) agri-environment schemes; (4) indicators of environmental quality. In the longer-term, such data will improve our ability to quantify the impact of potential drivers of population change.

-- Inspiring interest, awareness and enthusiasm for bats -

This project will bring science, nature and technology together to harness volunteers' enthusiasm for discovering more about the nocturnal biodiversity in their gardens and local area. By engaging with a much wider audience of volunteers, who will not require previous bat monitoring experience, we will increase interest, understanding and awareness of bats and bat conservation. The longer-term benefit is a public who take greater responsibility for the natural environment and wildlife within it."
14,CD0619AC-4A54-409C-9019-C209E0698B6D,RePriCo: Resolving Multi-party Privacy Conflicts in Social Media,"Hundreds of billions of items that are uploaded to Social Media are co-owned by multiple users, yet only the user that uploads the item is allowed to set its privacy settings (i.e., who can access the item). This is a massive and serious problem as users' privacy preferences for co-owned items usually conflict, so applying the preferences of only one party leads to privacy violations with severe consequences (e.g., users losing their jobs, being cyberstalked, etc.). A solution to this problem is most timely as it has been highlighted as one of the most important problems to be addressed for an appropriate online privacy management. An example of this problem is a photo in which Alice and Bob are depicted together: how can they agree on the third parties they will share the photo with? Mainstream Social Media would only allow Alice (assuming she uploads the item) to set the privacy settings for the photo, but what if Bob would not like to share with some of Alice's friends? Some initial approaches have been proposed in the past few years, but these have a number of limitations that make them unsuitable in practice. Users are forced to try to resolve such conflicts manually (by e-mail, phone calls, etc.), which is exhausting because social networks are very dynamic and huge in scale. Even more importantly, the process of resolving conflicts manually starts too late, when one user has already posted the item and the privacy violation has occurred.

RePriCo's most exciting, novel, and ground-breaking part will be the automated conflict detection and resolution mechanism based on new automated negotiation technologies. In particular, RePriCo's main challenge will be to automatically suggest solutions to the conflicts in such a way that these solutions are accepted by users most of the time and they do not need to resolve the conflicts manually. This is both ambitious and adventurous, as it requires understanding how users would actually reach an agreement if they were to solve the conflicts themselves manually. RePriCo's hypothesis is that users' negotiation behaviour is significantly influenced by users' relationships and their strength (as existing evidences seem to support), and that a computational negotiation mechanism should be based on this in order to produce acceptable outcomes. One of the main contributions of RePriCo will be a strong empirical base about how users' relationships influence negotiation behaviour. RePriCo will be highly transformative in many aspects: (i) it will empower users to manage their privacy for multi-party co-owned items; (ii) it will consider what is actually acceptable for users and the factors that contribute to that, which no previous approach to solve multi-party privacy conflicts has considered before; and (iii), the empirical base it will develop will not only influence RePriCo's automated conflict detection and resolution mechanisms but also help push research on the topic forward.",,"The project will run a programme with a view to maximising the impact of the project's results to deliver four key impact objectives: 1) sustainable economic impact; 2) influencing policy; 3) raising public awareness and improving quality of life in the cyberspace; 4) developing project team's skills. 

Sustainable economic impact will be delivered by means of a two-staged approach. In the first stage, a series of dissemination activities (magazine article, business videos, business event, attendance to and presentation at Industry conferences) will run to engage with potentially interested businesses. We have already secured interest, support and collaboration from Microsoft, SBL and Egress which will also participate in these events. In the second stage, activities with these already secured partners and possibly other selected partners from stage one will run. The commercial exploitation of the Social Media app to be developed will be explored (following Lancaster University's IP protection policy) with SBL, which will provide consultancies and access to partner companies and customers to facilitate this. We will also explore the applicability of RePriCo's results to other social domains with Egress, which has a secure collaboration tool for online data sharing that induces social networks among its users and faces the problem of multi-party privacy management. Finally, Microsoft has committed to provide a researcher to collaborate in the project to push research in this topic forward and establish a long-term mutually-benefiting collaboration base. RePriCo will utilise internal (IAA, CASE) and external (KTP) grant mechanisms for follow-up collaborations with these businesses. 

The PI will also leverage his previous experience in engaging with policy makers (see Track Record) to raise awareness about multi-party privacy management, to update them with project findings, and explore possible areas of impact in current and prospective regulations. To this aim, the PI will: (i) meet his policy contacts as member of the Policy Fellows Network at Centre for Science and Policy (a network of academics and policy professionals the PI is member of), and Security Lancaster's policy contacts (Academic Centre of Excellence in Cyber Security research the PI is member of); and (ii) contact regulation bodies like ICO,ENISA and EDPS. The information above will also be encapsulated as a whitepaper to share with policy stakeholders and the general public.

Raising Public Awareness and improving quality of life in the cyberspace will be delivered by means of a highly active Social Media and Press Releases strategy to disseminate educational videos and blog entries about the problem of multi-privacy management and hints to improve this as well as to disseminate project results. The project will also leverage Lancaster University's links with schools (South Lakes Teaching School Alliance), the MSc in CyberSecurity (in which the PI teaches) and Security Lancaster's bespoke programmes for national and international organisations, contacts with relevant professional bodies (IAPP,DPF,NADPO), and an invited slot on the IA practitioner's event (organised by SBL), as well as an invited article in SBL's CyberTalk Magazine.

The project's team will enhance their knowledge transfer skills via regular meetings with department's Knowledge Business Centre and Security Lancaster's Partnership Manager, and participation in knowledge transfer activities (Business education event, etc.). The RA will be given the opportunity to pursue research objectives more independently and gain academic network experience by organising together with the PI an academic workshop, and the PhD student (funded by LU) will gain research skills towards her/his PhD by working with the PI and other project researchers. The team will also learn new skills by participating in the activities described above to influence policy."
15,CE548D57-FADD-455B-ADB4-AC4447269F79,Natural speech Automated Utility for Mental health,"Promotion of mental well-being is at the core of the World Health Organisation's action plan on mental health 2013--2020, with particular emphasis on the prevention of mental illnesses. 
Indeed, prevention has long been neglected: if we were to make an analogy with dentistry, the state in mental health is such that we know how to treat caries, but we have yet to discover toothpaste. 
Although there is research to suggest that internet-based therapy can be beneficial, there has been little progress on automated mental health advice systems. In the last decade, machine learning has made a huge impact on various areas including spoken dialogue systems. 
Still, the application of statistical spoken dialogue systems has so far been limited to simple information-seeking tasks. Here we propose NAUM---Natural speech Automated Utility for Mental health---, a purely data-driven spoken dialogue system that can be used for maintaining mental well-being. Mental health experts will work on developing NAUM's knowledge, its behaviour will be optimised by novel reinforcement learning algorithms and it will support spoken interaction. 
This ground breaking research will bring the potential of machine learning in spoken dialogue modelling to an application which has a clear benefit for society. NAUM will provide anonymous support that can be accessed by anyone, any time, anywhere, for free.",,"The main objective of the project NAUM is the development of a spoken dialogue system for supporting people at risk of developing depression. To achieve this we set ourselves the following three milestones:
- Development of tools for a mental health ontology that can be used inside a spoken dialogue system
- Development of novel reinforcement learning algorithms that can model complex interaction
- Data collection of human-computer mental health therapy data
Successful development of such a system opens new avenues that go beyond this project. Such a system could then be used in clinical trials aimed at preventing the development of mental illness. Other research questions could be investigated, such as can the ontology be automatically learned from CBT manuals and can the dialogue system be multi-modal and support affective interaction."
16,C44A939C-41EA-4565-BE88-87E219D53498,"ROAD2H: Resource Optimisation, Argumentation, Decision Support and Knowledge Transfer to Create Value via Learning Health Systems","ROAD2H will develop novel Learning Health System (LHS) techniques to facilitate uptake of health interventions to achieve Universal Health Coverage (UHC) in Low- and Middle-Income Countries (LMICs). UHC is a specific target in the UN Sustainable Development Goals (SDG 3.8), with many LMICs attempting to extend population coverage of quality healthcare via public-subsidised health insurance schemes (HISs). Two issues are key to achieving UHC via these schemes: i) how budgets should be best allocated among competing clinical interventions (allocative efficiency) ii) under what timing and circumstances particular interventions should be offered for what kinds of patients in which settings and to what standard of care (quality).Current attempts in LMICs to address efficiency and quality issues include Health Technology Assessment (HTA) based on cost-effectiveness analysis of individual healthcare interventions, and evidence-based guidelines/pathways recommending best practice within whole disease areas. However there have been limited uptake of system-wide mathematical optimisation techniques (in part due to significant data requirements); no efforts to using data routinely collected by HISs for billing and claims purposes, as well as Electronic Health Records (EHRs), for improving efficiency and quality, even though existing attempts among LMICs to use such data retrospectively as inputs into HTA and guidelines show promise. 

We hypothesise that a LHS framework for national HISs, combining resource optimization, argumentation (as understood in AI), clinical decision support and clinical and policy analytics, will facilitate the uptake of evidence-based, cost-effective and data-driven healthcare interventions, and improve the overall efficiency and quality of integrated care systems in LMICs transitioning to UHC. 

ROAD2H will deliver DSS tools that integrate individual patient data (drawn from EHRs) with localized clinical guidelines (including HTA, clinical pathways) obtained via policy and clinical analytics from best available clinical and cost-effectiveness evidence as well as HIS billing/claim data. We will use argumentation to parameterize optimisation problems drawn from cost-effectiveness and resource optimization considerations, and then use standard methods in mathematical optimisation to maximize efficiency of clinical interventions. We will use argumentation also to integrate optimization for maximal efficiency and reasoning with localized guidelines, and resolve conflicts as well as transparently explain recommendations for clinical interventions. The DSS will make use of these explained recommendations as well as data provenance techniques to generate and transparently collect patient-tailored recommendations to enable further learning, in line with the LHS methodology. The DSS tools will operate at the EHR/clinician/patient interaction and integrate data from multiple sources as well as have localised interfaces (by country and possibly hospital/clinician).

The project will be carried out in three phases:
- Phase 1: To review existing HISs and infrastructures in three exemplar countries (China, Serbia, Myanmar), against the requirements for LHSs
- Phase 2: Iterative development of a prototype digital methodology (encompassing policy, data, cost-effectiveness analytics, optimization, argumentation, DSS and provenance) to be embedded into existing HISs 
- Phase 3: To evaluate, by means of proof-of-concept prototype DSS tools in Serbia and China and by scoping in Myanmar, the technical and clinical feasibility of the ROAD2H methodology, its acceptability among end users, and its potential and preconditions for generalization across disease areas and health systems settings in order to support UHC.",,"As its primary purpose, ROAD2H has the potential to directly benefit 3 ODA countries, ranging low-income (Myanmar) to middle income (Serbia and China, with a focus on the latter's rural poor), through:
- reducing out of pocket healthcare spending and financial impoverishment,
- minimising households' precautionary saving thus boosting economic growth,
- reducing waste and improving efficiency of healthcare spending by governments and insurers, allowing fairer and more comprehensive coverage, contributing to the Sustainable Development Goal 3.8 on UHC.

As LMIC budgets grow, demands on those budgets grow too, both from expensive new healthcare technologies, an expanding base of citizens entitled to care through UHC and growing non-communicable disease burdens with growing wealth. Health ministries have limited capacity to control their investments or to make a case to treasuries for targeted investments in high priority diseases and populations. The resulting inefficiencies and inequities affect health as well as economic growth and political stability. ROAD2H will address these challenges by empowering payers with better information as to where their investment goes, and how such allocation compares to global but locally adapted norms for best practice. Our diverse sample of countries will enable us to adapt our methodologies to address the different challenges faced by LMICs: Serbia, a small European nation with a well-established HIS and billing system; China, a vast country with a relatively new HIS and a fragmented healthcare delivery model with low penetration of best practice; and Myanmar, one of the world's poorest countries, spending about 1% of its GDP on health but with major potential in leapfrogging developed economies through better use of digital technology. 

There could be considerable impact from piloting ROAD2H methodologies among three regions of China (Xiamen City, Shaanxi, Henan) and Serbia, with a combined population of over 80m. There is also immense potential for scale-up across rural China (750m rural residents insured under the New China Medical Scheme), in addition for adapting and testing our methodologies in other ODA countries moving to UHC with nascent or established national health insurance schemes, where IGHI has strong links with senior policymakers (inc. India, Ghana, South Africa, Indonesia, Vietnam, Thailand) and where most of the world's poor reside. Our proposal is a much needed and timely contribution to developing countries' efforts towards the Sustainable Development Goal 3.8, aligning well with the strategic priorities of the Global Challenge Research Fund, namely 'sustainable health and wellbeing'.

Further, UN and multilateral organisations such as the World Bank recognise that Big Data will play an important role in monitoring countries' implementation of SDGs, however there currently exist few applications within health beyond infectious disease surveillance or m/eHealth platforms at the patient-clinician interface. We propose that to go much further by leveraging routinely collected data to optimise budget allocation and improve best practice at the system-wide level. This will help countries increase the efficiency and quality of their investment, improving patient referral and care integration across health system tiers, strengthen the generation and translation of locally-relevant knowledge, and ultimately realise UHC.

As a secondary objective, in addition to significant healthcare benefits in LMICs where the use cases will be developed and beyond, ROAD2H will ensure that the UK is at the forefront of the development of sophisticated knowledge and data management systems to advance healthcare provision and assist LMICs in enhancing their healthcare systems. ROAD2H will also strengthen foundations for UK-based commercial opportunities, expanding on our model of innovative public-private partnerships interfacing the healthcare, digital and creative sector"
17,C2A0F32F-B5E7-4747-BA75-2D5D0F08092E,"Tackling Malaria Diagnosis in sub-Saharan Africa with Fast, Accurate and Scalable Robotic Automation, Computer Vision and Machine Learning (FASt-Mal)","Malaria affect about 300 million people worldwide leading to around one million deaths each year. Up to eighty-five percent of the cases occur in sub-Saharan Africa with about 90% mortality in the under five years-of-age group due to severe malaria syndromes. Control of malaria remains a major public health issue in sub-Saharan Africa developing countries. A quarter of the global malaria cases and a third of malaria-attributable childhood deaths occur in the most populous country of Africa, Nigeria (160M inhabitants) and indicates the importance of the problem. Accurate malaria diagnosis relies on the recognition of clinical parameters and more importantly in the microscopic detection of malarial parasites, parasitised red-blood-cells in peripheral-blood films. Malaria parasite detection and counting by human-operated optical microscopy is the current &quot;gold standard&quot; and despite its major severe drawbacks, other non-microscopic methodologies have not been able to outperform it. Presumptive treatment for malaria (without microscopic confirmation) is wasteful of drugs and ineffective if the diagnosis was wrong, a drain on often precious health resources, fuels antimalarial resistance and have made control and elimination interventions unachievable. We aim to create and test in real-world conditions a fast, accurate and scalable malaria diagnosis system by replacing human-expert optical-microscopy with a robotic automated computer-expert system FASt-MalPrototype that assesses similar digital-optical-microscopy representations of the problem. The system aims to provide access to effective malaria diagnosis, a challenge that is faced by all developing countries where malaria is endemic.",,"The global incidence of clinical malaria is estimated at about 300 million cases leading to around one million deaths each year. Up to eighty-five percent of the cases occur in sub-Saharan Africa with about 90% mortality in the under five years-of-age group due to severe malaria syndromes. Control of malaria remains a major public health issue in sub-Saharan Africa developing countries. Our proposal addresses a key problem primarily related to these large group of low- and low-to-middle income countries with large burden Global Health Challenges such a malaria. A quarter of the global malaria cases and a third of malaria-attributable childhood deaths occur in the most populous country of Africa, Nigeria (160M inhabitants) and indicates the importance of the problem. Large all-year-round lethal malaria morbidity and mortality burden has hindered Ibadan wellbeing and economic development within the South-West Nigerian geopolitical region. Key strategies for malaria control has been to reduce mortality by rapid treatment with antimalarial drugs, but this has been stalled by the lack of scalable accurate diagnosis methods. Human-microscopic examination of blood smears remains the &quot;gold standard&quot; for malaria diagnosis and despite its major severe drawbacks, other non-microscopic methodologies have not been able to outperform it. Access to effective malaria diagnosis is a challenge faced by all developing countries where malaria is endemic. Presumptive treatment for malaria (without microscopic confirmation) is wasteful of drugs and ineffective if the diagnosis was wrong, a drain on often precious health resources, fuels antimalarial resistance and have made control and elimination interventions unachievable. This has prompted WHO, CDC and other Global Health organisations to emphasise the urgent need for tools to overcome the deficiencies of human-operated optical-microscopy malaria diagnosis. Our research aims to provide a novel solution for automated fast accurate scalable computational optical-microscopy identification of malaria parasites that will certainly underpin the design and development of future portable accurate and cost-effective malaria-detection devices. Our proposal has a clear path to immediate- and near-future impact outcomes across malaria endemic regions. The proposed timeline is to achieve the immediate-future impact outcome: the design and deployment of a cheaper and optimised robotic bench-top prototype at our primary beneficiary overseas partner at COMUI. Our Nigeria-UK team will carry-out a novel large-scale preclinical assessment of the validity of the automated computational system in real world clinical conditions. The COMUI University College Hospital Ibadan is a centre of academic excellence and attracts both wellbeing and ill people seeking affordable good quality healthcare. Our population footprint has allowed us to execute activities to engage users from different socio-economic backgrounds. Our engagement with primary users is extremely high given to the all-year-round burden of malaria at all ages of the population of the Ibadan metropolis. All inhabitants are at high risk of malaria infection with the most affected being pregnant women, children and the elderly. Fast accurate and scalable malaria-diagnosis methods such as ours will improve health and wealth on large sectors of the population living in extreme poverty across Nigeria and more importantly across the sub-Saharan Africa region (i.e. Togo, Ghana, Cameroon among others).The impact of our research is enormous as rapid and reliable accurate diagnosis of malaria, and therefore its accurate prompt treatment, is a crucial challenge for fulfilling the international development goals for the sub-Saharan African region and other regions of the World affected with malaria."
18,CD221612-C9F2-4E4D-A151-1B284E5499C6,Privacy-Protected Human Identification in Encrypted/Transformed Domains,"Biometrics has been widely utilized in the past two decades in many areas such as healthcare, banking, surveillance, and security control. Given the increased uptake of internet and mobile computing globally, many companies have been turning to biometric privacy and security to ensure secure communication. However, biometric verification over third-party or public network servers may be abusively exploited in an unauthorized way. To protect the privacy and improve the security, it has been advocated to carry out biometric verification in encrypted or transformed domains, where privacy and security can be more effectively guaranteed. 

The basic idea behind the project is that the biometrics in the irreversible encrypted/transformed domains contains exactly the same amount of information as its original one, and hence one can establish a pattern recognition methodology to determine/extract useful information from chaotic signals in encrypted/transformed domains. This First Grant Scheme project aims to investigate how to discover and evaluate the information from chaotic signals for discriminative power, and develop robust pattern recognition schemes for biometric/multi-biometric verification in encrypted/transformed domains. The proposed methods/schemes will be vigorously validated over typical wild face/speech/gait datasets, and two practical demo systems (biometric banking and pedestrian profiling) will be designed and tested in real world environments.

The project will focus on both theoretical understanding of chaotic information and application-specific exploitation of chaotic pattern recognition. Considering multiple data structures hidden beneath a set of given chaotic signals, I will develop a robust way to find out the underlying various data structures for data understanding, clustering and classification. On the other side, given a specific issue such as encrypted/transformed biometric verification, one need to examine the generic theoretic findings in this specific topic and develop a robust scheme for biometric human identification.

The work of this project is within the areas of signal processing, machine learning and pattern analysis. The research on encryted/transformed biometric verification has come from the practical new needs of the UK's emerging new businesses. The project will provide the understanding needed to allow the future development of robust biometric verification methods with novel applications.",,"The rapid advancement in biometric technology has generated huge impact in UK's usual life, ranging from financial service, public security, legal service, immigration control, to daily medical service and healthcare. As a result, biometric industry in UK has experienced drastic expansion in its market. For example, biometric banking has been widely endorsed by major UK banks. Public surveillance is also becoming a thriving market in UK. The value of new biometric markets has achieved $15 billion in 2015 and been estimated to grow further to $45 billion by 2021 (Global Biometrics Market, Markets &amp; Markets, 2015). However, accompanying with the wide spread of biometric uses, people are more and more concerned about its security and privacy issues, especially when Internet-of-Things becomes booming and biometrics are used over public network severs. Developing safe and robust biometric technologies is a key priority for the UK to maintain and reinforce its world-leading role in this new research area and business market. 

A key sector in the UK economy is banking and financial services, which accounts for 10% GDP (Financial services industry of the United Kingdom, Wikipedia) and 33% of the UK's trade balance. Biometric banking is now growing in importance due to the increased popularity and spread of mobile banking. People can easily access their accounts by scanning their face/iris/fingerpring. In the past two years, biometrics such as fingerprints/veins have been utilized by major banks such as HSBC and Barclays. Atom Bank has developed a solution to exploit face and voice recognition, which can be more conveniently integrated with mobile banking. This project will address key security and privacy issues linked to mobile banking, and Atom's involvement will give invaluable insight into industrial requirements for advanced technology exploiting biometric verification in the encrypted domain. I will also develop a mobile-based biometric banking demo with Atom Bank's professionals. Atom bank is a potential partner for future spin-off projects funded directly (under non-disclosure agreements), via InnovateUK (KTPs or otherwise) or with other partners in, for example in EU Horizon 2020 projects. 

Public surveillance is another UK's thriving industry with a market value estimated around &pound;6 billions (Securing the Nation's Future, British Security Industry Association). In this project, I will develop a surveillance demo system in collaboration with Warwick team and their spin-out, which is about pedestrian profiling using scrambled gait/face to provide real-time city monitoring on google map. Potentially, this may lead to a further KTP project via InnovateUK or a SME project via EU Horizon 2020.

The training of the postdoctoral researcher and a university funded PhD student with skills relevant to digital technologies will benefit both the individuals and UK industry. The experience gained will include signal processing, artificial intelligence, vision and image, and security systems. They will receive training in public understanding and engagement and will be involved in outreach work through Think Physics and Digital Living activities and attend exhibitions at Sunderland Software Cities and North East Catapult Centre (Sunderland). This will benefit the individuals and also attract wider interest from the public by promoting the spirit of science and highlighting everyday impacts which arise from this research."
19,CAE06094-FC3F-4CD9-AD68-4660C2F10996,Example-driven machine-human collaboration in mathematics,"In a recent study of what mathematicians talk about, we found that examples form the biggest single category. These may be examples of a concept, such as the set of natural numbers being an example of a group, and the numbers 3, 4, and 5 an example of a Pythagorean triple, or supporting or counterexamples to a conjecture, such as 1 and 3 being a counterexample to the conjecture that the sum of two odd integers is odd. The study found that examples are used for different reasons at different points in a conversation, for instance to understand a conjecture, to test it, or extend it.

As an example of example-use in mathematics, consider the following conversation, taken from an online forum for solving a conjecture:

&quot;If the points form a convex polygon, it is easy.'' [Anonymous - July 19, 2011 @ 8:08 pm]

&quot;Yes. Can we do it if there is a single point not on the convex hull of the points?'' [Thomas H - July 19, 2011 @ 8:09 pm]

&quot;Say there are four points: an equilateral triangle, and then one point in the center of the triangle. No three points are collinear. It seems to me that the windmill can not use the center point more than once! As soon as it hits one of the corner points, it will cycle indefinitely through the corners and never return to the center point. I must be missing something here...'' [Jerzy - July 19, 2011 @ 8:17 pm]

&quot;This isn't true - it will alternate between the centre and each vertex of the triangle.'' [Joe - July 19, 2011 @ 8:21 pm]

Here we see people raising simple examples in support of a conjecture, and proposing and discussing other examples as potential problems.

In this project we will build on our investigations into example-use in mathematics, and employ third party model generators, to design and build a system which can interface to online mathematical conversations by:

1. Manually and automatically inducing dialogue rules to determine when in a conversation it would be appropriate to introduce an example.

2. Discovering argument patterns as to what role an example will play at a given point.

3. Manually and automatically inducing sets of axioms as input to the model generator.

4. Selecting useful output examples from the model generator.

5. Adding contextual information to the examples such as whether it is a reply to a previous comment or the reason why this example is of interest at this point.

6. Adding the examples alongside the contextual information into a conversation in a way which is useful to other (human) participants.

This will draw together theories of argumentation, automated reasoning systems, and ethnographical, cognitive and philosophical studies of how people do mathematics. The prototype system will be evaluated by running it on mathematical conversations in real-time, and seeing, by a variety of measures, whether mathematicians regard it as useful, and whether they are prepared to interact directly with it.

Furthermore, over the duration of the project, we will build a new and broad network of potential research users, in order to determine further directions which such inter-disciplinary work may take.",,"Impact pervades our plans for research: in our first four work-packages we disseminate work via conference talks, publications and contributions to open mathematics fora. Our final work-package, which will run concurrent with the entire project over its eighteen-month period, has impact at its core, via building contacts and engaging with stakeholder bodies.

1. Uniting two fields (Knowledge)

The proposed research is a pilot attempt to merge methods and theories of reasoning from the distinct fields of (i) verification, interactive theorem-proving and automated reasoning, and (ii) argumentation, supported by underlying ethnographic theories of how people do mathematics. The ultimate goal is to take a first step towards a new mixed-initiative mathematics in which multiple parties, both human and machine, collaborate in order to produce novel research mathematics. The project is distinctly interdisciplinary and has an original research idea and methodology underlying it. We will give regular talks to both the Centre for Argument Technology at the University of Dundee, and the Mathematical Reasoning Group at the neighbouring University of Edinburgh. Additionally, we will give talks on the HCI and cultural aspects of the computer-mathematician interactions to the HCI group at the University of Dundee. 

The EPSRC First Grant scheme is an excellent opportunity to invest efforts into proving that the two fields can be combined in a way which is beneficial to each of the two fields.


2. Emphasising the role of examples (Knowledge)

This project will deliver a theory of how and when examples are used in online mathematical conversations, alongside supporting pilot software. The ability to automatically introduce appropriate examples at appropriate points in discourse will have application beyond mathematics -- helping to avoid confirmation bias in intelligence analysis, challenging group-think in focus groups, and lessening the framing effect in decision making. By investigating example-use in our focused area of mathematical conversations, we will lay the foundation for broader applications in the future. 


3. Verification and Correctness (Economy and Society)

The project contributes to the EPSRC growth area of verification and correctness. We will build up a network of industry specialists, with whom we will engage to discuss using our ideas in the development of tools and methods for applying formal, mathematical, methods for specifying, designing and verifying critical software systems.

4. Mathematics education (Economy and Society)

The 2015 Office of Science and Technology Policy factsheet entitled: `Empowering Students and Others through Citizen Science and Crowdsourcing' factsheet states that: &quot;Citizen science and crowdsourcing projects are powerful tools for providing students with skills needed to excel in science, technology, engineering, and math (STEM).'' Our theory of example-use in crowdsourced mathematics will be based on ethnographic methods, supported by cognitive and
philosophical theories. By engaging with curriculum developers and experts in computer-aided instruction in mathematics, our work will have practical application to mathematics education, impacting theories in the psychology of education as well as mathematics students directly, via our planned lecture series for PhD mathematics students.

5. Skills development (People)

In terms of new skill development, the project will affect the Centre for Argument Technology, as well as externally-funded Honours, MSc and PhD students, in contributing to a vibrant research environment. It will further allow the PI to reinforce existing collaborations with researchers from other universities in the UK and abroad, as well as building and strengthening new collaborations, thus helping to establish her research career."
20,0B6B8242-6902-4AE2-88E2-0C5AD9EBAB0A,Difference-of-Convex Convolutional Neural Networks (DC-CNN),"The goal of computer vision is to impart machines with the ability to see, that is, to understand an image similar to a human. This consists of identifying which object categories are present in an image and where (car, person, trees), their actions (running, driving, sitting), and their relative locations (person inside the car, car above the road). The challenge of computer vision lies in obtaining a powerful discriminative representation of an image that allows us to infer the scene encoded within it. Consider, for instance, the representation of an image as captured by a camera, which consists of color values of each of its pixels. Two images that differ greatly in their color values may still depict the same scene (for example, images of the same location at day and night). At the same time, small changes in the color values may result in a completely different scene where objects have moved considerably. This makes raw color values unsuitable for understanding the scene depicted in the image.

Traditional computer vision approaches have relied on hand-designed representations of an image that are more amenable to scene interpretation. Given such a representation, there exist several principled formulations for learning to interpret scenes, which take advantage of the powerful mathematical programming framework of convex optimization. Convex optimization offers many computational advantages: it scales elegantly with the size of the problem, it provides global optimality, it offers convergence guarantees and it can be parallelised over multiple machines without affecting the accuracy.

Recent years have seen the rise of deep learning, and specifically convolutional neural networks (CNN), which aim to automatically obtain the representation of visual data from a large training set. While an automated approach is highly desirable due to its scalability, it comes with the challenge of solving highly complex non-convex mathematical programs. The aim of our research is to overcome these challenges by finding connections between convex optimization and deep learning. The key observation is that the non-convex programs encountered in deep learning for computer vision have a special structure that is closely related to convexity. Specifically, while the mathematical programs are not convex, they are of a difference-of-convex (DC) form. A DC program can be optimized efficiently by an iterative algorithm, which, at each iteration, solves a convex optimization problem.

Our aim is to exploit the structure of DC-CNNs to design the next generation of algorithms for computer vision. Specifically, we will build customized algorithms that will scale up the dimensionality of the CNN by orders of magnitude while keeping the computational cost low. Our algorithms will retain many of the highly desirable benefits of convex programming (convergence, quality guarantees, elegant scaling, distributed computing) while still allow the automatic estimation of image representations.

The impact of such principled and efficient algorithms is potentially huge. The new CNN architectures that this enables will allow researchers to address significantly more complex visual tasks. For example, a generative network that can provide a set of diverse future frames of a given video sequence, or a intelligent agent that can crawl the web for images and videos and complete the captions in order to bridge the gap between visual data and searchable content. Our research results will be made publicly available via open source software. The project is also likely to have a large academic impact, consolidating the leadership of the UK in machine learning and computer vision.",,"Recent years have witnessed the deployment of computer vision in many real-world applications. Examples include autonomous navigation from companies such as MobilEye, where it is important to understand the scene captured by the sensors placed in a car, and the Kinect sensor for Microsoft XBox, where the human pose has to be estimated automatially from depth images. Established tech-based companies such as Google, Facebook and Microsoft are releasing new software on an almost daily basis including image search (a standard feature available on almost all search engines these days), camera stablization (e.g. Motion Stills), and 3D reconstruction (e.g. Seene). The UK, and indeed most countries in the world, are witnessing an unprecedented increase in the number of computer vision based start-ups. However, this is just the start, as computer vision now tries to answer significantly more challenging problems such as automatically infer all the high-level components of a scene depicted in a visual samples (images uploaded on Flickr, or videos uploaded on YouTube) in order to bridge the large gap between the amount of visual information and the amount of searchable content on the Internet. As we make more progress in computer vision and related areas of
artificial intelligence, the opportunities for new tech-based business will grow manifolds.

In this context, the proposed research will play a key role in the deployment of the next general of computer vision solutions. One of the limiting factors of the current technology is that the current optimization algorithms used in conjunction with the ubiquitous deep learning framework has several practical and theoretical drawbacks: (i) they are slow, often taking several days to train even on state of the art hardware; (ii) they offer no guarantees of convergence; and (iii) they cannot be easily distributed across multiple computational cores. Our research is geared towards providing a new principled optimization approach that can speed up the training of a deep learning framework significantly, provide conver
gence guarantees, and naturally lend itself to parallel processing. As an upshot, researchers and practitioners will be able to cut down the development time for new applications substantially and even address more challenging tasks such as predicting the several diverse sets of future frames for a given video, or generating high-resolution and realistic visual samples from textual description.

The potential impact of the project is exemplied by the PI's collaboration with Microsoft Research on two projects that rely heavily on deep learning. The first is aimed at learning a deep disciminative or general model for visual data, which can be applied to infer human poses from depth images is highly cluttered environments, or enable gesture based human-computer interaction by inferring the joint locations of a hand. The second is the optimization of code, which would enable programmers to focus completely on the correctness of their code and leave the optimization for speed to a neural network that is capable of adapting the code to a given data set of samples. The PI is also pursuing research on automatically estimating the network architectures with the help of a Google funded PhD student, which would enable even faster deployment of the core machine learning technology to novel applications. The PI will continue to pursue these collaborations to create practical applications of the methodologies developed through the proposal.

Successful completion of this research will have a high international impact. As mentioned above, it will be of interest both to the established tech-based companies as well as the emerging start-ups in the UK and worldwide. In addition, it will also be of great interest to the large academic community focused on deep learning."
21,1040BEB8-04A0-43C5-AC83-DB02771FFF7E,Hierarchical High Dimensional Clustering for Network Security,"Network security is an area of critical importance in terms of data protection and maintenance of services, which in turn affects every industry ranging from finance to medicine to communications. The World Economic Forum has just published &quot;The Global Risks Report 2017&quot; and lists cyberattacks as one of the biggest risks of any type in terms of both likelihood and impact; in terms of impact it is rated as the biggest technological risk we face.

The aim of the proposed project is to develop a software-based solution for network monitoring and security. This solution would be an adaptation of the software architecture, algorithms and expertise developed for the LHC ATLAS experiment trigger system under STFC funded research at University of Sussex.

Following proof-of-concept work in partnership with industry and further software and IP development we will be pursuing routes to commercialisation. The commercialised software will benefit businesses in UK economy through prevention of network attacks.",,
22,C136B29F-811B-450A-95B3-88589CE283D6,Supporting Feature Engineering for End-User Design of Gestural Interactions,"Sensors for analysing human gesture and activity (such as accelerometers and gyroscopes) are becoming increasingly affordable and easy to connect to existing software and hardware. There is great, unexplored potential for these sensors to support custom gestural control and activity recognition systems. Applications include include the creation of bespoke gestural control interfaces for disabled people, new digital musical instruments, personalised performance analysis systems for athletes, and new embodied interactions for gaming and interactive art. The ability to easily create novel interactions with motion sensors also benefits schoolchildren and university students who are learning about computing through the use of sensors with platforms such as BBC micro:bit and Arduino.

We have previously established methods for enabling people without programming expertise to build custom gesturally-controlled systems, using interactive machine learning. These methods allow people to easily create new systems by demonstrating examples of human actions, along with the desired label or computer response for each action. 

Unfortunately, many compelling applications of custom gesture and activity recognition require substantial pre-processing of raw sensor data (i.e., &quot;feature engineering&quot;) before machine learning can be applied successfully. Experts first apply a variety of signal processing techniques to sensor data in order to make machine learning feasible. Many people who would benefit from the ability to create custom gestural interactions lack the signal processing and programming expertise to apply those methods effectively or efficiently. It is not known how to successfully expose control over feature engineering to non-experts, nor what the trade-offs among different strategies for exposing control might be.

Our hypothesis is that it is possible to support creation of more complex gestural control and analysis systems by non-experts. We propose to develop and compare three methods for exposing control over feature engineering to non-experts, each requiring varying degrees of user involvement. 

The first method uses a fully automated approach, which attempts to computationally identify good features. The second method first elicits high-level information about the problem from the user, and then employs this information to better inform the automated approach. The third method directly involves the user in the feature engineering process. By leveraging users' ability to demonstrate new gestures, identify patterns in visualisations, and reason about the problem domain - as well as computers' ability to employ users' demonstrations to propose new relevant features - this interactive approach may yield more accurate recognisers. Such an approach may also help users learn about the utility of different features, enabling more efficient debugging of their systems and a better understanding of how to build other systems in the future. 

We are interested in understanding both the accuracy and usability of these methods. We will evaluate each method with people training several types of gesture and activity recognisers. We will compare each method in terms of the accuracy of the final system, the time required for users to build the system, users' subjective experiences of the design process and quality of the final system, and improvements in users' ability to reason about building gestural interactions with sensors. 

This research will enable a wider range of people to successfully use popular motion sensors to create bespoke gestural control and analysis systems, for use in a broader range of applications. Our techniques will be directly implemented in existing open-source software for interactive machine learning. The methods and study outcomes will also inform future work to support feature engineering for users creating real-time systems with other types of sensors.",,"This project puts at its heart user-centred design and stakeholder engagement, in order to ensure that benefits are recognised in a number of different target audiences. 

1. User groups in the general public:

New groups of people will gain the ability to create compelling, customised interactions with motion sensors, including:
- People with disabilities who can create new, customised gestural interfaces for controlling computers and games;
- Musicians creating new digital musical instruments;
- Athletes and trainers creating personalised performance monitoring and analysis systems;
- Artists creating new interactive experiences;
- &quot;Quantified self&quot; practitioners seeking new ways to understand or change their habits; 
- &quot;Hackers&quot; and &quot;makers&quot; developing new interactions with sensing hardware for practical or creative use. 

The ability to create more sophisticated gesture and activity recognition systems can enhance such people's quality of life, providing an increased sense of agency, new creative outlets, and a better understanding of themselves. We will integrate our techniques for end-user feature engineering directly into existing free software that provides GUIs for use by non-programmers. This means the above users will immediately be able to employ these techniques. 

2. Users in the creative and digital industries

Individuals who currently design interactions as part of their professional practice-including many artists, musicians, and indie game designers-will be able to use sensors more effectively and efficiently in their work. Other people who currently employ sensors for non-commercial, personal use, such as hackers/makers, may become more likely to commercialise their designs as the sophistication of interactions they can create improves.

3. School students using sensors to learn about computing. 

The introduction of 1 million BBC micro:bits into UK classrooms this year reflects recognition that work with sensors can engage students in learning computing, encouraging creative exploration of projects that connect computing to the physical world. Our project greatly expands the set of interactions that non-experts can easily build using micro:bit's accelerometers, and our findings will directly inform the design of future tools for students experimenting and creating interactions with sensors. The software tools released by our project will also be immediately usable by university students in physical computing classrooms who are building new interactions with sensors and platforms such as Arduino and Raspberry Pi. Increased student engagement with electronics presents opportunities for additional, longer-term economic benefits: students who build solid foundations in STEM skills will be better prepared to contribute to the 21st century workforce and to become technology innovators themselves."
23,A171F9D7-E04A-4AB6-9B6B-A9AB384C3629,PAMBAYESIAN: PAtient Managed decision-support using Bayesian networks,"Patients with chronic diseases must take day-to-day decisions about their care and rely on advice from medical staff to do this. However, regular appointments with doctors or nurses are expensive, inconvenient and not necessarily scheduled when really needed. Increasingly, there are low cost and highly portable sensors that can measure a wide range of physiological values. Can such 'wearable' sensors be used to improve the way that chronic conditions are managed? Patients could have more control over their own care if they wished; doctors and nurses could monitor their patients without the expense and inconvenience of visits, except when they are actually needed. Remote monitoring of patients is already in use for some conditions but there are barriers to its wider use: it relies too much on clinical staff to interpret the sensor readings; patients, confused by the information presented, may become more dependent on health professionals, whose work may be increased rather than reduced.

The project seeks to overcome these barriers by addressing two weaknesses of the current systems. First is their lack of intelligence. Intelligent systems that can help medical staff in making decisions already exist and can be used for diagnosis, prognosis and advice on treatments. One especially important form of these systems uses belief or Bayesian networks, which show how the relevant factors are related and allow beliefs, such as the presence of a medical condition, to be updated from the available evidence. However, these intelligent systems do not yet work easily with data coming from sensors. The second weakness is any mismatch between the design of the technical system and the way the people - patients and professional - interact. We will work on these two weaknesses together: patients and medical staff will be involved from the start, enabling us to understand what information is needed by each player and how to use the intelligent reasoning to provide it. The medical work will be centred on three case studies, looking at the management of rheumatoid arthritis, diabetes in pregnancy and atrial fibrillation (irregular heartbeat). These have been chosen both because they are important chronic diseases and because they are investigated by significant research groups in our Medical School, who are partners in the project. This makes them ideal test beds for the technical developments needed to realise our vision and allow patients more autonomy in practice.

To advance the technology, we will design ways to create belief networks for the different intelligent reasoning tasks, derived from an overall model of medical knowledge relevant to the diseases being managed. Then we will investigate how to run the necessary algorithms on the small computers attached to the sensors that gather the data as well as on the systems used by the healthcare team. Finally, we will use the case studies to learn how the technical systems can integrate smoothly into the interactions between patients and health professionals, ensuring that information presented to patients is understandable, useful and reduces demands on the care system while at the same time providing the clinical team with the information they need to ensure that patients are safe.

If successful, our results will be useful not only for the examples of chronic diseases studied on the project but also for managing other chronic medical conditions, when the same techniques can be applied. Although the project will produce prototype systems, several stages of product development and clinical trials will be needed before real systems are available for patients; we will prepare for these and make a first evaluation of the economic benefits of the proposed systems during the project. Also, several technology companies are involved in the project's Advisory Board to help ensure effective commercial exploitation in the long run.",,"Technology has the potential both to improve healthcare and reduce its cost, to the benefit of both patients and healthcare professionals. This project aims to develop a new generation of intelligent decision-support systems to work alongside the latest portable sensors so that patients can manage chronic medical conditions with efficient support from healthcare professionals. Achieving these benefits clearly relies on many strands of research (e.g. sensors, communications), but the contribution of this project will allow digital health technology developers to design systems that place less reliance on continuous supervision by health professionals and allow more patient autonomy.

The project's strategy for impact is: (i) early engagement with clinical stakeholders and patients, leading into clinical evaluation through a sequence of pilots and then trials, in parallel with (ii) progressive engineering refinement of the research prototypes developed within the project, in partnership with companies, all supported by (iii) attention to ethical and regulatory requirements and economic viability from an early stage so that these do not become barriers. This strategy will be applied firstly in the context of three medical case studies, in the management of rheumatoid arthritis, diabetes in pregnancy and atrial fibrillation but equally extends to the application of the technical advances to the management of other chronic medical conditions.

Early in the project, we will set-up clinical focus groups to ensure that technology solutions fit the actual problems. Initial engagement with patients and health care professionals will be around the case studies with patient advisory groups formed from existing PPI groups. Later, we will also engage with wider patient groups such as Patient Opinion, UK eHealth Association, Healthwatch and NHS Choices to explore the next stage of the translation of our research into practice. Agile and startup companies have an important role in introducing this disruptive technology, and a group of UK medical technology companies (BeMoreDigital, Mediwise, Rescon, SMART Medical, uMotif) as well as IBM UK and Hasiba Medical GmbH, have committed to advising the project (minimum 2 Advisory Group meetings per year) with a view to exploiting the technology. 

We will expand this with advice from our innovation team on IP, licensing and spin-outs. The project includes an initial study of the potential health economic impact in the case studies. To address the traditional 'regulatory barrier' we have made safety a core research objective and will progressively engage key regulators (e.g. the Information Governance Alliance) and the Caldicott Guardian and Digital Health lead for Barts Health, with whom initial contracts have already been made.
 
In addition to the medical beneficiaries, the project promises major impact in the data sciences with advances in methods for building and running Bayesian network (BN) models that combine expert judgment and data. Although many researchers and system developers already use BNs for decision-support and probabilistic analysis, more widespread use is limited by constraints on the size and complexity of models. The project will deliver a framework for integrating models at different levels of granularity, providing a practical way to build more complex models efficiently. Since current state-of-the-art BN technology has extremely limited support for the modelling concepts proposed the research will be of benefit to those applying BNs in other applications. The project will deliver open source code so that academic researchers and users can use the new techniques in their own work without restriction."
24,254B5547-6AEB-4A7F-8AD9-D54A2E9FAE18,Active Microscopy: Machine learning optimization of cell-based imaging microscopy.,"In cell biology, scientists investigate cells and cellular components to understand the biological mechanisms that enable these organisms to function. It is very common to grow cells in culture systems and then to stain certain parts of these cells with fluorescent or contrasting molecules. The staining procedure highlights specific parts of the cell or organism, which can then be subsequently viewed and imaged with a microscope. Microscopy experiments often require a skilled user to setup the measurements, but once configured, involve repetitive steps to be performed to complete the experimentation. Often cell biologists will spend a lot of their time manually seeking out the position of cells of interest (e.g. with specific properties) on their sample slide before imaging them. Cell biologists are highly trained individuals but often this procedure of image acquisition and optimisation can be time consuming and wasteful use of their time, and might also become objective. Could this process be automated? There have already been attempts to automate cell biology imaging assays. Unfortunately, these systems are designed to automate a particular type of imaging experiment and/or use specialised equipment that is not flexible enough to use for day-to-day prototyping of cell biology experiments. The purpose of this project is to integrate machine learning techniques, which are intelligent and adaptive, with conventional microscopy. Due to advances in the fields of computer science and artificial intelligence over the last 10 years, the technology required to achieve the proposed goals is very feasible. These algorithms that can learn from example will form the foundation of the approach. With minimal human guidance the proposed microscope system will be able to perform cell biology imaging experiments and the process will be fully logged, accountable and reproducible. The proposed system will be a significant contribution to the arsenal of tools which imaging scientists can use to perform their experiments and this is the perfect time to develop such a technology.","Machine learning (ML) is a sub-field of Artificial Intelligence (AI) and its purpose is to describe algorithms that can learn without being explicitly programmed. Due to recent advances in ML we are now at a stage whereby we can consider embedding AI into the equipment we use to support our productivity. In this project we propose to create a microscope system that is tightly integrated with state-of-the-art ML algorithms so that it will be able to autonomously perform imaging experiments. Using ML we can create a tool that is powerful and flexible and which can directly learn from human scientists.
 We will take an already existing microscope in the applicant's lab and propose to upgrade the stage, camera and illumination of the microscope so that the acquisition can be fully computerised and interfaced through a desktop computer. This computer will run LabView, a powerful hardware software interface and will also run python scripts that will facilitate the algorithmic control of the system and connect to the user interface. 
Our proposed algorithmic framework is divided into two parts (Assessment and Fine-Grain). For the Assessment, we intend to use a ML density estimation technique, which will coarsely locate the cells. This algorithm is implemented using fast decision tree ensemble methods. For the Fine-Grain classification we intend to use a combination of Convolutional and Recurrent Neural Networks that will learn to distinguish cells based on the association of structures within the cell and the human classification of the same cell. The user will show the system how to perform the experiment by selecting the first examples, from which the system will then learn and then will be able to perform the rest of the experimentation. The system's algorithms, which have been proven to work in other areas of computer science, are of great interest to the community and resulting findings will not only inform the biomedical community but the ML community also.","Our project will have a considerable impact on the microscopy, cell biology and biophysics academic communities. Although automated microscopes have been created in the past, this is the first time that a substantial amount of artificial intelligence (AI) has been applied to control a conventional microscope. Because of this, we expect the project to be appealing not only as a cutting-edge piece of research but also as a tool that can be leveraged in many different projects due to the fields in which our collaborations reside (e.g. peroxisome, immunology, cancer and haematology fields). Future implementation of our Active Microscopy approach on microscopes within the host institutes open-user image facility will make it accessible to a broad range of researchers within Oxford and UK-/world-wide. As a result, we expect this project to have a substantial impact on the community within Oxford, nationally and also internationally. The developments of this project will be communicated through peer-reviewed publications, conferences and through social media. Both Prof. Christian Eggeling (CE) and Dr. Dominic Waithe (DW) have a long history of publishing in these domains.
Because we are focusing on the acquisition phase of the experimentation and this involves directly interacting with microscopy hardware we expect that this research will be of interest to microscopy vendors. As part of this project we have established a project-specific collaboration with PicoQuant and we also expect interest with other microscopy companies such as Zeiss, Leica, Nikon and Olympus who are active in this domain and with whom we have collaborations. Furthermore we propose to develop a novel augmented reality system for the microscopes that we expect will have a big impact on the microscopy community as a whole due to its potential for improving user-interactivity. CE has significant experience working with microscopy companies and both CE and DW will communicate the developments of this project directly with these companies and through conference portals.
We expect this project to also have an impact on the AI academic research community. The algorithms we are using are cutting edge and because of the synergies this approach has with work in other areas we expect to make an impact in many sub-fields of machine learning discipline. DW already has a record of publishing within the machine learning community and will continue to do so. Because we are using cutting-edge machine learning algorithms which are being actively applied in other domains, we expect that this project will have a significant impact on companies which focus on applying machine learning in commerce and media (e.g. Microsoft, Alphabet DeepMind and also Facebook). These companies are interested in accessing new markets and so it is likely that this project will be of interest. We will communicate our findings to these companies through our academic connections with these firms as well as at biophysics, microscopy, computer vision and machine learning conferences where there is an opportunity to meet with the vendors.
Besides using conventional academic portals to ensure that our research is communicated we are also going to make use of social media. DW has significant experience using social media and has some 300,000 views for his scientific communication Youtube account and also maintains a popular Twitter profile. Furthermore DW also maintains an active Github and Quora account which can also be leveraged to share the findings of this project and to answer questions from interested parties. We intend to develop several short movies that will document and highlight the various aspects of this work and then will publicise these using all of the mentioned social media platforms along with CEs group page (http://www.nano-immunology.org/). AI and robotics is currently a very popular topic on the internet and so we are sure to have some impact in these arenas with our project."
0,66533EBD-29F1-4091-BCDC-325DA9B53D2C,Number Understanding Modelling in Behavioural Embodied Robotic Systems,"Mathematical competence can endow robots of the necessary capability for abstract and symbolic processing, which is required for improving their cognitive performance and their social interaction with human beings. But so far, only few attempts have been made to apply mathematical cognition in robots.

The objective of the NUMBERS project is to construct a novel artificial cognitive model of mathematical cognition by imitating human like learning approaches for developing number understanding. This will provide a novel tool for developmental psychology and neuroscience research on the development of mathematical abilities in children. 

The objective will be achieved through a highly interdisciplinary research program that will take advantage of the collaboration of leading academics in the fields involved, prof. Cangelosi (Cognitive Developmental Robotics), Plymouth University, and prof. McClelland (Computational Psychology), Stanford University, USA, and the technical support of an industrial partner (NVIDIA Corporation, USA and UK).

The NUMBERS' research activities will exploit the cutting-edge facilities offered by Sheffield Robotics, a joint initiative between the University of Sheffield and Sheffield Hallam University, which will host the project. Indeed, the model will be integrated with one of the most advanced child-like robotic platform (the &quot;iCub&quot;) and, therefore, validated through realistic experiments, resembling scientific experiments of mathematical cognition in children. 

The validation of the novel model of the numerical cognition in interactive robotic experiments will constitute a proof of concept of the enhanced capabilities offered by a modular approach to bio-inspired artificial intelligence architectures. Furthermore, an optimised implementation for mobile devices will be realised in order to downsize space and power requirements for the computation, increasing application opportunities.

This foundational research will provide the methodological basis and cognitively plausible engineering principles for the next generation of socially interactive robots, mimicking advanced capabilities of the human intelligence for real understanding and interaction with the external world. Results will help the design of more efficient cognitive robotic systems capable of learning abstract symbolic number processing in a more flexible and ecological manner.

The human-like learning and interaction are characteristics that might allow people to more easily identify the desired social overture that the robot is making, or facilitate the transfer of skills learned in human-human interactions to human-robot encounters. This envisioned humanization will positively affect the acceptance of robots in social environments, as they will be perceived as less dangerous, increasing the socio-economic applications of future robots that can take on tasks once thought too delicate or uneconomical to automate. This is particularly relevant in the fields of social care, companionship, therapy, domestic assistance, entertainment, and education.",,"The NUMBERS project is based on a highly interdisciplinary approach and has an important scientific and technological bearing that will have a significant impact in the technological fields (e.g. service robotics and HRI for interactive systems, GPU accelerated parallel computing, robotics engineering) covered by the project. 

From a socio-economic point of view, the technological advances expected will facilitate the design of novel control systems suitable for a wide range of commercial interactive system applications (e.g. social robots, smart intelligent devices, speech interfaces, autonomous and intelligent-assisted transport vehicles). Indeed, the NUMBERS project aims to be one seed for future UK industrial leadership in the emerging social robotics industry, as human-like cognition and interaction will drastically increase the acceptance of robots in social environments, widening future applications, such as care for elderly, child education, entertainment, and domestic use.
In particular, in the field of social care and innovation, the project foresees synergies with the H2020 MSCA CARER-AID project, which has the same host institution (SHU) and supervisor (PI). These synergies will allow extending the contribution to the assessment and therapeutic technologies for cognitively disabled children.

From an industrial point of view, the project will also contribute towards widening the application of deep learning technologies that are currently one of the most vital pieces of work in computing industry of today. Indeed, top industry players like NVIDIA, Google, and Microsoft are strongly investing in expanding this technology for their future products. The sponsorship for the NUMBERS project by the NVIDIA Corporation is evidence of the interest and offers a great opportunity for collaborating towards the development of a profitable technology for the future robotics applications in social environments. In addition, embedded microchip design houses, such as ARM Ltd, can profit from the targeted optimisation of the model for portable devices, which can be more likely to be integrated into mobile socially interactive robotic platforms.

For knowledge transfer into industries, the PI will communicate the project ideas and results to the NVIDIA GTC conferences which can allow for extending the project's impact by interacting with software developers, researchers, and technologists from some of the top companies, universities, research firms, and government agencies from around the world. The PI will organise an Industrial Open House, inviting delegates from NVIDIA Corporation, ARM Ltd and UK robotics companies, e.g. RU Robots Ltd, AA Robotics, Engineered Arts Ltd, and Shadow Robotics.

Public Engagement is one of the objectives of the present project, as more information is needed to overcome the scepticism that currently regards the use of robots in the social context. This will be achieved taking all the available opportunities, such as SHU open days, Sheffield Robotics outreach events and during the UK robotics week.

A dedicated project website will be accelerating dissemination and communication away from academic publications and public events to all relevant parties. An extensive online resource will include videos of the experiments and demonstrators of the advanced robot behaviours, presentations, highlights from publications. The software developed within the project will be available open source through the project website."
1,9919DF67-E136-46D8-B2A7-95B048CCD4A3,TRANSIT: Towards a Robust Airport Decision Support System for Intelligent Taxiing,"There is an imminent need to make better use of existing aviation infrastructure as air traffic is predicted to increase 1.5 times by 2035. Many airports operate at near maximum capacity, and the European Commission recognises the necessity to increase capacity to satisfy demand. In addition, inefficient operations lead to delays, congestion, and increased fuel costs and noise levels inconveniencing all stakeholders, including airports, airlines, passengers and local residents. 

A critical issue is routing and scheduling the ground movements of aircraft. Although ground movement is only a small fraction of the overall flight, the inefficient operation of aircraft engines at taxiing speed can account for a significant fuel burn. This applies particularly at larger airports, where ground manoeuvres are more complex, but also for short-haul operations, where taxiing represents a larger fraction of an overall flight. It is estimated that fuel burnt during taxiing alone represents up to 6% of fuel consumption for short-haul flights resulting in 5m tonnes of fuel burnt per year globally. This project aims to investigate a decision support system which considers multiple factors to provide more robust taxiing routes. 

Current decision support systems for routing and scheduling taxiing aircraft suffer from several limitations:

1) The only objective they consider is minimising taxi time, ignoring other important factors. These other factors include taking into account engine performance which is linked to fuel consumption, environmental impact and cost. Routes and schedules, which are efficient in terms of fuel and cost, are therefore compromised as a result of considering a one dimensional objective.

2) Airframe dynamics are not taken into account during planning of routes and schedules. Consequently, the taxing instructions issued may be hard to follow, making compliance with the allocated routes unrealistic.

3) Taxi time is typically based on average speeds of aircraft. This is an over-simplification meaning that any taxiing manoeuvre which falls outside the expected duration can affect the taxiing of other aircraft. Furthermore, if the approach of including overly conservative time buffers to absorb uncertainty is adopted, the resulting overall airport operating efficiency will be degraded.

4) It is difficult to specify taxiing speeds and heuristic rules for routing and scheduling systems as: they depend on airport layout and operational requirements, which can vary throughout the day according to the volume of air traffic. Consequently, routing and scheduling systems have to be reconfigured for specific airports and operational constraints.

5) Due to variability in taxi speed and over-simplistic models of aircraft, there is lack of understanding as to how much benefit can be achieved by automated routing and scheduling in real-world settings. 

TRANSIT will directly address these limitations of current systems, to make better use of existing airport infrastructure and lessen the impact of the growing aviation sector on the environment. Multi-objective optimisation algorithms will be integrated with models of aircraft to balance the reduction of taxi time, cost and emissions. We aim to make the routing and scheduling system easily reconfigurable to any airport. The uncertainty will be directly incorporated in planning, resulting in robust taxiing, verified by pilot-in-the-loop trials.

TRANSIT aims to investigate such a system and its associated benefits in collaboration across a broad range of disciplines and fields (Engineering, Operational Research, and Computer Science) needed to tackle such challenging problem. Cooperation with leading industrial stakeholders, and consultation with established academics, ensure that the work is cutting edge while reflecting needs of the industrial partners.",,"The immediate impact of TRANSIT includes better understanding of causes, behaviour and consequences of uncertainties, and the dynamic nature of ground movement obtained by the analysis of real-world data and simulation. Such knowledge can be used in short term (1-5 years) by airports/airlines in their day-to-day planning. However, the vision of TRANSIT, through the investigation of modelling techniques and optimisation methods, is to develop a basis for future decision support and flight deck automation systems for ground movement. Such a system, developed and implemented in the long term (5-15 years) will be of benefit to industry, environment and society. Pathways to impact are designed to deliver impacts by exchanging knowledge between academics and industry, educating the next generation of researchers, exploring future research directions, delivering public awareness, and in particular fostering economy performance and improving society in the following areas:

Quality of life: Taxi time will be optimised considering uncertainty in times, detailed taxi speeds and other operating conditions producing two benefits: 1) more precise and robust taxi schedules, which will reduce the chances of congestion and therefore subsequent delays; 2) Reducing the time spent on taxiing. Both of these benefits will contribute to the quality of life of passengers as they pass through the airport. 

Environment: Optimising airport ground movement with regard to fuel consumption will decrease the amount of fuel burnt during taxiing, resulting in lower emission of greenhouse gases and associated pollutants in the immediate vicinity of airports. This is an important consideration as, while taxiing is only a small portion of the overall journey, jet engines burn very inefficiently at low speed and therefore make a substantial contribution to the total emissions.

Health: Reduced taxi time and optimised aircraft engine performance, means aircraft engines are running for a shorter period of time with lower fuel consumption, decreasing noise and pollutants, benefiting residents in the immediate vicinity of airports.

Policy: The environmental impact of the proposed research directly helps the UK to fulfil its national and international commitments. Decrease in the emission of greenhouse gases aligns with the Climate Change Act 2008, which aims for the net UK carbon account for 2050 to be 80% lower than 1990. Furthermore, the European White paper 'Roadmap to a Single European Transport Area' calls to reduce greenhouse gas emissions to 20% of 2008 levels, and Flightpath 2050 envisions emission-free taxiing by 2050. 

Cost reduction for passengers, airlines and airports: Decreasing the number and length of delays and dynamic decision making for different airport operational scenarios will have a direct impact on reducing costs, in terms of wasted time or missed connections for passengers, and in terms of costs of using airport infrastructure, aircraft and crew costs for airlines. Preliminary studies on Active Routing framework indicate a reduction of up to 50% in both taxi time/fuel consumption. 

Competitiveness of air transport industry: Minimising transit time, fuel consumption and costs are key factors in an already highly competitive industry. Not only is it important for the aviation industry to remain competitive with alternative modes of transport, but also it should provide a reliable service, as a flight is part of a passenger's overall journey. Therefore, reducing delays at an airport is an important milestone in effectively transporting passengers from door to door, over an ever more interconnected transportation network.

Safety: While conflicts between taxiing aircraft usually do not pose a serious safety hazard, they result in costly damage and interrupted operation. By providing largely conflict-free taxi routes, generated by the proposed optimisation framework based on full-4DTs, this risk can be substantially reduced."
2,262A2FE7-FF9B-49B4-BB43-5B3F57BEC8F0,Learning to Efficiently Plan in Flexible Distributed Organizations,"Teams of robots are expected to revolutionise industry and other other parts of society. However, decision making in such so-called multiagent systems (MASs) under uncertainty is computationally very complex. The decentralized partially observable Markov decision process (Dec-POMDP) framework facilitates principled formulation of such decision making problems, but currently there are no scalable solution methods that provide guarantees on task performance. To simplify coordination in MASs, agent organisations assign an abstracted, easier problem to each agent. Typically only the most rigid organisations, which completely decouple the agents, have led to clear computational benefits. However, these come at the expense of task performance: full decoupling means that agents can no longer collaborate to divide the workload. 

This project will focus on flexible distributed organisations (FDOs) for Dec-POMDPs, which restrict considered interactions to spatially nearby agents without imposing full decoupling. Currently no scalable decision making methods with guarantees on task performance exist for FDOs: the main goal of the project is to develop such methods along with the theory that supports their formalisation. To accomplish this goal, it will investigate the use of deep learning techniques to learn representations of 'influence' in FDOs and use those representations to develop novel planning methods. If successful, this will provide the proof-of-concept that learned influence representations can enable principled decision making in large-scale MASs. This will be the basis for a larger research program investigating such influence representations for different forms of abstraction and will spark applied research that investigates deployment of the developed algorithms in real robotic teams.",,"This is a relatively short project that pursues basic research in the field of AI. As such, the expected short-term impact will mostly be in the form of knowledge (developed techniques), the scientific output (articles and software) and the influence on research questions picked up by peers (of which citations to those output are an indication). These academic impacts are a crucial link in the pathways to longer-term impact on society and economy. In particular, the project aims to make simulation-based planning dramatically more efficient, and thus more effective or even feasible in cases where it was not before. This could have a great mid-to-long-term impact since the potential of application of these methods is huge; they are advanced versions of model-predictive control (MPC) methods which are widely applied in industry. The recent application of simulation-based planning in the mastering of the game of Go is likely to attract attention in many areas. 

Another reason that simulation-based planning methods have not seen more application yet is their large computational costs. However, this is precisely what we address for the class of problems that admit flexible distributed organizations. For such problems, the proposed research will have a crucial impact in terms of just making it feasible at all to apply simulation-based planning to these domains. For instance, we expect that this could be the case for robotic teams collaborating in future factories or warehouses, but also for other problems that can be modelled as spatial task allocation problems, such as dispatching of emergency vehicles, or for applications areas such as optimizing traffic control by simulating large traffic networks, optimizing routing policies by simulation of communication networks, optimizing UAV patrolling policies in security domains or for law enforcement, etc. 

Therefore, a large part of the longer term impact will be of the economic kind (new companies, improved products and services) with, given the possible application areas, the potential to improve quality of life. The first companies to adopt these techniques will be logistics and manufacturing companies since these already are starting to use robotic teams. Many of the other aforementioned applications (UAV patrolling etc.) will probably need more time and are likely to be tackled by AI and robotics start-up companies. The discipline of high-performance computing (HPC) is involved in optimizing both hardware and software, e.g., to make simulations maximally efficient. The basic idea put forth in this proposal---that one can mostly use 'local model simulations' that consider only a small subset of the variables considered in 'full model' simulations, while still converging to the same behaviour---could potentially make an impact on this community, and thus may affect the design of computers dedicated to doing simulations. 

I will contribute to the impact of the developed methods by contributing to awareness (interacting with potential industrial partners, as well as with academic peers, and organising a workshop), clarifying potential gains (an expected output of the research), and facilitating adoption by releasing open source software."
3,83E2167C-4277-40F3-8B57-CA8DA49F43A5,TRANSIT: Towards a Robust Airport Decision Support System for Intelligent Taxiing,"There is an imminent need to make better use of existing aviation infrastructure as air traffic is predicted to increase 1.5 times by 2035. Many airports operate at near maximum capacity, and the European Commission recognises the necessity to increase capacity to satisfy demand. In addition, inefficient operations lead to delays, congestion, and increased fuel costs and noise levels inconveniencing all stakeholders, including airports, airlines, passengers and local residents. 

A critical issue is routing and scheduling the ground movements of aircraft. Although ground movement is only a small fraction of the overall flight, the inefficient operation of aircraft engines at taxiing speed can account for a significant fuel burn. This applies particularly at larger airports, where ground manoeuvres are more complex, but also for short-haul operations, where taxiing represents a larger fraction of an overall flight. It is estimated that fuel burnt during taxiing alone represents up to 6% of fuel consumption for short-haul flights resulting in 5m tonnes of fuel burnt per year globally. This project aims to investigate a decision support system which considers multiple factors to provide more robust taxiing routes. 

Current decision support systems for routing and scheduling taxiing aircraft suffer from several limitations:

1) The only objective they consider is minimising taxi time, ignoring other important factors. These other factors include taking into account engine performance which is linked to fuel consumption, environmental impact and cost. Routes and schedules, which are efficient in terms of fuel and cost, are therefore compromised as a result of considering a one dimensional objective.

2) Airframe dynamics are not taken into account during planning of routes and schedules. Consequently, the taxing instructions issued may be hard to follow, making compliance with the allocated routes unrealistic.

3) Taxi time is typically based on average speeds of aircraft. This is an over-simplification meaning that any taxiing manoeuvre which falls outside the expected duration can affect the taxiing of other aircraft. Furthermore, if the approach of including overly conservative time buffers to absorb uncertainty is adopted, the resulting overall airport operating efficiency will be degraded.

4) It is difficult to specify taxiing speeds and heuristic rules for routing and scheduling systems as: they depend on airport layout and operational requirements, which can vary throughout the day according to the volume of air traffic. Consequently, routing and scheduling systems have to be reconfigured for specific airports and operational constraints.

5) Due to variability in taxi speed and over-simplistic models of aircraft, there is lack of understanding as to how much benefit can be achieved by automated routing and scheduling in real-world settings. 

TRANSIT will directly address these limitations of current systems, to make better use of existing airport infrastructure and lessen the impact of the growing aviation sector on the environment. Multi-objective optimisation algorithms will be integrated with models of aircraft to balance the reduction of taxi time, cost and emissions. We aim to make the routing and scheduling system easily reconfigurable to any airport. The uncertainty will be directly incorporated in planning, resulting in robust taxiing, verified by pilot-in-the-loop trials.

TRANSIT aims to investigate such a system and its associated benefits in collaboration across a broad range of disciplines and fields (Engineering, Operational Research, and Computer Science) needed to tackle such challenging problem. Cooperation with leading industrial stakeholders, and consultation with established academics, ensure that the work is cutting edge while reflecting needs of the industrial partners.",,
4,0D230C12-978C-41E4-80A2-C39C4F9D18B0,Understanding infants' curiosity-based exploration,"As parents know, babies are curious learners. The vast majority of infants' time is spent freely exploring (Oudeyer &amp; Smith, in press), at home, at nursery or at playgroup. By sampling their learning environment based on their own curiosity infants quickly acquire two fundamental components of cognition without which they would never engage effectively with the world: categories and words. Understanding early category learning and how it interacts with word learning is therefore critical to teasing apart the complex processes by which infant cognition develops into an adult-like understanding of the world. However, although curiosity-driven exploration accounts for almost all of infants' experience, our understanding of category and word learning comes almost entirely from tightly-controlled, highly structured experiments. Decades of elegantly-designed studies show that these interacting phenomena are exquisitely sensitive to features of the environment (e.g., Younger, 1983; Plunkett, Hu &amp; Cohen, 2008; Quinn, Eimas &amp; Rosenkrantz, 1993). Because these experiments typically take place in artificial laboratory conditions, however, we do not know how babies themselves choose to learn, and as a consequence, we do not know the best way to help them do so.

Here I propose the first studies of infants' curiosity-based exploration and word learning. Using cutting-edge head-mounted eyetrackers I will record typically-developing infants interacting freely with objects, generating the first detailed description of curiosity-based exploration and laying the foundation for future research in atypical exploration. Objects will be custom-designed to vary systematically (e.g., in shape), allowing me to record exploratory sequences to reveal what level of complexity infants prefer to learn from. Half the infants will hear labels for the objects and be tested on their word learning, revealing how categorisation and word learning interact in an infant-centred, rather than adult-designed, environment. 

For a complete understanding of infants' exploration, however, we not only need to know what infants do, but also how they do it: what mechanisms drive curiosity? Computational models can clarify the cognitive processes underlying a behaviour (McClelland et al., 2010). However, as yet we have no model of infants' curiosity-driven exploration and word learning. Based on my existing modelling work (Twomey &amp; Westermann, 2015), I will develop the first testable, mechanistic theory of curiosity-based exploration and word learning in infants.

As the first investigation of human infants' curiosity-driven category and word learning this research has clear academic impact (papers, conferences, future studies of atypical exploration, collaborations). It also has societal impact: my findings will inform policymakers' understanding of development, help designers create evidence-based books and toys that facilitate learning, and equip parents and early years practitioners with the knowledge they need to support babies' cognitive development. There will be numerous opportunities to build networks and develop my knowledge exchange skills (e.g., data sharing, publications, conferences, talks) and public engagement experience (e.g., writing news articles, organising public engagement events). In parallel I will undertake a programme of researcher and Principal Investigator development at my RO. To cement my position as an innovator in curiosity research I will visit the INRIA Research Institute to learn to implement curiosity-based learning in the Poppy humanoid robot, becoming the first UK researcher to use Poppy and providing scope for designing future studies far beyond the current work. Finally, recruiting and supervising a research assistant will develop my leadership and mentorship skills. Overall, this project will not only prepare me for founding my own group, but will also secure my unique position as an international leader in curiosity research.",,"Who will benefit from the proposed project?

Understanding infants' exploration and its relationship to language acquisition is critical to our understanding of development more broadly, yet infants' curiosity has not been studied. By examining how infants drive their own learning the proposed work will be of substantial benefit to parents, policymakers, early years educators, industry and academia.

How will these communities benefit?

- Parents. The findings will alert parents to the fact that infants are self-driven learners and that the environment in which this learning takes place is important. This take-home message, as well as more detailed findings during the course of the research, will be communicated to parents visiting the Babylab to take part in the empirical studies, via post-study debriefing and printed/online materials, providing them with tools - and confidence - they need to successfully and independently support their babies' development and language learning.

- Policymakers and early years educators. &quot;Planned, purposeful play&quot; is at the core of the Department for Education's Statutory Framework For The Early Years (2014, p. 9), despite a lack of understanding of the mechanisms underlying this exploration. The planned work defines a pathway to developing principled, evidence-based guidelines for supporting learning through play.

- Members of the public. I will disseminate this work in a range of online and face-to-face venues. Online articles (e.g., The Conversation) and blog posts will discuss my findings and their implications for our understanding of development. Parents' days, science exploration days (with Science From The Start) drop-in sessions and nursery visits in the local Lancaster community will increase the visibility of developmental research and provide opportunities for the end-users of this research (i.e., parents) to speak directly to researchers in an informal setting. Overall, this research will bring about an increase in public understanding of infant development and language acquisition.

- Industry. A wealth of toys, books and apps marketed as &quot;educational&quot; are targeted at the parents of babies and toddlers, but as yet none are designed based on scientific evidence. Findings from this project will enable the design of books, apps and toys which provide optimal learning opportunities for a given age group.

- Academia. The proposed work will result in a substantial theoretical advance in developmental psychology as well generating the first knowledge base on infant's curiosity-driven exploration and word learning. It will define a new interdisciplinary field and as such will generate strong interest not only in psychology, but also in computational modelling and developmental robotics. Cross-disciplinary collaborations with roboticists at Aberystwyth University, UK, and INRIA, France, will drive forward our understanding of how artificial intelligence agents can benefit from incorporating insights from developmental psychology. These links will be strengthened and new collaborations inspired by an interdisciplinary curiosity symposium to be presented at a prestigious international conference. The research will also provide a baseline measure of curiosity-driven exploration in typically-developing infants, providing a vital comparison sample for researchers in atypical development."
5,7557D2C1-5E84-41F7-8548-D228D2BF53B9,EthicalML: Injecting Ethical and Legal Constraints into Machine Learning Models,"Our choice as to which movies to watch or novels to read can be influenced by suggestions made by machine learning (ML)-based recommender systems. However, there are some important scenarios where ML systems are deficient. Each of the following scenarios involves a situation where we wish to train an ML system so that it delivers a service. In each case, however, there is an important constraint that must be imposed on the operation of the ML system.
Scenario 1: We want a system that will match submitted job applications to our list of academic vacancies. The system has to be non-discriminatory to minority groups. 
Scenario 2: We need an automated cancer diagnosis system based on biopsy images. We also have HIV test results, which can be used at training time but should not be collected from our new patients.
Scenario 3: We wish to have a system that can aid us in deciding whether or not to approve a mortgage application. We need to understand the decision process and relate it to our checklist such as whether or not the applicant has an overdraft in the last three months and is on electoral roll. 

Scenario 1 asks an ML system to be fair in its decisions by being non-discriminatory with regards to, e.g., race, gender, and disability; scenario 2 requires an ML system to protect confidentiality of personal sensitive data; and scenario 3 demands transparency from an ML system by providing human-understandable decisions. 

Equipping ML models with ethical and legal constraints, scenarios 1-3, is a serious issue; without this, the future of ML is at risk. In the UK, this is recognized by the House of Commons Science and Technology Committee, which recommended an urgent formation of a Council of Data Ethics (&quot;The Big Data Dilemma&quot; report, 2016). Furthermore, since 2015, the Royal Society has started a policy project that looks at the social, legal, and ethical challenges associated with advancement in ML models and their use cases.

Building ML models with fairness, confidentiality, and transparency constraints is an active research area, and disjoint frameworks are available for addressing each constraint. However, how to put them all together is not obvious. My long-term goal is to develop an ML framework with plug-and-play constraints that is able to handle any of the mentioned constraints, their combinations, and also new constraints that might be stipulated in the future.

The proposed ML framework relies on instantiating ethical and legal constraints as privileged information. This privileged information is available at training time to better train a decision model and to make a decision model non-discriminatory, but it will not be accessible for future data at deployment time. For confidentiality constraints, personal confidential data such as HIV test results are the privileged information. For fairness constraints, protected characteristics such as race and gender are the privileged information. For transparency constraints, complex un-interpretable but highly discriminative features such as deep learning features are the privileged information.

This project aims to develop an ML framework that produces accurate predictions and uncertainty estimates about its predictions while also complying with ethical and legal constraints. The key contributions of this proposal are: 1) a new privileged learning algorithm that overcomes limitations of existing methods by allowing to plug-and-play various constraints at deployment time, by being kernelized, by optimizing its hyperparameters, and by producing estimates of prediction uncertainty, 2) a scalable and automated inference that makes the new privileged learning algorithm easily applicable for any large scale learning problem such as binary classification, multi-class classification, and regression, and 3) an instantiation of the new algorithm for incorporating fairness, confidentiality, and transparency restrictions into ML models.",,"Advancement in ethically and legally aware machine learning (ML) models has broad implications. In this project, I will focus on engaging with users in the following domains:
1. Predictive Policing
Predictive policing refers to &quot;computer systems that use data to forecast where crime will happen or who will be involved&quot; (Upturn's report, 2016). In the UK, the effectiveness of predictive policing is widely reported, for example, Strathclyde Police cited a reduction in domestic violence reoffenders (Joe Newbold's report in 2015). The fact that predictive policing technologies rely on historical and inherently biased crime data to build ML models raises several ethical and legal concerns such as fairness and transparency. Upturn's 2016 report on &quot;Early Evidence on Predictive Policing and Civil Rights&quot; concluded that the predictive policing tools, which are currently designed and implemented, reinforce discriminatory policing practices. This is a serious issue that puts the future of predictive policing at risk despite its success in reducing crime. This project will develop an ML model that corrects past biases via non-discriminatory fairness constraints.

2. Healthcare Analytics
For health workers, hospitals and governmental decision makers, it is extremely useful to have tools to predict healthcare problems such as maternal mortality rates and cancer. The AI and Life in 2030 Report stressed that ML-driven applications need to &quot;gain the trust of doctors, nurses, and patients&quot;. The proposed ML framework acknowledges the need to use patient confidential data only in a strict need-to-know basis and to not use them in the deployed system. Furthermore, the transparency constraints will aid health professionals in their decisions and will steer clear of the statement &quot;because the computer said so&quot;.

3. Improving the skills base
&quot;The Big Data Dilemma&quot; report has urged immediate action to tackle the crisis of data analytics skills. The appointed PDRA will develop skills and experience in data analytics, collaboration, scientific and public presentations, and organization of workshop and stakeholder meetings.

4. General public
This project will have societal impact by informing both ML enthusiasts and sceptics about the reality that ML technologies have permeated our everyday life, and that there is an active push within the ML community to develop models that respect ethical and legal constraints.

Although not a direct focus, I recognize the long-term implications of the study in the following areas, and will be alert to any opportunity for establishing links for future actions:

5. Human Resource Analytics
Companies and universities use ML models on candidates' background information (e.g. application form data including disability) and employee data to predict whether this candidate should be hired.

6. Mortgage Approval
Similarly, lenders use ML models on borrowers' background information, including licensed data such as credit score information, to predict whether it is risky to extend a mortgage offer.

7. Insurance Premium Setting
Also, insurance companies use ML models on applicants' driving history and biographical data to predict the driver type of an applicant, and subsequently to set an insurance premium accordingly.

An algorithmic assessment method, which is used for predicting human outcomes such as recruitment (5), loan approval (6), and insurance premium (7), contributes to a world with decreasing human biases. To achieve this, however, we need advanced ML models that are free of algorithmic biases (fairness), despite the fact that they are trained based on historical and biased data. Additionally, the deployed models should not collect personal sensitive data (confidentiality). Furthermore, in an interactive mode, where humans can check the computer's judgment, understanding the reasons behind predictions made by ML models (transparency) is a prescription for improved collaborative decisions."
6,93B54102-7A06-4880-8716-50CD5BF987D7,An engineering platform for rapid prototyping synthetic genetic networks,"Synthetic biology is an exciting new discipline which offers the potential to bring many benefits to human health and welfare. One near-market example is the use of engineered genetic networks to make biological sensors, or biosensors, which can rapidly detect toxins and harmful microorganisms. However, most synthetic biology systems are based on living genetically modified cells, and due to safety concerns and regulatory issues, they can not be used outside of a specially approved laboratory, whereas the greatest unmet need for biosensors is in the field, for 'point-of-use' and 'point-of-care' tests for health hazards. The laboratory of Professor James Collins recently reported a remarkable breakthrough, using non-living biological systems based on genetic components dried onto strips of paper. These systems can be prepared very cheaply, can be stored stably for long periods, and, since they are not alive and can not replicate, they pose no risks to the environment. This technology is therefore ideal for further development of sensors for human health. 

In addition, these cell-free systems can be prepared in large numbers very rapidly, in a matter of hours, and tested rapidly, in a matter of minutes, whereas living cell based systems may take weeks to prepare and days to test. This makes the new technology ideal for 'rapid prototyping' of genetic circuits. Many designs can be rapidly generated and tested, and the most successful can then be used to generate cell-based systems for applications where this is required, such as engineered metabolic pathways for manufacturing pharmaceuticals and other valuable compounds. 

In this project, we will further develop these remarkable systems and create new tools which will make it even easier to design and develop them. Firstly, we will create new computational tools which can be used to design genetic circuits for many applications. These will be made available on-line for the benefit of the research community. Secondly, we will establish methods for rapid automated assembly and testing of new circuits, allowing many thousands of variants to be generated and tested in a very short time with minimal human effort. Thirdly, we will seek to improve the basic technology, to improve the performance of the cell-free devices, and also develop low cost open-source electronic readers which can easily be used in the field along with the sensors we develop. Fourthly, we will demonstrate the usefulness of the technology by generating sensors which can rapidly and sensitively detect various external inputs. All of our new inventions will be made available to the research community. 

In addition to the other advantages mentioned above, this technology also makes it easy for users to develop their own assays simply by adding appropriate DNA components to a basic mixture, using standard protocols. Such devices can be manufactured and distributed cheaply on a very large scale. In conjunction with low-cost readers, ubiquitous mobile devices equipped with GPS and time data, and cloud-computing, this will offer the possibility to detect health hazards with unprecedented levels of speed and detail, with potentially huge effects on human health and welfare. Furthermore, these devices are ideal for use in education, allowing users to design and test their own genetic circuits without the issues inherent in using living cells. For these reasons, our proposal offers tremendous benefits and represents a step change in the real-word applicability of synthetic biology.",,"The success of this proposal will provide an integrated platform for rapid prototype synthetic genetic networks, in particular these bespoke cell-free sensors. This targets several strategic priorities of EPSRC: 1) to be a Productive Nation by enabling researchers in both academia and industrial to more effectively design and manufacture synthetic genetic networks for various applications, 2) to be a Resilient Nation by enabling rapid response to emerging disease outbreak (the Collins lab has demonstrate it takes hours to design and manufacture novel cell-free biosensors for Ebola and Zika viruses), 3) to be a Healthy Nation: the technology platform developed in this proposal can be used to develop genetic networks for pathogen detection, pollutant detection, and bacterial food-borne illness as well as zoonotic sensors for influenza and determinants of antibiotic resistance.


In this project, we will develop an integrated platform based on linguistic models to constraint viable design space and couple it with the cell free systems to enable researchers to rapid prototype genetic networks. Firstly, we will create new computational tools which can be used to design genetic circuits for many applications. These will be made available on-line for the benefit of the research community. Secondly, we will establish methods for rapid automated assembly and testing of new circuits, allowing many thousands of variants to be generated and tested in a very short time with minimal human effort. Thirdly, we will seek to systematically characterize more genetic parts to capture the functional attributes, and these characterization data will be invaluable for designing synthetic circuits. In addition to the other advantages mentioned above, this technology also makes it easy for users to develop their own assays simply by adding appropriate DNA components to a basic mixture, using standard protocols. Furthermore, these devices are ideal for use in education, allowing users to design and test their own genetic circuits without the issues inherent in using living cells. For these reasons, our proposal offers tremendous benefits and represents a step change in the real-word applicability of synthetic biology.

The research described here will be of both immediate and long-term benefit in many areas of research. The most immediate impact will be for researchers working on development of biosensors and other genetic networks. By providing a standardized set of well-characterized tools for rapid generation and characterization of genetic networks, our work will greatly facilitate activity in this area. In particular, our work will allow rapid automated assembly and prototyping of genetic networks with wide applicability in all areas of synthetic biology, including healthcare and public health applications, food safety and security, as well as applications in industrial biotechnology, such as conversion of renewable biomass to useful products, which is essential for development of a sustainable bio-economy"
7,6CC9A2B4-BFF8-4CE2-91A1-E7526252B1CD,Automated Software Specialisation Using Genetic Improvement,"This fellowship will change the face of software development by transferring the challenging and time-consuming task of software specialisation from human to machine. It will develop novel approaches for specialising and improving efficiency of generalist software for particular application domains in an automated way. The developed techniques will be program-agnostic and thus applicable to any type of software. Therefore, they will allow to speed-up computationally intensive calculations that arise, for instance, in the field of bioinformatics or healthcare. This fellowship thus can contribute to driving research development in other fields of research by providing an automated way of adapting and speeding-up existing software used in a plethora of areas, both in research and in the industry. 

The project will utilise and develop novel methods in the field of software engineering, called genetic improvement, to achieve the goals of the project. Genetic improvement is a novel field of research that only arose as a standalone area in the last few years. Several factors contributed to the development and success of this field, one of which is the sheer amount of code available online and focus on automated improvement of non-functional properties of software, such as energy or memory consumption. 

Dr. Petke is a world-leading expert on genetic improvement, publishing award-winning work on automated software specialisation and transplantation. She won two `Humies' awarded for human-competitive results produced by genetic and evolutionary computation and a best paper award at the International Symposium on Software Testing and Analysis. This work was also widely covered in media, including the Wired magazine and BBC Click. The potential of genetic improvement for automating certain aspects of the software development process has thus been already recognised in the academic community and beyond.

Dr. Petke will collaborate with a UK-based company, called Satalia, which provides the latest optimisation techniques to the industry. She will apply deliverables of this project to automatically adapt and speed-up their generalist optimisation algorithms to particular classes of real-world problems. An example is specialisation of a general routing program to devising an optimal network for broadband connections. Therefore, the deliverables of this project will contribute to the UK economy by providing techniques that will automate the process of software specialisation for real-world optimisation problems.",,"The project will have a revolutionary impact on the software development process. It will move the effort of modifying and adapting code from human to machine. By pushing the area of genetic improvement forward fully automated optimisation of large real-world software will be within reach.

The project will have a direct impact on the UK economy through collaboration with the industrial partner, Satalia. The company provides latest software optimisation technology to the industry. The project will lead to faster, automated solutions for Satalia, hence faster solutions to their clients. For example, Satalia optimised vehicle routes for a retailer to let more customers place orders without increasing fleet size. Similarly, generalist state-of-the-art routing algorithms will be automatically adapted for other route scheduling domains using techniques developed during the fellowship.

Through publishing at top software engineering and evolutionary computation venues and organisation of automated software specialisation and continuing the Genetic Improvement workshop series, deliverables of this project will be disseminated to academia. By releasing a tool for automated software specialisation via GitHub, other researchers will be able to optimise their software. More importantly, the techniques developed will be agnostic to the software being optimised. Therefore, researchers from fields outside of computer science will be able to use the tool. 

The automated software specialisation tool developed during the fellowship will be open source. Dr. Petke will advertise it on software developer fora to reach out to people outside of academia. Through seminars, advertisement and work with Satalia automated specialisation techniques developed will be released both to academia and industry. It is envisioned that the techniques will be adopted in real-world software automating the software adaptation and optimisation process.

The human vs. computer competition on automated software specialisation will allow students to learn and get involved in the genetic improvement field. Therefore, they will gain access to the latest technology and garner interest to pursue research in this novel field that has huge potential in changing the way people program nowadays by transferring the optimisation effort from human to machine."
8,6AAB2D45-60CA-40BE-930A-99CF85A3BA0B,Thales-Bristol Partnership in Hybrid Autonomous Systems Engineering (T-B PHASE),"Hybrid autonomous systems are those where groups of people are in direct, ongoing interaction with groups of autonomous robots or autonomous software. 

One prominent current example involves rush-hour traffic made up of a mixture of cars driven by people and cars driven by smart algorithms. However, emerging technologies in robotics, AI and ICT mean that hybrid autonomous systems of this kind will become increasingly common in a much wider set of situations: 

Emerging technologies in robotics, AI and ICT mean that hybrid autonomous systems of this kind will become increasingly common in a much wider set of situations: 

 - a mixture of autonomous and human-operated drones making deliveries or monitoring public spaces; 
 - a mixture of human traders and autonomous trading agents buying and selling stocks; 
 - a mixture of autonomous and human-operated trains and trams providing efficient, integrated public transport; 
 - autonomous systems assisting with search and rescue missions in disaster areas that are difficult or dangerous to access; 
 - robot carers assisting care workers with the provision of social care in the home

In each of these cases smooth, reliable, safe interaction amongst machines and people will be key to success. But how can we guarantee that self-driving cars won't cause a crash or gridlock? How can we understand how autonomous systems will respond to new situations (both acute shocks and long-term gradual changes in their environment), or changes in the way that people interact with them? Consequently, as we enter this new design space, a crucial challenge for the engineers of hybrid autonomous systems across all of these settings is ensuring that the system behaviour is Robust and Resilient and that it meets Regulatory demands: the R3 Challenge. 

T-B PHASE directly addresses this R3 Challenge for Hybrid Autonomous Systems Engineering, by bringing together expertise in robotics, AI, and systems engineering at the University of Bristol and Thales in a five-year project that targets fundamental autonomous system design problems in the context of three real-world Thales use cases: Hybrid Low-Level Flight, Hybrid Rail Systems, and Hybrid Search &amp; Rescue. 

Bristol and Thales have a long-standing track record of research collaboration, and by jointly pursuing fundamental research questions in the context of highly practical design problems, alongside a programme of engagement with industry, the public and regulatory bodies, T-B PHASE will significantly advance our capability to operate confidently in one of the most important emerging areas for modern engineering.",,"The business benefits of T-B PHASE are:

- Liability and responsibility for the behaviour of complex systems strongly inhibits the deployment of hybrid autonomous systems in the real world. By embedding Robustness, Resilience and Regulation as part of the development life cycle, T-B PHASE will provide those who commission and develop hybrid autonomous systems with tools that enable early-stage evaluation and demonstration in the development lifecycle.

- T-B PHASE will accelerate the adoption of new hybrid autonomous systems by reducing the costs of development, the risks of deployment, and the length of the development life-cycle. Current approaches involve exhaustive real-world validation and verification campaigns that are not scalable or sustainable for systems with emergent properties.

- Public and consumer acceptance of autonomous hybrid systems is currently fragile. T-B PHASE will improve the transparency of system Robustness and Resilience, which is an essential aspect of building acceptance and evolving Regulation frameworks that are suitable for hybrid autonomous systems. 

- Developing a critical mass of skilled researchers in autonomous systems engineering will benefit multiple relevant sectors in the UK, stimulating further technology development in this area and creating longer term strategic benefits for the UK engineering sector.

- T-B PHASE will contribute to advancing the development of regulatory frameworks for autonomous systems, which require attention to what can be assured at design stage and what can be assured post-deployment through online monitoring and adaptation.

The pathways to achieving this impact are described fully in the Pathways to Impact section of this proposal."
9,1C887BF2-E292-4498-A478-65A44CF18D43,Robotics and Artificial Intelligence for Nuclear (RAIN),"The nuclear industry has some of the most extreme environments in the world, with radiation levels and other hazards frequently restricting human access to facilities. Even when human entry is possible, the risks can be significant and very low levels of productivity. To date, robotic systems have had limited impact on the nuclear industry, but it is clear that they offer considerable opportunities for improved productivity and significantly reduced human risk. The nuclear industry has a vast array of highly complex and diverse challenges that span the entire industry: decommissioning and waste management, Plant Life Extension (PLEX), Nuclear New Build (NNB), small modular reactors (SMRs) and fusion.

Whilst the challenges across the nuclear industry are varied, they share many similarities that relate to the extreme conditions that are present. Vitally these similarities also translate across into other environments, such as space, oil and gas and mining, all of which, for example, have challenges associated with radiation (high energy cosmic rays in space and the presence of naturally occurring radioactive materials (NORM) in mining and oil and gas). Major hazards associated with the nuclear industry include radiation; storage media (for example water, air, vacuum); lack of utilities (such as lighting, power or communications); restricted access; unstructured environments.

These hazards mean that some challenges are currently intractable in the absence of solutions that will rely on future capabilities in Robotics and Artificial Intelligence (RAI). Reliable robotic systems are not just essential for future operations in the nuclear industry, but they also offer the potential to transform the industry globally. In decommissioning, robots will be required to characterise facilities (e.g. map dose rates, generate topographical maps and identify materials), inspect vessels and infrastructure, move, manipulate, cut, sort and segregate waste and assist operations staff. To support the life extension of existing nuclear power plants, robotic systems will be required to inspect and assess the integrity and condition of equipment and facilities and might even be used to implement urgent repairs in hard to reach areas of the plant. Similar systems will be required in NNB, fusion reactors and SMRs. 

Furthermore, it is essential that past mistakes in the design of nuclear facilities, which makes the deployment of robotic systems highly challenging, do not perpetuate into future builds. Even newly constructed facilities such as CERN, which now has many areas that are inaccessible to humans because of high radioactive dose rates, has been designed for human, rather than robotic intervention. Another major challenge that RAIN will grapple with is the use of digital technologies within the nuclear sector. Virtual and Augmented Reality, AI and machine learning have arrived but the nuclear sector is poorly positioned to understand and use these rapidly emerging technologies. 

RAIN will deliver the necessary step changes in fundamental robotics science and establish the pathways to impact that will enable the creation of a research and innovation ecosystem with the capability to lead the world in nuclear robotics. While our centre of gravity is around nuclear we have a keen focus on applications and exploitation in a much wider range of challenging environments.",,"The nuclear industry is of enormous importance to UK society and its economy both in the short and long-term. Decommissioning the UK's legacy facilities is currently costing the UK Government approximately &pound;3Bn/year, with estimates of its total cost over the next 120 years ranging from &pound;90Bn-220Bn. Investment in Nuclear New Build (NNB) before 2030 is expected to be approximately &pound;60Bn and the cost of building and operating a geological disposal facility is approximately &pound;14Bn.

Robotics and Artificial Intelligence (RAI) has a significant part to play in the UK's decommissioning programme with the National Nuclear Laboratory (NNL) estimating 20% of the costs of complex decommissioning projects will be spent on RAI, assuming the continued reliance on bespoke single use solutions. RAIN will deliver innovation in modular, reusable robotics with the target of achieving &gt;20% improvements in cost and productivity in decommissioning and zero-human entries in to radioactive facilities. Conservative estimates by NNL indicate that robotic systems have already resulted in productivity improvements of &gt;10% and that RAI has the potential to reduce decommissioning costs across the Nuclear Decommissioning Authority (NDA) estate by approximately &pound;11.7Bn.

New RAI technologies will lower costs and timescales, reduce risk and improve safety in high consequence interventions. The exposure of human workers to radiation and other hazards will be reduced significantly with the adoption of RAI and with reliable remote inspection of existing facilities possible, there is the potential to increase the life-span of the UK's existing reactor fleet, helping to secure the country's future energy provision.

Decommissioning legacy nuclear facilities and the inspection of new and old nuclear power plants is not a problem confined to the UK. The race to develop nuclear weapons and prototype reactors has left considerable decommissioning challenges in the USA, Russia, Korea and many other nations. In 2014 the IAEA recorded that there were a total of 438 nuclear reactors in operation with a further 59 under construction. The cost of decommissioning these reactors, legacy facilities and the additional 150 reactors that are already in permanent shutdown will cost approximately &pound;1T. The development of reliable, functional robotic systems has the potential to transform decommissioning operations in all these countries and hence the export market for nuclear robotics is vast. The UK has an established reputation for delivering world leading nuclear science research and innovative decommissioning solutions and thus has the potential to become the recognised world leader in nuclear robotics.

The widespread adoption of RAI in the nuclear industry will also have a knock-on effect, that will support the use of robotics in other extreme environments, such as space for off-planet missions and in-orbit satellite design and maintenance and offshore operations, where it is estimated that up to 150 platforms around the UK will require decommissioning in the next 10 years and that the decommissioning costs in the North Sea alone will total &pound;50Bn - &pound;60Bn over the next 25 years."
10,644D4B74-1807-4667-B165-C2A304D6447D,UK Robotics and Artificial Intelligence Hub for Offshore Energy Asset Integrity Management,"The international offshore energy industry currently faces the triple challenges of an oil price expected to remain less than $50 a barrel, significant expensive decommissioning commitments of old infrastructure (especially North Sea) and small margins on the traded commodity price per KWh of offshore renewable energy. Further, the offshore workforce is ageing as new generations of suitable graduates prefer not to work in hazardous places offshore. Operators therefore seek more cost effective, safe methods and business models for inspection, repair and maintenance of their topside and marine offshore infrastructure. Robotics and artificial intelligence are seen as key enablers in this regard as fewer staff offshore reduces cost, increases safety and workplace appeal. 

The long-term industry vision is thus for a completely autonomous offshore energy field, operated, inspected and maintained from the shore. The time is now right to further develop, integrate and de-risk these into certifiable evaluation prototypes because there is a pressing need to keep UK offshore oil and renewable energy fields economic, and to develop more productive and agile products and services that UK startups, SMEs and the supply chain can export internationally. This will maintain a key economic sector currently worth &pound;40 billion and 440,000 jobs to the UK economy, and a supply chain adding a further &pound;6 billion in exports of goods and services. 

The ORCA Hub is an ambitious initiative that brings together internationally leading experts from 5 UK universities with over 30 industry partners (&gt;&pound;17.5M investment). Led by the Edinburgh Centre of Robotics (HWU/UoE), in collaboration with Imperial College, Oxford and Liverpool Universities, this multi-disciplinary consortium brings its unique expertise in: Subsea (HWU), Ground (UoE, Oxf) and Aerial robotics (ICL); as well as human-machine interaction (HWU, UoE), innovative sensors for Non Destructive Evaluation and low-cost sensor networks (ICL, UoE); and asset management and certification (HWU, UoE, LIV). 

The Hub will provide game-changing, remote solutions using robotics and AI that are readily integratable with existing and future assets and sensors, and that can operate and interact safely in autonomous or semi-autonomous modes in complex and cluttered environments. We will develop robotics solutions enabling accurate mapping of, navigation around and interaction with offshore assets that support the deployment of sensors networks for asset monitoring. Human-machine systems will be able to co-operate with remotely located human operators through an intelligent interface that manages the cognitive load of users in these complex, high-risk situations. Robots and sensors will be integrated into a broad asset integrity information and planning platform that supports self-certification of the assets and robots.",,"Our project represents an unprecedented effort to use Robotics and AI to revolutionise Asset Integrity Management in the offshore energy sector to enable cheaper, safer and more efficient working practices and move towards a fully autonomous offshore energy field, operated, inspected and maintained from the shore. Up to &pound;17.8M of cash support from the industry demonstrates the timeliness and strong industry interest for our research agenda. Beyond its academic impact detailed elsewhere, this project will bring about a step change in the feasibility of large scale monitoring for subsea assets such as cables, risers, seabed structures and marine energy devices. To maximise the impact of our research, we have built in mechanisms to maximise the translation of research into innovation, products and industry take-up. 

1- Strong industry involvement: Our initial research focus is built around 6 developed Use Cases and 7 Capability Challenges for RAI applied to offshore energy asset integrity for marine, topside (i.e. on platform) and aerial domains developed in collaboration with industry (initial workshop July 2017, 40 people attending). Regular 6-monthly demonstrations of existing research will inform our partners of the evolving maturity of the research and trigger potential exploitation. Annual workshops to refine these and update the research roadmap will be organised by the Hub with the support and guidance of our Industrial Leadership and Opportunities Panel (ILOP). From these, Joint Industry Projects (JIPs) will form from Open Calls initiated by ILOP, the PI and the academic team. These projects will be funded from the &pound;20M Partnership Resource Fund (PRF) (&pound;2.6M EPSRC, &pound;17.8M industry partners). JIPs will comprise SMEs, startups and others from the supply chain to make capabilities more robust, reliable and certifiable in increasingly realistic domains up to TRL7. These efforts will be supported by a full time impact manager. 

2- Fostering wider collaborations: The hub will expand its research and innovation reach to other RAI Hubs initially through the bi-annual Hub PI meetings, where opportunities across hub boundaries will be identified. We will aim to align common science and innovation goals through the relevant Hub management structures to provide greater gearing whilst recognising the distinctness of the environments and markets. Regular Open Calls will seek research partners (funded by both EPSRC and Partners from the PRF) to close scientific gaps in the core WP programme based on advice by the International Scientific Advisory Panel (ISAP). This funding will be accessible by the best teams across the world, thus fostering collaborations at an international level. We will also seek to exploit synergies created from other hubs in other domains to have an impact on the anticipated ISCF RAI Challenge funding in 2019. Through this PI Forum, we will also reach out to the successful iUK projects (parallel ISCF RAI call) to seek synergies, overlap and opportunities for rapid innovation advancements. 
Standardisation: A crucial output of the project will be working closely with regulators, especially project partner Lloyds Register, to develop standards and formal requirements for offshore RAI self-certification. We will also explore defining an ISO industry standard for dialogue act annotation for interaction with autonomous systems and AI (similar to e.g. DiAML) to encourage adoption of the developed techniques in a variety of domains.

3- Engaging the wider public in research: We have a wide experience of public engagement through our RAS-CDT (BBC News @10, Blue Peter, Sky News, Twitter, Facebook) and will continue to our engagement strategy supported by our full time Hub Manager."
11,76539480-88B9-4869-BF0A-9EEE41FA484C,Engaging three user communities with applications and outcomes of computational music creativity,"In recent years, machine learning and artificial intelligence systems have been grabbing headlines by defeating humans at the game of Go and the American TV game-show Jeopardy! The same technology have also been applied to other domains considered essentially human, like art and music. This project proposes an alternative viewpoint to the human-vs-machine paradigm: these technologies can produce co-creative partners that are useful, meaningful, and non-threatening.

This project facilitates four innovative and creative activities to engage with three user communities: Composers, Performers, and Listeners. The principal aim of this project is to engage these user communities with applications and outcomes of computational music creativity. A subsidiary aim is for the investigators to collaboratively learn from these communities about the implications of emergent computational music creativity in non-academic contexts. 

Among its objectives, this project will develop a software application to enable anyone with an Internet connection to explore music co-creation. The project will also develop an online resource for the user community of this application and others, where users can share, comment and rate submitted outcomes. The technology underpinning this application comes from the AHRC-funded Care for the Future research project, &quot;Data science for the study of calypso-rhythm through history&quot; (DaCaRyH, AH/N504531/1). That project brings together ethnomusicologists, experts in data science and a composer to investigate how computational tools can be used to study calypso music, and how the study of music can be used to inform the design of computational analyses. An unanticipated outcome of DaCaRyH is that some of these analytical tools can be &quot;inverted&quot; to create systems that synthesise never-before-heard music that is surprisingly good. Preliminary work by the investigators (in the form of expert elicitations, a public workshop, and music concert) show there is great interest among listeners and practitioners for exploring computational creativity outside of the academic realm.

The project will engage public audiences with music arising from computational co-creativity. It will deliver a hands-on workshop for composers to learn to use the developed software application. It will also organise a symposium on computational music creativity for the general public. The conclusion of the project will feature a music concert showcasing new music created with the developed application. To motivate participation, the project will organise a competition to solicit original compositions created with the application, to be performed by a professional ensemble. The competition jury will include musicians, composers, and of course a critic built using artificial intelligence. 

An especially novel outcome of this project is a professionally produced album of &quot;machine folk&quot; music. The album will feature music generated entirely by the system and edited, arranged, and performed by experienced musicians. The preliminary work of the investigators has found that good music performed by good musicians serves as the best ambassador for the value and potential usefulness of computational creativity. This album will serve as a lasting illustration for the benefits of the co-creative approach to machine learning tools.

This project links directly with the Digital Transformations theme by bringing state of the art digital technology to democratise participation in creative music making. It also relates to the Care for the Future theme by encouraging innovation and renewal in community music practice, and thus helping to broaden and deepen engagement from listeners, performers and composers. The project will serve to diversify the legacy of the DaCaRyH project.",,"This project will impact users and beneficiaries outside the academic research community in three sectors.

# Public and third sector
- Professional music composers: One commercial composer with whom the investigators are collaborating has envisioned that the application to be developed in this project could help him overcome &quot;the intimidation of the blank page&quot;. Were he to begin by selecting among a wealth of materials, the process of composition would flow. Both investigators have composed new music using the current music models, and can see great benefit to this approach. The application will facilitate more of this kind of interaction.
- Professional music performers: Several performers the investigators have worked with so far already benefited from this work. One performer remarked that the process of preparing to perform tunes generated from the system led him to reevaluate his relationship with technology and his ideas about creative practice. Another performer said they didn't initially like the idea of engaging with a computer composing music, but once he heard the results he changed his mind.
- Education: The application to be developed in this project could be used as a pedagogical assistant to music students. In fact, part of the testing of the prototype application in this project will use music students at Kingston University. This classroom setting will be useful to test the usability of the application itself, but also the potential for this approach for teaching students about music and creativity.

# Commercial private sector
- New co-creative music enterprises: the topic of machine learning for music applications is attracting the interest of many commercial enterprises (e.g., London-based JukeDeck, Sony's Flow machines, Google's Magenta project). The application to be designed in this project (as well as the proof of the quality of the music in the form of the album) will motivate the development of similar co-creative music products.
- Standardised music testing industry: In the UK, there is a large cottage industry of preparatory services for national examinations in music. The music models developed in this project could be used to assist in the evaluation of the answers, such as stylistic relevance and originality. These will enhance the validity of such exams and also make the marking more efficient.

# Wider Public
- Amateur and semi-professional musicians: Non-professional music making is widespread in the UK with amateur choirs, orchestras, ensembles, and bands assembling to enjoy music making. This project will engage people who want music to be part of their lives, and offer them new opportunities for active and creative engagement with music through the application of new technology. In fact, one of the participants at the recent workshop of the investigators (http://www.insideoutfestival.org.uk/2017/events/folk-music-composed-by-a-computer/) commented that she felt unable to channel her creativity into making music because she lacks training. With the application to be developed in this project, she could start by generating tunes and selecting the bits she likes, building as she goes. The benefit, therefore, is both in the form of a tangible musical output but also in the form of learning and developing of musical skills.
- Public at large: Public interest in the subject of the project is evidenced by the number of views of our article for The Conversation (https://theconversation.com/machine-folk-music-shows-the-creative-side-of-ai-74708) and the examples posted to The Bottomless Tune Box YouTube channel (https://www.youtube.com/channel/UC7wzmG64y2IbTUeWji_qKhA). This indicates that there is a non-academic audience interested in engaging with the larger question of this project: how can artificial intelligence have a beneficial impact on music?"
12,0D674371-E47B-4585-9248-93EBF9C2BCB5,DeepSecurity - Applying Deep Learning to Hardware Security,"With the globalisation of supply chains the design and manufacture of today's electronic devices are now distributed worldwide, for example, through the use of overseas foundries, third party intellectual property (IP) and third party test facilities. Many different untrusted entities may be involved in the design and assembly phases and therefore, it is becoming increasingly difficult to ensure the integrity and authenticity of devices. The supply chain is now considered to be susceptible to a range of hardware-based threats, including hardware Trojans, IP piracy, integrated circuit (IC) overproduction or recycling, reverse engineering, IC cloning and side-channel attacks. These attacks are major security threats to military, medical, government, transportation, and other critical and embedded systems applications. The proposed project will use a common approach to investigate two of these threats, namely the use of deep-learning in the context of side-channel attacks and hardware Trojans.

Side-channel attacks (SCAs) exploit physical signal leakages, such as power consumption, electromagnetic emanations or timing characteristics, from cryptographic implementations, and have become a serious security concern with many practical real-world demonstrations, such as secret key recovery from the Mifare DESFire smart card used in public transport ticketing applications and from encrypted bitstreams on Xilinx Virtex-4/5 FPGAs. A hardware Trojan (HT) is a malicious modification of a circuit in order to control, modify, disable, monitor or affect the operation of the circuit. Although there have been no public reports of HTs detected in practice, in 2008 it was speculated that a critical failure in a Syrian radar may have been intentionally triggered via a hidden 'back door' inside a commercial off-the-shelf (COTS) microprocessor. 

The proposed project seeks to investigate the application of deep learning in SCA and HT detection, with the ultimate goal of utilising deep learning based verification processes in Electronic Design Automation tools to provide feedback to designers on the security of their designs. In relation to the call, the project addresses the challenge of 'maintaining confidence in security through the development process', and more specifically 'building supply chain confidence' and 'novel hardware analysis toolsets and techniques'.",,"The overall goal of the DeepSecurity research project is to investigate the use of deep learning for security verification in EDA tools, specifically in relation to hardware Trojan detection and side channel analysis, to allow non-security experts receive feedback on how to improve the security of their designs prior to fabrication. Hence, the research outputs will be of immediate relevance to entities for which supply chain confidence is of critical importance, for example, military, medical, government, transportation, and other critical infrastructure organisations.

In terms of direct economic impact, the project partners, BAE Systems and Cryptography Research (CRI) will be the first users and beneficiaries of the research outputs, but further beneficiaries will naturally ensue. Securing an untrustable hardware supply chain is an area of significant interest for BAE. CRI offers side channel countermeasures in addition to independent testing of devices to evaluate their side-channel resistance. Therefore, for them the research into DL-based attacks is particularly relevant, in addition to the proposed DL-based automated side-channel secure verification framework. 

Hardware security is regarded as the foundation of effective IoT security and is essential to realising the IoT value proposition. A common theme in all the realms of IoT is the need for dependability and security. This was highlighted in the 2015 HiPEAC Vision report as a primary challenge for IoT. It outlines that security has to become one of the primary design features of whole systems, thus, underlining the importance of the proposed DeepSecurity project. Hence, the provision of security assurances to IoT devices, acts as an enabling layer for IoT applications and analytics, which when in full deployment will result in significant societal impact through, for example, more intelligent food production, energy consumption, traffic congestion/collision avoidance and remote healthcare applications. 

The project will also enrich the skills pool in the UK with uniquely skilled researchers in the areas of hardware Trojan detection, side channel analysis and (secure) hardware design processes. CRI has offered to provide internship opportunities for the PhD students working on the project. In addition, experiences and insights developed in the project will be reflected back into the teaching curriculum of the MSc in Applied Cyber Security at QUB."
13,161B03E3-6EE2-4985-A708-D2B541BE1E80,Adaptive Automated Scientific Laboratory,"Our proposal integrates the scientific method with 21st century automation technology, with the goal of making scientific discovery more efficient (cheaper, faster, better). A &quot;Robot Scientist&quot; is a physically implemented laboratory automation system that exploits techniques from the field of artificial intelligence to execute cycles of scientific experimentation. Our vision is that within 10 years many scientific discoveries will be made by teams of human and robot scientists, and that such collaborations between human and robot scientists will produce scientific knowledge more efficiently than either could alone. In this way the productivity of science will be increased, leading to societal benefits: better food security, better medicines, etc. The Physics Nobel Laureate Frank Wilczek has predicted that the best scientist in one hundred years time will be a machine. The proposed project aims to take that prediction several steps closer.

We will develop the AdaLab (an Adaptive Automated Scientific Laboratory) framework for semi-automated and automated knowledge discovery by teams of human and robot scientists. This framework will integrate and advance a number of ICT methodologies: knowledge representation, ontology engineering, semantic technologies, machine learning, bioinformatics, and automated experimentation (robot scientists). We will evaluate the AdaLab framework on an important real-world application in cell biology with biomedical relevance to cancer and ageing. The core of AdaLab will be generic.

The expected project outputs include:

- An AdaLab demonstrated to be greater than 20% more efficient at discovering scientific knowledge (within a limited scientific domain) than human scientists alone.
- A novel ontology for modelling uncertain knowledge that supports all aspects of the proposed AdaLab framework.
- The first ever communication mechanism between human and robot scientists that standardises modes of communication, information exchange protocols, and the content of typical messages. 
- New machine learning methods for the generation and efficient testing of complex scientific hypotheses that are twice as efficient at selecting experiments as the best current methods.
- A significant advance in the state-of-the-art in automating scientific discovery that demonstrates its scalability to problems an order of magnitude more complex than currently possible.
- Novel biomedical knowledge about cell biology relevant to cancer and ageing.
 - A strengthened interdisciplinary research community that crosses the boundaries between multiple ICT disciplines, laboratory automation, and biology.

All outputs produced by the project will be made publicly available by the end of the project.",,"The proposed project has high potential for significant technological, economical, and societal impacts. This potential impact of robot scientists has been widely recognised, e.g. in the Nature editorial of 15.1.04 their potential synergistic collaboration with human scientists is stressed, &quot;an automated system that designs its own experiments will benefit young molecular geneticists&quot;; a reviewer of our article in Science (King et al, 2009) stated that the work was of &quot;historical significance&quot;; and Time magazine named it the 4th most significant scientific advance of 2009. The proposed project principally extends the previous work by developing a (semi-) automated framework for scientific discoveries by teams of human and robot scientists (AdaLab), making it far more applicable to general biomedical research. 

The AdaLab framework will contribute to realising Europe's 2020 strategy for smart, sustainable and inclusive growth. The project would contribute to the future development of (semi)-automated laboratories across Europe and wider. These intelligent laboratories have the potential to speed up the technological progress. Our cautious estimate is that the exploitation of the AdaLab framework in scientific laboratories would increase the efficiency of laboratory experimentation by 20% (see section 2). Such an increase would lead to more scientific discoveries, better technological solutions, and new products. 

Science is the greatest generator of economic wealth (through developments in technology), and the greatest driver of better health (through development in biomedical science). Therefore all EU citizens will potentially benefit from the proposed research. For example new better drugs could be delivered to the market faster and cheaper. Currently, ~25Billion euros is spent annually within the EU on pharmaceutical research. Most of this is spent on late-stage trials (which are less amenable to automation), but conservatively estimating that 10% is amenable to the AdaLab framework, then a 20% efficiency gain would result in savings of ~0.5Billion euros per annum."
14,4F4960AB-0376-48DB-A2C8-CE97A9E3009E,Multi-Disciplinary Pedestrian-in-the-Loop Simulator,"Pedestrians represented roughly 24% of road fatalities and 22% of the seriously injured in the UK in 2015 (Department for Transport, Reported Road Casualties Great Britain: 2015, Annual Report). The most commonly recorded factors were: &quot;in accidents where a pedestrian was killed or injured; pedestrian failed to look properly was reported in 59 per cent of accidents. Failed to judge other person's path or speed was the most typical secondary cause.&quot; (DfT, 2015)

In this context, the increased use of Autonomous Vehicles (AVs) and new urban warning systems that can help monitor and assist pedestrians and their interactions with vehicles has the potential to dramatically reduce road deaths. A major concern, however, is that the AVs and warning systems must be designed to take into account the capabilities and limitations of pedestrians. 

This project will develop a new pedestrian laboratory to support safe experimental research in a repeatable fashion in which a variety of variables with respect to AV design, warning system design, and intersection configuration can be studied. The experiments can also look at the impacts of a wide range of human factors including age, vision and mobility.

The pedestrian laboratory (PEDSIM) will consist of a Virtual Reality (VR) simulator that will allow a participant to experience a variety of urban configurations and interact with new vehicles and urban robots. The pedestrian laboratory will track the participant's performance in a variety of tasks to compare the effectiveness of various designs.

What makes the PEDSIM unique in the world is its very high resolution displays combined with its large walkable environment (9 metres by 4 metres) and its integration with driving simulators to test interactions between pedestrians and drivers.

As automated and autonomous vehicles get closer to deployment, research into their design and impact has rapidly increased. There are several studies currently funded by the EPSRC that can take immediate advantage of the new research capabilities of the PEDSIM. These include research to evaluate solutions for cooperative interaction of automated vehicles and urban robots with pedestrians and research that will test various lighting conditions and its impact on visibility, trip hazards, and understanding intentions of other pedestrians and vehicles.",,"The aim of PEDSIM is to provide a true-to-life, CAVE-based, simulated walking environment, for addressing a number of key research questions on human performance and interaction relevant to both today and tomorrow's needs. The PEDSIM will also be integrated with the other well established simulator facilities at the University of Leeds, to provide a distributed simulation suite, as part of the Virtuocity concept, delivering a pioneering and unique research environment in the UK for investigating the interaction of humans with new technologies and infrastructures, in a highly-controllable, low-cost and safe setting. Work achieved by the PEDSIM will provide the full range of impacts, including creating wealth, enhancing knowledge, informing policy, and improving quality of life.

Economic Impact -Work conducted on the PEDSIM and the distributed simulation suite is relevant to a vast range of disciplines, providing knowledge on matters such as: the design of effective road and pavement layouts for tomorrow's cities, understanding how technologies can assist impaired and disabled users, and how pedestrians will communicate with fully automated vehicles. This leads to direct economic benefit, via inward investment from relevant stakeholders wishing to investigate the interaction of humans with their prototypes, saving costs prior to production, and IP and commercialisation of new products and procedures. Research-led teaching programmes, using blended learning techniques, will provide a range of on-line learning platforms, accessible to a global student network, providing new revenue.

Societal Impact - The University of Leeds has a longstanding track record of translating research to knowledge for end-users, industrial partners, policy-makers, and the public at large. These activities are facilitated via direct collaboration, licencing of IP, or spin-out, all of which are used to provide a successful model for translational activity. The PEDSIM will be managed by the internationally renowned team at the University of Leeds Driving Simulator, who have longstanding strategic partnerships with a large group of industrial end-users in the UK, and beyond, including all leading vehicle manufacturers (e.g. Jaguar Land Rover, Volvo, VW, Ford, Fiat, Renault and Nissan). International development will be achieved by addressing future research needs on human factors of automated vehicles, discussed by members of the group at high-level UK, US and European steering group meetings such as the H2020 Transport Advisory Group, the EC's Strategic Transport Research and Innovation Agenda (STRIA), and the EC's high level GEAR2030 group, assembled to &quot;boost competitiveness and growth in the automotive sector&quot;.

Academic Impact forms part of the critical pathway towards economic and societal impact. The PEDSIM is accessible to a multidisciplinary team of leading academics in Leeds and beyond, who will disseminate results of research from the facility to an international audience, covering a range of disciplines; from Engineering and Infrastructure Design, to Human Interaction with Robotics, Psychology and Road User Behaviour, Virtual Reality testing, Artificial Intelligence, and Disability Studies. The facility will also provide exciting opportunities for collaboration across disciplines. Each academic partner and collaborator will also use the facility to provide training and new skills to students at undergraduate and postgraduate level, as well as enhancing the knowledge and expertise of current and future PDRAs and partnering on development of new teaching modules."
15,457241AE-C128-491E-B983-117B885CA640,Future AI and Robotics Hub for Space (FAIR-SPACE),"Advances in robotics and autonomous systems are changing the way space is explored in ever more fundamental ways. Both human and scientific exploration missions are impacted by these developments. Where human exploration is concerned, robots act as proxy explorers: deploying infrastructure for human arrival, assisting human crews during in-space operations, and managing assets left behind. As humans extend their reach into space, they will increasingly rely on robots enabled by artificial intelligence to handle many support functions and repetitive tasks, allowing crews to apply themselves to problems that call for human cognition and judgment. Where scientific exploration is concerned, robotic spacecraft will continue to go out into Earth orbit and the far reaches of deep space, venturing to remote and hostile worlds, and returning valuable samples and data for scientific analysis. 
The aim of FAIR-SPACE is to go beyond the-state-of-the-art in robotic sensing and perception, mobility and manipulation, on-board and on-ground autonomous capabilities, and human-robot interaction, to enable space robots to perform more complex tasks on long-duration missions with minimal dependence on ground crew. More intelligent and dexterous robots will be more self-sufficient, being able to detect and respond to anomalies on board autonomously and requiring far less teleoperation. 
The research will see novel technologies being developed for robotic platforms used in orbit or on planet surfaces, namely: future on-orbit robots tasked with repairing satellites, assembling large space telescopes, manufacturing in space, removal of space junk; and future surface robots, also known as planetary rovers, for surveying, observation, extraction of resources, and deploying infrastructure for human arrival and habitation; a further case study will target human-robot interoperability aboard the International Space Station. 
The research will merge the best available off-the-shelf hardware and software solutions with trail-blazing innovations and new standards and frameworks, aiming at the development of a constellation of space robotics prototypes and tools. This aims to accelerate the prototyping of autonomous systems in a scalable way, where the innovations and methodologies developed can be rapidly spun out for wide adoption in the space sector worldwide. 
FAIR-SPACE directly addresses two of the priorities in the Industrial Strategy Green Paper: robotics &amp; artificial intelligence and satellite &amp; space technologies. The clear commitment offered by the industrial partners demonstrates the need for establishing a national asset that will help translate academic outputs into innovative products/services. Our impact plan will ensure we can maximise co-working with user organisations, align our work with other programmes (e.g. InnovateUK) and effectively transfer our research outputs and technology to other sectors beyond space such as nuclear, deep mining and offshore energy. FAIR-SPACE will therefore not only help in wealth creation but also help develop a robotics UK community with a leading international profile.",,"The FAIR-SPACE Hub will establish a national asset in space intelligent systems and robotics that will help realise the target of creating a &pound;40Bn UK space industry by 2030, as set out in the government's Space Innovation and Growth Strategy. The Hub will provide research and innovation to a sector that generates &pound;13.7bn of income, supports 38,500 jobs and has worker productivity 2.7x greater than the national average (UK Space Agency, December 2016). The research challenges addressed by the Hub will confer significant benefits not only to the space industry but also to a number of other priority, crossover and downstream sectors, including telecommunication, broadcasting, navigation- and location-based services, meteorological and geospatial services, defence and security, and even healthcare. The Hub will contribute to safety-critical autonomy for extreme environments, e.g. deep mining and nuclear decommissioning, and the wider industrial robotics sector. 
The commitment offered by our project partners (from industry, government, independent research and educational bodies) underscores the wide diversity of impact that will be achieved through world-leading research and innovation. It also demonstrates that we will be well placed to engage with the other programmes led by InnovateUK. Among our non-space-centric industrial partners are multinationals such as Intel, the world's largest producer of semiconductors, and preeminent robot arm manufacturer KUKA. The director of Intel Health and Life Science Innovations, for example, foresees synergies between space robotics and healthcare applications, and the founder of cloud-based telehealth operator InTouch also sees many challenges in common between the healthcare and space domains.
The convergence of AI, deep learning and robotics is poised to deliver versatile, intelligent and robust solutions for environments that are inaccessible to humans. Orbital and planetary surface autonomous vehicles push robotic exploration and high-performance, low-energy computing to the limit - which can be an accelerant to the development of disruptive technologies.
The proposal is also of direct relevance to major players in the aerospace sector such as BAE Systems, Airbus Defence &amp; Space, Thales Alania Space, SSTL, ESA, UKSA, NASA (who are all project partners of the Hub) for application in the military, air, satellites, and adjacent markets, in addition to unmanned scientific exploratory missions conducted by space agencies. 
Considering the wider implications of the proposed work of the Hub, standards bodies and policy makers that set laws and regulations on autonomous systems will be afforded a useful evidence base from the Hub's work, particularly as an entire research theme has been given over to addressing verification challenges engendered by space robotics. Cybersecurity standards, unmanned vehicle certification also come into this, with ripple-on implications in commercialisation of terrestrial UAVs or driverless cars. Creation of new business and wealth is envisaged through UK tech startups and SMEs like the Hub's existing partners RURobots and NEPEC. 
The Hub will impact on the general public as we engage with them to shape the programme and show the value of robotics and AI. Inspiring the next generation will be an emphasis. In its 2017 synopsis, Engineering UK reports an annual shortfall of 20,000 graduate engineers. The Hub will help address this by working with schools and the National Space Centre to demonstrate the excitement of engineering to young people."
16,CDD240DE-0813-45E8-91BB-21F28AF0ECEF,nlvis: Natural Language Interaction for Visual Data Analysis,"The unprecedented increase in the amount, variety and value of data has been significantly transforming the way that scientific research is carried out and businesses operate. As data sources become increasingly diverse and complex, analysis approaches where the human and the computer operate in collaboration have proven to be an effective approach to derive actionable observations. This is achieved through an iterative human-computer dialogue where the knowledge and the creativity of the human meets the power of computation. In such human-in-the-loop data analysis approaches, interactive visualisation methods are core facilitators of this dialogue. However, these methods still rely on conventional, not often intuitive interaction mechanisms that can introduce unnecessary complexities into the process. There is an urgent need to rethink the ways how analysts interact with visualisations in data-intensive analysis situations. The recent advances in natural language based interaction methodologies offer promising avenues to address that. 

This project aims to develop a fundamental understanding of how analysts can use natural language elements to perform visualisation empowered data analysis and use that understanding to develop a framework where natural language and visualisation based interactions operate in harmony. The project then aims to demonstrate how such a multi-modal interaction scheme can radically transform the analysts' experience with the goal of achieving significant improvements in the value and the volume of actionable observations generated.

Within the project, we will initially identify and develop a taxonomy of natural language interaction elements for describing visualisations and for carrying out a visual data analysis process. Here, we will inform our investigation with findings from data collected through crowd-based survey methodologies. We will then design a conceptual framework that facilitates an iterative data analysis process through interactions with both natural language and visualisation elements. We will make use of the data analysis and visualisation related language taxonomy from the earlier stage to define the scope and the capabilities of the interaction elements.

The project will then move on to realising its vision through a prototype where the conceptual framework will operate through the help of an established conversational interface mechanism. The prototype will involve a combination of natural language and visual interaction capabilities and will also incorporate underlying computational capacities. We will then evaluate our approaches through a series of carefully designed use-cases that encompass common visual analysis scenarios. Our success criteria will be to achieve enhanced engagement and improved productivity during the visual analysis of complex data-intensive problems.

Potential beneficiaries of the outputs of this project ranges widely from academic researchers, professional data analysts, data analysis industry, and the general public. For visualisation and visual analytics researchers, findings will benefit researchers who are working on understanding user-intent and mechanisms of sense-making in interactive visual analysis processes. For businesses that offer visualisation-empowered solutions to their customers (according to some reports, the visualisation market size is expected to reach a $2.8 Billion by 2020), the framework developed will provide the basis for new forms of products that are easier to learn and engage with. For professional data analysts, the novel interaction capability will offer a more fluid and natural experience, improving their efficiency and positively impacting the quality of observations. For the general public, natural interaction mechanisms will provide an enhanced experience when using data-intensive products that are becoming to be widely adopted.",,"The project plans two main pathways to impact and a number of side activities to widen the impact further. The two main activities are Knowledge Transfer through Project Partnership and a public engagement workshop. We also plan to organise more informal public engagement activities, and also try and increase the local outreach of the project. The PI will use the last 6 months of the project to focus on widening the impact of the project. The following are the main pathways to impact:

KNOWLEDGE AND TECHNOLOGY TRANSFER THROUGH PROJECT PARTNERSHIP: The first and most concrete pathway to impact plan of the project is through the project partner company named Redsift. They are looking into innovative ways to inform the development of their conversational analysis solution and data analytics capabilities within their product range. Therefore the impact of the research results will be almost immediate in this pathway to impact.

DISSEMINATION and ENGAGEMENT WORKSHOP: The second pathway to impact is through an engagement and dissemination activity that aims to get together experts from a wide range of academic, industrial and governmental institutions, and facilitate a lively discussion around the topic of &quot;The role of the user/interaction in Data Science&quot;.

ACADEMIC PUBLICATIONS: During the duration of the project, we aim to produce a Short paper and a Workshop paper for the EuroVis 2018 conference to share the initial work-in-progress results. We then aim to produce a number of scientific papers: i) one detailing the observations from WP1 and the visualisation, task, and analytical intent related vocabulary ii) one describing the design process of the conceptual framework along with the results of the evaluation study. Even after the project has ended, we plan to work on position papers and other workshop papers that will discuss the implications of the utilisation of natural language approaches within visual analysis.

PUBLIC ENGAGEMENT: We aim to engage in activities where we can present project results to the general public. We will aim to take place in digital technology related activities that are organised in popular, social spaces. A couple of candidate events are: Digital Design Week at Victoria &amp; Albert Museum, or appropriate themed events within Science Museum activities such as Science Museum Lates.

OPEN ACCESS TO PROJECT RESULTS: We will make sure that all the projects' outputs are available as open source. Any published paper will be made available through the Open Access System of CITY. And whenever there are no limitations we will ensure the anonymity of the data and make the data available along with the research papers through services such as GitHub."
17,6D281DFC-CF12-4D84-B7DB-1FE7CBFB6FBB,Analysing Narrative Aspects of UK Preliminary Earnings Announcements and Annual Reports: Tools and Insights for Researchers and Regulators,"The quality of information provided to investors by corporate management in publicly traded companies is a matter of central importance to financial market participants. Narrative commentaries represent an increasingly significant component of financial communications. While financial narratives in the UK are shaped in part by prevailing regulations, senior management enjoys significant discretion over the content, structure and presentation of these disclosures. The informativeness of financial narrative disclosures and the way management apply their reporting discretion are key questions for academics and policymakers. 

Partnering with the UK body responsible for promoting high quality corporate governance and financial reporting - the Financial Reporting Council (FRC) - this interdisciplinary project will combine expertise from accounting with state-of-the-art methods from computational linguistics to examine two key elements of financial disclosure. The first aspect is preliminary earnings announcements (PEAs), which arguably represent the most important disclosure in UK firms' annual reporting calendar. The second aspect is the annual report to shareholders, which forms the largest single recurring disclosure commitment for management.

Two opposing perspectives exist on corporate narrative disclosures. On the one hand, proponents argue that narratives provide information beyond that contained in financial data. On the other hand, opponents claim that management exploit the discretion embedded in narrative reporting to obfuscate or present a biased representation of actual performance. While extant work on UK annual report and PEA narrative disclosures provides evidence consistent with both perspectives, both the scope of the research and the generalizeability of findings is compromised because conclusions rely on manual coding methods applied to small samples.

This project will develop and use state-of-the-art computerized textual analysis methods to study the properties and usefulness of financial narratives for a comprehensive sample of UK disclosures published between 2003 and 2016. While researchers are already using these methods to study disclosures made by US companies, problems accessing digital PEAs and annual reports coupled with inconsistent document structure has hindered computerized analysis of UK financial narratives and skewed research agendas away from studying UK reporting outcomes. This project will shine much needed light on two key aspects of UK narrative reporting. The work will provide the first large sample analysis of PEAs narratives. 

The project will also examine a set of contemporary policy-relevant themes relating to the content and structure of UK annual reports. Software tools and datasets from the project will also create new opportunities for the research community.

Policymakers are facing pressure to adopt evidenced-based approaches to regulation. While the FRC is committed to conducting impact and evaluation analyses, it is reliant on a relatively small team of research staff to undertake such work, much of which involves manual collection and analysis of unstructured data. The labour-intensive nature of the work inevitably yields results that are hard to generalize and constrains the scope of the FRC's work. As well as examining novel and policy-relevant research questions, this project will embed computerized text analytics methods in the FRC's formal policymaking processes. The methods will complement existing approaches by facilitating lower cost and more comprehensive assessments of regulatory changes and emerging issues in narrative reporting.",,"Who will benefit from the work?
The project will deliver economic and societal benefits as well as contributing to academic research.

The work involves co-funded and co-produced research with the UK financial reporting regulator, the Financial Reporting Council (FRC). The work seeks to enhance policymaking in corporate governance and financial reporting by: reviewing a key unregulated aspect of corporate reporting in the form of preliminary earnings announcements (PEAs) to determine the need or otherwise for regulatory guidance; evaluating the impact of recent developments in annual report narratives; and embedding large-sample textual analysis methods in the FRC's policymaking toolkit.

Other bodies with links to financial reporting are also expected to benefit from project outputs including the UK Investor Relations Society (UK IRS) and the Institute of Chartered Accountants in England and Wales (ICAEW), the European Financial Reporting Advisory Group (EFRAG), and the International Integrated Reporting Council (IRRC).

The academic community will also benefit from the project. Large-sample empirical research on corporate narratives is skewed heavily toward the US due in part to the ease with which financial narratives can be accessed and processed automatically in that market. This project will create new resources, insights, and agendas for researchers generally and UK researchers in particular.

What form will the benefits take?
The research will enhance policymaking through two ex ante impact assessments of prevailing financial reporting practice. First, we will undertake the first systematic analysis of the properties and economic impact of PEA commentaries as a basis for evaluating the need or otherwise for the FRC to issue regulatory guidance. (PEAs are largely unregulated in the UK, creating variation in practice and scope for both informative reporting and obfuscation.) Second, we will provide large-sample evidence on emerging trends in unregulated aspects of annual report narratives as a basis for identifying both best practice and areas where regulatory guidance may be required. We also expect these findings to be of interest to other bodies involved in financial reporting including UK IRS, ICAEW, EFRAG and IIRC.

The project will also contribute to FRC policymaking activities by providing comprehensive post-implementation reviews of recent developments in annual reporting. (The FRC is currently restricted to conducting small sample manual post-implementation reviews that are costly to produce and hard to generalise.)

Coincident with this instrumental impact, the project will also deliver capacity-building impact to policymaker and academic communities. For the policymaker community, the work will embed large sample textual analysis and big data methods in the FRC's policy toolkit, empowering it to conduct comprehensive, timely, and low cost analyses of UK firms' narrative reporting practices as part of its surveillance and post-implementation review activities (where only small sample manual work is currently possible). Training and documentation to support software and methods will enable FRC colleagues to harness the potential of these resources and ensure significant legacy benefits. Datasets of financial narratives will also enhance contemporaneous and future evidence-based policymaking activities.

For the academic community, the project will build sustainable UK-focused research capacity by: developing software resources that facilitate automatic retrieval and analysis of corporate financial narratives; providing new training opportunities in textual analysis for researchers; generating datasets summarizing the properties of narrative commentaries; and stimulating UK-focused research agendas in hitherto unexplored areas such as document structure, content integration, and data presentation."
18,4187339D-5FCD-4EB0-A83D-4BC7432071C1,RealPDBs: Realistic Data Models and Query Compilation for Large-Scale Probabilistic Databases,"In the recent years, there has been a strong interest in academia and industry in building large-scale probabilistic knowledge bases from data in an automated way, which has resulted in a number of systems, such as DeepDive, NELL, Yago, Freebase, Microsoft's Probase, and Google's Knowledge Vault. These systems continuously crawl the Web and extract structured information, and thus populate their databases with millions of entities and billions of tuples. To what extent can these search and extraction systems help with real-world use cases? This turns out to be an open-ended question. For example, DeepDive is used to build knowledge bases for domains such as paleontology, geology, medical genetics, and human movement. From a broader perspective, the quest for building large-scale knowledge bases serves as a new dawn for artificial intelligence research. Fields such as information extraction, natural language processing (e.g., question answering), relational and deep learning, knowledge representation and reasoning, and databases are taking initiative towards a common goal. Querying large-scale probabilistic knowledge bases is commonly regarded to be at the heart of these efforts.

Beyond all these success stories, however, probabilistic knowledge bases still lack the fundamental machinery to convey some of the valuable knowledge hidden in them to the end user, which seriously limits their potential applications in practice. These problems are rooted in the semantics of (tuple-independent) probabilistic databases, which are used for encoding most probabilistic knowledge bases. For computational efficiency reasons, probabilistic databases are typically based on strong, unrealistic completeness assumptions, such as the closed-world assumption, the tuple-independence assumption, and the lack of commonsense knowledge. These strong unrealistic assumptions do not only lead to unwanted consequences, but also put probabilistic databases on weak footing in terms of knowledge base learning, completion, and querying. More specifically, each of the above systems encodes only a portion of the real world, and this description is necessarily incomplete; these systems continuously crawl the Web, encounter new sources, and consequently new facts, leading them to add such facts to their database. However, when it comes to querying, most of these systems employ the closed-world assumption, i.e., any fact that is not present in the database is assigned the probability 0, and thus assumed to be impossible. As a closely related problem, it is common practice to view every extracted fact as an independent Bernoulli variable, i.e., any two facts are probabilistically independent. For example, the fact that a person starred in a movie is independent from the fact that this person is an actor, which is in conflict with the fundamental nature of the knowledge domain. Furthermore, current probabilistic databases lack (in particular ontological) commonsense knowledge, which can often be exploited in reasoning to deduce implicit consequences from data, and which is often essential for querying large-scale probabilistic databases in an uncontrolled environment such as the Web. 

The main goal of this proposal is to enhance large-scale probabilistic databases (and so to unlock their full data modelling potential) by more realistic data models, while preserving their computational properties. We are planning to develop different semantics for the resulting probabilistic databases and analyse their computational properties and sources of intractability. We are also planning to design practical scalable query answering algorithms for them, especially algorithms based on knowledge compilation techniques, extending existing knowledge compilation approaches and elaborating new ones, based on tensor factorisation and neural-symbolic knowledge compilation. We will also produce a prototype implementation and experimentally evaluate the proposed algorithms.",,"We are proposing to lay the foundations for a new generation of probabilistic database systems that will revolutionise how we deal with probabilistic data, and unlock their full data modelling potential. As a special kind of Big Data, probabilistic data are being produced by an increasing number of applications, devices, and users, and one of their main challenges is how to deal with their incompleteness, most notably in the context of the World Wide Web. The commercial value of probabilistic data management is also reflected by the fact that the company Lattice.io, which grew out of the probabilistic database system DeepDive, has just been acquired by Apple for $175 million to $200 million. Big Data in general are critically important for a variety of different areas in science, industry, governments, and healthcare; their economic potential is enormous, estimated to exceed &pound;50B annually in the UK (over 2.5% of the entire UK GDP). The beneficiaries could, in the long term, include anyone who uses or depends on probabilistic data, such as those collected on the Web. In the Western world at least, this effectively includes every business/organisation and every individual.

In the shorter term, the techniques for uncertainty and incompleteness handling to be developed in this project will exert a major influence on the theory and practice of probabilistic data management and of ontological data access in a more traditional database and information system context, both within and outside the academic community. Thus, our work will be of benefit to all those working in the broad area of information systems, including researchers - both in academia and industry - in the fields of databases and ontologies. Specifically, in the context of Web data, our work will allow to deal with uncertainty and incompleteness in knowledge bases that result from the ontology-based extraction and integration of probabilistic data from the Web. Thus, other short-term beneficiaries will include researchers in both academia and industry working on the extraction and integration of probabilistic data from the Web, as well as on query answering from the resulting knowledge bases. 

Unsurprisingly, a strong interest in the project is coming from companies working in these areas, like our project partner Wrapidity. One business case that Wrapidity is interested in is understanding and extracting business information from Dark Data, e.g., to analyse investment trends in the region. This requires a (currently unavailable) scalable, expressive, and flexible reasoning on especially uncertain and incomplete data. Since incompleteness and ontological commonsense handling in probabilistic databases includes as special cases uncertainty and incompleteness handling in single and distributed ontologies, other short-term beneficiaries are researchers and developers dealing with large uncertain and incomplete ontologies.

In the longer term, beneficiaries will be data analysts working with probabilistic data who need to handle incomplete data, as well as researchers and developers who need to exploit probabilistic data in applications. Hence, our work will also be of benefit to researchers in science, industry, governments, and healthcare in such diverse areas as, e.g., biology, medicine, geography, astronomy, agriculture, and aerospace. For example, together with a natural language interface, the realistic probabilistic data models and their scalable query answering techniques to be developed in this project will in the long term pave the way for powerful question answering systems in healthcare or even for medical diagnosis. 

The project will also be of wider benefit to the UK's research community by answering important open questions, contributing to the UK's research base, and helping to further cement the UK's world leadership in this research area. In summary, the project will contribute to enhance the UK's scientific relevance and excellence."
19,955BD7FE-1A1B-46AB-928E-C4C20C985291,Bilateral ESRC/FNR: Experimental Assessment of the Societal Impact of Algorithmic Traders in Asset Markets,"The has been proliferation of computerized algorithms traders (ATs) in electronic markets. This has generated considerable policy debate on whether the presence of ATs promotes or obstructs healthy markets. Further policy concerns are what steps - if any - should be taken to regulate AT behaviour and what responsibilities should be assign to market exchange providers regarding disclosures of AT participation, or whether they should be compelled to provide AT-free alternatives. Our study generates evidence to inform such policy debates and policy recommendations. 

We use the methodology of experimental finance to gather this information. We conduct asset markets with financially motivated human traders and introduce AT's and information about their presence in a controlled way. Unlike traditional asset markets we control, and therefore know what the underlying fundamental values of assets are and what information each traders knows about these. Also, we have clear identification of what markets actions are taken by the AT's and their underlying strategy. We get further clean measurements by exogenously controlling the types of exchanges that are available to the traders. 

We first assess whether retail and institutional investors are averse to having alternative types of ATs participating in the same markets. Alongside this analysis, we also measure the market impacts and wealth distribution effects of introducing AT-free &quot;safe haven&quot; exchanges. 

We then turn our attention to arbitrage, a fundamental principle that drives market efficiency. AT's that seek riskless opportunities for instantaneous profit monitor multiple exchanges which facilitate the trade of the same asset. They can either look for attempt to create mispricing opportunities in which they purchase the asset in one exchange and sell it at nearly the same time at a higher price in a different exchange. In our experiments, we examine the impact of liquidity consuming ATs who arbitrage through market orders and liquidity providing ATs who arbitrage through limit orders. We also assess the impact of high frequency trading by varying the speed of the ATs between that at which humans can take market actions to those who act at sub-human speed. 

There is a strong belief in the artificial intelligence and computer science community that a key attribute for ATs to outperform human traders, and therefore be a viable alternative for investors to use, is that they trade fast. In an efficient market speed leads greater opportunity to trade with misprices limit orders, and it also allows on to maintain the proprietary position in the order book to capture market at an advantageous price. The existing evidence supporting this belief comes from hybrid experiments/simulations in which equal numbers of ATs and human traders interact in a market. We introduce unbalanced designs, with ATs and human traders placed on opposite sides of the market, to measure whether this impacts aggregate performance of the market and traders. We also, vary the relative market power of the sides of the markets. 

Finally we conclude with an assessment of the impact of placing Dark Pool markets, where assets are traded bilaterally with delayed price announcements, alongside public markets. Then we assess the impact on allowing predatory ATs into either of these markets. This allows us to address contemporary regulatory problems in which private providers of Dark Pool markets have a moral hazard in preserving the ATs safe haven they promise to deliver.",,"The role of algorithms in asset markets is a current and ongoing policy question. The objective of a financial market is to process and clear information from subjective valuations. Algorithms can only process the information available to them from the history of prices and any specific pricing rules programmed in to them. Hence a purely algorithm driven market could have a far lower information content than one driven by human beings incorporating all available information resources. The major difference being the speed at which new information is processed. The predominant research paradigm for assessing the impact of high frequency algorithms in financial markets has been to look at historical patterns in quoting and transactions. Unfortunately, (this research has provided little in the way of definitive results, we propose to conduct as series of laboratory experiments to assess the precise effect of different market structures and typologies of algorithmic trading. This research should provide foundational results that can be used as basic evidence for the regulation of markets. 

The main, non academic, beneficiaries of the proposed research schedule are regulators and exchange providers such as the financial conduct authority, Bank of England and the London Stock Exchange. In addition over the next three years (the lifecycle of the project) a number of national and supranational legislative mechanisms will be drafted such as the European Commission Markets in Financial Instruments Phase II (MiFID II) and any follow up. Another interesting legislative adjustment may occur in the United States if there is a refinement on the regulation on National Market Systems (reg-NMS) which enshrines the best execution. The reg-NMS &quot;order protection rule&quot; requires that brokers execute against best price and not speed. A further decision to restrict co-location to authorized brokers in the US has also been enacted. So there is an active legislative agenda both in Europe and the United States. Our results will provide helpful evidence for future market design and any implementation of restrictions on trading.

Our results will also be of interest to market participants. Market participants are often divided into two groups, liquidity demanders (those who seeks a specific position in a security, long or short) and liquidity providers (those who trade to provide positions to liquidity demanders and charge a small fee for this service). Liquidity providers, such as NASDAQ market makers provide near automated services at high speed, but generally are not trading to obtain a particular position. The difference between a high speed market maker and a high frequency trading algorithm can be quite small, both are designed to seek out arbitrage opportunities between quotes. Hence, results that allow the discrimination between typologies of algorithm will be useful in ensuring that regulation is sufficiently nuanced to ensure that transaction costs are not increased. For instance, the THOR trading algorithm (Royal Bank of Canada) has received some publicity as it forces stock quotes to be displayed at the exact same time, even thought this might be a fraction slower than the best execution time. Our results can be used to establish if this type of algorithm &quot;hobbling&quot; approach is the most appropriate in financial markets.

Finally, our results will also be of interest to the psychology community that looks at the interaction between human beings and algorithms. Algorithmic trading is one of the few clean experiments that can be designed where there is a clearly delineated interaction between different types of algorithms and humans."
20,42FF1901-6518-48F9-9B24-A84ECA7485D9,Digging into Early Colonial Mexico: A large-scale computational analysis of 16th century historical sources,"The 'Colonisation of America' is a fundamental process in the history of the modern world. Along with archaeological remains, the historical writings related to the establishment of the so-called Virreinatos constitute primary sources of information for the understanding of this period. An extended compilation of information ordered by the Spanish crown in the 16th century, called Relaciones Geogr&aacute;ficas, served to gather vast amounts of information about the New World through multiple records and descriptions, both in Spanish and indigenous style. Traditional research of these documents has relied on the close reading of a handful of these texts, which can take the scholar a life-time to examine. Using a Big-Data approach, this project will apply for the first time ground-breaking computational methodologies to study one of the most important sources for the colonial history of America, and it will identify, extract, cross-link, and analyse information of vital importance to historical enquiry. Our highly interdisciplinary team will combine techniques from different disciplines, including Corpus Linguistics, Text Mining, Natural Language Processing, Machine Learning, and Geographic Information Systems, to address questions related to the recording of information about indigenous cultures, the Spanish exploration of indigenous social and religious concepts, the appropriation and ideas about place and space in the indigenous world, and their attitudes towards politics and economy. In doing so, the project will transform the way historical sources and large corpora are approached and analysed by modern scholars.",,"While the development of the project will have significant value for established practitioners of history, archaeology, anthropology, linguistics, digital humanities, and computer science, working in a range of subdisciplines, we are strongly committed to form a skill-base of new academics with a robust history of collaborative work and interdisciplinary mind set. As such, we think that working with different strategies related to education, networking, and publicity will generate a positive impact and encourage the adoption of interdisciplinary methods and relations by a broad audience. We have identified a number of ways in which we will do this: the project will employ research associates, post-doctoral researchers; it will offer a PhD studentship; and it will organise short courses, local seminars, and expert workshops. One studentship in Computer Science will be offered by the project and is described in section a of the PMDC. In addition, the project will employ three research associates working in Corpus Linguistics/NLP, two senior research associates, one in Computational Linguistics and one in Spatial Humanities, one post-doctoral researcher in NLP and Computational Social Science, and two postgraduate researchers specialised in Mexican Colonial History. All of them will work in a highly interdisciplinary environment and will develop considerable skills in their own fields, as well as in Digital Humanities. Where adequate, they will contribute to publications either as co-authors or on their own, and the team leaders will look to actively support their academic development. 
The Universities of Lisbon and Chester will contribute, at no cost to the project, with the organisation of two courses each year, one in Spatial Humanities (Chester), and another in NLP and Machine Learning (a summer school in Lisbon). In addition, a focused conference and extended workshop in computational approaches to historical texts will be organised in year 2 by our team in Mexico. The PI and the team leaders have extensive experience delivering courses in these topics, which have a huge demand across Europe and beyond.
The results of the project will be disseminated to students and an academic audience via workshops, papers in conferences, at least five articles in international peer-reviewed journals, and multiple contributions to other possible book collections. The outputs will target journals from a range of disciplines (archaeology, history, linguistics, computer science, geography, digital humanities) and period-specialisation, looking to reach a broad and diverse academic audience. In order to disseminate our findings to stakeholders beyond the academy, including heritage managers, librarians, educators, and the wider public, the team will contribute to public lecture series and will engage actively with social media, the project's blog, and media interviews. Our project will therefore enhance both academic and public appreciation of the possibilities in the use of data and the development of digital technologies and methods for a wide range of research in Humanities fields. From the start of the project, our aims and emerging results will be communicated to the public on a website maintained by the University of Chester. This will be linked to an active presence in social media via Twitter, Facebook and Google Plus. The blog and a dedicated YouTube channel will feature our research processes and experiments, as well as videos of lectures and presentations. 
All resources and code generated within the project will be stored in an open collaborative repository (i.e., the project's GitHub). The datasets (e.g., the annotated corpus and gazetteer) and other research outputs will be stored in the Digital Humanities Research Centre Repository and the Museo del Templo Mayor in Mexico, the UK Data Service and they will also be 'returned' to the Internet Archive and to Europeana."
21,048A1B40-DA0C-4299-9D3A-CF75C0833E19,FloraGuard: Tacking the illegal trade in endangered plants,"Over the last 60 years, commerce in exotic wild plants increased in Western countries (Sajeva et al 2007). Alongside the legal trade in plants, the profitability of the market also boosted illegal markets. Wild plant crimes have long been a focus of concern mainly in conservation science. In criminology, while the illegal trade in wild animals (and animal parts eg ivory) is receiving increasing attention, the illegal trade in plants has so far been under-investigated. However, wild plant trafficking threatens and destroys numerous species and important natural resources (Herbig &amp; Joubert 2006) and hinders the rule of law and security as profits are also used to finance other forms of trafficking (WWF 2016). The Internet has increased the illegal trade in wild plants, facilitating the encounter of supply and demand; no matter how highly specialised the market in certain wild plants, it is much easier to find potential buyers or sellers online than in the physical world (Lavorgna 2014a). There is consensus that the policing of such a criminal activity is still scarce and poorly resourced (Nurse 2011; Elliot 2012; Lavorgna 2014a; Lemieux 2014; Runhovde 2016). A major challenge is the fact that law enforcement agencies have limited training opportunities and lack of equipment and specific expertise to counter effectively this illegal trade (CITES2016). In this context, the question of how can we best control and prevent this criminal market needs to be addressed.

The proposed project combines innovative and cross-disciplinary ways of analysing online marketplaces for the illegal trade in endangered plants and analyses of existing policing practices to assist law enforcement in the detection and investigation of illegal trades of endangered plants. It focuses on the UK, which serves as a major transit and destination market for the European region (EU Commission2016).

The result of this research will be of significant importance for the work of law enforcement (eg national wildlife crime units, custom officers) in combating the illegal trade in endangered plants (in both its online and offline elements), disrupt criminal networks involved in such trade, and preserve biodiversity. In line with the latest WWF position paper (WWF 2016), the project fosters the improvement of awareness and technical capacity in investigation and prosecution services for wildlife crimes. The proposed approach will identify and disseminate best practice for other researchers and law enforcement officers with an interest in online crime markets and wildlife policing; in addition, it will improve our understanding of the online marketplaces and the offline market routes for the trafficking of endangered plants into Western countries, supporting new avenues of investigation. By integrating insights and expertise from criminology, computer sciences and conservation science, the proposed project has also important implications for demonstrating interdisciplinary methodological developments.

The research is structured around three cumulative work-packages (WP). WP1 comprises analysis of economic, social and geographical dynamics of a sample of online marketplaces active in the UK and associated with the illegal trade of endangered plants. WP2 focuses on the policing of this criminal activity by mapping current law enforcement practices and interventions, assessing their effectiveness in the light of the findings of WP1, and identifying law enforcement's needs for more effective policing. WP3 develops and tests a digital package of resources to assist law enforcement investigations into illegal trades of endangered plants in the UK. In doing so, it promotes engagement and effective communication with a non-academic audience (law enforcement, NGOs, botanic gardens, international institutions). The Royal Botanic Garden (Kew, the scientific authority for CITES plant trade in the UK) and the UK Border Force are formal non-academic partners to this project.",,"Who will benefit from this research?
Our approach integrates criminology, computer science and conservation science to improve our understanding of plant trafficking towards the UK and develop a package of digital resources to assist law enforcement in investigating the criminal market in endangered plants. This is of interest for national law enforcement bodies working to counter this illegal trade (e.g. the National Wildlife Crime Units within the UK Police Force), as well as for national and international policy makers and law enforcement agencies looking to review (traditional and cyberspace) policing approaches (e.g. the UK Partnership for Action Against Wildlife Crime, the Police Advisory board for England and Wales) and NGOs (e.g. TRAFFIC, WWF). 

How will they benefit from this research?
Our impact strategy aims to target UK enforcement agencies to raise awareness of the illegal trade in endangered plants and assist their policing strategies. Ultimately, &quot;FloraGuard&quot; will: 
1) Significantly improve the understanding of socio-economic and geo-social patterns in the online-facilitated trade of endangered plants. Currently this is an under-researched area of criminality. We expect &quot;FloraGuard&quot; to provide an up-to-date snapshot of the marketplace, how it operates and how policing interventions could make the most effective difference to disrupt this. 
2) Create a unique dataset of online illegal wildlife trade activity available today, which will be of immediate relevance for other researchers interested in this area.
3) Create and support a community of experts and practitioners crossing national, institutional and sector boundaries. The network is designed to outlast the duration of the project, providing a lasting impact for the countering of illegal plant trafficking in the UK.
4) Promote knowledge exchange and capacity-building for law enforcement. By sharing the digital tools, law enforcement agencies will be able to run analyses of the online aspects of the criminal market for themselves.

For 1) and 2), our methodology allows us to build an up-to-date dataset for selected online marketplaces for endangered plants. This approach has potential applications for other forms of online criminality (e.g. arms trafficking). The data collected will be released to authorised parties starting from month 15, so that law enforcement and cyber-security professionals in areas other than the illegal wildlife trade can build upon our work.

For 3) and 4), we will engage with Project Partners throughout the duration of the project, with their ideas and expertise shaping the project outcomes. This is important as we want to motivate our Partners to champion our policy paper and the digital package of resources in the medium term and see them shape future governmental policy. Bilateral meetings will discuss progress, methodological issues and preliminary results at key milestones (see Gantt Charts). Additional online meetings will take place to keep partners updated and get feedback. We will also establish a secure social network accessible from the project website for sharing updates and facilitate the direct involvement of partners in developing the digital resources.

The final part of each WP will focus on delivering policy level impact. We will use the expertise at PublicPolicyISouthampton to create &quot;science-to-policy&quot; outputs to be distributed via the project website and a final workshop in Southampton. We will also work with our Partners to provide briefing for governmental committees and charities including the Environment Agency, the Police Advisory Board for England and Wales and the British Wildlife Rehabilitation Council. At the final workshop, champions from each Partner will be identified to ensure our outputs are followed through to include our results in the next UK Border Agency strategy and the UK Government Commitment to Action on the illegal wildlife trade."
22,89FC94D8-2344-4ED8-B1BE-8E950A546137,Fast Runtime Verification via Machine Learning,"When programmers make mistakes, costs can be huge. Programmers make mistakes because they are human. To help them, researchers are developing tools. No tool is a silver bullet; each has advantages and disadvantages. We focus on one kind of tool: the runtime verifiers. Programmers use runtime verifiers more and more, which attests to their utility. Runtime verifiers observe a program while it runs and look for signs that something is amiss. This slows down the program, which is why runtime verifiers are used only by programmers, before releasing the program to users. If runtime verifiers would be more efficient and would not slow down programs so much, then they could remain active in released programs, which would make them more useful. For example, they could be used for detecting security intrusions in deployed systems.

This research project is focused on increasing the efficiency of runtime verifiers.


A Cambridge study from 2013 estimated that the worldwide cost of fixing software defects to be $310 billion per year. A large part of this cost is programmer time. The interval between finding a defect and fixing it is not so small because the task is often not so easy. Some defects are security vulnerabilities, which makes the fixing interval a window of opportunity for criminals. The window of opportunity is bigger than the fixing interval because it also includes the time until users decide to update their software. The size of the window of opportunity is important. For example, updating software regularly is recommended by the UK government, in its Cyber Essentials Scheme. This guidance was issued after a government study showed that, in 2014, nine out of ten large organisations in UK suffered security breaches, and each security breach had an average cost of 1.5 million pounds.

Runtime verifiers could be used to reduce the window of opportunity that criminals have, as follows. Programmers describe scenarios that should not happen. A runtime verifier then observes the program while it runs and, if one of the forbidden scenarios does happen, then the runtime verifier can take an action. For example, the runtime verifier could be instructed to kill the program, because a program crash is preferable to a security breach. Alas, such a use case for runtime verifiers is unrealistic at the moment, because no user would tolerate a program that runs ten times slower than usual just for the benefit of extra protection against security intrusions. In this project we aim at finding a better trade-off. We want the program to run only slightly slower than usual and, in return, we are willing to give up some of the guarantees offered by a runtime verifier, but not much. But, how to find a good trade-off? The key will be using a toolkit developed recently in the machine learning community.",,"Let us describe a vision, an ideal outcome, in which everything in this research proposal goes according to plan. By the end of 2019, the TOPL runtime verifier will be a reliable and efficient tool that developers use increasingly. Using TOPL, developers deliver software with two versions: standard and hardened. The hardened version is slightly slower but comes with extra protections. With high probability, when a developer assumption is violated, the hardened version informs them. Also with high probability, security intrusions relying on software defects are intercepted and interrupted.

There will be some limitations. For example, TOPL works only with the Java programming language and the TOPL property-specification language. It might be that the latter is not expressive enough in some situations. However, the basic techniques we will develop will not be tied-in with either Java or TOPL. Authors of other runtime verifiers will want to adapt our techniques to their tools. Some of these tools will be academic (such as MarQ, LARVA, Mufin); others will be commercial (such as RV-Monitor and RV-Predict). Tools like AdressSanitizer, which are widely used and have characteristics similar to runtime verifiers, may also make use of some of our techniques.

Ultimately, progress in the area of runtime verification will lead to higher quality software, which will reduce the economic cost of software. Developers will spend a smaller fraction of their time fixing mistakes. Security intrusions will be successful less often."
23,041BFA5E-F360-4DE5-90DC-92167F93750C,eNeMILP: Non-Monotonic Incremental Language Processing,"Research in natural language processing (NLP) is driving advances in many applications such as search engines and personal digital assistants, e.g. Apple's Siri and Amazon's Alexa. In many NLP tasks the output to be predicted is a graph representing the sentence, e.g. a syntax tree in syntactic parsing or a meaning representation in semantic parsing. Furthermore, in other tasks such as natural language generation and machine translation the predicted output is text, i.e. a sequence of words. Both types of NLP tasks have been tackled successfully with incremental modelling approaches in which prediction is decomposed into a sequence of actions constructing the output.

Despite its success, a fundamental limitation in incremental modelling is that the actions considered typically construct the output monotonically, e.g. in natural language generation each action adds a word to the output but never removes or changes a previously predicted one. Thus, relying exclusively on monotonic actions can decrease accuracy, since the effect of incorrect actions cannot be amended. Furthermore, these actions will be used to predict the following ones, likely to result in an error cascade.

We propose an 18-month project to address this limitation and learn non-monotonic incremental language processing models, i.e. incremental models that consider actions that can &quot;undo&quot; the outcome of previously predicted ones. The challenge in incorporating non-monotonic actions is that, unlike their monotonic counterparts, they are not straightforward to infer from the labelled data typically available for training, thus rendering standard supervised learning approaches inapplicable. To overcome this issue we will develop novel algorithms under the imitation learning paradigm to learn non-monotonic incremental models without assuming action-level supervision, relying instead on instance-level loss functions and the model's own predictions in order to learn how to recover from incorrect actions to avoid error cascades. 

To succeed in this goal, this proposal has the following research objectives:

1) To model non-monotonic incremental prediction of structured outputs in a generic way that can be applied to a variety of tasks with natural language text as output

2) To learn non-monotonic incremental predictors using imitation learning and improve upon the accuracy of monotonic incremental models both in terms of automatic measures such as BLEU and human evaluation. 

3) To extend the proposed approach to structured prediction tasks with graph as output.

4) To release software implementations of the proposed methods to facilitate reproducibility and wider adoption by the research community.

The research proposed focuses on a fundamental limitation in incremental language processing models, which have been successfully applied to a variety of natural language processing tasks, thus we anticipate the proposal to have a wide academic impact. Furthermore, the tasks we will evaluate it on, namely natural language generation and semantic parsing, are essential components to natural language interfaces and personal digital assistants. Improving these technologies will enhance accessibility to digital information and services. We will demonstrate the benefits of our approach through our collaboration with our project partners Amazon who are supporting the proposal both in terms of cloud computing credits but also by hosting the research associate in order to apply the outcomes of the project to industry-scale datasets.",,"- Economy

The two applications we will focus on in the project, natural language generation and semantic parsing, are key technologies in a variety of commercial products which require generating and understanding language. In particular, personal digital assistants such as Google Now, Microsoft's Cortana, Amazon's Alexa and Apple's Siri are used by millions of users at home or on their mobile devices and are of great importance to these companies since they act as gateways to many of the services and products offered by them. 

- Society

Personal digital assistants and natural language interfaces are used by a large number of users. Thus improving technologies of language generation and semantic parsing through non-monotonic incremental language processing is likely to affect these end users by improving their experience. We will explore this during the research visit of the RA at Amazon and test our approach in the context of Alexa.

- Knowledge

The project aims to address a fundamental limitation in an approach successfully applied to a variety of natural language processing tasks. Thus we anticipate that we will publish our results in high profile natural language processing conferences. Furthermore, we will accompany the paper publications with open source implementation of our approach on the project github repository. 

- People

The project will have a positive impact on the careers of both the PI and the RA. It will enable the PI to build on his success and expertise he has developed in incremental language processing using imitation learning, and thus solidify his position in the field while simultaneously addressing a fundamental shortcoming in the approach. An EPSRC first grant would be of great significance to the PI as it will be his first time proposing and delivering a project on his own, which will provide him with useful experience and strengthen his profile in applying for further funding. Finally, the named RA has been working in language generation throughout his career and most recently with the PI in applying imitation learning to this task achieving state-of-the-art results."
24,28719A85-2572-4679-9F84-2495A68C32B3,Creative Economy Engagement Fellowships - 3D3 Consortium,"The project will support fellowships that aim to support:
 - some of the UK's most talented researchers and nurture future leaders
 - the broader skills development of high-calibre recent doctoral graduates in the art and humanities, particularly in relation to working with partners to support the wider impact of research.
 - projects broadly aligned with the core themes, challenges and opportunities that are highlighted in the Industrial Strategy Green paper
 - research which is cross-disciplinary and innovation-orientated
 - the best international talent",,See Case for Support
0,44170666-54D8-4BB7-9A41-42CA28A2AD98,Newton STFC-Narit: Using astronomy surveys to train Thai researchers in handling Big Data,"The most effective way of reducing levels of poverty in developing countries is through their continued economic development, which leads to increased levels of income per person. To remain competitive, however, a developing economy needs access to a workforce with increasingly sophisticated skills. For Thailand today, this means skills that enable innovation, allowing it to successfully compete against other developing and developed economies. With more and more sectors collecting data on their customers, production lines, distribution networks, stock prices, etc., one of the most crucially needed &quot;high-level&quot; skills is the ability to handle large amounts of digital data. However, Thai data scientists and students typically lack ready access to very large datasets, which presents a barrier to their training in this area. Similarly, other scientists - including astronomers - typically lack the necessary data handling skills to efficiently store and analyse the large amounts of data they collect. Our project addresses both these problems by combining UK and Thai astronomers' access to and understanding of very large datasets with Thai data scientists' skills in databasing and machine learning to train Thai students in advanced data handling techniques.

Working under the supervision of the Thai and UK partners, the graduate students involved in the project will establish a data centre at NARIT to store and automatically analyse the hundreds of gigabytes of data generated each night by the Gravitational-wave Optical Transient Observatory (GOTO) - a major new survey telescope of which NARIT is a contributing member. In the process, the students will gain vital experience of database management and automated, machine learning-based data analyses. The resulting data centre will be an important research asset for NARIT astronomers and a key training resource for the broader Thai scientific community. Indeed, we will ourselves use the data centre as a training aid in teaching data handling skills to up to 60 other researchers and students during two 5-day practical workshops (one held each year of the grant with space for up to 30 trainees each). Through three graduate research projects, our team will develop the data centre into an automated storage and analysis system with the ultimate goal of outputting a prioritised list of targets for follow-up observations with NARIT's other observing facilities. In doing so, the Thai data centre will be a testbed for machine learning-based analyses, remaining at the forefront of all GOTO data centres in terms of data handling research. On completion of the project, the skills acquired by the Thai students will be readily transferrable to a diverse range of economic sectors such as information technology, medicine, finance, security, etc., thereby helping the further economic development of Thailand.",,"The immediate impact of this research will be on the Thai scientists and students who will receive training in high-level data handling skills either via their research projects or by attending one of the 5-day workshops. Following this training, the scientists and students will be able to apply these skills within a diverse range of growth sectors, such as finance, medicine, logistics, information technology etc. In doing so, they will benefit the wider Thai economy by helping it grow and compete internationally through innovation.

Further impact will be felt by the scientists and astronomers who are involved in GOTO - of which NARIT is a major contributing member. By establishing a data centre to host and automatically analyse large amounts of digital data, the outcome of the students' research will play an important role in extracting information from the vast amounts of data that GOTO will provide. This information can then be fed through to other NARIT facilities for follow-up observations, including by their network of Robotic Telescopes within outreach centres across Thailand. This latter point opens the prospect of the project raisng awareness amongst students, schoolchildren and the general public of the impact that Big Data can have on scientific research."
1,6D059268-6A9B-4B9F-A855-5AFB26A6CA2A,Learning Sparse Features from 4D fMRI Data for Brain Disease Diagnosis,"Machine learning endows computers with the ability to learn from data to help solve real-world problems. Due to the growth of big data, machine learning methods have become increasingly important tools in a wide range of applications including bioinformatics, computer vision, economics, and medicine. This project investigates machine learning for extracting useful information from fMRI data to help clinicians make more accurate diagnoses for certain brain diseases and develop more effective treatments for them. 

Currently, deep learning is the most popular machine learning method. However, it has highly complex architectures and needs vast amounts of data to learn a huge number of parameters. This leads to difficulties when the number of data examples available (n) is very small compared to the number of features in each data example (p), which is the &quot;large p, small n&quot; problem. Indeed, Geoff Hinton, the godfather of deep learning, said recently: &quot;One problem we still haven't solved is getting neural nets to generalise well from small amounts of data&quot;. 

Most existing solutions for the &quot;large p, small n&quot; problem represent data as vectors. With growing data dimensionality, such vector-based methods become inadequate for severe &quot;large p, small n&quot; problems, e.g., machine learning on fMRI data. fMRI data are sequences of 3D volumes, i.e., 4D data. They are noisy, big, and multidimensional, making comprehensive manual analysis infeasible and machine learning challenging. A typical whole-brain fMRI scan sequence has tens of millions features (voxel measurements), with a file size over 100MB. For such data, even a simple linear basis needs tens of millions parameters (deep learning will need far more) but in practice we often only have sequences for dozens of individuals available in a particular fMRI study due to high cost. 

Therefore, we aim to develop a new machine learning method for severe cases of &quot;large p, small n&quot; for multidimensional data such as whole-brain fMRI. We will take a tensor-based approach, where a tensor refers to a multidimensional array. Tensor-based methods have a much smaller number of parameters than vector-based ones. For typical whole-brain fMRI data above, a tensor-based multilinear basis needs only a few hundreds parameters, several orders of magnitude smaller than those needed by a vector-based, linear basis. We will generalise the state-of-the-art sparse feature learning methods for vector input to tensor-based ones for tensor input.

This will be the first study to learn sparse features directly from tensor representations of multidimensional data in a scalable and interpretable way. We will apply our algorithms to a large fMRI dataset on attention deficit hyperactivity disorder (ADHD) to accomplish two major tasks: prediction and interpretation. Firstly, we will detect ADHD and classify its subtypes via a small number of automatically selected voxels. Secondly, collaborating with a brain imaging expert, we will analyse the connectivity of brain regions corresponding to selected voxels to interpret the classification results, gain insights, and identify biomarkers to assist clinicians in further diagnosis and treatment. Our results will be fully reproducible with the dataset in the public domain and our software to be released as open source. The success of this project will advance the state-of-the-art of machine learning and provide a new enabling software tool to applications with severe &quot;large p, small n&quot; problems such as medical imaging with high-cost scanners (e.g., MRI or 3D mammography machines) and translational bioinformatics with big genomic data.",,"This research will contribute to strengthening the UK's world-leading position in both machine learning and brain imaging, with impact spanning four main areas.
 
A. Economy: Machine learning is an increasing economic driving force. With big data as the &quot;fuel&quot;, machine learning is the engine turning big data into great power in artificial intelligence applications for the next industrial revolution. This project focuses on expensive but limited &quot;fuel&quot; on which the state-of-the-art machine learning methods are having difficulties. Its success will lead to significant cost saving in data collection such as fMRI scans (about 400 GBP per scan) and other expensive medical devices. Successful machine learning from a smaller amount of data will greatly help with the analysis of rare diseases or disorders such as prion disease, where it is impossible to get large numbers of subjects. It will also enable data exploitation for new data analytic problems such as genomic data analysis in translational bioinformatics for personalised medicines. On the other hand, this project will help brain disease diagnosis, which will lead to significant healthcare cost savings and reduce many other direct or indirect costs. For example, autism and dementia alone cost the UK economy 32 and 17 billion GBP per year, respectively, and they both can benefit from this project. In the long term, this project will also benefit pharmaceutical companies with interest in developing drugs for brain diseases.
 
B. Society: Machine learning and artificial intelligence have significantly impacted our society in the past few decades. Nowadays, our everyday life has been significantly changed by and heavily depends on machine learning, from shopping, socialising, entertaining, to healthcare, banking, job hunting. This project will advance machine learning technology and thus further drive the impacts of machine learning on improving our quality of life, with healthcare being the most direct area. It targets brain diseases, such as autism and dementia, which are major societal challenges affecting children and the elderly respectively. It will help clinicians better detect and classify such brain diseases, understand their causes and different patterns, and eventually find effective treatments for their patients. In general, it will also help the diagnosis and treatment of rare or new diseases where only a limited number of data examples are available. 

C. Knowledge: This project will advance the state-of-the-art research in tensor-based machine learning and whole-brain fMRI data analysis, which are both areas of great importance. The findings of this project will also impact closely-related research fields including bioinformatics, computer vision, NLP, speech and language modelling, and even mathematics and statistics. Furthermore, while focusing on brain imaging applications, the method to be developed is general in nature like Lasso or Elastic Net so it will benefit researchers in many other areas of science and engineering in solving problems of similar nature. This project is interdisciplinary itself, which will foster more interdisciplinary works along this direction and beyond. 

D. People: This project will have a positive impact on the careers of the PI, RA, and collaborator. We will all gain additional knowledge and experience in machine learning, tensor analysis, and brain imaging. Such exposure will help us build a larger network of potential collaborators for future funding proposals. It will advance the RA's career with additional skills and benefit the PI's students via small projects on related problems. Despite his recent success with Hong Kong grants, the PI has not yet been involved in UK-funded projects. Having an EPSRC first grant is a very important step in his academic career, helping him consolidate his new lecturer position and raise his internationally profile in machine learning."
2,3BC6B63E-E6EF-4C60-B901-F574E62CD259,Assessment of Sea Surface Signatures for Naval Platforms Using SAR Imagery (AssenSAR),"In space imaging, enhanced image quality is key to the detection and characterisation of difficult and transient targets. For example, accurate evaluation of the sea surface conditions can help with the detection and characterisation of ship wakes. These provide key information for tracking (illegal) vessels and are also useful in classifying the characteristics of the wake generating vessel.

Until recently, one of the main factors hampering research into sea surface modelling was the lack of sufficient data of high enough quality, able to accurately describe the sea surface. Remote-sensing technologies have however shown remarkable progress in recent years and the availability of remotely sensed data of the Earth and sea surface is continuously growing. Several European missions (e.g., the Italian COSMO/SkyMed or the German TerraSAR-X) have developed a new generation of satellites exploiting synthetic aperture radar (SAR) to provide spatial resolutions previously unavailable from space-borne remote sensing. The UK is currently developing the first of a constellation of four satellites that will constitute the NovaSAR mission. This represents a milestone for Earth-observation capabilities but also requires the development of novel image modelling, analysis, and processing techniques, able to cope with this new generation of data and to optimally exploit them for information-extraction purposes.

Indeed, the mathematical modelling and understanding of wakes and other sea surface signatures can be greatly enhanced through image analysis and information extraction from SAR imagery. Hence, this project is concerned not only with the development and validation of new sea surface models, but also with the design of very advanced methods for enhancing SAR image quality and for subsequent information extraction.

The results of this project will be important in the detection and tracking of illegal vessels involved in smuggling goods or humans. They will also be indicative in terms of understanding and classifying the characteristics of the wake generating vessel. As a consequence, the work will directly benefit the design of stealthy vessels that can avoid such detections, reducing the risk to naval operations.",,"AssenSAR is highly relevant to space imaging and hence to the UK's satellite industry (one of the Eight Great Technologies) where enhanced image quality can be exploited to detect and characterise difficult and transient targets.

The results of this project will be important in the detection and tracking of illegal vessels involved in smuggling goods or humans. They will also be indicative in terms of understanding and classifying the characteristics of the wake generating vessel. The work will thus inform the design of stealthy vessels that can avoid such detections, reducing the risk to naval operations.

The range of commercial and societal impact objectives are as follows: 

Capability: We will help to shape UK capability across the satellite industry through better understanding of remote sensing imagery and its utility in information extraction in marine environments. Our researchers will be trained with the technical and enterprise skills needed to deliver the impact of this research across the wider community. 

Commercial: Close coupling with partner organisations will facilitate pull-through into advanced products, opening new opportunities for innovation in both marine surveillance and remote sensing imaging. By liaising with the UK Satellite Applications Catapult, it is expected that the innovation potential of this project will be nurtured and eventual commercialisation of research will be facilitated. There are significant foreseeable opportunities for the creation of new starts-up to exploit the outcomes of this project.

Operational Practice: We will concentrate on the design of new denoising and super-resolution protocols for situations where automated processing is used to support human decision making. Obvious examples are in the viewing and analysis of remote sensing imagery and information extraction therefrom."
3,B46A2D26-1225-4A08-8303-FD55210AC5DD,Discovering Individual and Social Preferences through Inverse Reinforcement Learning,"Organisations that provide services and create products often base their decisions on questionnaires and/or other explicit forms of communication with their user base (e.g. patients, customers, citizens). The aim of this information exchange between providers and users is to uncover the users' &quot;reward function&quot;, i.e. what users actually want from their interactions and what issues exist with the current product/service line-up. Explicit forms of information exchange can be cumbersome and expensive to design for organisations and are intrusive to the user. Furthermore, response bias is a well-known problem for survey based methods, particularly around sensitive topics, where respondents maybe unwilling to engage due to social or cultural concerns. Some practical solutions to response bias are provided by indirect questioning methods (item count and randomized response techniques). However, none of these solutions are practical for large scale and real time settings.
We postulate that ideally an organisation should try to elicit the reward function of its user base (i.e. what states are preferred by users) by using observational data generated from user activity. Inspired by recent literature in AI research, we propose a three-facet programme that aims to directly attack the problem of what users want by a) trying to infer the user reward function through the collection of behavioural data (e.g. website clicks, traffic behaviour, movie preferences); b) creating short, non-intrusive online questionnaires that will remove any uncertainties; and c) exploiting user preferences in order to improve service and product provision.
The proposed research aims to contribute to developing methods that can be embedded in artificial intelligence systems which must elicit and understand preferences by interacting with humans in order to adapt their behaviour and allow for a more natural experience and interaction.
Through this research we have four key objectives: (a) understand user preferences and develop methods to uncover and learn the reward function through data and behaviours; (b) develop interactive and conversational methods for eliciting responses and interactions from users that allow for a more natural user experience with automatic systems; (c) explore the social limitations of our approach (for instance, to what extend are personal rewards not dictated by individual preferences, but rather by social coercion?); and (d) investigate what steps can be taken to fully automate the procedure of provisioning new services and products through eliciting preferences via the methods developed under (a) and (b).
This Fellowship provides a unique opportunity to bring together artificial intelligence techniques and social science to tackle problems that are faced by a range of businesses and organisations in dealing with clients and customers and attempting to elicit preferences and needs through behaviours and interactions. We will be working closely with our industry partners in this project, British Telecom (BT) and the Essex County Council (ECC), to investigate the issues and challenges of eliciting and understanding preferences as being faced in their own contexts to inform and shape the programme of work.",,"The project has been conceived in partnership with the collaborating business partners with an emphasis on practical deployment of the developed methods and results of the project during the latter stages of the research. The business partners will be working with us during the Fellowship to guide and co-design the research and ensure a two-way feedback loop that will maximise the impact of the work undertaken and also facilitate dissemination beyond academic. Close engagement with stakeholders will also ensure the relevance of the generated results and outcomes and their take-up and embedding in business practices of organisations. Our user (partner) and stakeholder activities consist of a number of elements design to enable cross-fertilisation of ideas and truly participatory research and dissemination of outcomes:
Initial scoping workshop. To help shape the research questions and context of the work to be undertaken, the Fellowship will commence with a a focused workshop to involve key people from the business partners, the Fellow, and the Academic Team (PI and CIs). The purpose of the workshop will be three-fold: (i) the Fellow and Academic Team will have the opportunity to meet the key people to be involved in the project from the business partners' side and form a close working relationship; (ii) the business partners will have the opportunity to describe the problems that they face and provide the context of the work; (iii) the mode of collaboration will be discussed and agreed. This will then help the Fellow to work more closely with the business partners in the next step to refine the questions that the research will aim to address and position them in the context of the problems that these organisations face. Following the workshop, a detailed workplan with the research questions, milestones, and timelines will be drafted with further input from the partners. 
Continuous engagement and refinement of research questions and approach. The research questions will continue to be refined as part of the short-term placements of the Fellow within the respective Data Science teams of the partners. Both organisations have committed to hosting the Fellow within their Data Science teams for short periods of time to enable the research to remain aligned and relevant to the problems that they face and to enable true knowledge exchange, cross-fertilisation of ideas and imparting new skills. These short placements will also enable the Fellow to build close working relationships and obtain a more comprehensive understanding of the business problem and needs in-situ. Presentations at meetings of the Data Science teams and other teams within the business partners will facilitate raising awareness of the project and dissemination of intermediate project outcomes.
Dissemination of project outcomes to the business partners. At the end of the Fellowship a workshop will be organised to disseminate the results of the project. This will bring together the two business partners as well as the other businesses that have expressed an interest to be involved in the dissemination of the project outcomes (as well as new business partners and organisations that the Fellow may develop links with during the Fellowship) in order to discuss and evaluate the results and see how these can be embedded in their business practices. One of the project outputs at the end will be a report for each partner on how the project results could be embedded within their businesses practices and recommendations on how this can be taken forward."
4,1C0B4130-0FF1-479D-88EE-E632773CAB02,Stable Prediction of Defect-Inducing Software Changes (SPDISC),"Context: software systems have become ever larger and more complex. This inevitably leads to software defects, whose debugging is estimated to cost the global economy 312 billion USD annually. Reducing the number of software defects is a challenging problem, and is particularly important considering the strong pressure towards rapid delivery. Such pressure impedes different parts of the software source code to all receive equally large amount of inspection and testing effort. 

With that in mind, machine learning approaches have been proposed for predicting defect-inducing changes in the source code as soon as these changes finish being implemented. Such approaches could enable software engineers to target special testing and inspection attention towards parts of the source code most likely to induce defects, reducing the risk of committing defective changes. 

Problem: the predictive performance of existing approaches is unstable, because the underlying defect generating process being modelled may vary over time (i.e., there may be concept drift). This means that practitioners cannot be confident about the prediction ability of existing approaches -- at any given point in time, predictive models may be performing very well or failing dramatically.

Aim and vision: SPDISC aims at creating more stable models for predicting defect-inducing changes, through the development of a novel machine learning approach for automatically adapting to concept drift. When integrated with software versioning systems, the models will provide early, reliable and automated defect-inducing change alerts throughout the lifetime of software projects. 

Impact: SPDISC will enable a transformation in the way software developers review and commit their changes. By creating stable models to make software developers aware of defect-inducing changes as soon as these are implemented, it will allow targeted inspection and testing attention towards defect-inducing code throughout the lifetime of software projects. This will reduce the debugging cost and ultimately lead to better software quality. 

Proposed approach: an online learning algorithm will be developed to process incoming data as they become available, enabling fast reaction to concept drift. Concept drift will be detected using methods designed to cope with class imbalance, which typically occurs in prediction of defect-inducing software changes. Class imbalance refers to the issue of having a much smaller number of defect-inducing changes than the number of safe changes. The proposed approach will also make use of data from different projects (i.e., transfer learning between domains) to speed up adaptation to concept drift.

Novelty: SPDISC is the first proposal to look into the stability of predictive performance over time in the context of defect-inducing software changes. Most previous work ignored the fact that predictions are required over time, being oblivious of the instability of predictive performance in this problem. To deal with instability, SPDISC will develop the first online transfer learning approach for predicting defect-inducing software changes. 

Ambitiousness: online transfer learning between domains with concept drift is not only a very new area of research in software engineering, but also in machine learning. Very few approaches exist for that, and none of them can deal with class-imbalanced problems. Therefore, SPDISC will not only advance software engineering by enabling a transformation in the way software developers review and commit their changes, but also advance the area of machine learning itself. 

Timeliness: given the current size and complexity of software systems, the increased number of life-critical applications, and the high competitiveness of the software industry, approaches for improving software quality and reducing the cost of producing and maintaining software are currently of utmost importance.",,"SPDISC's beneficiaries are the software industry, software users and related scientific communities. 

1) Software Industry
The software industry is SPDISC's main beneficiary. The UK software industry is estimated to be worth more than 9bn GBP, and is the second largest market by value in the EU. Globally, the software industry's estimated value is over 407bn USD. And yet, the global cost of debugging software is estimated to be 312 billion USD annually, representing an enormous loss of revenue. SPDISC will lead to an impact on the economy by reducing debugging cost and increasing software quality. 

In particular, SPDISC will empower software developers with early, reliable and automated alerts of defect-inducing software changes throughout the lifetime of software projects. It will enable a transformation in the way software changes are reviewed and committed in software development companies who use software versioning and bug-tracking systems. Defect-inducing changes will be automatically pinpointed for attention right after their implementation, allowing easy and wise allocation of the limited testing and inspection resources. This is specially desirable in companies leaning towards a more agile software development process. 

As the software changes will be fresh in the developers' minds when defect alerts are triggered, their inspection will be much cheaper than later debugging cost. In addition, changes typically have few lines of code, further facilitating inspection. Therefore, SPDISC's approach will reduce the risk of committing changes that will lead to defects, reducing debugging cost and increasing software quality. The lower debugging cost will translate into cheaper software cost, as finding and fixing defects typically takes 50% of a software developer's time. 

From a project management perspective, as each software change is inherently associated to a single developer, the assignment of developers to inspect defect-inducing changes will be straightforward. With SPDISC, the task of deciding which parts of the source code should receive increased attention and by whom can be delegated to the software developers themselves, freeing project managers to other tasks. 

Both large enterprises and SMEs can benefit from SPDISC, as its approach automatically adapts to different environments. I anticipate that software development tools based on SPDISC will be commercialised in the future. One of SPDISC's industrial partners has already expressed interest in doing that. This will assist SMEs in benefitting from SPDISC, increasing their competitiveness and driving faster and more balanced economic growth. This will in turn lead to an impact on society by increasing wealth and employment. 

2) Software Users
The more cost-effective software development enabled by SPDISC will consequently bring benefits to software users, who can be private users, users of public services, or other enterprises. Cheaper cost will facilitate access of private users and public services to software. Higher quality will improve quality of life through better and safer software experience. This is key to a world of smart cities, which are greatly controlled by software. It is also important to life-critical software applications, which could pose serious threats if defective. Cheaper and higher quality software will increase the competitiveness of other enterprises who depend on software, driving faster economic growth. Extensions of SPDISC's approach can also potentially help to solve other data analytics problems than defect prediction. 

3) Scientific Communities
SPDISC will create a tighter bond between software engineering and machine learning through its new machine learning approach for software engineering. These two areas will benefit from this research. There will also be some impact on mathematical sciences, as part of SPDISC's foundation lies in this area. More details are in the academic beneficiaries summary."
5,065EE0ED-5A3C-4415-8589-8BE0B4E312D0,Social and Economic Implications of Transport Sharing and Automation,"This study will link the changing nature of jobs due to automation and the platform economy to regional infrastructure planning and transport operations, and the role specifically of transport automation within this context. The patterns and forms of jobs are changing due to many different reasons, leading to non-traditional work schedules and differences in commuting patterns, non-standard work travel patterns, and even elimination of certain jobs and creation of new ones, with significant implications for regional infrastructure planning and transport operations. At the same time, there are enormous changes anticipated in infrastructure and operations, due to large-scale automation in the transport sector (eg autonomous and connected vehicles). 

This project will make estimates of the changing nature of jobs due to these considerations at the regional level towards the goal of deriving the transport and regional infrastructural planning consequences. The project will use labour market survey data as well as privately-held labour market data on jobs, skills and industry to estimate regional variations due to these trends, given regional industry-occupation mix. These changes will be linked to the Spatial Urban Data System (SUDS), which is a UK-wide geospatial data infrastructure under development within UBDC containing transport infrastructural and operational conditions. , and which has been recently used to identify areas of transport poverty throughout the UK and the extent to which and which we will expand through work with the project's industrial partners. 

Using these data sources, we will identify regional automation risks due to unique industry and skill concentrations and derive transport and infrastructure planning implications. Within this context, we will also evaluate the role of autonomous vehicles given potentially different commuting patterns using specialist transport simulation models. We will further develop specialist transport simulation models to ascertain which packages of &quot;last-mile&quot; transport solutions (low-energy station cars, autonomous vehicles, shared transport, active travel and demand-response services) are likely to bring about high-quality, sustainable and socially-equitable forms of transport accessibility in areas at risk of changing nature of jobs. We will then combine the results of our various model scenarios, using ensemble forecasting methods utilising Bayesian Model Averaging or related techniques to ascertain which packages are more likely to bring about high-quality transport accessibility in the selected areas.",,"With 66% of the world's population estimated to be living in urban areas by 2050, the need to provide new transport infrastructure and to address traffic congestion, road fatalities, air pollution, and associated problems continue to generate debates in policy circles. 

Impact on local economic development and labour market planning: The work related to estimating the potential impact of automation and AI on jobs given skills required in the occupation-industry mix regionally available is likely to be of immense value to regional infrastructure planners and business owners, as well as for skills-development initiatives and organisations in the local economic development planning. Through UBDC's networks, we will engage local authorities and other stakeholders in co-creating our results on this topic, so as to involve our work in their planning processes.

Transport and infrastructure planning and operations impact: Additionally, there is a range of shared technology, automation and use of AI and Machine Learning (ML) being proposed in transport, and a growing business community involved in their development and use. Although the trends surrounding automation and sharing transport are being driven primarily by private companies, governments around the world are increasingly developing policies to support as well as to regulate many of these developments, and the UK government has an active programme on Connected and Autonomous Vehicles (CAV); among the high-value economic infrastructure to be funded through the 2016 National Productivity Investment Fund (NPIF) is &pound;390 million for future transport including ultra-low emission vehicles and CAVs. Additionally, various types of shared mobility services, e.g., car-sharing, dynamic ride-sharing, on-demand personal mobility vans, and express, crowd-sourced urban delivery services, under the banner of Mobility-As- A-Service (MaaS) are now offered by private companies in UK cities. Just as the introduction of the private car in the beginning of the twentieth century transformed the way we live our daily lives and the ways in which cities developed, large-scale automation, connectivity and sharing of mobility resources (sharing economy) are expected to be a step-change changing daily lives in future society and the form and functions in cities.We expect that our results will help highlight significant regional planning and operations impact of such technology, against the backdrop of changing commuting and work-related travel patterns resulting from the changing nature of jobs.

Industrial Impacts: The approach will allow us to evaluate spatial and regional effects of varying degrees of automation and sharing mobility, and to identify new markets in smart cities and urban planning. Our industry partner, Peter Brett Associates, views that the risks of emerging technology being left out of the planning agenda are great due to lack of empirical data, leading technology disruption in transport to occur in an ad-hoc way. Having the results of the analysis and the associated data would help them reach new markets and also to reduce uncertainty in their forecasts. Scottish MaaS, has similarly noted that the methods and results being proposed will help them reach new markets both geographically thereby opening up UK companies to a global pipeline of contracts in integrating CAVs into infrastructure planning and construction, and MaaS solutions in addressing expensive last-mile problems facing city managers worldwide."
6,72795679-4967-4648-B376-E96E5B6F80C5,Space and Narrative in the Digital Humanities: A Research Network,"Interacting with computer-based maps has made many tasks, such as navigating from one place to another, significantly simpler for many users. The technology behind these developments, however, depends on being able to pin-point roads, buildings, paths, and other features in a precise way. The space of the world people move around in day by day is not the only kind of space which it is important to represent on computers. For example, historical documents can describe where buildings were, or where events happened. Writings about journeys made in the past, can help us today understand our own environment and how it was shaped and is still being shaped by the ways it has been perceived and described. Narratives of journeys are significant too in imagined worlds whether created in literature, or through media such as film. Studying such journeys, as well as other kinds of imagined spaces in literature, also helps us understand the ways in which people tell stories as a way of communicating with each other.

The digital humanities includes the study of history, literature, and many other aspects of human experience, through digital technology. Ths technology may be used to process large amounts of information, which might come from historical documents or literary texts for example. This enables scholars to find patterns in the information, through techniques including visualisation of the data. These patterns can generate new questions or new perspectives on the world from which the data came. The digital humanities has successfully used Geographical Information Systems (GIS) for the study of data using computer-based maps. However, what can be done if all that we know is that the church is next to the house, or that the path went through the forest, or that the flood covered most of the town? These examples all include qualitative spatial relations: 'next to', 'through', 'covered most of'. These kinds of relations have been studied in computer science because, although they do not give enough information by themselves to plot things on a map, they can be represented computationally using logic. One motivation for the study of Qualitative Spatial Representation (QSR) in computer science is that humans often don't use very detailed spatial information. We might say ''move the chair next to the window'' instead of having to calculate the exact distances and angles involved. This kind of flexibility when dealing with locations is very similar to the need in the digital humanities to handle events, objects, journeys, and so on that cannot be pin-pointed on a map. 

Thus, the digital humanities needs more flexible computer-based ways of representing spatial information, and computer science has extensive research on QSR. These two areas of study have had almost no interaction before now. This network brings together experts from the humanities in areas including history, literature, and archaeology, with experts from computer science in areas including artificial intelligence and geographical information systems. The network will focus on how the digital humanities can use adaptations of the ways qualitative spatial relations are used in computer science. Two workshops will explore detailed case studies based on documents and other resources from the participants' areas of expertise. A third workshop will encourage the formation of a cross-disciplinary community centred on spatial information in the digital humanities. This workshop will report on the case study explorations. It will also allow researchers, and organisations who make archives and other historical records accessible to the public, to plan together the work needed to build digital tools that will help people handle spatial information in the humanities which is qualitative, metaphorical, vague, uncertain or ambiguous .",,"The main area for potential impact lies with organisations which make archive data and museum collections available to the general public as well as to academic researchers. Archive data can include references to places and locations which are vague or imprecise. This makes linking different documents relating to the same place challenging and also can make it difficult for users, especially the public, to find documents relevant to them. Beyond merely locating sources, the ability to enrich records of historic buildings, landscapes, or places with digital representations of personal narratives is one way that techniques for qualitative spatial representation are likely to have an impact on individuals and community groups interacting with information about localities of significance to them.

The project has four specific partner organisations who will all participate in some of the activities of the network including the concluding Outreach Workshop, which will also be open to representatives from other organisations working in related areas. Three of our partner organisations are; The National Archives, Historic England, and Leeds Museums and Galleries. The impact of the network for these organisations will initially be in introducing an awareness of techniques for dealing with qualitative spatial information. Access to expertise in computational representation of qualitative space. Beyond the lifetime of the network, this is expected to lead in the longer term to specific systems that will allow individuals improved ways of accessing vaguely located qualitative information.

The impact of the understanding of spatial information can extend beyond the digital humanities. Our fourth partner organisation, is the Defence Science and Technology Laboratory (DSTL), which sees potential impact in its need to understand spatial information present in sources such as text messages, phone conversations, emails, and other written and spoken means of communication. The relevance of the network here lies in the likelihood that qualitative spatial constructions appearing in historical narratives will, although culturally dependent, relate partly to ways that humans move and experience their environment. The involvement of DSTL could also help contribute to impact for the other organisations through providing a view of issues in a scientific context and bringing an awareness of a wider range of spatial information technologies."
7,A9EEEFE2-A07E-47C3-B328-8168D4ED3690,Hummingbird: Human-machine integration for biometric authentication,"We live in a technological age in which we can use our voice as a password to access online banking, and our children can pay for school lunches with a fingerprint. Biometrics, which reflect our physiological or behavioural characteristics, are now common as a way to prove our identity in order to access secure information, services or spaces. Given the important uses associated with biometrics, there is a fundamental need for accuracy in biometric analysis in order to encourage trust amongst both citizens and service providers. The feasibility study undertaken within the HUMMINGBIRD project will provide a human-inspired framework to address both needs.

The recent publication of two high profile report converge to make this endeavour timely and necessary. The first is the UK Governmental review on 'Future Identities', which recognised the transformative effect that digital technologies are having on identity. In particular, it noted the myriad of ways we now have to convey our identity, and to have it spoofed. The second is the UK Parliamentary Select Committee review on 'The Current and Future Uses of Biometrics' which highlighted two necessary future steps for biometric analysis: Analysis should draw on behavioural as well as physiological measures; and it should take full advantage of the combination of data across multiple biometrics and across decision makers in order to improve decision-making.

To address all factors, we propose an exciting project that will deliver a human-inspired multi-expert, multi-modality framework for biometric analysis. This will satisfy three aims: First, it will deliver enhanced algorithms for automated biometric analysis by incorporating successful strategies used by humans. Second, it will deliver a method of combining decisions made by humans and (enhanced) algorithms in order to boost accuracy. Third, it will deliver the potential to combine multiple biometrics, providing resilience in scenarios in which a single modality may be sub-optimal.

The HUMMINGBIRD project team possesses a unique combination of skills to explore this idea and indeed, we build on recently published theoretical work on this topic. In this proposal, we examine two biometrics - face and voice - which reflects the move to combine static physiological measures (facial images) and dynamic behavioural measures (temporal voice samples). We also concentrate on two decision-makers - the human and state-of-the-art automated algorithm - providing direct relevance to scenarios in which the human must be part of the decision-process (such as in forensic decisions). Our work will establish the fundamental performance levels of humans and machine algorithms when recognising faces and voices under optimal and sub-optimal presentation conditions. It will then seek to enhance the machine algorithms through incorporation of human rules and heuristics. Such a move offers the potential to boost accuracy and efficiency by streamlining automated solutions. More importantly, it exploits the fact that humans can outperform machine algorithms under some conditions, such as when trying to recognise a face under dim light, or a voice amidst noise. Finally, our work will apply an innovative data fusion model to combine the decisions of humans and machine algorithms from one biometric, and then from multiple biometrics. This novel and creative element of our work addresses issues of accuracy, disagreement resolution, and resultant confidence in an identity decision, when the situation is inherently uncertain. 

Arguably, biometrics reflect identity more directly than token or password systems because they rely on who we are rather than what we have or know. As such, biometric analysis is likely to remain a mainstay of identity management. The HUMMINGBIRD project presents real promise as a way to improve accuracy and confidence in that analysis, enabling accuracy of, and trust in, identity management as technology advances.",,"The HUMMINGBIRD project addresses the priority area of human identification through biometric analysis. Success in this area is paramount if the benefits of modern living are to be fully realised. For example, systems that accurately verify our identity allow us to move seamlessly through secure spaces such as work areas or airport boarding gates. Moreover, proof of our identity allows us to access e-systems such as online banking and eCommerce. Biometrics now provide an integral element to enable human identification. However, there are concerns regarding reliability of the automated systems responsible for biometric analysis. The HUMMINGBIRD project aims to enhance biometric analysis through application of a novel human-like computing approach.

Key societal and economic impacts are anticipated from the HUMMINGBIRD project, and several stakeholder groups have been identified, including citizens, as well as government and industry parties. Civic interest stems from a need on the part of the public to be assured of reliable means to prove their identity, and to protect their identity from misuse or spoofing. This requires that the tools we use are optimised for the job at hand, in a way that allows the intelligent combination of information across multiple measures and across multiple decision-makers. The HUMMINGBIRD project explores a novel framework to achieve exactly this. First, we seek to enhance machine analysis by integrating successful human strategies. Second, we seek to improve reliability by combining human and machine decision makers into the final decision. Third, we seek to improve resilience by incorporating more than one biometric measure into the identity decision. It is anticipated that the improved performance of the system, and the transparency gained through human involvement, will increase both accuracy and trust, and will boost civic confidence, particularly within low-throughput, high-risk scenarios. Within this feasibility study, our involvement with, and feedback to, civic groups comes through our dialogue with human participants, and our dissemination plans via science fairs, museum exhibits, media reports, and social media channels.

Societal impact will also be felt by government and industry stakeholders. Government agencies are especially interested in biometric identification, as evidenced through ongoing debate of the new Identity Assurance schemes, and the introduction of novel technologies such as eGates into UK airports. Within the forensic sector too, the Chief Scientific Advisor's report on 'Forensic Science', and the GO Science report on 'The Current and Future Uses of Biometrics', both published within the last 2 years, witness the continued interest in biometric analysis. The HUMMINGBIRD project will provide clear benefit, through contribution to governmental understanding and capability. This will be delivered through knowledge transfer channels, both through active Steering Committee involvement and dissemination to wider government groups. Alongside this, Industry interest centres on the practical question of how to deliver an identity management solution which combines user acceptability with a minimum of error. The relationship between the HUMMINGBIRD project and IBM as a hub for Emerging Technologies, provides a clear route to deliver benefit in a way that is ahead of the technology curve.

Finally, the HUMMINGBIRD project is also likely to support sustained economic impact for UK wealth and technology through renewed public trust in e-Commerce and digital business. Inward investment will enable an improvement in UK competitiveness within a digital marketplace. Indeed, through providing the scaffolding for increased reliability, transparency and resilience of identity management, technologies may emerge in diverse areas such as mobile phone providers, eHealthcare systems, social media platforms, and online banking that make a material difference to modern living."
8,3D53B313-DFF8-4539-91D2-AD233B32E863,Sleep cycling for Probabilistic Generative Models,"The media have lately been full of excitement about progress in Artificial Intelligence. Not only can computers now beat humans at Go, and detect cats in Youtube videos, but soon we will have robots in the house, self-driving cars, and many jobs might become automated. Apart from the societal challenges that this revolution will bring, many hurdles are still to be overcome before Artificial Intelligence will obtain truly human-like capabilities. 

In particular, current artificial systems might be very good at specific tasks, they cannot easily apply their processing power to other problems. Moreover, in order to become experts in a certain problem these machines often need millions of training examples. Current AI systems follow a strategy very different from humans and obtain their strength from brute compute power and massive amounts of data rather than by cleverness. This is also the reason why it is hard to communicate with these machines, understand their decisions and instruct them. The fact that computers use an approach that is so different from that used by humans seriously hinders application of AI to real world applications. The research community is well aware of these issues, and it generally believed that the problem arises because machines don't construct higher level understanding of the problems that they are solving. How this should be addressed is however not clear.

In humans and animals sleep plays an important role in creating high level representations. During sleep, the brain consolidates information, rearranges it, finds links between different types of knowledge, reformulates problems, and comes up with creative solutions. Most people have experienced this at some point - as they feel better able to solve a problem after a good night of sleep. It is only very recently that researchers have become able to manipulate the processes that go on during sleep, and thereby pick apart the roles of the various sleep phases play and the reason why the sleep phases are ordered in a particular way.

Here we propose to research how to processes occurring during sleep can be mimicked in computational models, and thereby open the possibility to build more human-like artificial systems.",,"By examining whether it is feasible to mimic the cognitive processes that occur during sleep in computational models, the research is in the first instance designed to benefit both computational and cognitive researchers. 

For computational researchers, there are benefits for those who seek to build better artificial systems, including academic researchers such as those grouped in the Alan Turing Institute, and the rapidly growing community of machine learners and data scientists employed in government, such as GCHQ, and industry, such as Google Deepmind in London. These researchers will profit by having insight to algorithms that process in a more human-like fashion, systems that use higher level and hence more compact ways to reach decisions, and systems that need less training data and can transfer knowledge between domains, and thereby increase energy efficiency, widen application domains, and reduce development cycles.

For cognitive researchers, the main benefits will stem from the increased understanding which stems from creation of a good computational model. The importance of the SWS and REM sleep stages, and the interleaving of these, for memory consolidation is too complex to understand without a computational model. To date, no such model exists, and we are unaware of any group working to develop one. Thus, the model which we propose to develop will be hugely beneficial to studies in this area, as it will make explicit testable predictions about how these sleep stages interact with memory. Beneficiaries of this will include academic researchers interested in memory consolidation, sleep, and the combination of these fields, as well as researchers interested in human development, and cognitive reasoning. 
Our access to these communities, including the iCub project, is detailed in the Pathways to Impact attachment.

There is an emerging market for technology that can enhance our memory, and a quickly growing crop of start-up companies aiming to do this through manipulation of sleep. Because our research will lead to a better understanding of how memory replay in each sleep stage, and in fact at each time in the night, impacts on consolidation, it will directly benefit these companies. For example, PZIZZ Ltd. is enhancing sleep through delivery of sounds on an mobile phone app, Rythm Inc. and Deep Wave Technologies Inc. are companies seeking specifically to enhance slow wave sleep. We are already working directly with these companies (for details see Pathways to Impact).

On a longer time-scale, medical researchers are also interested in sleep, as many cognitive problems are linked to sleep problems. These communities will profit from both the insights gained by our experiments, and the computational framework that will act as a source of hypotheses and subsequent experiments.

Finally, the training of the RAs and the exposure to each other's complementary research will impact the research landscape in the UK and help to address the current lack of computationally trained cognitive scientists. The cognitive researchers involved in this project will learn computational tools which they could implement in the future in other laboratories, including those that develop therapeutic devices to modulate various sleep stages. The computational researchers in the project will be exposed to applications of their research to cognitive paradigms. The training delivered in the context of this grant will thus contribute to the UK's competitiveness and further emphasize the UK's prime position for integrating computer and cognitive sciences."
9,F1A00DA9-3A8B-4606-93C0-EA5BCFC8D96D,Sleep cycling for Probabilistic Generative Models,"The media have lately been full of excitement about progress in Artificial Intelligence. Not only can computers now beat humans at Go, and detect cats in Youtube videos, but soon we will have robots in the house, self-driving cars, and many jobs might become automated. Apart from the societal challenges that this revolution will bring, many hurdles are still to be overcome before Artificial Intelligence will obtain truly human-like capabilities. 

In particular, current artificial systems might be very good at specific tasks, they cannot easily apply their processing power to other problems. Moreover, in order to become experts in a certain problem these machines often need millions of training examples. Current AI systems follow a strategy very different from humans and obtain their strength from brute compute power and massive amounts of data rather than by cleverness. This is also the reason why it is hard to communicate with these machines, understand their decisions and instruct them. The fact that computers use an approach that is so different from that used by humans seriously hinders application of AI to real world applications. The research community is well aware of these issues, and it generally believed that the problem arises because machines don't construct higher level understanding of the problems that they are solving. How this should be addressed is however not clear.

In humans and animals sleep plays an important role in creating high level representations. During sleep, the brain consolidates information, rearranges it, finds links between different types of knowledge, reformulates problems, and comes up with creative solutions. Most people have experienced this at some point - as they feel better able to solve a problem after a good night of sleep. It is only very recently that researchers have become able to manipulate the processes that go on during sleep, and thereby pick apart the roles of the various sleep phases play and the reason why the sleep phases are ordered in a particular way.

Here we propose to research how to processes occurring during sleep can be mimicked in computational models, and thereby open the possibility to build more human-like artificial systems.",,
10,7AD70E47-5AF3-4194-80C8-213B2AD47280,Humanlike physics understanding for autonomous robots,"How do you grasp a bottle of milk, nestling behind some yoghurt pots, within a cluttered fridge? Whilst humans are able to use visual information to plan and select such skilled actions with external objects with great ease and rapidity - a facility acquired in the history of the species and as a child develops - *robots struggle*. Indeed, whilst artificial intelligence has made great leaps in beating the best of humanity in tasks such as chess and Go, the planning and execution abilities of today's robotic technology is trumped by the average toddler. Given the complex and unpredictable world within which we find ourselves situated, these apparently trivial tasks are the product of highly sophisticated neural computations that generalise and adapt to changing situations: continually engaging in a process of selecting between multiple goals and action options. Our aim is to investigate how such computations could be transferred to robots to enable them to manipulate objects more efficiently, in a more human-like way than is presently the case, and to be able to perform manipulation presently beyond the state of the art.

Let us return to the fridge example: You need to first decide what yoghurt pot is best to remove to allow access to the milk bottle and then generate the appropriate movements to grasp the pot safely- the *pre-contact *phase of prehension. You then need to decide what type of forces to apply to the pot (push it to the left or the right, nudge it or possibly lift it up and place the pot on another shelf etc) i.e. the *contact* phase. Whilst these steps happen with speed and automaticity in real time, we will probe these processes in laboratory controlled situations to systematically examine the pre-contact and contact phases of prehension to determine what factors (spatial position, size of pot, texture of pot etc) bias humans to choose one action (or series of actions) over other possibilities. We hypothesise that we can extract a set of high level rules, expressed using qualitative spatio-temporal formalisms which can capture the essence of such expertise, in combination with more quantitative lower-level representations and reasoning. 

We will develop a computational model to provide a formal foundation for testing hypotheses about the factors biasing behaviour and ultimately use this model to predict the behaviour that will most probably occur in response to a given perceptual (visual) input in this context. We reason that a computational understanding of how humans perform these actions can bridge the robot-human skill gap. 

State-of-the-art robot motion/manipulation planners use probabilistic methods (random sampling e.g. RRTs, PRMs, is the dominant motion planning approach in the field today). Hence, planners are not able to explain their decisions, similar to the &quot;black box&quot; machine learning methods mentioned in the call which produce inscrutable models. However, if robots can generate human-like interactions with the world, and if they can use knowledge of human action selection for planning, then this would allow robots to explain why they perform manipulations in a particular way, and also facilitate &quot;legible manipulation&quot; - i.e. action which is predictable by humans since it closely corresponds to how humans would behave, a goal of some recent research in the robotics community. 

The work will shed light on the use of perceptual information in the control of action - a topic of great academic interest and simultaneously have direct relevance to a number of practical problems facing roboticists seeking to control robots working in cluttered environments: from a robot picking items in a warehouse, to novel surgical technologies requiring discrimination between healthy and cancerous tissue.",,"This project has the potential to lead to major advances in situations where human skills exceed modern robot capabilities and thus will impact on a number of user groups.
 
Societal Impact:
------------------

1. Human-modelled robotics that are capable of grasping and manipulating objects will be crucial for future service robots deployed to help people in their daily lives (from being in our homes and supporting tasks such as fetching the remote through to hospitals and supporting the care needs of immobile patients).
 
2. To model robots beyond repetitive factory line tasks and specialist labour settings into homes to benefit end-users- more variable uncertain and unstructured environments- where goals locations are not pre-defined- a model that learns and generalises (like a human) is necessary for robustness. Our work will provide a proof of concept that can be extrapolated to applications that involve novel and dynamic contexts (and this would be an aim for future work).

3. Explaining the Black box: The general public will benefit- if robots have human-inspired actions and more transparent reasoning for their decision-making, interactions with humans increase the acceptability of these devices and confidence in their capabilities. Similarly, modelling natural human interaction can improve the design of requirements for human-robotic interactions (HRI). Successful co-operation in HRI is a fundamentally challenge-this framework would help improve spatial and temporal co-ordination of activities. 

Impact on knowledge base:
------------------------------

1. Artificial intelligence researchers and roboticists, through the demonstration of human-inspired control-schemes enabling skilful robotic interactions with the environment and robotic control designers through a more precise specification of how robotic systems can interact with the underlying visual-motor action selection of the user.

2. If robots share behavioural characteristics of humans, these systems can be used to provide insights into the frailties of human decision-making and the aetiology these biases- can present an alternative to animal models- inform how we understand motor behaviour in individuals with neurological conditions in ways that would be impossible or unethical to study in humans.

3. Facilitating the cross pollination of ideas: Research staff engaged in the project through exposure to methodologies that range from artificial intelligence, motion planning, reinforcement learning, decision-making, cognitive science and engineering solutions. The project will promote working in tandem to develop for mutually beneficial advances in our understanding of human perceptual-motor behaviour e.g. through computational modelling of action selection to progress the sophistication of robotic technology.

Economic impact:
-------------------

Increasing the productivity of businesses. Here, we will focus our application on picking robots in warehouses with a particular focus on e-commerce. One of our test cases will involve competing in the Amazon Picking Challenge- improvements in these systems will yield tangible benefits in efficiency and cost-savings to businesses - allowing them to process and deliver orders faster. Natural extensions of this work are to other situations where skilled planning and motor actions e.g. autonomous vehicles, search and rescue robots. 
 
What we will do to ensure that benefits are realised?
---------------------------------------------------------
1) Our publication strategy will focus on targeting high-impact engineering and neuroscience outlets (e.g. IJRR, Autonomous Systems, the Journal of Neuroscience as well as conferences such as ICRA)

2) Conference presentations to robotics, computer science &amp; psychology research audiences

3) Liaison with industrial partners on the potential for knowledge transfer

4) Dissemination with policy bodies guided by Nexus and our industrial partners."
11,19249874-38BB-4BD0-8B32-B8D1641BFBE7,SpeechWave,"Speech recognition has made major advances in the past few years. Error rates have been reduced by more than half on standard large-scale tasks such as Switchboard (conversational telephone speech), MGB (multi-genre broadcast recordings), and AMI (multiparty meetings). These research advances have quickly translated into commercial products and services: speech-based applications and assistants such as such as Apple's Siri, Amazon's Alexa, and Google voice search have become part of daily life for many people. Underpinning the improved accuracy of these systems are advances in acoustic modelling, with deep learning having had an outstanding influence on the field.

However, speech recognition is still very fragile: it has been successfully deployed in specific acoustic conditions and task domains - for instance, voice search on a smart phone - and degrades severely when the conditions change. This is because speech recognition is highly vulnerable to additive noise caused by multiple acoustic sources, and to reverberation. In both cases, acoustic conditions which have essentially no effect on the accuracy of human speech recognition can have a catastrophic impact on the accuracy of a state-of-the-art automatic system. A reason for such brittleness is the lack of a strong model for acoustic robustness. Robustness is usually addressed through multi-condition training, in which the training set comprises speech examples across the many required acoustic conditions, often constructed by mixing speech with noise at different signal-to-noise ratios. For a limited set of acoustic conditions these techniques can work well, but they are inefficient and do not offer a model of multiple acoustic sources, nor do they factorise the causes of variability. For instance, the best reported speech recognition results for transcription of the AMI corpus test set using single distant microphone recordings is about 38% word error rate (for non-overlapped speech), compared to about 5% error rate for human listeners. 

In the past few years there have been several approaches that have tried to address these problems: explicitly learning to separate multiple sources; factorised acoustic models using auxiliary features; and learned spectral masks for multi-channel beam-forming. SpeechWave will pursue an alternative approach to robust speech recognition: The development of acoustic models which learn directly from the speech waveform. The motivation to operate directly in the waveform domain arises from the insight that redundancy in speech signals is highly likely to be a key factor in the robustness of human speech recognition. Current approaches to speech recognition separate non-adaptive signal processing components from the adaptive acoustic model, and in so doing lose the redundancy - and, typically, information such as the phase - present in the speech waveform. Waveform models are particularly exciting as they combine the previously distinct signal processing and acoustic modelling components.

In SpeechWave, we shall explore novel waveform-based convolutional and recurrent networks which combine speech enhancement and recognition in a factorised way, and approaches based on kernel methods and on recent research advances in sparse signal processing and speech perception. Our research will be evaluated on standard large-scale speech corpora. In addition we shall participate in, and organise, international challenges to assess the performance of speech recognition technologies. We shall also validate our technologies in practice, in the context of the speech recognition challenges faced by our project partners BBC, Emotech, Quorate, and SRI.",,
12,102D617F-36E0-44BE-BE02-48B8ECB1598D,How to (re)represent it?,"AI engines are ubiquitous in our lives: we talk to our mobile phones, ask directions from our sat-navs, and learn new facts and skills with our digital personal assistants. But most of the time, we need to learn how these systems work first: we have to adapt to them as they are not aware of our level of experience, expertise and preferences. AI engines are fast and can deal with a deluge of data much better than us, but they do so in machine-oriented ways, which are often inaccessible and unintelligible to humans. 

The aim of this project is to identify and study how humans \textit{represent} information that they want to work with and from which they will obtain new knowledge. Humans have the capability to choose the representation that is just right for them to enable them to solve a new problem, and moreover, if the representation needs to be changed, they can spot this and change it. Unlike humans, machines in general have fixed representations and do not have the understanding of the user. For example, sat-nav systems will only give directions with elementary spatial commands or route planning functions, whereas humans give directions in many forms, for instance in terms of landmarks or other geographic features that are based on shared knowledge. 

We want to model in computational systems this inherently human ability to choose or change appropriate representations, and make machines do the same. We want to find out what are the cognitive processes that humans use to select representations, what criteria they use to choose them, and how we can model this ability on machines. Our hypothesis is that when humans choose a representation of a problem, they use cognitive and formal properties of the problem and its representation to make their choice. In this project, we will test this hypothesis by achieving the following goals:

1. Collect a corpus of problems and candidate representations to study and categorise their cognitive and formal properties.

2. Devise coding schemes and conduct cognitive studies to identify cognitive and formal properties that people use in choosing representations. Develop cognitive theories based on these experiments.

3. Design and implement computational algorithms that allow users to choose alternative representations. Build a ranking and recommendation system based on the taxonomy from cognitive studies to suggest appropriate representation given a particular problem and user.

4. Evaluate the utility of the system and generalise the approach to other domains outside of mathematics. Investigate how to apply our cognitive and computational models in education in the form of AI tutors that are adaptable to users.

Our work is novel in that it will address the problem of appropriate representation choice. Moreover, we will build novel cognitive theories and computational models that will allow AI systems to operate in more human-like ways and adapt to the requirements of the problem and the needs of the user. Thus, the potential impact will span numerous domains where systems interact with humans to represent information and use it for extracting new knowledge.",,"&quot;How to (re)represent it?&quot; is an ambitious project where we want to find out how humans choose and also change representation during problem solving. We want to develop engines that will give machines the same capability. Such machines will accrue many of the benefits that humans obtain from changing representation, including: more effective communication with a human by selecting a representation that is well-suited to their level of familiarity with the target topic; better understanding of a human by identifying and adopting the representation that the human favours; and greater flexibility in adapting to the needs of the user. As such, the project will have economic, societal and knowledge impact.

The impact of this work will be wide as AI systems are becoming ubiquitous. When interacting and exchanging information with humans, developers of such products need to represent information in human understandable ways. We will devise techniques that will help developers of AI systems to build products that choose representations appropriate for their users -- and thus have potential economic benefit to them.

Making machines more human accessible will also benefit the society at large, in particular, as it will potentially bring technical tools to those that are less technically versed. 

We have already outlined how our work will contribute to the expansion of knowledge in Academic Beneficiaries. The techniques we will develop are novel and will provide a scientific advance in all areas (in academia and in industry) that must represent information for solving problems, reasoning, etc.

Whilst we exposed education as our initial concrete application target, our contributions will be general and could apply across different domains. They will lead to better understanding of representations and their relation to human expertise and preferences. As intelligent tools make part of our every day life, we will all benefit from having more human-like systems that adapt to us, rather than us adapting to them.

To achieve maximum impact, we will follow a comprehensive dissemination strategy. We will publish our results in all relevant communities (e.g., artificial intelligence, cognitive science, computer science, automated reasoning, diagrams, knowledge representation, human-computer interaction, information visualisation) to achieve wide dissemination. We will demonstrate our work by organising a workshop and preparing tutoring material aimed at academic as well as more specific industrial communities. This will also provide a community building opportunity for people interested in representations and more generally, human-like computing. We will continue to participate at outreach activities to raise the awareness of the importance of human-like computing. We will create a web repository to enable free and public access to our papers, corpus of problems and their solutions, software and tutorials. Finally, we have enlisted an advisory board of experts spanning all areas relevant to this project, and coming from academia and industry - their advice will help us stay focused on relevant problems, influence diverse communities that they lead, and transfer our technology widely."
13,F18AFA4E-3672-4609-B8EB-8597476C10C8,Leveraging functional profiling datasets with machine learning to uncover proteins and cellular processes important for ageing,"Ageing is the largest risk factor for most human diseases in developed countries, including progressive diseases such as Alzheimer's and Parkinson's, diseases like cancer that show variable rates of onset, and catastrophic system failures such as heart-attack and stroke. While the study of specific disease processes has long been a major focus of research, there is a growing realization of the importance of studying the normal ageing process itself as an essential part of the problem, and of exploring ways to slow or reverse its effects. Ageing is a multi-factorial process that can be seen as an inevitable feature of the ravages of time. Recent discoveries, however, demonstrate that ageing can be modified in dramatic ways by simple interventions. For example, single gene knockouts can delay ageing and improve health late in the life of laboratory animals. The processes involved in ageing are similar in different organisms, and genetic mutations affecting these processes are associated with longevity in humans. A central challenge of ageing research, however, remains to tease out a complete and unified picture of the biological factors and processes determining lifespan. 

Ageing is highly complex and affected by diverse proteins and processes. Modern biological assays can simultaneously measure properties and interactions of thousands of proteins or genes, but it is challenging to make sense of such large datasets. Advances in computational data-analysis methods, called 'machine learning', provide exciting opportunities to get the most from large biological datasets and thus increase our understanding of complex processes like ageing. Machine Learning can find hidden patterns in data that is too complex for humans to process. Advances in computer power, algorithms and data sizes allow recent machine-learning architectures (known as 'deep learning') to accurately find and classify intricate patterns in combined datasets of different types. 

We plan to use fission yeast as a model organism, together with multi-step machine learning, to comprehensively identify biological processes with fundamental importance for ageing. Remarkably, many of these processes are similar from yeast to human, but are much easier to study in the simple yeast. Yeast cells enter a dormant, non-dividing state under limiting nutrients. Such dormant cells provide a useful system to analyse proteins and processes affecting the lifespan in this state. In previous studies, we have identified 116 proteins that, when absent, allow the yeast to live longer (long-lived knockout mutants). So these proteins are involved in ageing, and can be used to train machine-learning programs to predict new ageing proteins by a method known as 'guilt by association'. We will combine large systematic data on mutant features (phenotypes) with diverse existing data to empower the machine-learning predictor. We will test the predicted ageing proteins in the laboratory for lifespan effects in yeast, and feed this information back to the computer for it to learn more about ageing proteins. We will then use mutants of the new ageing proteins identified by the computer and confirmed in yeast to measure links with all other mutants. Such 'genetic-interaction' data provide rich information on functional relationships, which will be used to explore other, potentially more powerful deep-learning methods to predict the biological processes that are involved in ageing. We will then test the most attractive predictions with laboratory experiments. Moreover, we will make all the new data, methods and predictions available to interested scientists to help with their research. We anticipate that this project, using intimate cycles of experiments and machine-learning, will provide a valuable platform to better understand all the biological factors involved in ageing, to eventually develop interventions that extend healthy lifespan in humans.","We want to establish comprehensive sets of proteins and biological processes involved in cellular ageing in fission yeast. This project combines functional-profiling experiments (large-scale phenotyping and genetic-interaction assays) with powerful new machine-learning prediction algorithms. Our integrated approach will benefit from iterated computational predictions and experimental validation, with different techniques used in two computational stages to get the most from the rich experimental data. The first computational stage will apply Bayes Multiple Kernel Learning, informed by phenotyping and heterogeneous network/homology datasets, to rank proteins based on their predicted associations with 116 ageing-associated proteins that we recently identified. In each of 5 iterations, we will test 50 top-ranked proteins for altered lifespans in the corresponding mutants to improve the predicted ranking in the next iteration. We will then screen the top-125 validated ageing proteins for genetic interactions using Synthetic Genetic Array analyses. In the second computational stage, we will exploit our functional-profiling data, integrated with in-house homology and network data, to build deep-learning predictors for GO Biological Processes relevant to ageing. 

CAFA-2 recently ranked our CATH homology-based predictor top; CATH is unique in providing functional sub-families that outperform Pfam for functional purity. Combining this unique data with the functional-profiling data generated in this project will enhance the power of our predictors optimized for ageing-related processes. Deep learning is computationally expensive, but advances in computing (e.g. GPUs are ~15x faster than CPUs) and efficient code bases (e.g. TensorFlow) are helping in this respect. Furthermore, we have access to the JADE Centre for Deep Learning Computation, which provides excellent computational facilities to speed up our training and investigation of many architectures.","Who will benefit from this research? 
This proposed research is basic by its nature, and the immediate impacts from this work relate to scientific and knowledge advancement and the development of skills, capacity and capability. In the longer term, this research has the potential to impact areas of wealth and health. Beneficiaries beyond academia therefore are the commercial private sector and the wider public. 

How will they benefit from this research? 
The proposed research takes state-of-the-art experimental and computational approaches to address fundamental questions relating to biological processes involved in ageing. The research will deliver increased capacity and capability in strategically relevant areas of genomics and machine learning, through the provision of inter-disciplinary training and the further development of key methods and resources. Establishment of these methods is significant as they have a wide range of applications that reach beyond basic science into fields relating to human healthy ageing, the commercial (pharmaceutical) sector and beyond. The commercial sector might benefit by recruiting highly skilled and experienced scientists trained through this project. 

Ultimately, the pharmaceutical sector will clearly benefit from all the publicly available experimental data that we will make available through the DeepAge resource. They might also benefit by exploiting fresh drug targets (ageing-associated proteins that slow ageing when down-regulated), to effectively reduce the effects of ageing as the major risk factor for multiple diseases. The ageing population is a huge and increasing problem in our society, with enormous cost implications due to the economic and social burden of the rise in associated diseases and diminished quality of life for both patients and carers. It is evident that any measures that promote healthy ageing will be of massive, broad ranging benefit to our society with respect to economy, quality of life, health and creative output. In the longer term, the general public may thus benefit from our fundamental contribution to the understanding of genetic mechanisms and universal principles involved in ageing-related phenotypes that will guide and empower research in more complex systems and may help to develop safe broad-spectrum, preventative measures against age-associated diseases.

Immediate and concrete deliverables with respect to impact beyond academia will be in public engagement, which we recognize as an important responsibility of scientists. We already have experience and established links that will facilitate good communication and public engagement of the research outputs. Details of our specific plans and timelines with respect to public engagement are outlined in the Pathways to Impact."
14,927FD0F9-6797-4B27-B6CE-CAF6469ADE61,Seeing the future,"Newspapers do not report stories about the sun rising or setting. This because what is expected or unchanging is of little interest, what is newsworthy is the unexpected or changing. In 1961 Horace Barlow, a British vision scientist, hypothesised that the sensory systems of the brain work on the same principle, it devotes little investigation to the expected and reports the unusual. It does this by trying to predict what is going to happen and when it fails it funnels resources at the &quot;prediction errors&quot;. This makes intuitive sense, the things in a scene that merit attention are typically those that unpredictable or changing. This idea, &quot;predictive coding&quot; has proven very fruitful and today it underpins much research on how the brain works. 
 
The aim of this project is to translate the idea of predictive coding from the fields of psychology and biology to the design of a computer vision system. The system will try to predict the upcoming camera image. Prediction will be based on the previous images seen (over immediate, intermediate and longer time scales), information about movement of the system through space (from the inertial sensor), and knowledge of how objects and people move. Prediction errors will be flagged.

This is a feasibility study, to determine and demonstrate the benefits of such a system and form a basis for future work.

Our specific objectives are to: 
 
i) Evaluate the technical feasibility of building a predictive vision system. 
ii) Assess the projected efficiency and accuracy of a predictive vision system. 
iii) Evaluate the effectiveness of a predictive vision system for detecting changes or incongruencies in the scene. 
iv) Assess the computational completeness and sufficiency of the predictive coding approach (this result will be fed back to the biology/psychology community). 
v) Identify potential partners and explore potential industrial, security or healthcare applications of the technology. 
vi) Take appropriate steps to protect intellectual property and then release the data and code to the broad research community. 
vii) On successful completion seek follow-on funding (research council and/or industrial) for further work. 
 
The project brings together expertise in computer vision, human vision and computational biology across two partner (GW4; Cardiff and Bristol) universities to work on this innovative and cutting edge work. The system is to be built from a stereo camera, an inertial sensor, and a CPU/GPU. This technology is available in some smartphones today. If the efficiency and accuracy benefits are realized then the technology could be based on a smartphone which opens up a broad range of applications, including in healthcare, security and ubiquitous computing.",,"This 18 month feasibility study will build a demonstrator predictive vision system, evaluate its strengths and weaknesses, and explore potential application areas and broader use. During planned activities (e.g. workshop with industrial, security and healthcare delegates) we will explore the future applications of a predictive vision system. Below we outline potential beneficiaries of the work. 
 
Economic Impact: Building Industrial Niches in the UK 
The worldwide machine vision industry is expected to grow from 8 billion USD to 12 billion USD by 2020. Within this industrial niche, there are a large number of potential applications of our new technology. If deployed on a smart phone (see below) there are an abundance of applications in reach at very little cost. This opens up major industrial/business opportunities many of which can be envisioned (some examples below) but many unexpected applications will be envisioned by innovative smart phone developers. 
 
Video Surveillance is a commonplace, wide ranging and massively growing application of video camera, entire &quot;Smart Cities&quot; are under increasing surveillance and many public and work places are observed for security and safety monitoring. Police forces worldwide are currently being deployed with body worn camera, as are military personnel. Many private vehicles and Police/Security forces have in car cameras (e.g. GoPro). An obvious application of our 'smart' video capture system would be to record and feed back anomalies to security agents, without the need for massive data storage and operator screening requirements. 
 
Next-Generation Video Compression, Change Detection and Saliency mapping: A successful demonstrator system could provide a foundation for new approaches to video compression, event detection and salience mapping in a variety of video analysis scenarios (in particular monitoring and surveillance). 
 
Greener smart sensor technologies: The efficiency advantages of a predictive system should allow the development of lower-energy smart sensors. 
 
Ubiquitous computing, smartphone apps: A key feature of the predictive system is that it detects &quot;interesting&quot; events (changes or incongruities) in the scene in real time. All components of our system already exist on a smartphone. In a few years time, smartphone sensors will no doubt improve in accuracy and smartphone processing power will increase to allow our methods to be effectively deployed on such devices, opening potential for many far reaching applications and future technologies. 
 
Societal Impact: Building a healthy, competitive and resilient society 
 
Predictive sensing in care of elderly or infirm: Medicine has many ready-made challenges that our technology could assist. For example, a device that could be placed in a person's home, a care home or hospital to monitor for abnormal movement patterns, at one extreme that could be falls, at the other it could be early signs of Parkinson's gait or lameness etc., both would be flagged as prediction errors, deviations from the priors. 
 
Predictive sensing as an aid for those with attentional or perceptual deficits: Visually impaired people, due low vision, or people with attentional problems, following brain injury, stroke, or just general ageing, could be equipped with an assistive sensor (incorporated in a handheld device, or a device worn on the body, on the chest or on the head in a google Glass type arrangement), to scan the environment and alert them to important events in the scene. 
 
Developing World Leading Scientists: A key role will be played by this project's PDRA who work close collaboration with the investigator team. They will present at international conferences (see resources) where they can build their own reputation and establish, moving forward, their own scientific career (with multidisciplinary collaborators)."
15,34388A54-1E72-4380-A973-B2B8EC04BAD9,How to (re)represent it?,"AI engines are ubiquitous in our lives: we talk to our mobile phones, ask directions from our sat-navs, and learn new facts and skills with our digital personal assistants. But most of the time, we need to learn how these systems work first: we have to adapt to them as they are not aware of our level of experience, expertise and preferences. AI engines are fast and can deal with a deluge of data much better than us, but they do so in machine-oriented ways, which are often inaccessible and unintelligible to humans. 

The aim of this project is to identify and study how humans \textit{represent} information that they want to work with and from which they will obtain new knowledge. Humans have the capability to choose the representation that is just right for them to enable them to solve a new problem, and moreover, if the representation needs to be changed, they can spot this and change it. Unlike humans, machines in general have fixed representations and do not have the understanding of the user. For example, sat-nav systems will only give directions with elementary spatial commands or route planning functions, whereas humans give directions in many forms, for instance in terms of landmarks or other geographic features that are based on shared knowledge. 

We want to model in computational systems this inherently human ability to choose or change appropriate representations, and make machines do the same. We want to find out what are the cognitive processes that humans use to select representations, what criteria they use to choose them, and how we can model this ability on machines. Our hypothesis is that when humans choose a representation of a problem, they use cognitive and formal properties of the problem and its representation to make their choice. In this project, we will test this hypothesis by achieving the following goals:

1. Collect a corpus of problems and candidate representations to study and categorise their cognitive and formal properties.

2. Devise coding schemes and conduct cognitive studies to identify cognitive and formal properties that people use in choosing representations. Develop cognitive theories based on these experiments.

3. Design and implement computational algorithms that allow users to choose alternative representations. Build a ranking and recommendation system based on the taxonomy from cognitive studies to suggest appropriate representation given a particular problem and user.

4. Evaluate the utility of the system and generalise the approach to other domains outside of mathematics. Investigate how to apply our cognitive and computational models in education in the form of AI tutors that are adaptable to users.

Our work is novel in that it will address the problem of appropriate representation choice. Moreover, we will build novel cognitive theories and computational models that will allow AI systems to operate in more human-like ways and adapt to the requirements of the problem and the needs of the user. Thus, the potential impact will span numerous domains where systems interact with humans to represent information and use it for extracting new knowledge.",,
16,C57E7EF7-267B-4B03-B345-E1F0F5782EA3,SpeechWave,"Speech recognition has made major advances in the past few years. Error rates have been reduced by more than half on standard large-scale tasks such as Switchboard (conversational telephone speech), MGB (multi-genre broadcast recordings), and AMI (multiparty meetings). These research advances have quickly translated into commercial products and services: speech-based applications and assistants such as such as Apple's Siri, Amazon's Alexa, and Google voice search have become part of daily life for many people. Underpinning the improved accuracy of these systems are advances in acoustic modelling, with deep learning having had an outstanding influence on the field.

However, speech recognition is still very fragile: it has been successfully deployed in specific acoustic conditions and task domains - for instance, voice search on a smart phone - and degrades severely when the conditions change. This is because speech recognition is highly vulnerable to additive noise caused by multiple acoustic sources, and to reverberation. In both cases, acoustic conditions which have essentially no effect on the accuracy of human speech recognition can have a catastrophic impact on the accuracy of a state-of-the-art automatic system. A reason for such brittleness is the lack of a strong model for acoustic robustness. Robustness is usually addressed through multi-condition training, in which the training set comprises speech examples across the many required acoustic conditions, often constructed by mixing speech with noise at different signal-to-noise ratios. For a limited set of acoustic conditions these techniques can work well, but they are inefficient and do not offer a model of multiple acoustic sources, nor do they factorise the causes of variability. For instance, the best reported speech recognition results for transcription of the AMI corpus test set using single distant microphone recordings is about 38% word error rate (for non-overlapped speech), compared to about 5% error rate for human listeners. 

In the past few years there have been several approaches that have tried to address these problems: explicitly learning to separate multiple sources; factorised acoustic models using auxiliary features; and learned spectral masks for multi-channel beam-forming. SpeechWave will pursue an alternative approach to robust speech recognition: The development of acoustic models which learn directly from the speech waveform. The motivation to operate directly in the waveform domain arises from the insight that redundancy in speech signals is highly likely to be a key factor in the robustness of human speech recognition. Current approaches to speech recognition separate non-adaptive signal processing components from the adaptive acoustic model, and in so doing lose the redundancy - and, typically, information such as the phase - present in the speech waveform. Waveform models are particularly exciting as they combine the previously distinct signal processing and acoustic modelling components.

In SpeechWave, we shall explore novel waveform-based convolutional and recurrent networks which combine speech enhancement and recognition in a factorised way, and approaches based on kernel methods and on recent research advances in sparse signal processing and speech perception. Our research will be evaluated on standard large-scale speech corpora. In addition we shall participate in, and organise, international challenges to assess the performance of speech recognition technologies. We shall also validate our technologies in practice, in the context of the speech recognition challenges faced by our project partners BBC, Emotech, Quorate, and SRI.",,"Robust speech recognition is a key technology needed to make digital infrastructure simple, accessible, invisible, and reliable, supporting a range of mobile and other applications. It is driven in particular by the need for new interfaces to small and mobile devices in which traditional modalities like touch-screens may be inappropriate or not physically possible.

The EPSRC delivery plan highlights four &quot;prosperity outcomes&quot; to which SpeechWave will contribute:

1. Productivity: Robust speech recognition can significantly enhance productivity across a range of industries and is a key enabler for a range of new smart technologies which are enabled or enhanced by speech interfaces.

2. Connectedness: Robust speech recognition enables natural interaction with novel devices and applications, and can unlock the audio and video data that makes up 80% of the web.

3. Resilience: The core of the project is to develop speech technology that is robust and resilient to changing conditions of use.

4. Health: Robust speech recognition enables assistive technologies and accessibility.

SpeechWave is well-aligned to EPSRC's Cross-ICT Priorities 2017-2020. Speech recognition technologies that are robust to realistic and natural acoustic conditions help to underpin the Future Intelligent Technologies and the People at the Heart of ICT priorities, enabling broad utilisation of intelligent spoken interfaces. SpeechWave's collaboration between researchers from signal processing, speech technology, and machine learning addresses the priority Cross-Disciplinarity and Co-Creation.
The UK has a vibrant and expanding speech technology sector built on a healthy ecosystem comprising multinational companies with a UK R&amp;D base (including Amazon, Apple, and Google), home-grown medium-size companies, and exciting startups (many located in Edinburgh or London). SpeechWave will help the UK maintain this world-leading research activity, through its collaboration with project partners, and will increase innovation potential. 

Within SpeechWave we have taken measures to maximise the impact of our research. These will be in two main areas:

1. Broadcast and Media (via project partners BBC and Quorate). The focus of this work is to develop robust media transcription prototypes, able to cope with the diverse range of broadcast media. Media transcription has direct benefits (for example supporting accessibility through automatic subtitling), as well as enabling intelligent processing of broadcast media through natural language processing and text analytics.

2. Distant Speech Recognition (via project partner Emotech). The focus of this work is to develop prototype software for speech recognition in personal robots. Speech is perhaps the most natural communication modality for such robots, but the acoustic conditions can be extremely challenging due to reverberation and competing acoustic sources. Improving speech recognition accuracy for such devices in challenging environments is likely to have a significant impact on their usability and uptake.

We also plan to enhance the global impact of our research through project partner SRI International who have a specific R&amp;D interest in speech recognition in highly challenging acoustic environments."
17,544BD1BE-07F3-453A-B7E4-D28A560E789B,Complex Chemical Systems Platform Exploring Inorganic Intelligence,"Our vision is to establish the new field of inorganic intelligence by defining the key fundamental science problems, and by developing researchers equipped with the right skills to explore this emerging area of science. The Cronin Group has made world-leading contributions to foundational aspects of this research and now we need to explore, unify, and develop some of the central science problems. These include how to explore and control, and understand complex chemical systems using robotics and real-time data. We anticipate that the coordinated development of these four topics will lead into applications as diverse as self-assembly control in nano molecules, chemical synthesis and discovery automation or artificial intelligence (AI) optimisation of reactions and exploration and discovery of new underpinning principles. The new grant will continue to unify and develop synergies already established during the previous Platform, but most importantly will ensure continuity and stability. This will enable the team to evolve from focusing on inorganic systems to the digital control and exploration of complex chemical systems. The new Platform will not only contribute to unify the many strands already existing in the team, but will also allow an extension to new disciplines including robotics, machine learning, and development of synergies across those areas - a combination of topics very rarely merged and hence extremely hard to raise funding using other mechanisms. Thus, the new Platform is essential for continuation and the evolution of the research activity, giving added value in integrating the group, allowing us to be strategic and develop the team into the chosen new areas defining the area of 'inorganic intelligence'. The previous grant was instrumental in letting us extend our critical mass, enhance key existing international collaborations, and support inter-group collaborations in Glasgow, which allowed us to speculate and develop our exploratory work in chemical robotics. In addition, we had the flexibility to support and further consolidate some of the existing team, and to hire in new expertise, as well as restructure the team with help from the EPSRC mentor scheme. We need the new platform to continue our team development and provide stability and flexibility especially important during the next few years. As before, we will aim for our best results to be published in Science and Nature, protect innovations by patent applications, and engage a user group and industrialists as well as other world-leading academics to maximise both the academic and technological impact. This will be achieved by making full use of funding from various sources, aiming at areas that need to be developed using the Platform as a consolidating component. We will also seed 'pump-prime' projects within the Platform, provide bridging funding, and be ready to exploit unexpected and high impact results. The Platform will ensure the group remains at critical mass at a critical time, and at the cutting edge of science in a range of new areas.",,"The digitization of chemistry has a trillion-dollar potential as a disruptive technology (remote drug manufacture), to develop a chemical data driven economy (chemistry on the cloud), as well as networked chemical synthesis (distributed chemical infrastructure). This application bridges many of the EPSRC outcome frameworks e.g. resilience and productivity (chemical manufacturing based upon digital code and decentralised), connected (chemical code will be on the cloud and aid collaborative developments in digital space), and healthy (algorithms to power a serendipity engine for the discovery of new drug molecules).

We will disseminate our work as widely as possible through publication in high impact journals. We will aim to publish in open access journals or have our publications on the open-archive within 3 months of publication (aiming for 1 month). We will also build web resources for wide dissemination of data (open data), and have a digital platforms website established to help translate affordable robotic systems for use by chemists between labs. We will also ensure that our open / collaborative agenda will be advertised through the large number of invited talks given by the team both in the UK and abroad, including major national and international conferences. We intend to interact directly with the Glasgow University Impact Agenda (GUIA) (including EPSRC-funded Impact Acceleration Accounts) as well as the Cronin-group Glasgow University Research and Enterprise Officer (Melville Anderson). These existing resources will provide support with strategic management of IP. The University of Glasgow prides itself of being the instigator of an innovative, award-winning model for the management of IP known as Easy-Access IP. This represents the University's commitment to maximize the impact from research by adopting a flexible approach to interactions with industry. The GUIA is a key element that we will exploit to give this work visibility, interact with end-users, and develop a forum of interested parties that will receive information and progress updates about the project as it proceeds. Both Scottish Enterprise and the IP Group (technology investors who have a partnership with Glasgow University) have been interacting with us and have been evaluating the overall portfolio with a view to developing a range of investments in order to develop exploitable technology, know-how, and product innovations.

Also, engagement with the companies will allow us to write a road-map developing the platform beyond inorganics to organic synthesis (championed by GSK) and reactionware (championed by MilliPoreSignma). SpacePharma is interested in developing flight-ready miniature chemical synthesis laboratories for drug synthesis in low earth orbit. We will allow the companies to 'embed' researchers in the team and with the platform for collaborative visits / projects as well as encouraging some of our team to second to industrial sites for training and knowledge exchange."
18,5C3E8621-D259-4C39-A77A-89E7138BD66F,ACTION on cancer,"Death from cancer is typically both slow and painful, and few families have been spared its scourge. Cancer is also one of the world's greatest killers (13 million deaths and 22 million new cases per year by 2025), and it is estimated that every second person on the planet will develop cancer at some stage of their life. 

Over the last 30 years our knowledge about cancer has increased enormously, and now, for the first time, we understand the fundamental nature of the disease(s): malfunctioning in the way that cancer cells process information. All the cells in our bodies process information about their internal state, and communicate with their neighbours, and when this goes wrong cancer can occur.

As everyone's cells are different, and there are very many different ways that this information processing can go wrong and cause cancer, it is not possible to design a single treatment for cancer, or even for a sub-type of cancer such as breast cancer. Instead what is needed are personalised treatments tailored to each patient's cancer. However, such personalised treatments are very expensive to design, and the expertise to do so is limited. In addition, it is often necessary to execute custom designed experiments to better understand what is the best treatment. Therefore the only way to make personalised cancer treatment available to everyone is through laboratoryautomation, and the use of artificial intelligence (AI).

In this project we will develop ACTION, which will be a prototype AI system for the design of personalised cancer treatments. ACTION will focus on chemotherapies - design of drug cocktails. Given initial information about a cancer ACTION will extract all the relevant knowledge it can find about the cancer, both from databases and computational models of cellular information processing that scientists have developed. ACTION will rationally integrate this knowledge, and infer what extra knowledge is required to make the best decision on how to treat the cancer. ACTION will then automatically execute custom designed experiments using laboratory robotics to determine the missing information. Finally, using all the knowledge it has gathered, ACTION will decide on the best chemotherapy.

We will evaluate ACTION using different types of cancer cells grown in the laboratory. This avoids the ethical complexities of working with patients, and is much cheaper and faster. If the development of ACTION is successful it will then move to testing with patient derived cancers.",,"Impact Summary
The proposal is an ambitious one with a high potential for significant technological, Medical, economical, and societal impact.

Value
Cancer is one of the world's greatest killers (13 million deaths and 22 million new cases per year by 2025), and it is estimated that every second person on the planet will develop cancer at some stage of their life. The annual treatment of cancer costs the world ~$900 Billion per annum (more than any other disease), and there is deep concern that cancer treatment is becoming unsustainable, as new therapies can cost &gt; &pound;100,000 per patient per year. 

The Funding Landscape
The proposal aims take the concept of AI designed personalised drug treatment from TR2 (Principles demonstrated through experimentation) to TR3 (Early proof of concept demonstrated in the lab). The main objective measure of success of TR3 will be that ACTION will make automated recommendation on personalised cancer treatment that is as good or better that current 'best practice'

RDK has significant commercial experience. He was a founder of the spin-out company PharmaDM, which developed data mining tools for the pharmaceutical industry. RDK also consults for a wide range of multi-national companies. LS has less commercialisation experience, and she will undertake appropriate business and commercial training.

The applicants understand and have experience with the variety of follow on governmental and medical charity. For example, RDK was involved with Cambridge in a MRC Biomedical Catalyst project to develop an anti-malaria compound.

One year before the end of the project we will institute a meeting of the project Advisory Board to decide on an appropriate commercialisation pathway. To take ACTION to TR4 we will first seek proof-of-principle funding for this through the University of Manchester EPSRC Impact Acceleration Account. However, this will be insufficient to cover the high costs. We will therefore seek following on funding from Cancer Research UK, the MRC Developmental Pathway Funding Scheme (DPFS), etc. We will also approach our VC contacts.

The evidential support to produce a high quality follow on funding application will come from two main sources: publications, and Intellectual Property

Publications
The applicants have an excellent publication track-record, our papers have been published in Science, Nature, PNAS, etc. We will also attend and submit papers to relevant conferences.

Stakeholder Engagement 
Through participation in Big Mechanism we are aware of the complex landscape of stakeholders in cancer. There is a growing enthusiasm for the potential of AI to help in cancer treatment and diagnosis, however this can only succeed if all stakeholders are included and involved: 

RDK is engaged in cancer research in Manchester through membership of Manchester Cancer Research Centre (MCRC). To ensure a close relationship with MCRC we will invite Professor Andrew Hughes onto the ACTION advisory board. 

To advise ACTION in industrial involvement we will invite Professor Kevin White onto the ACTION advisory board. We have worked closely with him in Big Mechanism. He is a US National Cancer Institute board member, and President of Tempus, where he oversees its scientific operation.

Patient involvement is also essential. Through Big Mechanism we have worked closely with Cancer commons: a 'patient-centric not-for-profit network of patients, physicians, and scientists that help identify the best options for treating an individual's cancer'. Dr. Marty Tenenbaum, Chairman of Cancer Commons, is excited by our research, and we will ask him to be on the Advisory Board. He is both a renowned computer scientist, and a cancer survivor. 

Research capacity building 
The project will train three PDRAs in key areas of future science and technology: AI, machine learning, bioinformatics, medical informatics, and cancer biology."
19,54AACCAD-7126-4496-910F-B43A31B111C5,Robustness-as-evolvability: building a dynamic control plane with Software-Defined Networking,"Highly available information networks are an increasingly essential component of the modern society. Targeted attacks are a key threat to the availability of these networks. These attacks exploit weak components in network infrastructure and attack them, triggering side-effects that harm the ultimate victim. Targeted attacks are carried out using highly distributed attacker networks called botnets comprising between thousands and hundreds of thousands of compromised computers. A key feature is that botnets are programmable allowing the attacker to adapt to evolve and adapt to defences developed by infrastructure providers. However current network infrastructure is largely static and hence cannot adapt to a fast evolving attacker.

To design effective responses, a programmable network infrastructure enabling large-scale cooperation is necessary. Our research will create a new form of secure network infrastructure which detects targeted attacks on itself. It then automatically restructures the infrastructure to maximise attack resilience. Finally, it self-verifies whether global properties of safety and correctness can be assured even though each part of the infrastructure only has a local view of the world.

Our research will examine techniques to collect and merge inferences across distributed vantage points within a network whilst minimising risks to user privacy from data-aggregation using novel privacy techniques. We make a start on addressing the risks introduced by programmability itself, by developing smart assurance techniques that can verify evidence of good intention before the infrastructure is reprogrammed.

We set three fundamental design objectives for our design: 
(1) Automated and seamless restructuring of network infrastructure to withstand attacks aimed at strategic targets on the infrastructure.

(2) A measurement system that allows dynamic allocation of resources and fine control over the manner, location, frequency, and intensity of data collected at each monitoring location on the infrastructure.

(3) Assurance of safety and compliance to sound principles of structural resilience when infrastructure is reprogrammed.

Our aim is to develop future network defences based on a smart and evolving network infrastructure.",,"Our joint research program (with industrial and academic partners) will provide new foundations for a resilient network infrastructure. The medium term beneficiaries of this research will be parties who have a stake in the development of future networking infrastructure, and, in the much longer and wider term, everyone who relies on resilience of the global digital infrastructure.

Our technologies will give private sector companies competitive advantage, by making their SDN-based networking tools provide additional functionality, aimed directly at tackling security issues. Our techniques will give greater confidence in network infrastructure; thus enabling utility companies and government to move more critical infrastructure onto standard backbones. 

Software-Defined Networking (SDN) promises massive reduction in costs. The proposed work programme has the potential to demonstrate that not only could SDN switches provide fundamentally better security (by incorporating programmability and hence dynamic algorithms for measurement and response), they might achieve it cheaper than conventional hardware routers."
20,D5DE843F-BE50-4F3E-97C9-05877EDC229A,Data Science of the Natural Environment,"We will develop a data science of the natural environment, deploying modern machine learning and statistical techniques to enable better-informed decision-making as our climate changes. While an explosion in data science research has fuelled enormous advances in areas as diverse as eCommerce and marketing, smart cities, logistics and transport, health and wellbeing, these tools have yet to be fully deployed in one of the most pressing problems facing humanity, that of mitigating and adapting to climate change. This project brings together world-leading statisticians, computer scientists and environmental scientists alongside an extensive array of key public and private stakeholder organisations to effect a step change in data culture in the environmental sciences.

The project will develop a new approach to data science of the natural environment driven by three representative grand challenges of environmental science: predicting ice sheet melt, modelling and mitigating poor air quality, and managing land use for maximal societal benefit. In each motivational challenge, there is already an extensive scientific expertise, with intricate models of processes at multiple scales. However this sophisticated modelling of system components is usually let down by naive integration of these components together, and inadequate calibration to observed data. The consequence is poor predictions with a high level of uncertainty and hence poorly-informed policy making. As new forms of environmental data become available, and the pressures on our natural environment from climate change increase, this gap is becoming a pressing concern, and we bring an impressive team to bear on the problem.

A key theme of the project is integration, developing a suite of novel data science tools which work together in a modular fashion, and with existing scientifically-informed process models. By building a team that spans the inter-disciplinary divisions between data and environmental scientists we can ensure the necessary interoperability of methods that is currently lacking. Working with the full range of stakeholder environmental organisations will enable continual co-design of the programme and training of end-user scientists to ensure a reduction of the skills gap in this area. The resultant culture shift in the data literacy of the environmental sciences will enable better decision-making as climate change places ever greater strains on our society.",,"We seek a coupling of cutting edge intellectual endeavour with a strong focus on impact. This work is supported by an impressive set of twenty-two partners that represent a who's who of the environmental community alongside key data science players:

Our partners include (contacts in brackets): the Environment Agency (Stuart Homann), Defra (Andy Stott), the Met Office (Vicky Pope, Alberto Arribas, Fiona O'Connor), JBA Trust (Rob Lamb), CEFAS (Jon Barry), the National Oceanography Centre (Kevin Horsburgh), the Centre for Polar Observation and Modelling (Andy Shepherd), British Antarctic Survey (David Vaughan), Natural England (Ruth Waters), Natural Resources Wales (Jim Latham), the Joint Nature Conservation Committee (Deborah Procter), the National Centre for Atmospheric Research (Jean-Fran&ccedil;ois Lamarque), the Scottish Government (Andrew Taylor), SEPA (Colin Gillespie), J&uuml;lich Forschungszentrum (Thomas Lippert), and the DAFNI Consortium (Jim Hall). In terms of health impacts, we also have the Asthma UK Centre for Applied Research as a partner (Aziz Sheik, Colin Simpson). Mike Berners-Lee (Small World Consulting) has a crucial role in translating science into policy. The RIDE Forum (Vicky Pope) - formerly LWEC - is on board to support outreach to the public sector. In terms of data science, we have Microsoft Research (Kenji Takeda), BT (Fraser Burton) and EDF (Hugo Winter) as partners. We also look forward to working closely with the Alan Turing and Farr Institutes, with links to both.

In approaching partners, we have observed a real hunger for more sophisticated data science methods tailored for the environment. The partners will be intrinsically folded into the research through the ongoing co-design of the project (continuing the process that started with the scoping of the motivating challenges). The key mechanism to support continual co-design is the use of an agile development methodology where we fold in the end user community as an intrinsic part of the development process, and this includes challenge and methodological workshops, supplemented by monthly show and tell sessions, where we get frequent feedback from different end user groups at key stages of the research.

The partners also support a multi-faceted impact strategy:

1. Impact on science and its organisation. We place significant emphasis on achieving a transformative impact on science by working closely with our partner organisations to achieve the necessary organisational culture shift towards one that embraces the full potential of data science and its role within a new kind of open, integrative and collaborative science. Key mechanisms: challenge themes, continuous co-design, workshops, our agile methodology and show and tell sessions.

2. Impact on training. We will address the acute skills shortages in environmental data science through the development of a new breed of researchers that understand both contemporary data science practices and the challenges of environmental science. We also place emphasis on training to amplify this impact to partner organisations and beyond. Key mechanisms: training events and online materials, summer schools.

3. Impact on policy. We also focus heavily on the role of data science to support the development of mitigation and adaptation policy, with this work being enhanced by associated research around data, trust and communication (as a planned PhD topic). We will also utilise Small World Consulting, JBA and the RIDE Forum in this translational work. Key mechanisms: policy workshops, secondments.

4. Impact on the public. We plan a number of public outreach events as part of the programme of research. Key mechanisms: Data Science Meetups, public lectures, Caf&eacute; Scientifique.

The combined contribution from the partners is &pound;542,680. Further details of our impact strategy can be found in Pathways to Impact (also refer to letters of support)."
21,A48263C5-3828-4A2E-BCA8-86C50C11FB4A,"Closed-Loop Data Science for Complex, Computationally- and Data-Intensive Analytics","Progress in sensing, computational power, storage and analytic tools has given us access to enormous amounts of complex data, which can inform us of better ways to manage our cities, run our companies or develop new medicines. However, the 'elephant in the room' is that when we act on that data we change the world, potentially invalidating the older data. Similarly, when monitoring living cities or companies, we are not able to run clean experiments on them - we get data which is affected by the way they are run today, which limits our ability to model these complex systems. We need ways to run ongoing experiments on such complex systems.
 
We also need to support human interactions with large and complex data sets. In this project we will look at the overlap between the challenge someone faces when coping with all the choices associated with booking a flight for a weekend away, and an expert running complex experiments in a laboratory.

The project will test the core ideas in a number of areas, including personalisation of hearing aids, analysis of cancer data, and adapting the computing resources for a major bank.",,"To catalyse a long term culture change and build skills capacity in Data Science, the School is planning to co-locate up to 30 J.P. Morgan staff and the Glasgow Hub of The DataLab in the same building as the academics with an Innovation Hub building in construction. Further data providers are Glasgow Polyomics, the Urban Big Data Centre and Skyscanner. The long term legacy in data science will be achieved through changing practice together with data owners, but also by sharing the tools developed in open-source resources that can be developed by others.
 
Closed-loop data science is a new, challenging topic which will take time for both researchers and end-users to grasp. This project will be closely integrated with two data-rich companies (J.P. Morgan, Skyscanner) who will ground the research in a real world industrial context, as well as academic institutions from life sciences and urban studies (Glasgow Polyomics, Urban Big Data Centre). We will exploit industrial collaborations via the QuantIC innovation hub and the DataLab Innovation Centre. In each case, this project leverages existing tight-knit links to local teams. The core research topics have applications with multiple end-users (both companies/data providers, as well as their customers), allowing us to test theories in multiple domains, and transfer experience between application areas. For instance, Skyscanner customers will benefit from more effective recommendations and search results that are less biased from closed-loop interactions.
 
We have embedded staff with end-users (in J.P. Morgan's case they are embedded with us), and there will be separate task groups and meetings on projects with specific data providers. Each post-doc will also act as a liaison point for at least one of the external data providers. 50% of a Grade 7 PDRA will be supported at the academic data providers (UBDC, Polyomics). We will arrange periodic away days for all staff students and end-users to discuss new developments, and look for cross-fertilisation of ideas among application areas and data providers.
 
A senior RA will support adaptation of research-quality systems to work within the constraints of the target systems of the data providers. To ensure a lasting impact, this RA will ensure the quality of the open-source software made available for immediate use with project partners, and to manage the dissemination to the broader technical community, ensuring its longer-term legacy. Project partners will provide ongoing feedback on compliance with current standards for deployability with their teams. The impact process will be supported by Jill Ramsay, the SoCS Business Development professional, as well as BD staff at the DataLab and QuantIC Hubs.
 
The UoG is building a &pound;100M interdisciplinary Research &amp; Innovation Hub - a commitment to excellence which will include a Data Science Centre to provide an environment in which researchers involved in the generation, interrogation, analysis and visualisation of large, complex datasets can come together to stimulate new cross-disciplinary collaborative projects. Creating a culture of co-creation is at the heart of the Innovation Hub vision; embedding the Data Science Centre in the hub will enable to fully support and sustain this project and its plans for co-creation well beyond the initial EPSRC funding."
22,543AB456-C091-48C1-B0EC-02C4FC1AB153,MIMIC: Musically Intelligent Machines Interacting Creatively,"This project is a direct response to significant changes taking place in the domain of computing and the arts. Recent developments in Artificial Intelligence and Machine Learning are leading to a revolution in how music and art is being created by researchers (Broad and Grierson, 2016). However, this technology has not yet been integrated into software aimed at creatives. Due to the complexities of machine learning, and the lack of usable tools, such approaches are only usable by experts. In order to address this, we will create new, user-friendly technologies that enable the lay user - composers as well as amateur musicians - to understand and apply these new computational techniques in their own creative work.

The potential for machine learning to support creative activity is increasing at a significant rate, both in terms of creative understanding and potential applications. Emerging work in the field of music and sound generation extends from musical robots to generative apps, and from advanced machine listening to devices that can compose in any given style. By leveraging the internet as a live software ecosystem, the proposed project examines how such technology can best reach artists, and live up to its potential to fundamentally change creative practice in the field. Rather than focussing on the computer as an original creator, we will create platforms where the newest techniques can be used by artists as part of their day-to-day creative practices.
 
Current research in artificial intelligence, and in particular machine learning, have led to an incredible leap forward in the performance of AI systems in areas such as speech and image recognition (Cortana, Siri etc.). Google and others have demonstrated how these approaches can be used for creative purposes, including the generation of speech and music (DeepMinds's WaveNet and Google's Magenta), images (Deep Dream) and game intelligence (DeepMind's AlphaGo). The investigators in this project have been using Deep Learning, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory Networks (LSTMs), and other approaches to develop intelligent systems that can be used by artists to create sound and music. We are already among the first in the world to create reusable software that can 'listen' to large amounts of sound recordings, and use these as examples to create entirely new recordings at the level of audio. Our systems produce outcomes that out-perform many other previously funded research outputs in these areas.

In this three-year project, we will develop and disseminate creative systems that can be used by musicians and artists in the creation of entirely new music and sound. We will show how such approaches can affect the future of other forms of media, such as film and the visual arts. We will do so by developing a creative platform, using the most accessible public forum available: the World Wide Web. We will achieve this through development of a high level live coding language for novice users, with simplified metaphors for the understanding of complex techniques including deep learning. We will also release the machine learning libraries we create for more advanced users who want to use machine learning technology as part of their creative tools. 

The project will involve end-users throughout, incorporating graduate students, professional artists, and participants in online learning environments. We will disseminate our work early, gaining the essential feedback required to deliver a solid final product and outcome. The efficacy of such techniques has been demonstrated with systems such as Sonic Pi and Ixi Lang, within a research domain already supported by the AHRC through the Live Coding Network (AH/L007266/1), and by EC in the H2020 project, RAPID-MIX. Finally, this research will strongly contribute to dialogues surrounding the future of music and the arts, consolidating the UK's leadership in these fields.",,"We will directly engage stakeholders in the process of music making with creative tools, exploring the role that AI will play in the future of the creative industries. We will bring complex AI and machine learning technologies to the general user of creative software; we will democratise technologies that are still emerging in academia and corporate R&amp;D labs.

These groups will benefit from new software, course materials, events, artistic outputs and industry collaborations:

a) creative practitioners and their audiences; specifically musicians, composers and their audiences;
b) the hacker/maker community;
c) industry professionals; including through existing industry partnerships with record labels (XL, Universal), music technology companies (Akai, Roli, Ableton, Reactable, Cycling74, Abbey Road Red) and our project partner, Google Magenta;
e) learners; including those from secondary and higher education, home learners, and academics and professionals
f) the general public.

A key aim of our project is to create a simplified live coding language for coding in the browser where novices can learn about AI and machine learning through a clear and simple, yet powerful, live coding programming language written on top of JavaScript , which is well supported and popular. This simplified live coding language will be designed specifically for musicians and artists and will allow them to pursue new routes for creating music. 

We will facilitate a range of high-quality use cases with creative professionals. This will bridge gaps between research and industry, accelerating the impact of artificial intelligence by deploying it in real-world and professional music-making and listening contexts.

Our events series will bring musicians, composers and audiences together, providing an important platform for the continued dissemination our work and the work of those practitioners whom we support through the creation of new tools. 

In addition to concerts, we will run symposia to expand and further develop critical thought in these fields, inviting participation from a range of stakeholders. We will also disseminate and support artistic output through the creation of our platform, making it simple not just to create work, but also to share it amongst friends and colleagues, from both outside and inside our connected communities.

Our background technologies will be open source and available to academics and SMEs alike, allowing them to use contemporary AI in ways that are currently very challenging for novices. 

We will generate significant, engaging and unique course materials, associated with our existing MOOC provision, and targeted at a range of different learners, from secondary education, through HE, to home learners, academics and professionals. This will help people to acquire skills in machine learning at any stage. 

Our track record indicates we are capable of meeting the significant interest from the general public around these issues. Recent public engagement activities from team members have included:

- applying Deep Learning to the creation of artworks we are currently exhibiting at the Whitney Museum of American Art, with an accompanying paper at SIGGRAPH
- significant press around the use of AI and Machine Learning for music 
- generative AI music software created used by Sigur Ros for the release of their most recent single (Route One), and a generative remix of the track broadcast for 24 hours on Icelandic National TV and watched by millions of people online
- contribution to the first computer generated West End musical 
- high profile experiments on live national radio, as well as experience developing large scale, online collaboration platforms
- machine learning software for composers and musicians, downloaded over 5,000 times and the world's first MOOC on machine learning for creative practice; 
- design of various popular live coding systems (ixiQuarks, ixi lang, the Threnoscope)."
23,5091EBCF-369A-4BB2-99C0-7A40E621ED95,The Internet of Food Things,"The &quot;Internet of Food Things&quot; will create an interdisciplinary network that defragments and expands the UK's food digital economy. Food and drink is the largest manufacturing sector of the UK economy. The food supply chain from farm to consumer generates &pound;108bn GVA per year and employs 3.9m people. In addition, food has highly significant social and environmental impacts. Obesity alone, including downstream health impacts such as diabetes, heart disease etc, costs the UK economy &pound;49bn per annum. There are still c. 1,000,000 cases of food poisoning per year costing &pound;1.5bn p.a.. Food generates up to 30% of the UK's road freight, but 10MT of food, generating 20MTCO2e of GHG emissions, are wasted each year. 

Digital technology has the potential to transform the food chain, for example, opportunities (that map onto the EPSRC DE Network strategy) include but are not limited to;
- New business models via distributed ledger technology (DLT) to underpin the traceability of food. The recent Holmes report identified food as one of the key seven UK industry sectors most likely to benefit from DLTs.
- The creation of a &quot;data trust&quot; for the food sector to underpin data sharing, trust and interoperability within complex supply chains. 
- Wide scale application of the internet of things (IoT) for the service community, for example, the use of IoT by domestic users (refrigerators, cooking devices etc) to improve health outcomes and reduce waste. 
- The development of new digital labelling protocols that assist with consumer use of food as well as supply chain optimisation, 
- The use of novel digital technologies (e.g. artificial intelligence) to reduce food waste by optimising whole supply chains from manufacturer to consumer.

Hitherto these opportunities have not or are only partially realised. There is an urgent need to defragment the digitally inspired academic community and connect it to food industry practitioners.

Although the digital focus is in within EPSRC's remit (IoT, blockchain, data trusts, interoperability issues), we will multiply impact by including interdisciplinary contributions from food science and technology practitioners, policy makers, engineers, management specialists and colleagues in social and behavioural sciences. The network will include academia, industry and consumer interests. The industry interest covers the whole food and digital innovation chain including food manufacturers (e.g. Food and Drink Federation, EPSRC Food CIM), IoT and digital specialists (Siemens and IMS Evolve), the HVM Catapult and regulators such as the Food Standards Agency and GS1 the international agency that sets data standards (bar codes) for retail. Consumers will be represented through out, but the inclusion of food retailers within the consortium provides access to unrivalled data sets demonstrating behaviours.

The DE network will facilitate a number of key actions, including a marketing, social media and work shop / conference campaign that yields a large scale (up to 500 persons) network who have mutual interests within the food digital domain. We will host one main conference per year and in addition 3 facilitated workshops p.a. to deep dive key questions within the food domain. We will fund a range of pilot studies (&pound;350K applied) and detailed reviews to underpin horizon scanning. All the research challenges will be co created with industry. We expect that the network will facilitate onward research funding and catalyse interest in the food digital economy. In addition to network activities, we will deliver a comprehensive pathway to impact that engages professional practitioners as well as the general public and schools.",,"The food chain generates a GVA of &pound;108bn, with 3.9m employees in a truly international industry that realises &pound;20bn of exports p.a. It is the largest manufacturing sector within the UK, greater than aerospace and automotive combined. 
However, it's total R&amp;D spend is low, the Institute of Physics (Oct 2016) estimates that it accounts for 2% of UK R&amp;D funding compared to 21% for automotive and aerospace combined.

Furthermore, the environmental and societal impacts of the food chain are significant; for example,
a) obesity (dealing with e.g. diabetes and heart disease) costs the UK &pound;47 billion per annum;
b) the food chain uses 17% of UK energy consumption; post farm-gate the food chain is responsible for 10MT of food waste and more than 20 MTCO2e of GHG emissions; 
c) there are still 1,000,000 cases of food poisoning per year; 
d) regular food &quot;scares&quot; undermine consumer confidence and cause alarm, i.e. &quot;Dutch Egg Scandal, 2017&quot;; &quot;Hepatitis E sales of pork, 2017&quot;; &quot;Horsemeat, 2013&quot;. 
e) post Brexit the pressures on the food chain will increase, it is muted that the UK will see imports of foods from countries with different regulatory environments (chlorinated chicken, hormone impregnated beef), as well as competition from low cost importers and potentially reduced availability of migrant labour. 

Digital technology has the potential to transform the food chain. In this project we will develop a sustainable and cross disciplinary DE Network that focus's on developing and enabling the potential of digital technology within and through the food supply chain. We will examine a range of digital technologies (IoT, blockchain, architectures, analytics, twins etc) but focus on how they can deliver economic, environmental and societal impacts. This will be facilitated by the defragmentation of digital resources (especially human) with an interest in the food chain, network development, pilot studies, horizon scanning and novel digital training approaches to underpin industrial adoption.
The specific impacts will be;
1) Economic. Digital technology has the potential to drive productivity within the food manufacturing sector. The food sector was considered as a vertical strand within the BEIS Made Smarter Review (Nov, 2017) led by Juergen Maier (Siemens CEO) and outlined in the Industrial Strategy White Paper (Nov, 2017). Within this review, Accenture determined that digitisation of food manufacturing would yield &pound;55bn of economic benefit from 2018 to 2030. This project alone will not deliver these benefits but it is a critical stepping stone in the process. 
2) Environmental. The food chain has huge environmental impacts, for example 30% of all UK road freight is deployed to distribute food. UK food waste is unacceptable (10MT) and comes at great environmental cost (GHG and water). Digital approaches could transform the environmental impact of the food chain and Accenture estimate that food waste could be reduced by 20%. This will be via digital technology that connects the whole supply chain, improved data analytics to match supply and demand as well as human centric techniques that challenge behaviours (digital diet plans, recommendations on storage and recipes etc).
3) Social. Food has significant social impacts, it underpins human health (obesity, diabetes, food poisoning) and well being. We will explore novel approaches to develop a food digital economy that facilitates an improved interaction between people and food. This may include a wide range of techniques including digital approaches to raise awareness of food health issues, digital diet plans, novel concepts to underpin food safety, traceability and authenticity."
24,9B2C0EED-76F5-469D-9E67-2A95A4BB9817,Copacetic Smartening of Small Data for HLC,"The need for more human-like computing, which involves endowing machines with human-like perceptual reasoning and learning abilities, has becoming increasingly evident in the last year. The inexplicable 'black box', highly complex and context dependent models of deep learning techniques and conventional probability approaches, are not always successful in environments like Improvised Explosive Device Disposal (IEDD), which can have severe consequences for incorrect judgements. Moving towards a more transparent, explainable and human-like approach will transform the human-machine relationship and provide a more efficient and effective environment for humans and machines to collaborate in, leading to improved prospects for UK growth and employment.

This feasibility study focuses on those high risk situations where human cognition is superior to any machine, when humans are called to make judgements where information is sparse, time is poor and their previous knowledge, experience and 'gut feel' often play a critical part in their decision making. Unlike machines, humans rely on small scale data and small scale models (e.g. schema or frames) to make their judgements, reflecting on the possibilities or likelihoods of surprise events to improve their sense making in a given situation. A key challenge is to identify those few critical learning and inference kernels (CLIKs) that are at the heart of these schema humans use to make their judgements in a satisficing manner that feels right, i.e. things appear to be in copacetic or perfect order. Using the IEDD context as its setting, this research moves away from the conventional Bayesian and probability-based approaches, instead moving towards a novel approach inspired by the cognitive sciences to develop human-like inference techniques and learning schema. The schema will then be encoded into explainable artificial intelligence (XAI) agents so they can work alongside humans to enhance performance during high cognitive load tasks and for the learning and training of future experts.",,
0,870E2D25-56FB-40C1-A27A-7CA7E694281F,Rapid fault-recovery strategies for resilient robot swarms,"Robots are increasingly becoming an important part of our day-to-day lives, automating tasks such as keeping our homes clean, and picking/packing our parcels at large warehouses. An aging population and the need to substitute human workers in dangerous and repetitive tasks have now resulted in new tasks on the horizon (e.g., in agriculture automation and environmental monitoring), requiring our robots to do more, to work in large-numbers as part of a swarm (a large team of robots), to coordinately sense and act over vast areas, and efficiently perform their mission. However, our robot swarms to date are unprepared for deployment; unable to deal with the inevitable damages and faults sustained during operation, they remain frail systems that cease functioning in difficult conditions. The goal of this project is to remedy this situation by developing algorithms for robot swarms to rapidly -- in no more than a few minutes -- recover from faults and damages sustained by robots of the swarm.

The existing fault-tolerant systems for robot swarms are limited. They are constrained to only diagnose faults anticipated a priori by the designer, which can hardly encompass all the possible scenarios a robot swarm may encounter while operating in complex environments for extended periods of time. The multitude of robots in a swarm and the large number of intricate ways they can interact with each other makes it difficult to predict potential faults and predefine corresponding recovery strategies; which may explain why none of the existing fault-detection and fault-diagnosis systems have been extended to provide fault-recovery mechanisms for robot swarms. Therefore, in order to design fault-tolerant algorithms for robot swarms, we need to move beyond the traditional approaches relying on fault-diagnosis information for fault recovery.

Fault recovery in a robot swarm may instead be formulated as an online behavior-adaptation process. With such an approach, the robots of the swarm adapt their behavior to sustained faults by learning via trial-and-error new compensatory behaviors that work despite the faults. However, the current approaches to learning new robot swarm behaviors are time-consuming, requiring several hours. Therefore, such approaches are inappropriate for behavior adaptation (learning new swarm behaviors) for rapid fault recovery.

Behavior adaptation for effective fault recovery requires the robot swarm to creatively and rapidly learn new compensatory swarm behaviors online, that work despite the sustained faults, effectively recovering the swarm from the faults. The proposal will address these requirements by investigating data-efficient machine learning techniques for rapid online behavior adaptation, guided by creatively and automatically generated intuitions -- evolved offline -- of working swarm behaviors. The resulting system would have a significant impact on long-term operations of robot swarms, and open up new and interesting applications for their deployment, such as the monitoring of large bodies of water for pollutants using a swarm of autonomous surface vehicles.",,"The proposed project aims to develop algorithmic approaches for large-scale low-cost robot swarms to rapidly recover -- in no more than a few minutes -- from faults and damages sustained by individual robots of the collective. This will be achieved through the development of a novel combination of data-efficient machine-learning, with nature-inspired computing algorithms. Though close collaboration with project partners, the developed fault-recovery algorithms will be used to help improve the long-term autonomy of robot swarms, evaluated on a indoor swarm of mobile robots, and an outdoor swarm of aquatic surface drones. The project outcome is geared towards developing resilient robot swarms, working seamlessly around us, delivering the following economic and societal contributions.

Economic impact: A June 2014 report by the Intellectual Property Office indicates that the UK Government has identified Robotic and Autonomous Systems (RAS) as one of eight impactful technologies with the potential to propel the UK to future growth (goo.gl/YBSJ24). Developed RAS technologies are predicted to enable the conception of new products and services, a market of over &pound;70 billion by 2025, disrupting existing markets as divergent as environmental monitoring, transport, manufacturing, and medicine (goo.gl/hkxDJY). Therefore, it is essential for the UK to be at the forefront of this research area, to capture value as the markets disrupt and change.

Autonomous operation will be an essential feature of the next generation of robot swarms, allowing the robots to continue functioning despite faults resulting from common wear and tear of their functional parts, and unexpected changes in their operational environments. This ability increases the usefulness of the robots, and extends the amount of time they can continue operating without human intervention, thus providing a substantial economic advantage. For instance, industries deploying robot swarms do not expect their system to grind to a halt, every time the robots encounter an unanticipated situation, and individual robots suffer damages. They would preferably avoid disruptions in their regular operations, and the hefty start-up costs required to provide existing robot swarms with carefully controlled working environments, if that were even possible (e.g., robot swarms operating outdoors). The proposed research will contribute towards the development of long-term autonomous robot swarms, ideal for such scenarios.

One of the potentially impactful real-world application scenario for robot swarms is that of autonomous surface vehicles (ASVs) in rivers, lakes and marine environments. In such environments, robots are required to operate over vast areas, with minimal human intervention. ASV robots have been commercialized, and widely used in commerce, industry and military applications; such an established and active market for the platform now opens up impactful avenues for applications with ASV swarms. Finally, a number of industry players in the UK have keen interests in this area. Engagement with these players will be fostered by industrially relevant (out of the lab) robot swarm case-studies, developed in collaboration with the Southampton Marine and Maritime Institute, MBDA-UK Ltd. and ASV Global.

Societal impact: The robot swarms envisioned in this project, capable of extended autonomy, also have the potential to directly impact important scientific endeavors, beyond the robotics community. In these endeavors, robots will be employed as a fundamental data-gathering tool, allowing a swarm of robots to provide us with a rich and continual stream of high-resolution data on observed environmental phenomena. For instance, a large swarm of aquatic surface drones fitted with Passive Acoustic Monitoring sensors, may intelligently monitor a water-body for the presence of marine mammals, providing valuable data for the regulation of shipping lanes."
1,41827AF5-EC0A-4067-A46A-D76CBB601FF0,Bayesian Artificial Intelligence for Decision Making under Uncertainty (BAYES-AI),"Scientific research is heavily driven by interest in discovering, assessing, and modelling cause-and-effect relationships as guides for action. Much of the research in discovering relationships between information is based on methods which focus on maximising the predictive accuracy of a target factor of interest from a set of other related factors. However, the best predictors of the target factor are often not its causes and hence, the motto &quot;association does not imply causation&quot;. Although the distinction between association and causation is nowadays better understood, what has changed over the past few decades is mostly the way by which the results are stated rather than the way they are generated. 

Bayesian Networks (BNs) offer a framework for modelling relationships between information under causal or influential assumptions, which makes them suitable for modelling real-world situations where we seek to simulate the impact of various interventions. BNs are also widely recognised as the most appropriate method to model uncertainty in situations where data are limited but where human domain experts have a good understanding of the underlying causal mechanisms and/or real-world facts. Despite these benefits, a BN model alone is incapable of determining the optimal decision path for a given problem. To achieve this, a BN needs to be extended to a Bayesian Decision Network (BDN), also known as an Influence Diagram (ID). In brief, BDNs are BNs augmented with additional functionality and knowledge-based assumptions to support the representation of decisions and associated utilities that a decision maker would like to minimise or maximise. As a result, BDNs are suitable for modelling real-world situations where we seek to discover the optimal decision path to maximise utilities of interest and minimise undesirable risk.

Because BNs come from statistical and computing sciences, and whereas BDNs come mainly from decision theory introduced in economics, research works between these two fields only occasionally extend from one field to another. As a result, it is fair to say that the landscape of these approaches has matured rather incoherently between these two fields of research. It is possible to develop a new generation of algorithms and methods to improve the way we 'construct' BDNs. 

The overall goal of the project is to develop an open-source software that will enable end-users, who may be domain experts and not statisticians, mathematicians, or computer scientists, to quickly and efficiently generate BDNs for optimal real-world decision-making. The proposed system will allow users to incorporate their prior knowledge for information fusion with data, along with relevant decision support requirements for intervention and risk management, but will avoid the levels of manual construction currently required when building BDNs. The system will be evaluated with diverse real-world decision problems including, but not limited to, sports, medicine, forensics, the UK housing market, and the UK financial market.",,"The proposal falls within the priority area of New approaches to data science, and is a perfect fit for the Decision making under uncertainty initiative, which has been identified as an area of significant interest by multiple research councils in the UK. New approaches and tools that enable improved decision-making have become particularly important for policy makers, industrial organisations and academic research. This is because such tools are becoming increasingly useful for optimal decision making to maximise or minimise a particular outcome of interest, whether this involves the most cost-effective route, maximum impact, minimum risk, or an equilibrium between them. Policy makers and industrial organisations who invest in big-data solutions also tend to be particularly interested in these alternative advancements because they can provide them with potential to reduce at source much unnecessary data collection, and at the same time improve decision making performance. 

This proposal is timely in the sense that it combines three emerging technologies; data science, causal modelling, and decision making under uncertainty. This project is expected to extend the state-of-the-art in disciplines related to a) data science and machine learning, with new algorithms that can be used to build Bayesian Decision Networks (BDNs), b) causal modelling, with extended techniques to establish whether a relationship between two factors can be assumed to be causal or some other type of relationship for decision making representation, c) Information fusion, with new methods that will allow us to construct machine learnt models that can take into consideration knowledge-based information about what must, can, and cannot be discovered for decision support purposes and d) Decision Science, with an improved way of constructing models for optimal decision making under uncertainty. 

The main benefits to the areas identified above will be the published algorithms and methods, with open-source access to the code of a software that we will develop during this project. The software will enable end-users, who may be domain experts and not statisticians, mathematicians, or computer scientists, to efficiency and quickly generate BDNs for optimal real-world decision-making. Making the software open-source will help other scientists to tailor existing Bayesian Network (BN) modelling algorithms to enable the efficient development of especially complex BDN models, but will also allow us to perform further improvements to the software to address future end-user requirements.
 
Additionally, the project will contribute to the case-study areas of sports, medicine, forensics, the UK housing market, and the UK financial market. As a result, it is realistic to expect that the project will have positive repercussions in a broad range of disciplines."
2,E20A9CD4-F398-4F1C-945A-23F48D2AE157,Automatic disease detection and monitoring in calves,"Bovine respiratory disease (BRD) is the most common and costly disease affecting cattle in the world. BRD is a complex bacterial infection that can be fatal and is estimated to cost the UK cattle industry &pound;80M annually. Although manual scoring systems exist to aid early identification of the disease, they are time consuming and rarely used in practice. Commonly, identification of BRD is only in later stages of the disease when antibiotics are essential for treatment. Early and automated identification of BRD will have significant impact: 1) on the economic cost to farmers; 2) reducing the quantity of antimicrobial medicines used to treat the disease; and 3) improving the general welfare of animals.

The proposed project uses artificial intelligence techniques, coupled with visible-range and thermal cameras, to identify BRD at the earliest possible stage with main goals of establishing: 1) how early in disease development affected animals can be reliably identified; 2) the best way to scale up image capture and machine learning to automatically screen animals and alert farmers to those needing treatment; together with 3) developing a protocol for effective use of the trained system. The aim of the proposal is to develop a system based on providing the best possible information in a timely manner, which is key to making right judgements for farmers and vets alike. It is believed that a system based on low cost cameras and sensors, together with state of the art deep neural networks, can provide this.

The completed fellowship will result in a working and tested prototype system capable of development into a viable commercial product. During the fellowship a network of industry collaborators (including farmers, vets, advisory/regulatory bodies, equipment manufacturers and food producers) will be developed to support and promote the research and resulting product.",,"This proposal details of the activities needed for early automatic detection of disease in collaboration with the farming industry. In the first instance and for the purposes of research and development, the focus of the proposal is Bovine respiratory disease (BRD). BRD is a complex bacterial infection and is the most common and costly disease affecting cattle. In particular, planned collaboration with UK industry in the form of commercial farms and vets, will be developed in order to support the programme outlined in the proposal. Completion of the programme not only offers opportunities within the UK, based on a commercial start-up, but also the potential to develop the product for export, both of which will be important for the UK's future industrial strategy. Beneficiaries of the proposed programme are in both the academic and professional communities, as well benefits for society more generally. 

Academically, benefits will accrue in artificial intellegence through applying deep learning to a combination of (low quality) visual and thermographic spectrum data, with the aim of characterisation of pre-clinical BRD. Of particular interest will be the potential to identify which stage an animal is in the disease development, rather than a simple binary classification. Indeed, it is expected that this will highlight subtleties and relationships in the data that are not currently known and will aid disease characterisation. The ability to characterise disease in this 'automatic' way will also be of considerable interest to academic vets who study animal diseases. In veterinary medicine one of the deliverables will be a method of screen-based scoring of disease, based on assessment of visible-range and thermographic images, which will further contribute to the general approach to study diseases in animals. Overall and significantly, the project will contribute to knowledge of how early in the development of disease (BRD in the first instance) it can be reliably identified, this in turn provides information for vets when choosing the most appropriate treatment.

Being able to identify disease earlier has obvious benefits to farmers, such as fewer sick animals for shorter periods, that contribute to lower costs and better welfare. Professional veterinary surgeons also benefit from an addition to the diagnostic tools at their disposal, together with more appropriate treatment. Of particular interest and benefit from the point of view of professional vets is that early diagnosis leads to reduced use of antimicrobial medicines. This is a significant and pressing issue that leads to benefits to wider society through reducing the levels of antimicrobial resistance.

These benefits have already been acknowledged by our collaborators as the following quote (from an email to the applicants, talking about BRD) illustrates

&quot;In the last few weeks - at least in my practice - initial treatment was often followed by re-occurrence and in such cases the previous treatment was not effective anymore. This leads to a prolonged treatment period, which increases the amount of medicine required. 

A repeated and prolonged treatment profile regularly results in side-effects; particularly problems with digestion, anorexia, increased gas levels in the rumen, diarrhoea, changes in gastric flora, enterotoxemia, pododermatitis, lameness, and recumbency followed by emaciation. These individuals generally fall behind, others push them away from feeders and harass them, which leads to continuous fear and stress, loss of growth, and increased mortality due to other diseases.

These are real issues that I experience on a daily basis, causing significant amount of loss for the farmers, not to mention aspects of animal welfare. The proposed early detection coupled with timely treatment promises lower medicine consumption, which will minimise the side-effects of antimicrobial treatment.&quot;"
3,6BC7C9E8-A717-438A-9B0E-F35357907105,Revisiting optical scattering with machine learning (SPARKLE),"The surface topography of a component part can have a profound effect on the function of the part. In tribology, it is the surface interactions that influence such quantities as friction, wear and the lifetime of a component. In fluid dynamics, it is the surface that determines how fluids flow and it affects such properties as aerodynamic lift, therefore, influencing efficiency and fuel consumption of aircraft. Examples of the relationships between the topography of a surface and how that surface functions in use can be found in almost every manufacturing sector, both traditional and high-tech. To control surface topography, and hence the function and/or performance of a component, it must be measured and useful parameters extracted from the measurement data. There are a large number instruments that can measure surface topography, but many of them cannot be used realistically for real-time in-process applications due to the need for scanning in either the lateral axes and/or the vertical axis. There have been developments in area-integrating (scattering) methods for measuring surface topography that can be fast enough to use during a manufacturing process, but these are limited in the height range of surface topography with which they can be used.

In conventional machining, there has been a significant research effort to determine the surface topography of the machined parts during the manufacturing process. The dominant technology for this has been machine vision approaches, where a relationship between a texture parameter and an aspect of the measured field from an intensity sensor is determined. Such approaches have two major drawbacks: 1. they are usually applied to surfaces with geometrical features over a limited range and 2. they do not have the benefit of a physical model of the measurement process, i.e. they are purely empirical. As an example, the measurement and characterisation of the surface topography of additive manufactured parts remains a significant challenge, especially where measurement speed may be an issue. Typical metal additive manufactured surfaces have a large range of surface features, with the dominant features often being the weld tracks with typical wavelengths of a few hundred micrometres and amplitudes of a few tens of micrometres; such structures are beyond what can be measured effectively with existing commercial approaches. 

In the proposed project, we aim to demonstrate that it is possible to measure rough and structured, machined or additive surfaces using a simple, cost-effective real-time measurement system. This will involve the development of a fully rigorous three-dimensional optical scattering model, which will be combined with a machine learning approach to mine optical scattering data for topographic information that is not within the range of commercial scattering instruments. The proposed system could be mounted into a machining or additive operation without slowing down the process, therefore, reducing the cost of many advanced products that require engineered surfaces. To demonstrate the commercial potential of the project outputs, we have several advanced manufacturing partners who will supply industrially relevant case studies and one partner who could act as the commercial exploitation route for the instrument.",,
4,55C8D792-52C8-4447-9419-34DD9CCEC6B5,Copacetic Smartening of Small Data for HLC,"The need for more human-like computing, which involves endowing machines with human-like perceptual reasoning and learning abilities, has becoming increasingly evident in the last year. The inexplicable 'black box', highly complex and context dependent models of deep learning techniques and conventional probability approaches, are not always successful in environments like Improvised Explosive Device Disposal (IEDD), which can have severe consequences for incorrect judgements. Moving towards a more transparent, explainable and human-like approach will transform the human-machine relationship and provide a more efficient and effective environment for humans and machines to collaborate in, leading to improved prospects for UK growth and employment.

This feasibility study focuses on those high risk situations where human cognition is superior to any machine, when humans are called to make judgements where information is sparse, time is poor and their previous knowledge, experience and 'gut feel' often play a critical part in their decision making. Unlike machines, humans rely on small scale data and small scale models (e.g. schema or frames) to make their judgements, reflecting on the possibilities or likelihoods of surprise events to improve their sense making in a given situation. A key challenge is to identify those few critical learning and inference kernels (CLIKs) that are at the heart of these schema humans use to make their judgements in a satisficing manner that feels right, i.e. things appear to be in copacetic or perfect order. Using the IEDD context as its setting, this research moves away from the conventional Bayesian and probability-based approaches, instead moving towards a novel approach inspired by the cognitive sciences to develop human-like inference techniques and learning schema. The schema will then be encoded into explainable artificial intelligence (XAI) agents so they can work alongside humans to enhance performance during high cognitive load tasks and for the learning and training of future experts.",,"The impact from this project will benefit the inter-disciplinary research community developing novel ways for enhancing human-machine collaboration. One key impact is expected to be growth in the scale and diversity of the research into human-like computing approaches and algorithms that are not restricted by more conventional probabilistic approaches. This will lead to greater understanding of human-like computing and the ways in which explainable artificial intelligence (XAI) can support humans in high risk, uncertain, critical decision-making environments; also how the lessons drawn from critical learning and inference Kernels (CLIK) development could lead to improved business processes, risk assessment and economic growth from new human-like computing tools or services. The project will also impact government departments and organisations across industry that would gain value from implementing human-like computing for enhanced judgements in a wide variety of applications. The development of the CLIKs and XAI will benefit people working increasingly with cyber-forms of business, collaborating with computers and AI; resulting from increased awareness of these challenges of leaders of UK organisations with influence across business and government (primarily in the UK, but also internationally). The longer-term impact, achieved by exploitation of the agent prototypes and approaches developed during the project, will enable co-creation of new XAI services, and creation of new, smarter, approaches to government and new businesses. It is anticipated that these will impact the public, as users of the smarter systems and services created by the project. The case study draws from experienced senior leaders in UK business, industry and government who will in turn benefit from the reflective practice nature of the elicitation interviews. This appreciative, explainable and reflective form of judgement-support working will, in the short-term, help people to make more robust judgements leading to a more stable basis for policy-making in financial sectors and government. This will benefit UK public as they rely on safer systems (e.g. banking) and the more contractual nature of government projects post-Brexit. Improved, explainable judgements in policy-making and more reflective decision-making will generally provide a more stable and sound foundation for leadership, leading to improved prospects for UK growth and employment."
5,97D50901-DB00-4810-8A50-4C49AFCA941D,EPIC: An automated diagnostic tool for Potato Late Blight disease detection from images,"The yields of crop plants are deleteriously affected by various diseases. It is estimated that almost 25% of worldwide crops are lost to diseases, which may cause devastating economical, social and ecological losses. In China, as the fourth important food crop, yield losses from potato late blight diseases can vary from 20%-40% in common years. In severe cases, the yield loss may reach 50%-100%. The estimated yearly economic losses due to this disease are around $5 billion in China. Early accurate detection and identification of crop diseases plays an important role in effectively controlling and preventing diseases for sustainable agriculture and food security.

In our previous funded projects, we have developed an innovative automated machine vision system for efficient crop disease diagnosis from images, which have proven the technical feasibility of using advanced image processing, machine learning, mobile and cloud computing approaches. 

This project will take it forward and develop a near-market product ready for commercialisation, which can provide more accurate real-time information for crop disease surveillance. The tool can run on mobile devices. Farmers with basic training can perform disease diagnosis immediately. Compared to the current practices using human visual observation (which is labour intensive, costly and error-prone), this machine vision system can dramatically speed up diagnosis, and give growers more accurate information on which to base their disease control strategies and stop crop yields from being reduced by infection. This technology can overcome lack of expertise, help make a significant impact on agricultural productivity and farmer incomes, ensuring food security, and deliver highly cost-effective, long-term economic and social impact in China.

To achieve actual impact and demonstrable benefits in China, this project will work closely with Chinese partners from academia, industry and farmers including: the project partner (Hebei Agriculture University (HEBAU)), and end users (Beijing Mengbangda Biotechnology Co. Ltd (BMB) and Guyuan County Potato Association (GCPA)). They will provide support in gathering the field data, setting up trial systems and domain knowledge input from plant pathologists for local agriculture and fine tuning the systems in the fields, as well as potential commercialization of this technology in China (Hebei province initially). The project focuses on three stages of translational/user engagement: 
1) System requirement gathering from users; 
2) System evaluation with input from users; 
3) Potential impact and commercial exploitation with end users. 

The tool will be initially deployed in the real fields provided by end users (BMB and GCPA) in Hebei province at the end of project. This will help protect potato-planting area of over 380k MU initially from the disease infection with reduced annual costs on fungicide usage and damages to environment. 

Other translational activities include organization of workshops, conference attendance and paper publications, which will be used for dissemination and engagement with a wide range of user groups on a large-scale.

This project will not only develop a near-market product but will also generate a significantly measurable impact, promote long-term sustainable growth, economic development and welfare in China and beyond.","This project seeks to exploit our previous funded work, develop a near-market, automatic diagnostic machine vision tool for accurate detection of crop diseases using advanced image analysis and cloud/mobile computing approaches and deliver benefits and impact in China, with an initial focus on potato late blight (the most sever potato disease in China and worldwide). Our cloud-based machine vision system can rapidly identify the disease based on smartphone images. The farmers with only basic training can perform diagnosis immediately, with no need for experts based in the fields. In collaboration with our project partner Hebei Agriculture University (HEBAU) and end users (Beijing Mengbangda Biotechnology Co. Ltd (BMB) and Guyuan County Potato Association (GCPA)), the tool will be initially deployed in Hebei Province protecting over 380,000MU planting area. This will help make a significant impact on agricultural productivity and farmer incomes, and generate robust social and commercial impact in China.",N/A according to the call document
6,69C7E0FB-9315-4B65-A95F-FD5534F7B87F,EPSRC Network+ proposal: Human-Like Computing,"Human-Like Computing (HLC) research aims to endow machines with human-like perceptual, reasoning
and learning abilities which support collaboration and communication with human beings. Such abilities
should support computers in interpreting the aims and intentions of humans based on learning and accu-
mulated background knowledge. The Network fits within the EPSRC national priority areas of a) New and Emerging areas, b) People at the Heart of ICT, c) Cross-Disciplinarity and Co-Creation and d) Safe and Secure ICT.",,"The main stakeholders in our project are: other researchers within both Artificial Intelligence and Cognitive
Science. These stake-holders are drawn from academia, industry and governmental and non-governmental
organisations. We predict that the main impacts will be on the development of a cohesive community of
scientific researchers in the area of Human-Like Computing (HLC). This community will be built around the
development of new theory, implementations and applications of HLC. Such systems will, in the longer term
impact the role of computation within tasks involving close and open-ended interactions between humans
and machines. In particular, a more symmetric interaction between human and machines has the potential
for a closer, more symbiotic relationship between humans and machines in knowledge intensive tasks.

We will communicate directly with our fellow researchers and developers in both academia and industry.
Impact on end users will be indirect: via our contacts in industry, especially with the BBC, Microsoft, Intel
and Syngenta. These companies already have good contact with end users, understanding of their needs
and experience in delivering solutions that meet these needs. It would be an ineffective and inefficient use
of our resources to try to duplicate these contacts, understanding and experience."
7,FEE932FD-1B7F-49A3-91FE-D70E4C293250,Towards Explainable and Robust Statistical AI: A Symbolic Approach,"Data science provides many opportunities to improve private and public life, and it has enjoyed significant investment in the UK, EU and elsewhere. Discovering patterns and structures in large troves of data in an automated manner - that is, machine learning - is a core component of data science. Machine learning currently drives applications in computational biology, natural language processing and robotics. However, such a highly positive impact is coupled to a significant challenge: when can we convincingly deploy these methods in our workplace? For example: 

(a) how can we elicit intuitive and reasonable responses from these methods?

(b) would these responses be amenable to suggestions/preferences/constraints from non-expert users? 

(c) do these methods come with worst-case guarantees?

Such questions are clearly vital for appreciating its benefits in human-machine collectives. 

This project is broadly positioned in the context of establishing a general computational framework to aid explainable and robust machine learning. This framework unifies probabilistic graphical models, which forms the statistical basis for many machine learning methods, and relational logic, the language of classes, objects and composition. The framework allows us to effectively codify complex domain knowledge for big uncertain data. 

Concretely, the project aims to learn a model that best summarises the observed data in a completely automated fashion, thereby accounting of both observable and hidden factors in that data. To provide guarantees, two distinct algorithms are considered:

(a) an algorithm that learns simple models with exact computations; 

(b) an algorithm that learns complex models but rests on approximations with certificates. 

To evaluate the explainable, interactive nature of the learned models, the project considers the application of dialogue management with spatial primitives (e.g., &quot;turn south after the supermarket&quot;). We will study the scalability of these algorithms, and then evaluate the closeness of the learned models to actual suggestions from humans. 

Computationally efficient and explainable algorithms will significantly expand the range of applications to which the probabilistic machine learning framework can be applied in society and contribute to the &quot;democratisation of data.&quot;",,"The work in this proposal has the potential for substantial economic benefits, because better modelling languages, faster and robust inference algorithms will enable the graphical modelling framework to be applied with less effort and more confidence to a broader variety of problems. Graphical models provide a principled reasoning framework that has proven successful for problems in which there is noisy data and unknown hidden variables. Naturally, this applies to many applications and domains, both in academia and industry. Indeed, graphical models have supercharged the use of statistical methods in robotics, vision, and medical diagnosis, as well as core technologies thereof, such as deep generative neural networks. Despite this success, actually specifying these models is very challenging, especially for non-experts, and in that regard, probabilistic relational models have helped enormously. Our project directly builds on these successes, and will allow these models to finally reason about continuous data, which are very common in real-world applications (e.g., measurement errors, temporally-indexed values such as stock price fluctuations).

The following will benefit from the direction of this research: 

- The general public: If users do not understand the inner workings of machine learning models and also cannot extract meaningful behaviour from them, their applicability is likely to be limited to the select few who are technologically-gifted; this is likely to deepen social inequality. Explainable algorithms would help realise machine learning systems as an enabling technology, especially in human-machine collectives, to achieve goals collaboratively. 

- Private sector: Smart services and intelligent programs are becoming increasingly ubiquitous, and are often situated in larger smart environments. By identifying the foundations for compositionality in machine learning, technology-oriented companies can capitalise on these techniques to engineer their systems. Moreover, robustness can help such companies understand the risks of deploying a probabilistic machine learning method in a social context, in the sense of avoiding catastrophic outcomes. Finally, regulations on explainability in algorithms will certainly affect the packaging of AI technologies in products. 

- Research community: They will gain insights on expressive modelling languages and robust machine learning techniques. The problem is that graphical models can be difficult to apply for non-experts, especially if they have to design their own custom inference technique. Our outcomes will contribute to alleviating these challenges. 

To reach out to these beneficiaries on the project's outputs, we will: 

(a) publish the results in the top venues for AI, and engage with industry labs; 

(b) engage with non-experts (via discussion forums such as the Edinburgh International Development Society); 

(c) make benchmarks and software open access and publicise them on, e.g., beyondnp.org, which is a software repository for solvers of problem domains beyond the NP complexity class; 

(d) deliver seminars, lectures, and enhance teaching modules based on the outcomes of this project; and 

(e) organise a research workshop on symbolic methods for explainable AI, to which we will invite industry thought leaders invested in statistical relational learning and human-machine interaction, such as IBM, Microsoft and Google."
8,CC05785E-1067-44E9-A4EA-41FA902F8EAE,Psychological identity in a digital world: Detecting and understanding digital traces of our psychological self,"Boundaries between digital technologies and ourselves become blurred as technology is integrated into our work, home and even our bodies. Interdisciplinary research is needed to understand how our sense of self - our psychological identity - affects and is affected by technology use. During this fellowship, I will lead an interdisciplinary team of psychologists and computer scientists to explore how our identity shapes and is shaped by technology in the fields of security and healthcare. This work will be underpinned by a programme of collaboration with industry partners, Polaris Consulting, the National Crime Agency (NCA), Dstl, Milton Keynes University Hospital (MKUH) and EDP Drugs and Alcohol Services to explore applications that support rather than replace human analysts and consultants.

Sophisticated technologies are rapidly becoming part of our lives. Research on digital technology needs to urgently address concerns about privacy, trust, and ethical implications. I will extend current research in this area by considering how our different psychological identities shape what we find acceptable in different situations. For instance, a person might be less concerned about the tracking of personal information when thinking of themselves as a patient rather than as a parent. Throughout the fellowship, I will work closely with user groups (e.g., patients in the Lived Experience Group Exeter), industry partners and the general public to understand privacy concerns and privacy behaviour.

I will also continue to develop the capacity to detect psychological identities from naturally occurring digital data (e.g., forum posts, blogs, e-mails). This research will allow us to understand which psychological identity (e.g., parent, addict, criminal network identity) is relevant in a particular situation. I will extend my current work to test whether it is possible to distinguish between several identities by analysing text data, whether detecting identities in text is robust to deception, and whether it is possible to tell how committed an individual is to their group from the way in which they communicate online. I will work closely with my industry partners, Polaris, NCA and Dstl, to explore how findings can enhance current machine learning capabilities and analytic approaches in defence and security.

Finally, building on the identity detection work, I will examine how individuals develop new psychological identities (e.g., becoming a parent) and leave identities behind (e.g., leaving behind an addict identity during therapy), and the consequences of such transitions for mental health (e.g., post-natal depression, addiction recovery). I will work closely with industry partners MKUH and EDP to explore how these findings can be translated into diagnostic and monitoring solutions of the future that augment the work of therapists and medical consultants.

The project will be integrated with research on software engineering through the EPSRC SAUSE platform grant. It will be conducted at the psychology department of the University of Exeter, which has a long and successful history of high-impact research. The project will draw on the strengths of the Social, Environmental and Organisational Psychology Research Group (SEORG), which is world-leading in research on social identity, privacy, and well-being, and the Clinical and Cognitive Research Groups, which are world-leading on depression and addiction. The University of Exeter also fosters interdisciplinary work, for instance through the co-supervision of EPSRC students across colleges, access to world-leading experts on machine learning and data science at the Alan Turing Institute and experts in healthcare at the Wellcome Centre for Cultures and Environments of Health and the EPSRC Centre for Predicitive Modelling in Healthcare. Taken together this project will establish Exeter as a key centre for EPSRC work on psychological identity and digital technologies.",,"The main impact aim of this fellowship is to create social and economic benefits to the UK in healthcare and security by combining social psychology with computational linguistics and machine learning.

In particular, the proposed research will contribute to the development and application of AI by understanding how psychological identities shape privacy and, in turn, are shaped by technology use. Importantly, the proposed research will ultimately provide human analysts in security and healthcare with additional information on the psychological identities of target populations, thus augmenting rather than replacing humans. The research therefore addresses the EPSRC/NPIF industrial strategy area &quot;High productivity services through artificial intelligence, data and digital technologies&quot;. 

Psychological identities are closely related with several mental health problems. The proposed research aims to contribute to digital technology that transforms the delivery of healthcare by anticipating and preventing specific mental health problems (e.g., post-natal depression) and providing opportunities for monitoring patients outside clinical settings (e.g., during addiction therapy), thereby enabling independent living. Poor mental health has effects well beyond the individual: A recent government report estimates the costs to employers between &pound;33-42 billion per year and annual costs to the UK economy at around 4.5% of GDP. The fellowship research addresses the EPSRC/NPIF area &quot;Leading edge healthcare and medicines&quot;.

In the security and defence sector, I will work closely with Polaris Consulting to provide UK defence and security analysts with additional information based on the proposed psychological identity detection work to enable them to prioritise and act on relevant information. Outputs generated within this fellowship are anticipated to form part of future software packages. Work with Polaris as well as Dstl serve as a potential route to market for some of the project outputs - with potential to licence IP to Polaris for exploitation of my work through their software. The project will extend current work with the NCA on criminal identities as part of an EPSRC-NPIF studentship that started in September 2017. Working closely with the NCA from the start of the project will ensure that the findings can be translated into appropriate support for their operational and preventative work.
 
In the healthcare sector, I will work closely with Milton Keynes University Hospital (MKUH), EDP Drugs and Alcohol Services and the Lived Experience Group in Exeter to work towards detecting psychological identities relevant to the healthcare context (e.g., addiction identity) from written text. This research ultimately aims to help healthcare providers to achieve cost savings through early detection of (mental) health issues and cost-effective ways to identify and monitor patients remotely.

The planned research will impact on a variety of stakeholders including industry, professional communities of practice and the general public. Throughout the research I will engage with these stakeholders through:
- Patient and public involvement including focus groups, e.g., MKUH, Lived Experience Group
- Steering groups of stakeholders and academic experts in the field
- Research exchange visits with industry partners Polaris and MKUH
- Joint funding applications with industry partners Polaris and MKUH
- Exploration of different routes to market with industry partners, including IP licencing
- Development of proof of concept solutions with software engineering contacts from EPSRC platform SAUSE
- Public engagement, e.g., Soapbox Science, Pint of Science, @Bristol etc. and social and mainstream media
Additional impact will arise through career development for early career researchers (i.e. the fellow and her team of a PDRA and PhDs) including training and visits to leading industry and academic research labs (e.g., University of Queensland."
9,633404F7-44AC-4EA2-9A4C-0AB0F1C386F2,MOA: High Efficiency Deep Learning for Embedded and Mobile Platforms (Full EPSRC Fellowship Submission),"In just a few short years, breakthroughs from the field of deep learning have transformed how computers perform a wide-variety of tasks such as recognizing a face, tracking emotions or monitoring physical activities. Unfortunately, the models and algorithms used by deep learning typically exert severe energy, memory and compute demands on local device resources and this conventionally limits their adoption within mobile and embedded devices. Data perception and understanding tasks powered by deep learning are so fundamental to platforms like phones, wearables and home/industrial sensors, that we must reach a point where current -- and future -- innovations in this area can be simply and efficiently integrated within even such resource constrained systems. This research vector will lead directly to outcomes like: brand new types of sensor-based products in the home/workplace, as well as enabling increasing the intelligence within not only consumer devices, but also in fields like medicine (smart stethoscopes) and anonymous systems (robotics/drones). 

The MOA fellowship aims to fund basic research, development and eventual commercialization (through collaborations with a series of industry partners) algorithms that aims to enable general support for deep learning techniques on resource-constrained mobile and embedded devices. Primarily, this requires a radical reduction in the resources (viz. energy, memory and computation) consumed by these computational models -- especially at inference (i.e., execution) time. The proposal seeks will have two main thrusts. First, build upon the existing work of the PI in this area towards achieving this goal which includes: sparse intra-model layer representations (resulting in small models), dynamic forms of compression (models that can be squeezed smaller or bigger as needed), and scheduling partitioned model architectures (splitting models and running parts of them on the processor that suits that model fraction best on certain processors found inside a mobile/embedded device). This thrust will re-examine these methods towards solving key remaining issues that would prevent such techniques from being used within products and as part of common practices. Second, investigate a new set of ambitious directions that seek to increase the utilization of emerging purpose-built small-form-factor hardware processor accelerators designed for deep learning algorithms (these accelerators are suitable for use within phones, wearables and drones). However, like any piece of hardware, it is still limited by how it is programmed - and software toolchains that map deep learning models to the accelerator hardware remain infancy. Our preliminary results show that existing approaches to optimizing deep models, conceived first for conventional processors (e.g., DSPs, GPUs, CPUs), poorly use the new capabilities of these hardware accelerators. We will examine the development of important new approaches that modify the representation and inference algorithms used within deep learning so that they can fully utilize the new hardware capabilities. Directions include: mixed precision models and algorithms, low-data movement representations (that can trade memory operations for compute), and enhanced parallelization.",,"Expanded discussion upon aspects of how MOA relates to national importance are contained within the Pathways to Impact document. But here summarize in the following paragraphs, the core aspects of importance related to specific EPSRC priority areas, the broader economy, societal impact and contributions to knowledge.
MOA fits within the 'Robotics and artificial intelligence systems' priority area of the UKRI call. It aims to produce technology for enabling machine learning models that otherwise need to run remotely in the cloud, to instead run directly within mobile and embedded systems like robots, drones and small-form-factor devices. As a result, it is an enabling technology useful in the development of new applications or systems. There is direct usage, for example, to enabling healthy/independent living as it would allow a device such as 'Alexa' to be a smarter and better companion to the elderly; or within safety, it can allow a cheap battery-powered workplace or home camera to better understand the semantics of what it captures and react calling the police if it observes dangerous activities. Significantly, the proposed research will likely amplify and extend existing or already on-going machine learning research - this is because it allows a machine learning model that perhaps today must reside in powerful cloud computers to run directly within a drones, robots or devices; this in turn opens up an extend range of new possible application scenarios and use cases for such models.
At a societal level MOA contributes to more ethical, safer and privacy preserving forms of machine learning. Because it enables the use of machine learning directly on limited platforms like phones and devices it reduces the need to transmit and process sensitive data on 3rd party cloud servers not controlled or owned by consumers. Through MOA outcomes, consumers will be able to demand devices (like next-generation medical instruments) that retain their data, yet still offer the benefits of the latest in machine learning.
From the perspective of knowledge, MOA seeks to develop truly innovative concepts in models that can best utilize the latest in commodity and accelerator processor architectures (see workpackage 2). MOA also aims to mature and further invest in studying the latest techniques in a brand-new academic area (efficient deep learning) within a commercial level of quality and rigour (see workpackage 1) that can transfer to offering commercial benefits to companies like Nokia and Samsung that have multiple UK based teams."
10,0B622527-8F59-4C81-BE8F-DBE2E9CFEE8B,"Transfer Learning for Robust, Resilient and Transferable Cyber Manufacturing Systems","Digital Manufacturing relies on pervasive and ubiquitous use of Information and Communication Technology (ICT), sensors, intelligent robots to deliver the next generation of intelligent, co-operating and interconnected manufacturing systems. The research is aimed to improve techniques that can be used to develop digitalised manufacturing systems to reduce existing inefficiencies in production processes that impact on production costs, unplanned downtime, quality and yields. This is not only detrimental to manufacturing businesses but has a negative impact on the UK Economy. The current productivity levels of UK manufacturers and suppliers is lagging behind global competitors and prevents the UK from successfully competing with other countries in the manufacturing domain - which is vital to keep businesses and jobs in the UK rather than relocate production abroad. 

The UK Government wants to increase the strength of the UK Manufacturing Sector. A key means of doing this is the widespread adoption of industrial digital technologies (IDT). Cyber Manufacturing Systems (CMS) are the building blocks of digitalised manufacturing and generate vast amount of data that can be used for real time decision making to achieve optimised performance through predictive and prescriptive analytics. The latter are techniques that use, combine and analyse available data to develop computational models that can predict future outcomes and determine the best course of action.The research, under the fellowship, solves some of the existing problems in this area (CMS), developing new techniques and resources for predictive and prescriptive analytics with the potential to increase efficiency, accuracy and productivity of manufacturing processes. Businesses are therefore more likely to adopt IDTs and improve profitability and sustainability and provide high-quality jobs in a thriving part of the economy.

This project will study novel and robust data analytics methods that will enable to build predictive models that take into account uncertainty, complexity and dynamic behaviour of productions systems. The project will involve:

Objective 1 - develop algorithms that can reuse previously acquired data/knowledge to build more accurate predictive models that work well in the presence of noise (i.e. 'robust'), are able to adapt to changes over time (i.e. 'resilient') and can be scaled up across multiple factories (i.e. 'transferable').

Objective 2 - develop and test novel non-parametric methods for estimation of uncertainty and risks associated to a decision to enable real time mission and safety critical decision making (both automated and human driven) based on predictions.

Objective 3 - iteratively develop, deploy and test predictive and prescriptive models in real and simulated industrial scenarios to obtain acceptable level of performance, usability and robustness. 

There will be significant involvement from industrial collaborators who will provide labelled and aggregated datasets for testing the proposed methods through computer simulations and enable feasibility studies to be conducted in factory environments.

The outcomes of the research, as mentioned above, are ultimately to improve the quality of products, achieving less wastage and unnecessary costs. Through increased adoption of IDTs, the production of goods will, importantly, be more efficient, reliable and profitable. This will support the regeneration of the Manufacturing Sector and boost the global competitiveness of the UK.",,"The UK Government has crystallised the necessity for growth in the manufacturing sector via the adoption and integration of industrial digital technologies [1, 2]. The Government's aspiration is for the UK to become a World leader in Digital Manufacturing by 2030. However, at present the manufacturing sector has a 20% deficit in productivity compared to its main European competitors. Against this backdrop, my research will help to combat the UK productivity gap, stimulating the incorporation of digital technology within UK industry. Cyber Manufacturing Systems (CMS) are the foundations of digitalised production systems - my research will develop new machine learning techniques and toolkits for robust, resilient and transferable CMS. This will manifest in improved understanding of the benefits of IDTs adoption, and their value,and inspire confidence for manufacturers and suppliers to invest in this area. The beneficiaries of my research are a) the UK Economy, b) Organisations within the UK and International Manufacturing Sector, c) members of the public.

The UK Economy will ultimately benefit from my research, as aforementioned, via uptake in industry and the subsequent increase in productivity within the Manufacturing Sector (also potentially leading to job creation and re-shoring to the UK). This will foster global competitiveness, whilst helping to realise well publicised political aims. 

Organisations within the UK and International Manufacturing Sector, beginning with my industry collaborators (TATA Steel, Crown Packaging and Vortex IoT), will prosper in terms of economic enhancement. My work during this Fellowship will bequeath new and improved processes that can be implemented in multiple manufacturing domains. This will achieve increased accuracy, efficiency and flexibility, less wastage of products through quality improvement, reduced costs and enhanced profit margins through failure reduction. Corporate performance will be markedly improved. This will have strong impact, particularly on SMEs, supporting sustainability, investment and expansion. 

Members of the public will be engaged, through the outreach activity that is integrated in the Fellowship. This includes Science Festivals and initiatives such as the Fujitsu Young Ambassadors Programme. The dissemination of the research will acutely increase public awareness and comprehension of Engineering and the relevance of industrial digital techniques in the Manufacturing Sector. Specific audiences that will be targeted, in alignment with my commitment to developing the future workforce and supporting economic regeneration, will be School Children and those in Further Education. 

In short, the research will produce a range of far reaching impacts that will culminate to drive forward economic prosperity; help to promote and propel the incorporation of digital technologies within the Manufacturing arena; and complement the Governments agenda for achieving productivity growth and economic competitiveness.

[1] UK Government. Building Our Industrial Strategy Green Paper. 2017 (https://beisgovuk.citizenspace.com/strategy/industrial strategy/supporting_documents/buildingourindustrialstrategygreenpaper.pdf)
[2] Group IDRW, Made Smarter Review 2017 (https://www.gov.uk/government/publications/made-smarter-review)"
11,59179C76-E183-41E0-BD1F-86A04EA7966C,MTStretch: Low-resource Machine Translation,"Neural machine translation (NMT) has recently made major advances in translation quality and this technology has been rapidly adopted by industry leaders, such as Google and Amazon, and international organisations, such as the UN and the EU. However, high performing neural models require many millions of human translated sentences for training. For many real-world applications, there is not enough data to build useful MT systems. In this project I plan to stretch the resources and capabilities that we have, in order to develop robust MT technologies which are capable of being deployed for low-resource language pairs and for highly specialised low-resource domains. 

I will investigate making translation significantly more robust by using the intuition that translated (or parallel) corpora contain enormous redundancies, and are an inefficient way to learn to translate. Inspired by human learning, we will study Bayesian models which build up meaning compositionally and are able to learn to learn, thus creating models which only need a few training examples. We will also develop machine learning techniques, such as transfer learning and data augmentation, to extract knowledge from monolingual and parallel resources from other languages and domains. This proposal combines fundamental research in rapid deep learning with lower-risk data-driven machine learning research in order to deliver useful products to our industry partners. 

My team will provide translations for language pairs which were not previously well served by automatic machine translation. This will allow our partners, BBC World Service and BBC Monitoring, to cover under-resourced languages. Building on an existing scalable platform, created within the EU project called Scalable Understanding of Multilingual MediA (SUMMA), we can already deploy multilingual capabilities in the newsroom. The innovation fellowship will contribute to the commercialisation and sustainability of SUMMA translation components, but crucially it will allow us to cover a wider range of topical and strategic languages. Access to a high-quality translation platform for low-resource languages will help the BBC deliver impartial reporting across the world. Collaboration with our industry partner Quorate, will demonstrate the commercial potential of our research in the highly specialised domain of financial trading. 

In the long term, this project will have a wider impact on British industry by breaking down language barriers affecting international trade, and by significantly improving the quality and resilience of transformative AI language technologies.",,"Machine translation (MT) is an important technology enabling communication in an increasingly globalised economy. Both commercial and government organisations are increasingly relying on MT technology in their translation workflows. However, the usefulness of MT is severely constrained by the lack of sufficient training data in most language pairs and domains. This project will deliver machine translation models which are robust enough to be deployed where little or no training data is available, reaching strategic and commercially important languages and domains which have previously been excluded. 

MTStretch has major potential impact in these areas:

1. Media sector: This innovation fellowship will allow the BBC World Service to manage content creation across many languages efficiently. Teams will be able to instantly discover new topics across languages and it will help them to reversion and re-use content from other language departments. This project builds on our strong existing links with the BBC. We work together on the EU H2020 media monitoring project called Scalable Understanding of Multilingual Media (SUMMA). The fellowship will extend SUMMA's capabilities to a wider range of topical and strategic languages around the world.
2. Finance sector: Through collaboration with our industry partner Quorate, we will demonstrate the commercial potential of my research. Currently there are no solutions for reliable machine translation where the domain is highly specialised and there is little or no training data. Through our research we will open up new markets for translation solutions, leading to successful commercialisation of our research. 
3. Innovation: We will explore the potential for a spin-off company based on the unique selling proposition of providing the highest quality coverage of important low-resource languages and domains. The initial target customers will be in the media and finance industries."
12,D01EF67C-39A0-4A84-A670-604144E3424A,ARTICULAR: ARtificial inTelligence for Integrated ICT-enabled pharmaceUticaL mAnufactuRing,"There are considerable challenges around digitalisation in science, engineering and manufacturing in part due to the inherent complexity in the data generated and the challenges in creating useful data sets with the scale required to allow big data approaches to identify patterns, trends and useful knowledge. Whilst other sectors are now realising the power of predictive data analytics; social media platforms, online retailers and advertisers, for example; much of the pharmaceutical manufacturing R&amp;D community struggle with modest, poorly interconnected datasets, which ultimately tend to have short useful lifespans.

A result of poor, under-utilised datasets, is that it is largely impossible to avoid &quot;starting at the beginning&quot; for every new drug that needs to be manufactured, which is very costly with new medicines currently doubling in cost every nine years; $1 billion US Dollars currently &quot;buys&quot; only half a new drug so addressing this issue is key for sustainability of the industry and future medicines supply. This project, ARTICULAR, will seek to develop novel machine learning approaches, a branch of artificial intelligence research, to learn from past and present manufacturing data and create new knowledge that aids in crucial manufacturing decisions. Machine learning approaches have been successfully applied to inform aspects of drug discovery, upstream of pharmaceutical manufacturing, where large genomic and molecule screening datasets provide rich information sources for analysis and training artificial intelligences (AI). They have also shown promise in classifying and predicting outcomes from individual unit operations used in medicines manufacturing, such as crystallisation. For the first time, there is an opportunity to use AI approaches to learn from the data and models from across multiple previous development and manufacturing efforts and then address the most commonly encountered problems when manufacturing new pharmaceutical products, which are knowing: (1) the processes and operations to employ; (2) the sensors and measurements to deploy to optimally deliver the product; and (3) the potential process upsets and their future impact on the quality of the medicine manufactured.

All of these data and the AI &quot;learning&quot; will be made available via bespoke, personalisable AR and VR interfaces incorporating gesture and voice inputs alongside more traditional approaches such as dashboards. These immersive interfaces will facilitate pharmaceutical manufacturing process design, and visualisation of the complex data being captured and analysed in real-time. Detailed, interactive 3D visualisations of drug forms, products, equipment and manufacturing processes and their associated data will be created which provide intuitive access across the length scales of transformations involved from the drug molecule to final drug product. This will be unique tool, allowing the user to see their work and engage with their data in the context of upstream and downstream processes and performance data. Virtual and Augmented Reality technologies will be used in the lab/plant environment to visualise live data streams for process equipment as the next step in digitalisation. These advanced visualisation tools will add data rich, interactive visualisation to aid researchers in their work, allowing them to focus on the meaning of results and freeing them from menial manual data curation steps.",,"The ARTICULAR team engage and collaborate with a wide range of research beneficiaries: leading pharma companies (e.g. AZ, GSK, Bayer, Pfizer, Lilly); companies in the process industries (e.g. Mars, AB Sugar, Syngenta); technology and supply chain companies (e.g. Siemens). ARTICULAR team members are actively involved in the innovation landscape with collaborators including; HVM Catapult (CPI); the National Formulation Centre; medical charities (e.g. CRUK), Medicines Manufacturing Industry Partnership (MMIP) of the UK pharma manufacturing industry; International academic partners (Rutgers, Graz, Singapore) through the newly formed International Institute of Advanced Pharmaceutical Manufacturing. This strong, active and influential network of collaborators, stakeholders and business leaders maximises the potential to achieve the project Aims with associated impact and benefits including:

Economic: The project will contribute to the UK economy by increasing the competitiveness of pharmaceutical and technology partner companies with whom we will co-develop our commercialisation and exploitation plans. Companies will have access to the modular AI-driven ICT tools and applications that emerge from the project. This will increase competitiveness by accelerating process development using minimal amounts of API, improve quality control and minimize waste. The programme will also deliver cost effectiveness via novel adaptive control using machine learning to interpret data. In addition to implementation across CMACs &pound;34m physical hub, establishing a world leading digital lab capability, our connectivity with the HVM (CPI) and Digital Catapults, Diamond and NPL will form critical links with the wider innovation system helping drive industrial translation where opportunities for existing and startup company benefits to be realised are identified 

Societal: A key societal impact is to improve the accessibility and affordability of new and existing drug products and the speed at which they can be brought to market. ARTICULAR will do this by lowering development and production costs through intelligent systems, improving equipment utilisation, reducing energy consumption, waste and reject reduction, improved efficiency of material consumption and reduced environmental impacts. With machine learning, automated, intelligent control has the potential to enable intelligent decision making. These technology developments will have a major international impact, with significant potential for distributed manufacturing: small modular, reconfigurable, manufacturing capability with automated, intelligent control has the potential to enable manufacture close to the point of need. Furthermore, opportunities for anti-malarial or anti-retroviral products being made at low cost in developing economies for local distribution to patients will improve access to safe and effective healthcare in these poorer regions of the world. 

Academic: The world-leading academic partner combination incorporating data analytics, control, visualisation, pharmaceutical sciences and chemical engineering will contribute within and across disciplines producing significant advances in theory and understanding directed towards innovative methodologies, equipment and techniques. A key output will be delivering a highly skilled talent pipeline of trained researchers capable of transferring the generated research and knowledge into UK based multinationals and SMEs to enhance the knowledge economy and revolutionise capability in AI for medicines manufacturing. As a further route to impact, we will provide new opportunities for them to obtain academic and industrial experience via researcher exchanges. CMAC is a recognised academic leader and its unique role was outlined in the Medicines Manufacturing Industry Partnership 'Manufacturing Vision for UK Pharma'. Through our active dissemination and outreach programmes we will maximise the lasting legacy of this research."
13,1FE167C5-667C-484B-BEC1-9421F35ECFCA,Bringing the Social City to the Smart City,"Technological developments, such as the rise in GPS enabled devices and Web 2.0 technologies have created social transformations in how we connect and share information through the mass uptake of smart phones and social media platforms (Croitoru et al, 2014). This new generation of mobile technologies work as individual sensors capturing data on a wide range of human behaviours that have been previously hidden. These include data on individual movement, preferences and opinions. Understanding these behaviours is crucial if we are to create a joined up approach to simulating how cities breath and grow. However, considerable work is required in adapting and developing new technologies from machine learning to extract behaviours which can be embedded into cutting-edge modelling techniques. Creating this bridge between 'big' data representing the 'real' world, and simulations producing alternative versions of reality is of value to both academics and policymakers looking to develop new solutions to many of the challenges that today's cities face. To do this we need to understand how factors within the &quot;Social City&quot; (the impact of individual movements and decisions) play out every day in the &quot;Smart City&quot; (data collected from fixed sensors on for example, traffic counts, air pollution or movements of populations). 

However, standard &quot;Smart&quot; City understanding assumes previous flows (e.g. traffic at a specific time of the week, energy requirements or pollution levels over a 24-hour period) will be replicated into the future, lacking both adaptability (how does this alter if a major event in the city is happening?) and predictive power (what is the impact on health if all petrol and diesel vehicles are banned?). This disconnection between the Smart and Social city means policymakers are unable to obtain answers to complex interrelated questions such as: what is the optimal transport infrastructure to promote healthy behaviours and reduce the City's carbon footprint?

Being able to answer these questions is increasingly important as cities are facing significant challenges associated with the pressures from rapidly increasing urban populations. These include improving water and transportation infrastructure, air pollution and waste management as well as provision of adequate housing, energy, health care, education and employment. These pressures on future cities has brought the Smart City agenda to dominate many government initiatives with governments and policymakers looking to new forms of (big) micro data to provide innovative solutions to these challenges. While many models have been developed to forecast future transport, housing or healthcare initiatives, most uses are purely empirical: they lack any consideration of the social processes behind the individual generating the data or the impact of their actions and decisions. This Fellowship will explore how machine learning inspired tools can be used to recognise such emergent patterns and processes within micro-level data sources such as data on how individuals move and use city spaces. Along with the additional methodology of Agent Based Models, which allows smart and social city data to be readily combined, this suite of methods will be explored through looking at the case-studies of (i) the impact of air pollution on individuals and (ii) urban mobility. This work will fundamentally transform our ability to show how the social elements of the Smart City can be recognised and understood, and how to bring 'lived experience' to the analysis of Smart City data.",,"One of the key elements of the Fellowship is to develop economic and societal impact through engagement with a wide range of stakeholders. The Fellow will be based within the Consumer Data Research Centre (CDRC). The CDRC has established a network of partnerships that the Fellow will thread together over a number of activities. There are a wide range of active partnerships within CDRC including Callcredit, Axciom, Sainsbury's, Zoopla, Whenfresh, Heart Research UK, M&amp;S, Virgin Media, YouGov and Link. Access to data and stakeholders from these and other CDRC partners will be available to the Fellow. Engagement with external partners is facilitated through events including an annual User Forum, which brings together more than 60 commercial partners of CDRC. 

How will they benefit? 
Research project (RP) 1 and PhD 1: Creating tools to uncover hidden emergent trends and processes in spatial data. Can new tools give greater insight to explain why a phenomenon is prevalent in one area, but not another? Can new data tools reveal new insights into current data, and uncover gaps? Beneficiaries from this work include organisations that need to probe deeper into spatial data to understand the interplay of emergent patterns over time. Examples: Local government planning and transportation departments, public health organisations. 
RP 2 and PhD 2: Testing methodologies through ABM: ABM gives the opportunity to bring together Smart and Social city data through simulating individuals. This aspect of the work will create new understanding about the utility of new social media data sets and how they can be readily combined with Smart data. This will create impact for those beneficiaries who are interested in both simulating and understanding individual behaviour and the consequences of individual decisions. Examples: Retail (commercial) and health organisations, local and national government.
RP3: Application to case-studies. This part of the Fellowship has the greatest potential impact as the work can be readily applied to specific case-studies to answer questions about the consequences of new policies or interventions. Examples: Can the transportation infrastructure be optimised to encourage active travel and city spaces to be more readily used? Whilst methodological innovations will have the greatest short-term impact on researchers in academic institutions, the medium to long-term impact of this work will feed through into research orientated public sector organisations, such as those interested in health behaviours, mobility patterns, impact of new infrastructure and new economic initiatives. Examples: Town and Country Planning Association; West Yorkshire Combined Authority.

How will I ensure stakeholders will benefit from the research? 
Stakeholder advisory meetings: Creation of a stakeholder group, the purpose of which is to promote interactions and exchanges. Meetings will be scheduled for the beginning of the Fellowship and each subsequent year to allow time to shape research questions and identify additional case-studies.
Creation of new resources including training courses: Open access code, online training materials will be made available to professionals and academics alike. Workshops will be held to train stakeholders, academics and CDT PhD students in new methods and data, thus impacting on research capacity and skills. 
Seminars: Turing and CDRC both offer seminar programmes at which research findings from this Fellowship will be disseminated. Webinars will also be used to increase the impact and range of engagement with different stakeholders. 
Further funding applications: Engaging with stakeholders and building bridges is of prime importance for future funding opportunities. At month 18, a research incubator event will be held at CDRC. This will involve Turing Fellows as well as invited stakeholders and researchers"
14,B0DD9FA6-609D-465A-9A58-F512E7EBFE55,ExTOL: End to End Translation of British Sign Language,"British Sign Language (BSL) is the natural language of the British Deaf community and is as rich and expressive as any spoken language. However, BSL is not just English words converted into hand motions. It is a language in its own right, with its own grammar, very different from English. Also BSL uses different elements of the body simultaneously. Not just the movement and shape of the hands but the body, face, mouth and the space around the signer are all used to convey meaning. 

Linguistic study of sign languages is quite new compared to spoken languages, having begun only in the 1960s. Linguists are very interested in sign languages because of what they can reveal about the possibilities of human language that don't rely at all on sound. One of the problems is that studying sign languages involves analysing video footage - and because sign languages lack any standard writing or transcription system, this is extremely labour-intensive. This project will develop computer vision tools to assist with video analysis. This will in turn help linguists increase their knowledge of the language with a long term ambition of creating the world's first machine readable dataset of a sign language, a goal that was achieved for large amounts of text of spoken language in the 1970s. 

The ultimate goal of this project is to take the annotated data and understanding from linguistic study and to use this to build a system that is capable of watching a human signing and turning this into written English. This will be a world first and an important landmark for deaf-hearing communication. To achieve this the computer must be able to recognise not only hand motion and shape but the facial expression and body posture of the signer. It must also understanding how these aspects are put together into phrases and how these can be translated into written/spoken language.

Although there have been some recent advances in sign language recognition via data gloves and motion capture systems like Kinect, part of the problem is that most computer scientists in this research area do not have the required in-depth knowledge of sign language. This project is therefore a strategic collaboration between leading experts in British Sign Language linguistics and software engineers who specialise in computer vision and machine learning, with the aim of building the world's first British Sign Language to English Translation system and the first practically functional machine translation system for any sign language.",,"User beneficiaries of this project include those in the following groups:

Deaf people in society: Machine translation from sign language (SL) to written/spoken language will contribute to the status of deaf people in modern society &amp; enhanced hearing-deaf communication, bringing SLs up to par with machine translation between spoken languages. It also meets the requirements of the UN Convention on the Rights of Persons with Disabilities (UNCRPD) which was ratified by the UK in 2009 &amp; by the EU in 2011. The UNCRPD sets a framework for deaf people's rights, mentioning SL seven times in five different articles. Additionally, the research will take us one step closer towards achieving the first fully machine-readable SL corpora - a goal achieved for text corpora of spoken languages in the 1970s. This is important for deaf communities as validation of their linguistic/cultural heritage &amp; enabling wider access to archives.

Education: SL teachers &amp; their students will benefit from machine translation technology as it will provide new, faster ways of translating, annotating &amp; manipulating videos that include SL. Also, it paves the way for automated analyses in the assessment of second language acquisition of SLs and/or non-verbal behaviour in spoken language.

Deaf Researchers: We will aim to attract deaf applicants to the research posts. Deaf people often do not see HE employment as a viable option due to communication challenges. This project will enable us to train &amp; mentor more young deaf researchers, contributing to co-creation &amp; capacity building. The project will lead to increased participation of deaf people which will be ensured in three ways: priority-setting in collaboration with the deaf community, capacity building through the training &amp; employment of deaf researchers, &amp; ensuring native SL skills of deaf researchers are used.

Researchers in linguistics &amp; ICT: This project will be of benefit to linguists working on analysing visual language videos by providing tools to assist in a) faster annotation, given that slow annotation has precluded progress in SL corpus research, and b) richer annotation of visual language data than is currently feasible, especially concerning facial expression. This will benefit computer scientists working on recognition/synthesis of SL, gesture, multi-modal interaction, non-verbal communication, human-machine interaction, &amp; affective computing. Additionally, low-level phonetic transcription of manual &amp; non-manual features in face-to-face communication will contribute to better movement models needed for natural-looking synthesis. 

Researchers in arts, social science &amp; medicine: The project will benefit a wide group of researchers by providing tools for the analysis of video data of human interaction: those studying multi-modal communication including linguistics, psychology, sociology, economics, &amp; education; those concerned with gesture studies &amp; features of audio-visual interaction; researchers of online video &amp; social media; those studying developmental &amp; acquired language &amp; communication impairments in spoken &amp; signed languages, including studies of therapeutic discourse; anthropologists &amp; ethnologists. More generally, the technology could also be used for studies of human movement beyond language &amp; communication.

Commercial private sector: The tools from this project will be of interest to businesses in the area of computer vision as they will provide new marketable techniques &amp; therefore new opportunities for revenue. Automated subtitling from SLs to meet accessibility requirements for broadcast video, video on social media etc are obvious areas but as highlighted above, the application areas go far beyond SL.

In summary, the strategic interdisciplinary partnership in this project between experts in linguistics &amp; computer vision also has direct reciprocal benefits not only to those communities but also to social science, ICT and other fields more generally."
15,D273566E-F113-4FD1-A117-9DA6A5E91095,EPSRC NetworkPlus on Social Justice through the Digital Economy,"Technological advances in Artificial Intelligence and Big Data, have already given rise to extensive socio-economic transformation and new and emerging technologies, such as distributed ledgers and the Internet of Things, are set to further revolutionise the information and service economy, and public services. Yet, technological innovation has the potential to also dis-benefit the most vulnerable, amplify existing forms of injustice and create new forms of exclusion in socio-economic life, thus further exacerbate socio-economic inequality and social division. 

That the whole of society benefits from progress in the Digital Economy is national priority, both morally and economically as those who are most vulnerable have the greatest need of opportunities for socio-economic participation. Taking a Social Justice approach, this NetworkPlus focuses on how the design of new and emerging technologies in the Digital Economy, and their application, can empower, emancipate and more equitably distribute opportunities for economic development to all citizens, consumers and employees. This EPSRC NetworkPlus: Social Justice through the Digital Economy aims to bring together and resource partners from academia, industry, government and civil society to understand, explore and respond, together, to the potential of new and emerging technologies to make the UK socio-economic life fairer for all.

The NetworksPlus activities will focus on three challenge areas: Algorithmic Social Justice; Digital Security for All; Fairer Futures for Businesses and Workforces.
 
Algorithmic Social Justice examines fairness in the design and application of AI algorithms in automated and semi-automated decision-making processes. It asks how can large data sets be classified and interpreted to inform, for example, care or health interventions programs or city planning and how can AI algorithms be made less opaque and criteria used to design them fairer and transparent. 

Digital Security for All investigates new and better ways to model digital security that increase people's sense of agency, while meeting their security needs and protection of assets in public and commercial online service delivery. For example, this challenge area asks in what ways can online services be designed to better support people's sense of agency and trust, while assuring security in sharing personal data online. 

Fairer Futures for Businesses and Workforces considers how new 'sharing economy' platforms can be designed to realise more ethical business models and equal opportunities for economic development. For example, this theme asks what platforms can be designed to support peer-to-peer markets places that cater for those who have little or no assets; and what are the implications for a fair workforce representation in the digital era. 

The NetworkPlus will enable new ways to support effective collaborations between academic and non-academic communities and organisations through a range of activities, including a curated series on events in the three thematic priorities and an innovative and more directed process of project commissioning. The NetworkPlus will deliver curated events and activities-including symposia, hands-on workshops, theory-hacks and design and development sprints, aiming to increase capacity, upskilling and foster trans-disciplinary dialogue, knowledge exchange between academic and non-academic communities as well as. The NetworkPlus will deliver a novel curated commissioning process of activities designed to support EPS doctoral researchers and Early Career Researchers developing impactful project proposals in partnership with industry, government, third sector and civil society.",,"The beneficiaries of our NetworkPlus: Social Justice through the Digital Economy NetworkPlus are social justice clients (e.g. Charities, National Government bodies and Local Authorities), technology and digital service providers (e.g. SMEs, National and Global Corporations) as well as (formally) un-constituted groups and organizations currently at the margins or altogether excluded from participation in technological innovation and opportunities for socio-economic growth and development (youth, adults with complex needs, aging population, low paid workers, etc.). The NetworkPlus brings value to its beneficiaries through its activities and outputs as follows. 

(i) Public, private and third sector organisations and their service users will be able to explore the real-world problems they currently face within each identified challenge area with other experts and organisations who have different expertise but similar concerns through NetworkPlus activities (Symposia, and Open Event Program's Workshops, Theory Hacks and Design and Development Sprints); 

(ii) Public, private and third sector organisations and their service users will benefit from participating in pilot /micro projects that directly responds to the specific challenges they face within each identified challenge area: e.g. qualitative studies to better understand issues of fairness in algorithm design and its applications; frameworks for the design and application of AI algorithms within a social justice framework; rapid prototyping and testing of novel and emerging technologies and/or the reconfiguration of existing technologies that applies social justice principles in automated decision-making or that create spaces for agency and trust in online services; new insights to inform policy recommendations for fairer business models and workforce representation in the digital economy.

(iii) NetworkPlus partners not directly involved in a micro/pilot project will benefit from NetworkPlus projects outputs (all with open IP), through our dissemination activities and through NetworkPlus parterns who will advocate and disseminate outputs to their own external networks. 

(iv) Through our commissioned policy papers from all projects outputs, the NetworkPlus will benefit and contribute to the ongoing work of advocacy organizations, think tank and regulatory bodies working in each identified challenge area. 

(v) Youth groups, primary and secondary school pupils will benefit from the NetworkPlus Youth Engagement Program. The pupils directly involved in our Youth Engagement pilot activities will benefit from spaces to both learn about the issues the NetworkPlus tackles and participate and influence digital innovations arising from NetworkPlus projects; the learning material that will be produced from the Youth Engagement Programme, will be then be piloted by 20 more schools (delivered by schools) with a view to a larger roll out. 

(vi) Through our Project Assessment Citizen Panel and through our Community Champion engagement work, marginalized and underserved citizens will have a voice in shaping digital innovations that can ameliorate their lives."
16,C5BFB1A7-3413-4387-B565-1CD6A90EE4C6,Embedding Machine Learning within Quantifier Elimination Procedures,"This project concerns computational mathematics and logic. The aim is to improve the ability of computers to perform ``Quantifier Elimination'' (QE). 

We say a logical statement is ``quantified'' if it is preceded by a qualification such as &quot;for all&quot; or &quot;there exists&quot;. Here is an example of a quantified statement: 
&quot;there exists x such that ax^2 + bx + c = 0 has two solutions for x&quot;.
While the statement is mathematically precise the implications are unclear - what restrictions does this statement of existence force upon us? QE corresponds to replacing a quantified statement by an unquantified one which is equivalent. In this case we may replace the statement by:
&quot;b^2 - 4ac &gt; 0&quot;, which is the condition for x to have two solutions.
You may have recognised this equivalence from GCSE mathematics, when studying the quadratic equation. The important point here is that the latter statement can actually be derived automatically by a computer from the former, using a QE procedure.

QE is not subject to the numerical rounding errors of most computations. Solutions are not in the form of a numerical answer but an algebraic description which offers insight into the structure of the problem at hand. In the example above, QE shows us not what the solutions to a particular quadratic equation are, but how in general the number of solutions depends on the coefficients a, b, and c.

QE has numerous applications throughout engineering and the sciences. An example from biology is the determination of medically important values of parameters in a biological network; while another from economics is identifying which hypotheses in economic theories are compatible, and for what values of the variables. In both cases, QE can theoretically help, but in practice the size of the statements means state-of-the-art procedures run out of computer time/memory. 

The extensive development of QE procedures means they have many options and choices about how they are run. These decisions can greatly affect how long QE takes, rendering an intractable problem easy and vice versa. Making the right choice is a critical, but understudied problem and is the focus of this project. At the moment QE procedures make such choices either under direct supervision of a human or based on crude human-made heuristics (rules of thumb based on intuition / experience but with limited scientific basis). The purpose of this project is to replace these by machine learning techniques. Machine Learning (ML) is an overarching term for tools that allow computers to make decisions that are not explicitly programmed, usually involving the statistical analysis of large quantities of data. ML is quite at odds with the field of Symbolic Computation which studies QE, as the latter prizes exact correctness and so shuns the use of probabilistic tools making its application here very novel. We are able to combine these different worlds because the choices which we will use ML to make will all produce a correct and exact answer (but with different computational costs). 

The project follows pilot studies undertaken by the PI which experimented with one ML technique and found it improved upon existing heuristics for two particular decisions in a QE algorithm. We will build on this by working with the spectrum of leading ML tools to identify the optimal techniques for application in Symbolic Computation. We will demonstrate their use for both low level algorithm decisions and choices between different theories and implementations. Although focused on QE, we will also demonstrate ML as being a new route to optimisation in Computer Algebra more broadly and work encompasses Project Partners and events to maximise this. Finally, the project will deliver an improved QE procedure that makes use of ML automatically, without user input. This will be produced in the commercial Computer Algebra software Maple in collaboration with industrial Project Partner Maplesoft.",,"Quantifier Elimination (QE) allows for the removal of quantifiers within logical statements to produce clear unambiguous algebraic constraints. In this sense QE is the act of simplifying or solving a problem exactly, like pen and paper symbolic mathematics. The impact of improved QE procedures is felt not just in mathematics and computer science, but throughout engineering and the natural sciences - wherever there is a desire for such solutions. Some examples are given below. 

Initial impact from the project will be secured via collaboration with industrial Project Partner Maplesoft, whose European HQ is in Cambridge. Maplesoft produce the Maple Computer Algebra System used widely in science, education and industry. The PI will implement work in the Maple language and work with the Maple development team on the transfer of these project outcomes. This will guarantee a wide impact of the new procedures, not just by including them in a widely used piece of software, but by having them linked up as sub-routines to the main user commands. Maplesoft also produce the modelling and simulation tool MapleSim which uses Maple as a backend, thus offering a further route to impact. All outcomes (publications and data) will be published open access to facilitate additional transfers.

More broadly, the project aims to impact the thinking around software development in Computer Algebra Systems, by proving the effectiveness of machine learning as a route to optimisation. This impact will be insured not only via engaging with Maplesoft but also the wider field. We will hold two project events to take place at existing Symbolic Computation conferences, to maximise engagement and participation. We also plan further engagement with academic Project Partners who lead development teams on Computer Algebra Software packages: Redlog at CNRS, Inria, CoCoA at U. Genoa, and SMT-RAT at U. Aachen.

There will also be some local impacts of the project. It will directly support the UK career of both an RA and the PI, the latter gaining the particularly valuable first RCUK grant and also experience of line management. Further it will indirectly support a PhD student as Coventry U. have committed to fund a studentship associated with the project. Finally, it will feed into curriculum development: providing suitable examples, guest lectures and topics for final year projects on the Computer Science BSc at Coventry U.

We finish by discussing three potential real world beneficiaries of QE the PI is involved with. In each of these improved QE procedures would allow for much greater impact. 

- Biology/Epidemiology community: All sorts of biological processes may be modelled using systems of differential equations, such as epidemic spread of disease or protein reactions indicative of cancer. The identification of conditions on the parameters for stable solutions of these equations may be achieved using QE. The PI has recently collaborated with biologists on identifying bi-stability regions of the MAPK network.

- Formal Verification and Safety community: The use of computers to check a logical statement beyond all doubt (by successively combining existing results with basic axioms) is often used to ensure the correctness of safety-critical systems. For example, KeYmaera combines a theorem prover with real QE procedures for verification in the railway, automotive, and aviation industries, and even surgical robotics. The PI is a member of a separate EU project engaging with such applications.

- Validation of Economic Hypotheses: Economists make models under a variety of assumptions, many of which can be expressed in the language of QE. The compatibility of given assumptions is often not clear, but QE could validate the compatibility of assumptions, or identify regions of compatibility. The PI is in correspondence with a leading economist at U. Chicago who is the first to make use of QE in this context."
17,7E29FB40-C336-4E85-A801-7B08FF6A915D,Artificial and Augmented Intelligence for Automated Scientific Discovery,"AI is a widely used term that conjurers up many of the computers from science fiction. Its stands for a whole collection of ideas, algorithms, computational models and knowledge systems. Recent success of particular types of machine learning (e.g. deep neutral nets) have again excited the interest of the scientific community in delivering insight into the complexity of the real world. This type of approach compliments the knowledge engineering systems that have previously been used, however they require massive amounts of data to be trained. Taking the chemical and materials sciences as exemplar areas we can see that the traditional approaches to scientific discovery work with relatively small amounts of often uncertain data which is distilled by human insight to yield predictions and testable theories which may evolve as new data becomes available. In these areas of science more data is becoming available and the impact of 'larger data' parallels the reality that almost all science now depends on computational assistance. Never-the-less the quantity of quality data needed to train the new AI systems is simply not directly available even with recent advances in automation. As a basis for the network we propose to use 'amplification by simulation' as a key element of the cycle of automated experiments, simulation, AI learning, prediction, comparison, design, further experiments, to create the environment in which leading AI developments can be applied to the chemical and materials discovery.",,"The network partners for the proposed Network+ provide an extensive reach to the academic and industrial communities in the UK interested in research in computing and the physical and life science providing access to the relevant people to create impact throughout the whole community. The cutting edge areas of chemical and material science to be support with cutting edge AI, mean that successful research projects will be of immediate importance to industry giving rise directly to new pharmaceuticals, antimicrobials, green production techniques, functional materials and more importantly they will yield new techniques to generate future critically needed chemicals and materials.

The membership of the network will mean that relevant researchers and industrialists will get to hear about the work though the network and be able to support applications of the work. Through the network connections we will have access to all the major players in the chemical and materials areas and the nature of the proposed network is such that we will also be able to bring on board the SMEs, providing opportunities for them to interact with academic groups and other commercial entities. The network will also engage with the media and governmental (regulatory) agencies as the potential role of AI in driving UK productivity and innovation is a current and significant public agenda.

Open Science, quality software, supported by the network will lead we believe to greater open innovation in this space. Combining the best of open source software with open source hardware and wetware. We see this as a very effective underpinning of wider public engagement, for example in supporting presentations and discussions at Schools and public events, to be able to demonstrate to the public what AI approaches offer to science and through these activities to the public."
18,D4FE6B0B-67DA-40CC-9ED7-19C1341412DF,Autonomous Robot Evolution: Cradle To Grave,"Robotics is changing the landscape of innovation. But traditional design approaches are not suited to novel or unknown habitats and contexts, for instance: robot colonies for ore mining, exploring or developing other planets or asteroids, or robot swarms for monitoring extreme environments on Earth. New design methodologies are needed that support optimising robot behaviour under different conditions for different purposes. It is accepted that behaviour is determined by a combination of the body (morphology, hardware) and the mind (controller, software). Embodied AI and morphological computing have made major progress in engineering artificial agents (i.e., robots) by focusing on the links between morphology and intelligence of natural agents (i.e., animals). While such a holistic body-mind approach has been hailed for its merits, we still lack an actual pathway to achieve this.
While this goal is ambitious, it is achievable by introducing a unique methodology: a hybridisation of the physical evolutionary system with a virtual one. On the one hand, it is appreciated that an effective design methodology requires the use and testing of physical robots. This is because simulations are prone to hidden biases, errors and simplifications in the underlying models. Simulating populations of robots (rather than just simulating specific parts) leads to accumulated errors and a lack of physical plausibility: the evolved designs will not work in the real system. This is the notorious reality gap of evolutionary robotics. On the other hand, evolving everything in hardware is time and resource consuming. One of our major innovations is to run simulated evolution concurrently with the physical and hybridise them by cross-breeding, where a physical and a virtual robot can parent a child that may be born in the real world, in the virtual world or in both. The advantages of such a hybrid system are significant. Physical evolution is accelerated by the virtual component that can run faster to find good robot features with less time and resources; simulated evolution benefits from the influx of genes that are tested favourably in the real world. Furthermore, monitoring of and feedback from the physical system can improve the simulator, reducing the reality gap.",,
19,6B77025F-1D31-450A-AAE3-A0D236CCAC35,Start Making Sense: Cognitive and affective confidence measures for explanation generation using epistemic planning,"Consider a tourist on a walking tour of a city. After reaching a place where they can see they are almost back to the starting point, their tour guide says &quot;Let's go up that hill&quot;, pointing to a large hill. &quot;We can get a good view of the city from there.&quot; However, on seeing the tired expression on the tourist's face, the guide adds &quot;Or we can stop in that pub over there and take a break.&quot; This scenario has two important features: (1) people like to know what is going on. This is especially true in situations where a decision may not have been anticipated or expected. Here, an explanation may be needed not just to justify the decision but also to establish confidence in that choice-in other words, to trust it; (2) being able to read the situation and adapt to the needs of the moment is important when ranking the possible actions that could be taken. Here, a decision may need to be made dynamically. These two features add up to 'dynamic trust maintenance', and are needed for a broad range of the AI systems that are expected to be deployed in the near future, e.g., automated vehicles, service robots, or interactive voice-based assistants.

This project addresses the need for dynamic trust maintenance (which is not generally available in interactive and autonomous systems) by bringing together experimental research in cognitive science involving cooperative joint action with the practical construction of AI planning tools to apply to the task of explanation generation. This challenge will be addressed through these concrete objectives: (1) to study cooperative joint action in humans to identify the emotional, affective, or cognitive factors that are essential for successful human communicative goals, (2) to enhance epistemic planning techniques with heuristics derived from the cognitive science studies, and (3) to deploy the resulting system to generate human-like explanations and evaluate the effectiveness of the resulting approach with human participants.",,"Knowledge. The main scientific advances will be new ways to help people interact with planner-based artificial intelligences, and new ways to measure people's trust in such systems. Contributions will include: identifying emotional, affective, and cognitive factors relating to levels of trust in explanations; epistemic planning techniques enriched with heuristics derived from the cognitive science studies; and an empirical evaluation of an implemented system which generates human-like explanations. These have benefits for fields of enquiry including: AI, natural language processing, cognitive science, affective computing, and human computer interaction. The plan is to publish 9 papers in journals and conferences, and to run an expert workshop to help bridge communities.

Society. Social beneficiaries of the project include policy makers who act as intermediaries between technical experts and the general publics, on topics such as the &quot;Right to Explanation&quot;. However, the most important &quot;society&quot; impact will ultimately be on public end users empowered by the development of explainable AI. Through our tour guide domain of application, we aim to help people to understand better the strengths and weaknesses of current and future AI, and to benefit from the next generation of intelligent advisors. The plan is to run a co-design workshop, and a public engagement event.

People &amp; Capabilities. This project will contribute to EPSRC's planned network around explainable AI and human-like computing in the UK. We will seek to share our work with researchers at the interface between AI and cognitive science, wherever they are located, helping build a critical mass of researchers engaged with R&amp;D in this area. The project team will be mentored in sharpening their academic profiles, supported in commercialisation, and receive transferrable skills training. To deliver education and inspire the next generation, the team will work with excellent students, especially from the Masters in Design Informatics. The plan is to supervise at least 6 student projects.

Economy We will gain impact via the release of data, software, APIs, and documentation in the public domain under an appropriate license. We will seek to initiate work with an SME project partner with a novel biometric sensor system. Via Informatics Ventures (IV), there is effective entrepreneurship training, coaching for spinoffs, and business development support. The plan is to expose a Design Informatics student cohort (~30 students) to the project ideas, and use IV to support at least 1 entrepreneurial student."
20,B4569675-5D9E-40A8-A2CE-CB4A2A838CB6,DADD: Discovering and Attesting Digital Discrimination,"In digital discrimination, users are treated unfairly, unethically or just differently based on their personal data. Examples include low-income neighborhoods targeted with high-interest loans; women being undervalued by 21% in online marketing; and online ads suggestive of arrest records appearing more often with searches of black-sounding names than white-sounding names. Digital discrimination very often reproduces existing instances of discrimination in the offline world by either inheriting the biases of prior decision makers, or simply reflecting widespread prejudices in society. Digital discrimination may also have an even more perverse result, it may exacerbate existing inequalities by causing less favourable treatment for historically disadvantaged groups, suggesting they actually deserve that treatment. As more and more tasks are delegated to computers, mobile devices, and autonomous systems, digital discrimination is becoming a huge problem. 

Digital discrimination can be the result of algorithmic biases, i.e., the way in which a particular algorithm has been designed creates discriminatory outcomes, but it also occurs using non-biased algorithms when they are fed or trained with biased data. Research has been conducted on so-called fair algorithms, tackling biased input data, demonstrating learned biases, and measuring relative influence of data attributes, which can quantify and limit the extent of bias introduced by an algorithm or dataset. But, how much bias is too much? That is, what is legal, ethical and/or socially-acceptable? And even more importantly, how do we translate those legal, ethical, or social expectations into automated methods that attest digital discrimination in datasets and algorithms? 

DADD (Discovering and Attesting Digital Discrimination) is a *novel cross-disciplinary collaboration* to address these open research questions following a continuously-running co-creation process with academic (Computer Science, Digital Humanities, Law and Ethics) and non-academic partners (Google, AI Club), and the general public, including technical and non-technical users. DADD will design ground-breaking methods to certify whether or not datasets and algorithms discriminate by automatically verifying computational non-discrimination norms, which will in turn be formalised based on socio-economic, cultural, legal, and ethical dimensions, creating the new *transdisciplinary field of digital discrimination certification*.",,"DADD will run a programme of activities with a view to maximising: 

*Long-term cultural change* will be pursued by: i) actively engaging with the four research themes in King's Research Strategy and Vision 2029 to foster and encourage cross-disciplinary research in the College, and in particular, DADD is part of and will work closely with the Social Justice theme (one of the four themes); ii) building project team's skills (PDRAs and investigators), leveraging the co-creation and the way of working together in the project, and KCL's Centre for Research Staff Development and KCL's Engagement Services Team; and iii) training the next generation of digital discrimination researchers and professionals (PDRAs, PhD and MSc students), leveraging the PhD studentship committed by KCL, the London Interdisciplinary Social Science Doctoral Training Partnership, and KCL MSc programmes like the MSc in Data Science. 

*Influencing other ICT researchers to foster adoption of cross-disciplinarity and co-creation* will be delivered by organising workshops at academic conferences, at King's and in conjunction with the Science and Engineering South (SES), and at other venues in collaboration with other projects funded in this call, together with publications (magazines like CACM) on cross-disciplinary and co-creation methodologies, their benefits, and know-how gained throughout the project.

*Sustainable economic and public sector impact* will be delivered by working directly with companies and the public sector (e.g. e-government) using a two-staged approach. In the first stage, a business and public-sector-focused workshop will run to engage with potentially interested businesses and professionals, seeking engagement with the Digital Catapult. We have already secured support and collaboration from Google and AI Club who will also be invited to these events. In the second stage, activities with already secured partners and other selected partners from stage one will run, e.g., Google confirmed that they will collaborate with DADD (advisory board and project activities). DADD will utilise internal (IAA) and external (KTP, Google schemes) grant mechanisms for follow-up collaborations beyond its life, and it will engage with KCL's IP &amp; Licensing Team to explore commercialization options for the Toolkit. 

*Influencing policy and regulation on digital discrimination* will leverage the extensive expertise at KCL through its Policy Institute to reach out to and engage with policy makers, and campaign for and raise awareness about digital discrimination, to update them with project findings, and explore possible areas of impact in current and prospective regulations. In particular, relevant regulation bodies like the Information Commissioner's Office (ICO) and Prudential Regulation Authority (PRA) will be contacted and invited to engagement workshops. Also, the PI will leverage his policy contacts as member of the Policy Fellows Network at Centre for Science and Policy, University of Cambridge (a network of academics and policy professionals). 

*Raising public awareness of digital discrimination* is embedded in the participatory research methods used in DADD in which the general public will take part (e.g. techno-cultural workshops). Public Awareness will also be delivered by means of a highly active Social Media and Press Releases strategy."
21,31222879-8DBE-4B61-9EA9-61CB129E2293,Improving the product development process through integrated revision control and twinning of digital-physical models during prototyping,"An orchestration of physical and digital models of varying fidelities, and in differing sequences, is required for the product development process. The choice of these models depends upon the: skills of the design team; resources and tools available; purpose of the model; and nature of the design task. In all engineering disciplines a combination of digital and physical models is necessary to support the progression of the design process, with each model and iteration thereof generating new understanding and knowledge to inform decision-making. While extensive modelling - both physical and digital - is imperative to develop right-first-time products, the parallel use of digital and physical models gives rise to two interrelated issues. These are: the lack of revision control for physical prototypes; and the need for designers to manually inspect, measure, and interpret modifications to either digital or physical models, for subsequent update of the other. This manual process of revision control for physical models and what is referred to herein as 'twinning of digital-physical models' impacts on the cost, quality and time of the design and development process. In particular, the lack of revision control leads to multiple near-identical model instances, which contribute to issues of process management, traceability, decision-making, design duplication and inefficiency, and design rationale capture. It also makes optimisation of the product development process in terms of the digital-physical tool-chain all but impossible. In this project we will fundamentally redefine the revision control and twinning processes for digital and physical models from a manual, cumbersome, error-prone and expensive procedure to one that is seamlessly integrated (digital-to-physical and physical-to-digital), rapid, reliable and knowledge rich.",,"UK Manufacturing output totals over &pound;150bn and is ranked 6th in the world , the UK Construction industry is worth over &pound;300bn , and the creative industries employ almost 6% of the UK population and generate over &pound;122bn . This position is being maintained in the face of immense competition from developing nations. Central to maintaining this position is the ability to maximise the efficiency and effectiveness of manufacturing processes (productivity), and in particular, the design process. To date, while it is widely believed that digital tools and rapid prototyping can bring about a process transformation, it has not yet been fully realised. This research programme will address this deficiency and bring about a game changing capability to underpin a step-change in design process capability and productivity. In addition, the multi-disciplinary nature of the project will have a wider impact on technology and processes. The medium and longer-term impacts include: 
1. Techniques and approaches to support revision control of mixed reality assets/artefacts. 
2. An order of magnitude reduction in new product development cycle time.
3. Design methods &amp; processes to increase innovation &amp; new product development levels.
4. New paradigms of design development e.g. rapid interdisciplinary teams convened for short periods - similar to hackathons. 
5. Democratisation of design with communities and groups executing the process largely independently through modification of physical prototypes rather than CAD systems which require specialist skills. 
6. New approaches to enhance productivity of traditional manufacturing processes / technologies e.g. late customisation. This includes process planning for hybrid additive and subtractive technologies.
7. Methods and standards for twinning in fields such as asset management and building information modelling. 
8. New classes of tangible interfaces for almost seamless digital-physical integration and personalisation of user-interfaces."
22,58EB0401-BFB4-4345-935D-BE2A9881E648,Transforming networks - building an intelligent optical infrastructure (TRANSNET),"Optical networks underpin the global digital communications infrastructure, and their development has simultaneously stimulated the growth in demand for data, and responded to this demand by unlocking the capacity of fibre-optic channels. The work within the UNLOC programme grant proved successful in understanding the fundamental limits in point-to-point nonlinear fibre channel capacity. However, the next-generation digital infrastructure needs more than raw capacity - it requires channel and flexible resource and capacity provision in combination with low latency, simplified and modular network architectures with maximum data throughput, and network resilience combined with overall network security. How to build such an intelligent and flexible network is a major problem of global importance. 

To cope with increasingly dynamic variations of delay-sensitive demands within the network and to enable the Internet of Skills, current optical networks overprovision capacity, resulting in both over- engineering and unutilised capacity. A key challenge is, therefore, to understand how to intelligently utilise the finite optical network resources to dynamically maximise performance, while also increasing robustness to future unknown requirements. The aim of TRANSNET is to address this challenge by creating an adaptive intelligent optical network that is able to dynamically provide capacity where and when it is needed - the backbone of the next-generation digital infrastructure.

Our vision and ambition is to introduce intelligence into all levels of optical communication, cloud and data centre infrastructure and to develop optical transceivers that are optimally able to dynamically respond to varying application requirements of capacity, reach and delay. We envisage that machine learning (ML) will become ubiquitous in future optical networks, at all levels of design and operation, from digital coding, equalisation and impairment mitigation, through to monitoring, fault prediction and identification, and signal restoration, traffic pattern prediction and resource planning. TRANSNET will focus on the application of machine techniques to develop a new family of optical transceiver technologies, tailored to the needs of a new generation of self-x (x = configuring, monitoring, planning, learning, repairing and optimising) network architectures, capable of taking account of physical channel properties and high-level applications while optimising the use of resources. We will apply ML techniques to bring together the physical layer and the network; the nonlinearity of the fibres brings about a particularly complex challenge in the network context as it creates an interdependence between the signal quality of all transmitted wavelength channels. When optimising over tens of possible modulation formats, for hundreds of independent channels, over thousands of kilometres, a brute force optimisation becomes unfeasible. Particular challenges are the heterogeneity of large scale networks and the computational complexity of optimising network topology and resource allocation, as well as dynamical and data-driven management, monitoring and control of future networks, which requires a new way of thinking and tailored methodology.

We propose to reduce the complexity of network design to allow self-learned network intelligence and adaptation through a combination of machine learning and probabilistic techniques. This will lead to the creation of computationally efficient approaches to deal with the complexity of the emerging nonlinear systems with memory and noise, for networks that operate dynamically on different time- and length-scales. This is a fundamentally new approach to optical network design and optimisation, requiring a cross-disciplinary approach to advance machine learning and heuristic algorithm design based on the understanding of nonlinear physics, signal processing and optical networking.",,"TRANSNET is focused on one of our society's greatest technical challenges and economic drivers with impact on public, business and government activities. The question of how to ensure the availability of ubiquitous, high-capacity, low-delay, resilient and secure digital infrastructure, likely to transform and improve people's lives, forms the subject of our proposal. Every sector of the population and Government/private agencies is likely to be affected (well reflected in the participating team of project partners). TRANSNET's programme, though the approach of cross-disciplinarity and co-creation, will allow new digital application markets to become a reality with new approaches to build adaptive intelligent optical network that is able to dynamically provide capacity, where and when it is needed. TRANSNET will have numerous means of creating impact, from the people involved, the knowledge created, to the impact on the economy and broader society. 
A. People: TRANSNET will impact the UK and international science and engineering ICT communities by training a new generation of researchers that are able to operate at the interface between optical fibre communication and machine learning, hence creating a pipeline of highly skilled people.
B. (i) Knowledge: TRANSNET will impact through knowledge creation with a focus on dissemination, advocacy and the public understanding of science. Dissemination - in addition to high-quality publication in leading international peer-review journals and conferences, a substantial effort will be dedicated to organise technical workshops including with industrial partners within and outside the programme. Major impact of the work will be through technology demonstrations over the National Dark Fibre Infrastructure (NDFIS) to give carriers the confidence to move towards an intelligent optical network infrastructure. (ii) Advocacy: The Investigators are all known as effective and strong advocates of optical communications and will effectively use their substantial and diverse set of links to industry, opinion formers and the general public to enhance the impact of the project. (iii) Public understanding of science: TRANSNET will seek to use the creation of an intelligent optical network as a means of improving the public understanding of science. This will include public lectures (e.g. at the Royal Society), but also and perhaps more critically through outreach, through science festivals including the Big Bang Fair (targeting UK Young Scientists and Engineers) and the Cambridge Science Festival (targeting the general public).
C. Economy: TRANSNET will impact the economy through improvement in productivity, wealth creation and inward investment as industry seeks to utilise the outcomes of TRANSNET. Specifically, there are a number of beneficiaries beyond the project partners including: (i) Telecommunications/data service providers: TRANSNET will impact both traditional carriers and cloud services/internet service providers in planning, operating and evolving their networks. TRANSNET will have a major impact on reducing the management and operating costs of networks by providing a mechanism for automated service provisioning across all the layers of the network. (ii) Equipment and optical fibre manufacturers and vendors are facing the overall challenge of providing the necessary capacity to satisfy the growing data demands in the most cost-effective way and flexible way. The results of the research will inform industry's R&amp;D directions. 
D. Society: The digital infrastructure facilitated through TRANSNET, offers improved quality of life - from facilitating family friendly flexible working through to remote health monitoring of elderly. TRANSNET will inform and contribute to standards bodies and Government policy on infrastructure capability, security and broadband delivery - enabling informed policy formulation as networks evolve from being manually provisioned to autonomous entities."
23,CCBC7787-6D91-4987-AAE9-E9D3217DF7DA,People Powered Algorithms for Desirable Social Outcomes,"Algorithms increasingly govern interactions between state and citizen and as the 'digital by default' model of government-citizen interaction spreads this will increase. This increase, combined with the value of data science and how AI and machine learning is embraced as a way to achieve efficiency and carry out public policy we need to consider how algorithms mediate real-world relationships between the state and individuals.

Without confidence in the legitimacy and credibility of the algorithms the trust between government and citizens will dramatically degrade. Our research will therefore focus on algorithmic interactions between the citizen and the state and examine how we form productive and trusted relationships between those designing, deploying and using the algorithmic interactions and the communities affected by the decisions.

We will examine three key public policy areas where algorithmic decision-making is used for aspects of policy deployment: refugee resettlement, welfare and healthcare provision. These three areas have been selected as they are at the forefront of services that developed as part of digital by default, where issues of cost are addressed, in part, by algorithmic decision making to evaluate legitimate service access and use. Additionally, these are areas of significant public spending where the intended users of these services are more likely feel excluded and disenfranchised from mainstream society. Our research will examine how the re-designing of the system interactions and the communication of the political and economic logic will enhance the security and well-being of individuals, protects the security of the state and increases the confidence in digital service design.",,"Our proposed research will synthesise the outputs of market design research with information security research and, by visualising and abstracting the outputs, facilitate new dialogues across a range of stakeholders from individuals through to policy makers. The resulting dialogues will:
1. Promote discussions within communities about fairness and legitimacy in data-driven decision making;
2. Facilitate inter-community conversations between system designers, policy makers and communities that discuss the impact of informal information flows and the conceptual models on which systems are built;
3. Diversify the security dialogue so that a more nuanced discussion can be had about the connection between personal security, state security and technical security.
4. Allow us to engage with market design researchers to explore how an understanding of informal information flows might impact on their research.

We will drive impact in four beneficiary groups: User communities; Government Policy makers; Data scientists, system designers and security architects and finally academia.

User communities: This societal impact will ensure that our research not only involves the user communities but also genuinely supports the increased well being of these communities. This will be achieved through the improved social outcomes from better algorithmic policy making and through the increased agency and awareness from discussing these important topics. We will work across a range of policy domains to reach particular communities. For example our work with algorithmically implemented refugee policy will reach Refugees, Hosting communities, Communities in zones of first asylum and Communities in zones of transit. While in the UK our research in healthcare and welfare will consider a number of community groups and local government in Sunderland (see letter of support), communities with which members of the team have already engaged.

Government policy makers: It is intended that this work will assist in supporting and developing algorithm design at a national level as well as a local government level. Note that all members of the team have been active in supporting policy development at government (see also letter of support from GCHQ). In the specific policy domains our research will benefit multiples stakeholders. In the domain of refugee policy the following government beneficiaries will be reached: State ministries (e.g. UKHO, German BAMF, etc) Intergovernmental agencies involved in coordinating resettlement (e.g. EASO, IOM, UNHCR resettlement office), Local government involved in meeting resettlement needs (councils, municipalities, quangos such as JOMAST), Humanitarian workers and CSOs and Migration management ministries and border. In the domain of welfare policy, the following government beneficiaries will be reached: Department of Work and Pensions both centrally and regionally.

Data scientists, systems designers and security architects: There is a significant increase in the use of data-led decision-making both in government and industry. Driving awareness and of the issues raised in this research in these practitioners through their education and skills pipeline will ensure that our research will have a long-lasting impact on their professional practice, not only in their current posts but also their future influence.

Academia:Our inter-university consortium will support the training of highly skilled researchers and facilitate the generation and transfer of new methods of research synthesis. This will be supplemented by a network of visiting academics from fields including Political Science, Economics, Computer Science and Cyber-Psychology - this will result in an energetic and vibrant community. Further engagement with projects such as DAPM, UnBias and Ox-Chain in addition to the Orbit and TIPS1 funded projects will ensure that the exciting research is disseminated widely amongst the UK and International communities."
24,3FE36537-F994-44BD-9A05-B33E918F65A8,Learning to move as a human: one-shot learning of human motion,"Computational models for human motion analysis and synthesis have applications in fields as diverse as healthcare, computer graphics, and robotics. In healthcare, analysis of human movements can be used, for example, for tracking motor decline in the elderly. In computer graphics, human motion analysis can be used for human pose tracking from a single camera when measurements might be noisy or missing due to occlusion. In robotics, human motion analysis and synthesis can be used for teaching robots new skills by imitating demonstrations of a human, reducing the effort required to program an industrial robot or a service robot.

One approach to understand how humans move consists of collecting examples of a particular human activity and designing a machine learning model that extracts patterns from those examples. The more examples we collect, the more likely it is for the model to find common features in the data that can be exploited for solving predictive tasks. However, in different applications that require human motion analysis and synthesis, particularly in robot programming by demonstration, collecting many examples is expensive and time-consuming. I.e. we would like a robot to learn a new skill with as few demonstrations as possible, more like a human does. Indeed, humans learn efficiently by imitation with just one or few examples, which is further validated by their ability to generate new examples or creating abstract motions that were not previously seen in the examples that were used to imitate.

In this project, our objective is to develop a data-efficient machine learning model for human motion using the cognitive science concept of one-shot learning.

In cognitive science, one-shot learning (OL) refers to the idea of building intelligent agents using one or few examples. Successful illustrations of the use of this concept for building data efficient models include OL models for generating speech concepts and handwritten characters with human-like appearance. Recent research in cognitive science suggests that humans achieve OL through the combination of three core principles applied to primitive concepts: causality, compositionality, and &quot;learning to learn&quot;. It also claims that these ingredients could play an active role in producing machine learning models that replicate human intelligence.

We will achieve our objective through the two key novelties of this proposal: (i) a generic methodology that simultaneously combines causality, compositionality and learning to learn of motor primitives and (ii) a particular instantiation that uses physics-inspired Gaussian process (GP) representations of such motor primitives.

With respect to (i), although there are machine learning models that incorporate some of the ingredients of OL, their simultaneous combination to build data-efficient models for human motion analysis and synthesis has not been proposed yet. With respect to (ii), our GP representation of a motor primitive uses a physics-inspired covariance function with two features: the efficient use of data due to its non-parametric nature; and the inclusion of the principle of causality of OL, providing a generative mechanism for trajectory data. Compositionality of these GP motor primitives will be approached using ideas from formal language theory, in particular, hidden Markov models with explicit state durations. Learning to learn will be accomplished by providing hierarchies of such hidden Markov models.

In order to use the model in practice, we will provide a statistical inference framework for fitting the parameters of the OL model to given data, and for computing probability distributions for prediction. We will test the performance of the OL model for different tasks related to motion capture data, and for imitation learning using kinesthetic demonstrations from anthropomorphic robots. Our results will be fully reproducible and our software to be released as open source.",,"Developing data-efficient machine learning models for human motion can potentially have a positive impact in the following areas.

-- Economy. The Fourth Industrial revolution is taking place at the time this proposal is being written. It has been coined with the term Industry 4.0 (or Digital Manufacturing) and its aim is to use advances in information and communication (IC) technologies to promote the computerisation of manufacturing, consequently increasing productivity. Adopting IC technologies in the industry has the potential to increase the UK manufacturing revenue by 12.5%, equivalent to &pound;20bn (based on the same experience in Germany) by 2020. A key driver force of this Fourth Industrial revolution is Industrial Robotics, a market that is expected to reach $41bn by 2020. These new industrial robots are expected to be more intelligent and flexible enough to develop different tasks in the factory. Data-efficient machine learning models for imitation learning of human motion can impact positively on the advancement of these capabilities for industrial robotics in the UK. Likewise, the UK spent &pound;8.34 billion on social care for the elderly in 2015/2016 and it is estimated that today, one in eight older people live without the proper level of care. The methods that we will develop in this project will potentially provide technologies to build assistive robots with the ability to easily being programmed by imitation for an older person to carry out a daily living task, alleviating the pressure on the social care system.

-- Society. Computational models for human motion have a range of applications in healthcare. For example, they can be used for measuring the progress of a rehabilitation therapy for a previously injured limb, or for early diagnosis of motor-related diseases, e.g. Parkinson's. Developing more faithful models of human locomotion can potentially be used to build more accurate biomarkers in these applications. In addition, analysis of human motion in healthcare is progressively moving from a laboratory-based analysis to daily life monitoring. Therefore, developing probabilistic models for human motion that can cope with noisy measurements obtained from wearable sensors can help to establish them as reliable tools for assessing a medical condition correlated to human locomotion.

-- Knowledge. The generative models that we will develop in this project will have an impact on machine learning, and on imitation learning for robotics. Within machine learning, the probabilistic models that we will develop are examples of semi-parametric models, where the parametric part of the model is modular and given by different types of hidden Markov models, and the non-parametric part is given by powerful representations of observed data in the form of Gaussian processes distributions. Further research can exploit these architectures for designing machine learning models in different applications like computational biology and geostatistics. Within imitation learning for robotics, we claim that using the principles of human-like intelligence for building machine learning models for human motion paves the road for data-efficient robot learning. Further research projects can be developed by increasing the complexity of the models that take these principles into account.

-- People. The project will have a positive impact on the careers of the PI and the RA, who will both gain additional experience in the formulation of novel probabilistic machine learning models and hands-on experience with real robotic systems. An EPSRC New Investigator Award is the ideal opportunity for the PI to start building himself as a leading researcher in human-like computing in the UK.

To achieve these potential impacts, in the shorter and longer term, we will develop Pathways to Impact as described in the corresponding Pathways to Impact section."
0,4EDCE6D6-4AA0-40CD-8CA2-E5B5313F13BD,FinTrust: Trust Engineering for the Financial Industry,"The FinTech industry is one of the major growth industries in the United Kingdom. These companies create new, cheaper and faster services, utilizing the latest technologies such as cloud, mobile and blockchain. To succeed they need to gain the trust of customers in a period that society's trust in the financial industry is still impacted by the mortgage crisis almost a decade ago. They need to gain this trust while technologies are changing rapidly and data breaches are continuously in the news. FinTrust will research the issue of trust in FinTech, identifying the generic research challenges and establishing fundamental research results. A particular focus will be on increased automation through the use of machine learning algorithms, which may have implications that affect consumer trust in the new services.

For the first time, research results as derived in FinTrust will be made available as a Trust Engineering Tool Kit, allowing service provider, regulators and consumer organizations to assess levels of trust, quantify the extent to which trust matters to consumers and trust attitude differentiates consumers, assess bias in automated advice, establish whether algorithms may lead to financial exclusion or financial distress and protect services from erroneous algorithmic results. In our work we balance the commercial interest with society's interests, addressing issues such as financial exclusion through algorithmic bias. 

FinTrust offers a focused pathway to academic, societal and economic impact. Starting from the co-creation effort with Atom Bank that resulted in the proposal, we grow the Stakeholder Panel to include customer organizations, regulators and other stakeholders. We then establish a joint-up academic community in a FinTech Network, bringing together ICT, EPSRC and other research and intertwine this with existing research networks in finance and business research. We will make the software tools available freely and to a wide audience, training the research staff in interacting and public engagement to optimize the impact from our research.",,"The research in FinTrust aims to derive software and management tools for trust engineering in the financial industry, especially focusing on automation through machine learning algorithms . FinTrust delivers a tool kit for assessing human trust, assessing bias of algorithms, and offers approaches to control how socio-technical systems interface with algorithms. Our tools are based on FinTrust's generic research insights and can therefore find application across the industry (even beyond the FinTech industry, although initially this is not the focus of the project and its pathways to impact). The tools will be open source, and downloadable from the web and the data we use will confirm to the policy framework on research data of UK Research Councils. That is, we make all data openly available as quickly and with as few restrictions as possible, realising the commercial constraints that may be posed by our partner Atom Bank. 

When co-creating the FinTrust proposal we worked closely with Atom Bank, one of the most prominent FinTech companies in the UK. In the six months of FinTrust we will establish a Stakeholder Panel to broaden the base of interaction, including FinTech companies, regulatory bodies (Financial Conduct Authority) and consumer protection organisations. The stakeholders will collaborate through further co-design of the research approaches, as well as in co-research through participation in the research in human factors and in testing our tools. In this manner, FinTrust will optimize the suitability of its tool for industrial take-up and for functionality that protect the consumer while fulfilling requirements of the regulator. 

A specific area of interest in our work is the issue of financial exclusion and financial distress. These have been highlighted as important societal challenges by the House of Lords ('Tackling Financial Exclusion: A country that works for everyone? Report of Session 2016-17', House of Lords Select Committee on Financial Exclusion, 2017) and the Financial Conduct Authority ('Predicting financial distress by predicting unaffordable consumer credit agreements: An applied framework', Financial Conduct Authority Occasional Paper 28, 2017). By interfacing with FCO and consumer organisations we can optimize the tools we develop in terms of avoiding bad loans and, at the same time, financial exclusion. 

FinTrust will deliver benefits in terms of raising public awareness over the operations and implications of banking services, including fully automated 'robo-advice'. It will also reach out and discuss with consumers, politicians and other about the implications of automated advice for financial exclusion, distress and consumer trust. 

The project will pay particularly close attention to career development if its research associates, through public engagement training, training sessions with our partner Atom Bank, and through involving them intensively with the various network activities FinTrust leads and participates in. We will develop tutorials and presentations for academic and non-academic conferences and meetings, reserving budget for professional developers of presentations materials, flyers and web site."
1,46776CDC-B81E-4392-B306-AEC253C4694B,Autonomous Robot Evolution (ARE): Cradle to Grave,"Robotics is changing the landscape of innovation. But traditional design approaches are not suited to novel or unknown habitats and contexts, for instance: robot colonies for ore mining, exploring or developing other planets or asteroids, or robot swarms for monitoring extreme environments on Earth. New design methodologies are needed that support optimising robot behaviour under different conditions for different purposes. It is accepted that behaviour is determined by a combination of the body (morphology, hardware) and the mind (controller, software). Embodied AI and morphological computing have made major progress in engineering artificial agents (i.e., robots) by focusing on the links between morphology and intelligence of natural agents (i.e., animals). While such a holistic body-mind approach has been hailed for its merits, we still lack an actual pathway to achieve this.
While this goal is ambitious, it is achievable by introducing a unique methodology: a hybridisation of the physical evolutionary system with a virtual one. On the one hand, it is appreciated that an effective design methodology requires the use and testing of physical robots. This is because simulations are prone to hidden biases, errors and simplifications in the underlying models. Simulating populations of robots (rather than just simulating specific parts) leads to accumulated errors and a lack of physical plausibility: the evolved designs will not work in the real system. This is the notorious reality gap of evolutionary robotics. On the other hand, evolving everything in hardware is time and resource consuming. One of our major innovations is to run simulated evolution concurrently with the physical and hybridise them by cross-breeding, where a physical and a virtual robot can parent a child that may be born in the real world, in the virtual world or in both. The advantages of such a hybrid system are significant. Physical evolution is accelerated by the virtual component that can run faster to find good robot features with less time and resources; simulated evolution benefits from the influx of genes that are tested favourably in the real world. Furthermore, monitoring of and feedback from the physical system can improve the simulator, reducing the reality gap.",,"Impact on science
This project will lead to new ideas behind the co-evolution of brains and bodies applied in the context of robotics. Researchers working in the field of robotics will be an obvious group that this research will be highly relevant to. Additionally, the novel evolutionary methods and techniques developed will be of interest to the general evolutionary computation community. New and interesting evolutionary results will be uncovered, through the use of challenging case studies to give insight into the complex interaction between controller and body shape, which will be of wider scientific interest. 
Our system also offers a tool for biologists in that it represents a physical, rather than digital, model of evolution. Unlike digital models in simulation, a hardware-based model is guaranteed not to violate the laws of physics and can help test biological hypotheses. To this end, our demonstrator forms a prototype of a research instrument that can be used to carry out a wide range of investigations. Just as a cyclotron is a tool for nuclear physics, our system will be a tool for research into evolution and embodied intelligence.

Impact on technology and the economy
The Robotics Technology Market is expected to reach $82.7 billion by 2020 (https://www.alliedmarketresearch.com/robotics-technology-market). Robotics technology is used in a wide range of industries including healthcare, defense, aerospace, automotive and infrastructure, and allows consumers to automate processes, increase productivity, enhance quality and reduce human errors. In the UK, the government has identified robotics as one of the &quot;8 Great Technologies&quot; central to the future growth of the UK economy - this project is central to delivering this growth. 
It will deliver a technology prototype demonstrator that enables the creation of intelligent machines while minimising the reality gap. This will have a large and lasting impact on robotics by initiating radically new robotic systems where robots are conceived and born, rather than designed and manufactured. This project is designed to provide the world's first demonstration of the co-evolution of brains and bodies of robots in real time and real space. This represents a significant technological advance that potentially opens up new approaches to design and manufacture, with significant potential to challenge and disrupt conventional approaches. Technology licensing and technology transfer will provide economic benefit to many of the industries listed. However, in particular, the project specifically targets nuclear decommissioning. The cost of the future clean up of nuclear plants across the UK was forecast in a government report in 2016 to be around &pound;117 billion spread across the next 120 years. Therefore, providing new technologies to accomplish this has the potential to have significant economic impact. This project will result in significant potential for exploitation through technology licensing and technology transfer. This could result in UK leading companies and the economic impact that would flow. That in turn could lead to future economic benefit could flow from the potentially substantial cost savings to the taxpayer in improved methods for nuclear decommissioning.

Impact on society 
Work resulting from this project will pave the way for reduced exposure of humans to hazardous environments, e.g. nuclear decommissioning. Evolved, specialist robots, will be well suited and adapted to these challenging environments, thus reducing the need to expose humans to either, long-term potential harm, or in some cases, using robots instead of humans. This will also lead to longer-term benefits to the environment, through effective decommissioning. Also, since this project will undoubtedly generate significant public and press interest (developed through outreach and public engagement), there will be a positive impact in showing how advanced robots can help people and the environment."
2,3DBA65DE-855D-415B-A177-1DC4ABE44F9D,Autonomous Robot Evolution (ARE): Cradle to Grave,"Robotics is changing the landscape of innovation. But traditional design approaches are not suited to novel or unknown habitats and contexts, for instance: robot colonies for ore mining, exploring or developing other planets or asteroids, or robot swarms for monitoring extreme environments on Earth. New design methodologies are needed that support optimising robot behaviour under different conditions for different purposes. It is accepted that behaviour is determined by a combination of the body (morphology, hardware) and the mind (controller, software). Embodied AI and morphological computing have made major progress in engineering artificial agents (i.e., robots) by focusing on the links between morphology and intelligence of natural agents (i.e., animals). While such a holistic body-mind approach has been hailed for its merits, we still lack an actual pathway to achieve this.
While this goal is ambitious, it is achievable by introducing a unique methodology: a hybridisation of the physical evolutionary system with a virtual one. On the one hand, it is appreciated that an effective design methodology requires the use and testing of physical robots. This is because simulations are prone to hidden biases, errors and simplifications in the underlying models. Simulating populations of robots (rather than just simulating specific parts) leads to accumulated errors and a lack of physical plausibility: the evolved designs will not work in the real system. This is the notorious reality gap of evolutionary robotics. On the other hand, evolving everything in hardware is time and resource consuming. One of our major innovations is to run simulated evolution concurrently with the physical and hybridise them by cross-breeding, where a physical and a virtual robot can parent a child that may be born in the real world, in the virtual world or in both. The advantages of such a hybrid system are significant. Physical evolution is accelerated by the virtual component that can run faster to find good robot features with less time and resources; simulated evolution benefits from the influx of genes that are tested favourably in the real world. Furthermore, monitoring of and feedback from the physical system can improve the simulator, reducing the reality gap.",,
3,CD05DB1F-27D7-4EBA-B35A-4E0EE8360EDE,Living with Machines,"Living with Machines is both a research project, and a bold proposal for a new research paradigm. In this ground-breaking partnership between The Alan Turing Institute, the British Library, and the Universities of Cambridge, East Anglia, Exeter, and London (QMUL), historians, data scientists, geographers, computational linguists, and curators have been brought together to examine the human impact of industrial revolution.

It is widely recognised that Britain was the birthplace of the world's first industrial revolution, yet there is still much to learn about the human, social, and cultural consequences of this historical moment. Focussing on the long nineteenth century (c.1780-1920), the Living with Machines project aims to harness the combined power of massive digitised archives and computational analytical tools to examine the ways in which technology altered the very fabric of human existence on a hitherto unprecedented scale. The central theme - the mechanisation of work practices - speaks directly to present debates about how society can accommodate the revolutionary consequences of AI and robotics in what has become known as the fourth industrial revolution. To understand the fraught co-existence of human and machine, this project contends that we need research methods that combine technological innovation and human expertise.

The project will utilise the British Library's National Newspaper collection, and event-based records (census, electoral registration, births/ marriages/deaths, trade directories) collected by contributing partners Findmypast. By developing intuitive computational interfaces, and adapting collaborative practices developed in the field of software development, we will enable close interaction between computational methods and historical inquiry. 

Outreach and Engagement will be central to the project from the outset, and will take two forms: familiar outcomes such as television programmes and regional exhibitions; and working with individuals and communities to create common understandings of their shared histories. Participatory aspects will embody best practices in crowdsourcing and citizen history.

Project benefits:

1. The UK's first large-scale synergy between data science, artificial intelligence research, and the arts and humanities, building capacity and catalysing new research areas.

2. The development of new computational techniques to marshal the UK's rich archival collections (digitised and born-digital), to enable new research questions to be posed of the holdings.

3. Enriched and interlinked data holdings for the British Library, to add additional context and value to content.

4. The development generalisable tools, code, and infrastructure that can be adapted for and inspire future interdisciplinary research projects.

5. New historical perspectives on the effects of the mechanisation of labour on the lives of ordinary people during the long nineteenth century.

6. The creation of computational models to represent how language and meanings change across time and geography.

7. Research breakthroughs maintaining UK global leadership in Digital Humanities and driving large-scale international partnerships and opportunities.",,Optional.
4,FAFF1235-1DB9-4CBC-A582-0AFC13364216,A semi-autonomous robot synthetic biologist for industrial biodesign and manufacturing,"The last decade has seen significant advances in the fields of synthetic biology as well as robotics and artificial intelligence (AI). Synthetic biology is an emerging multidisciplinary field with potential to have step-change benefits in many fields from medicine through to industrial biotechnology. This advance is dependent on the ability to rationally engineer biological organisms in a more predictable and defined way than has previously been possible. 

Bio-manufacturing is an increasingly important platform for a sustainable manufacturing future. Many natural products have potentially valuable nutraceutical or pharmaceutical applications, but cannot be chemically synthesised or harvested from nature without significant ecological disruption. The engineering of biology by design seeks to construct new biological entities that are optimised for specific functionality such as bio-production within a 'cellular factory'. Synthetic biology provides a method for optimising production of complex natural products using sustainable methods in a microbial production host, much like ethanol is produced in yeast. Advanced synthetic biology tools will enable us to tackle more complex targets. Here, by integrating synthetic biology tools with robotics and AI we aim to make a significant advance to reducing the cost and development time of new biologically derived products.

It is now evident that robotics is essential for synthetic biology to fulfil its potential and is of particular relevance to industrial biotechnology. In parallel, big data has become increasingly important in many areas of technology as well as the biological domain. This is leading to new and powerful applications of AI in everyday life. Here we seek to address the application of AI to synthetic biology, using AI approaches to direct automated synthetic biology experiments. 

These advances will have the potential to create new products, companies and even industries that will ultimately benefit the economy, health, quality of life and security of the UK general public and beyond. It will also have far-reaching effects on policy and society.",,"The last decade has seen significant advances in the fields of synthetic biology as well as robotics and AI. Synthetic biology is an emerging multidisciplinary field with potential to have step-change benefits in many fields from medicine through to industrial biotechnology and defence/security. This advance is dependent on the ability to rationally engineer biological organisms in a more predictable and defined way than has previously been possible. It is now evident that robotics is essential for synthetic biology to fulfil its potential and is of particular relevance to industrial biotechnology. In parallel, big data has become increasingly important in many areas of technology, including the biological domain. This is leading to new and powerful applications of AI in everyday life. Here we seek to address the application of AI to synthetic biology, using machine learning approaches to direct automated synthetic biology experiments. This will have important and potentially far reaching applications in the industrialisation of synthetic biology tools and processes. These advances will have the potential to create new products, companies and even industries that will ultimately benefit the economy, health, quality of life and security of the UK general public and beyond. It will also have far-reaching effects on policy and society."
5,6280CA5A-035E-4A29-ACDB-015EA05D6B59,Trust in Human-Machine Partnership,"Interaction with machines is commonplace in the modern world, for a wide range of everyday tasks like making coffee, copying documents or driving to work. Forty years ago, these machines existed but were not automated or intelligent. Today, they all have computers embedded in them and can be programmed with advanced functionality beyond the mechanical jobs they performed two generations ago. Tomorrow, they will be talking to each other: my calendar will tell my coffee maker when to have my cuppa ready so that I can arrive at work on time for my first meeting; my satnav will tell my calendar how much time my autonomous car needs to make that journey given traffic and weather conditions; and my office copier will have documents ready to distribute at the meeting when I arrive in the office. And they will all be talking to me: I could request the coffee maker to produce herbal tea because I had too much coffee yesterday; and the copier could remind me that our office is (still) trying to go paperless and wouldn't I prefer to email the documents to meeting attendees instead of killing another tree?

This scenario will not be possible without three key features: an automated planner that coordinates between the various activities that need to be performed, determining where there are dependencies between tasks (e.g., don't drive to the office until I get in the car with my hot drink); a high level of trust between me and this intelligent system that helps organise the mundane actions in my life; and the ability for me to converse with the system and make joint decisions about these actions. Advancing the state-of-the-art in trustworthy, intelligent planning and decision support to realise these critical features lies at the centre of the research proposed by this Trust in Human-Machine Partnerships (THuMP) project.

THuMP will move us toward this future by following three avenues of investigation. First, we will introduce innovative techniques to the artificial intelligence (AI) community through a novel, intra-disciplinary strategy that brings computational argumentation and provenance to AI Planning. Second, we will take human-AI collaboration to the next level, through an exciting, inter-disciplinary approach that unites human-agent interaction and
information visualisation to AI Planning. Finally, we will progress the relationship between Technology and Law through a bold, multi-disciplinary approach that links legal and ethics research with new and improved AI Planning.

Why do we focus on AI Planning? A traditional sub-field of artificial intelligence, Planning develops methods for creating and maintaining sequences of actions for an AI (or a person) to execute, in the face of conflicting objectives, optimisation of multiple criteria, and timing and resource constraints. Ultimately, most decisions result in some kind of action, or action sequence. By focussing on AI Planning, THuMP captures the essence of what a collaborative AI decision-making system needs to do.

We believe that most AI systems will (need to) involve a human in-the-loop and that it is crucial to develop new AI technologies such that people can use, understand and trust them. THuMP strives for complete understanding and trustworthiness through transparency in AI. We will develop and test a general framework for &quot;Explainable AI Planning (XAIP)&quot;, in which humans and an AI system can co-create plans for actions; and then we instantiate two use cases for this framework that focus on resource allocation in two very different critical domains.

A cross-disciplinary project team of seven investigators, four collaborators and four postdoctoral research assistants will work with three project partners--a leading oil &amp; gas services corporation; a leading international charity; and a leading global law firm--to move us into this envisioned future. An ambitious and realistic programme of networking, development, evaluation and public engagement is proposed.",,"According to a June 2017 report from PricewaterhouseCoopers/PwC on the impact of Artificial Intelligence (AI) on the UK economy, the economic growth directly attributable to AI will be no less than 5% of GDP, and may be up to 10% by 2030. That figure represents a projected increase of more than 230 billion pounds in less than 15 years, and means that AI holds significant potential benefit for the UK. However, that potential is dependent on AI being seamlessly integrated throughout the economy: at work and at home; in factories, in offices, in shops and in schools. And that integration will not happen while people do not trust the technology. Unhappily, distrust in black-box AI techniques seems to be growing. THuMP directly addresses the issue of trust in AI, and one side effect of the overarching aim of the project--to achieve complete understanding and trustworthiness through transparency in AI systems--is to ensure that the promise of that additional 230 billion pounds a year is realised.

The main impact to be delivered by THuMP will be to demonstrate that trust in AI systems can be fostered if the AI can explain how it arrives at its recommendations. The demonstration will be in the context of AI planning, a sub-area of artificial intelligence that is already seeing deployment in industries such as our project partner, Schlumberger. Through the project evaluation, THuMP will demonstrate that &quot;Explainable AI Planning&quot; (XAIP) systems engender greater trust than planning systems that do not provide explanations. More specifically, THuMP will carry out this demonstration not only in a laboratory setting, but also in the real world setting of two of our project partners: an international oil &amp; gas service company and an international charity. Thus, in addition to scientific results on the increase of trust that XAIP systems engender in humans, we will produce two use case studies for dissemination.

Of course, if AI is to become an accepted part of our lives, this will not happen by purely technological means. Magic technology will not make people's worries evaporate. Rather we need to understand the causes of people's concerns about AI, and overcome them. We need to establish a sound legal, ethical and regulatory framework in which AI systems will operate, and we need to help understand people's fears, and make sure that they are addressed by the composers of laws and regulations and the programmers who create the AI systems. Through a series of public engagement activities, THuMP seeks to identify and assuage the AI fears of the general public, to help educate not only the lay person, but also the engineers and policymakers responsible for enabling and regulating AI in society.

THuMP aims to have an impact on multiple audiences. On the question of a suitable legal and ethical framework, THuMP will identify how AI systems can be made to fit within the EU General Data Protection Regulation that is coming into force in the near future. On the question of understanding and addressing people's fears, THuMP will make progress through an ambitious programme of public engagement activities. These aim to both discover the issues around AI that concern young adults, a group chosen both for their tech savvy as &quot;digital natives&quot; and for the fact that they will be the first generation to live alongside AI systems for the bulk of their lives, and to provide a vehicle for working through these concerns."
6,6A2492FA-217F-4CFD-B37C-639C6FB09B43,Stable Prediction of Defect-Inducing Software Changes (SPDISC),"Context: software systems have become ever larger and more complex. This inevitably leads to software defects, whose debugging is estimated to cost the global economy 312 billion USD annually. Reducing the number of software defects is a challenging problem, and is particularly important considering the strong pressure towards rapid delivery. Such pressure impedes different parts of the software source code to all receive equally large amount of inspection and testing effort. 

With that in mind, machine learning approaches have been proposed for predicting defect-inducing changes in the source code as soon as these changes finish being implemented. Such approaches could enable software engineers to target special testing and inspection attention towards parts of the source code most likely to induce defects, reducing the risk of committing defective changes. 

Problem: the predictive performance of existing approaches is unstable, because the underlying defect generating process being modelled may vary over time (i.e., there may be concept drift). This means that practitioners cannot be confident about the prediction ability of existing approaches -- at any given point in time, predictive models may be performing very well or failing dramatically.

Aim and vision: SPDISC aims at creating more stable models for predicting defect-inducing changes, through the development of a novel machine learning approach for automatically adapting to concept drift. When integrated with software versioning systems, the models will provide early, reliable and automated defect-inducing change alerts throughout the lifetime of software projects. 

Impact: SPDISC will enable a transformation in the way software developers review and commit their changes. By creating stable models to make software developers aware of defect-inducing changes as soon as these are implemented, it will allow targeted inspection and testing attention towards defect-inducing code throughout the lifetime of software projects. This will reduce the debugging cost and ultimately lead to better software quality. 

Proposed approach: an online learning algorithm will be developed to process incoming data as they become available, enabling fast reaction to concept drift. Concept drift will be detected using methods designed to cope with class imbalance, which typically occurs in prediction of defect-inducing software changes. Class imbalance refers to the issue of having a much smaller number of defect-inducing changes than the number of safe changes. The proposed approach will also make use of data from different projects (i.e., transfer learning between domains) to speed up adaptation to concept drift.

Novelty: SPDISC is the first proposal to look into the stability of predictive performance over time in the context of defect-inducing software changes. Most previous work ignored the fact that predictions are required over time, being oblivious of the instability of predictive performance in this problem. To deal with instability, SPDISC will develop the first online transfer learning approach for predicting defect-inducing software changes. 

Ambitiousness: online transfer learning between domains with concept drift is not only a very new area of research in software engineering, but also in machine learning. Very few approaches exist for that, and none of them can deal with class-imbalanced problems. Therefore, SPDISC will not only advance software engineering by enabling a transformation in the way software developers review and commit their changes, but also advance the area of machine learning itself. 

Timeliness: given the current size and complexity of software systems, the increased number of life-critical applications, and the high competitiveness of the software industry, approaches for improving software quality and reducing the cost of producing and maintaining software are currently of utmost importance.",,"SPDISC's beneficiaries are the software industry, software users and related scientific communities. 

1) Software Industry
The software industry is SPDISC's main beneficiary. The UK software industry is estimated to be worth more than 9bn GBP, and is the second largest market by value in the EU. Globally, the software industry's estimated value is over 407bn USD. And yet, the global cost of debugging software is estimated to be 312 billion USD annually, representing an enormous loss of revenue. SPDISC will lead to an impact on the economy by reducing debugging cost and increasing software quality. 

In particular, SPDISC will empower software developers with early, reliable and automated alerts of defect-inducing software changes throughout the lifetime of software projects. It will enable a transformation in the way software changes are reviewed and committed in software development companies who use software versioning and bug-tracking systems. Defect-inducing changes will be automatically pinpointed for attention right after their implementation, allowing easy and wise allocation of the limited testing and inspection resources. This is specially desirable in companies leaning towards a more agile software development process. 

As the software changes will be fresh in the developers' minds when defect alerts are triggered, their inspection will be much cheaper than later debugging cost. In addition, changes typically have few lines of code, further facilitating inspection. Therefore, SPDISC's approach will reduce the risk of committing changes that will lead to defects, reducing debugging cost and increasing software quality. The lower debugging cost will translate into cheaper software cost, as finding and fixing defects typically takes 50% of a software developer's time. 

From a project management perspective, as each software change is inherently associated to a single developer, the assignment of developers to inspect defect-inducing changes will be straightforward. With SPDISC, the task of deciding which parts of the source code should receive increased attention and by whom can be delegated to the software developers themselves, freeing project managers to other tasks. 

Both large enterprises and SMEs can benefit from SPDISC, as its approach automatically adapts to different environments. I anticipate that software development tools based on SPDISC will be commercialised in the future. One of SPDISC's industrial partners has already expressed interest in doing that. This will assist SMEs in benefitting from SPDISC, increasing their competitiveness and driving faster and more balanced economic growth. This will in turn lead to an impact on society by increasing wealth and employment. 

2) Software Users
The more cost-effective software development enabled by SPDISC will consequently bring benefits to software users, who can be private users, users of public services, or other enterprises. Cheaper cost will facilitate access of private users and public services to software. Higher quality will improve quality of life through better and safer software experience. This is key to a world of smart cities, which are greatly controlled by software. It is also important to life-critical software applications, which could pose serious threats if defective. Cheaper and higher quality software will increase the competitiveness of other enterprises who depend on software, driving faster economic growth. Extensions of SPDISC's approach can also potentially help to solve other data analytics problems than defect prediction. 

3) Scientific Communities
SPDISC will create a tighter bond between software engineering and machine learning through its new machine learning approach for software engineering. These two areas will benefit from this research. There will also be some impact on mathematical sciences, as part of SPDISC's foundation lies in this area. More details are in the academic beneficiaries summary."
7,00120800-7CB7-4724-9D49-C92F0E28203C,"Human Data Interaction: Legibility, Agency, Negotiability","Within almost every discipline related to the digital economy, there are critical and emerging issues around humans and the data they generate either directly, or as a byproduct of their endeavours. Equally, the data economy has stimulated a range of initiatives responses within each of the three sectors (public, private and third), as well as a broad portfolio of research across relevant disciplines. However, while such important work is ongoing, such these efforts are often disparate and tend not to feed directly back into the science of data-driven systems itself. There is an urgent need to guide the realisation of system design principles that are productive, and yet fit with the ethics and values acceptable to wider society. Those who are expert in development of the systems, algorithms and analytics that raise such issues face challenging culture gaps: firstly, with regard to those who are expert in areas such as the arts and humanities, and secondly with regard to those who are inexpert in technology but who are increasingly impacted by it in their everyday lives. Core to these divisions are issues such as a lack of social understanding of the technical capabilities of data-driven systems, inconsistency of research and development effort across sectors and disciplines, and tensions between industrial, societal and academic drivers, and human needs. Such tensions are visible in several domains, though few as pointedly critical as health. One need only look at NHS' efforts to protect individuals' medical records, in contrast to contrasted against the corporate monetization of DNA samples, as individuals take advantage of advances in low-cost mobile self-monitoring and diagnosiseek low cost solutions to their health-managements. Here, state, corporate and individual-level drivers create inconsistent approaches to the management and value of data. 

It is time to draw together, consolidate and formalise our efforts across disciplines. We must now seek to structure further endeavour, while considering how new and emerging systems are realised, received and responded to-not just within the bounds of the DE but cross-sector, i.e. within the range of organisations and communities that reflect and support daily human activity and concern. At a sectoral level, industry has often focused narrowly on either corporate monetisation of data from individuals, or individuals' efficiency and short-term optimisation of personal metrics (e.g. the 'quantified self'). Market pressures mean that technical advances are increasingly implemented before social and cultural effects can be determined. This means, however, that data-intensive systems to support long term social, cultural and creative benefits are rare. At the same time, academic research has often focused on questions of interest more to itself than to other sectors. Academic work with public and third sector organisations has been fragmented, with interactions often weighted in favour of shorter term innovation cycles rather than longer term social needs. Such challenges, divergences and tensions lead to duplications, contradictions, and unproductive effort. This is the problem space within which we operate.

Our network a holistic and inclusive network approach, sensitive to the socially situated nature of such systems. To achieve this we will (a) develop and sustain a collaborative, cross-sectoral community under the banner of Human Data Interaction, (b) develop a portfolio of system design projects addressing underexplored aspects of the DE (c) create cross-sectoral interdisciplinary synthesis of research under the HDI banner (d) conceptually develop and flesh-out the HDI framework, (e) create a suite of policy and public-facing case studies, papers, prototypes and educational materials, and (f) develop a set of core guidelines intended to inform the design of human-facing data-driven systems.",,"The Human Data Interaction Network+ will have direct impact with industry, the wider public, media and education and skills. The concept is core to the vast majority of new and emerging systems with which we all interact on a daily basis, both above and below our immediate level of awareness. As such it will transform many areas under the Digital Economy theme. This is evident through the strength of support and interest articulated by our partners and collaborators, even before commencement of the work. There is no question that this is a critical area and, as such, will attract multiple opportunities for impact. 
Specifically, this network will progress the state of the art in respect of systems design and research by (1) establishing the foundations for a new science of data-driven systems through (2) collaborative development of the Human Data Interaction framework. In order to achieve this, this Network+ will fund a portfolio of 45 projects, culminating in a review and showcase event. The work will establish the foundations for a new science of data-driven systems through the framework of human-data interaction. 

We will promote the new approach within our research communities. We will organise a workshop at premier conferences, and present our work at leading research institutions (including those of our network members). We will pursue opportunities for outreach and advocacy among the general public, industry, the public and third sectors and the wider scientific community, e.g. demonstrations and displays at science festivals, trade fairs and industrial outreach events, and debate through learned societies. Indeed, part of this outreach is inherent in the design of the network. We will also draw upon the networks of our partners in order to communicate both network opportunities and findings of the research. 

Our collaborations already include a broad range of industries and organisations. We will build on our ongoing collaborations with companies including Google, Microsoft Research, IBM, Spotify and Arup -reaching out to SMEs, startups and others through Digital Catapult and ICAEW. Through these mechanisms we will establish further collaborations with major companies, to expand our set of collaborators as the network matures. We will be assisted in this work by our partners, the planned workshops and our universities' Business Development Teams and Research &amp; Enterprise units. Such collaborations will enable us to support and promote research, and demonstrate that any prototypes can operate in a definite market. Our collaborating partners will benefit from access to the leading researchers in this emerging area, the ability to shape the ongoing research as it happens, funded collaborations, and the ability to demonstrate and evaluate the work in deployments aligned with their commercial areas of interest.

The funded project arising from of the network activities will act as demonstrators for media, the public, and government, highlighting new areas of research, asking new questions and drawing together existing endeavour. In this way, the network+ in Human Data Interaction has the potential to be the driving force and public face of research under this banner, exploiting the relationships forged during the life of the network into the long term. Through our government partners, this network will contribute towards evidence based policy-making and influencing public policies and legislation at a local, regional, national and international level. 

Lastly, we will inform skills development through our partnerships with professional bodies (ICAEW and ALT) in addition to our strong links into the network of local authorities. In summary, the impacts of this network will be broad and far reaching, even beyond the life of the project."
8,74C75210-3671-4A88-AA90-805DCD4FD2BF,Digitally Enhanced Advanced Services Network+,"This Digital Economy (DE) Network Plus will deliver a vibrant community that will position the UK as the internationally leading research hub for Digitally Enhanced Advanced Services. Rather than focus on the product or service that is delivered DEAS focuses on how the product or service is used. This is a major change in how firms earn money and is being enabled by transformative digital technologies that allows for example, payment per use or availability or outcome. The impact of these changes is in firm productivity. The traditional focus of productivity (outputs/inputs) is on internal efficiency. However, digital technologies can also transform the value of the output (payment for use, availability or outcome). Haldane (Chief Economist of the Bank of England) in his recent report on productivity puzzle (essentially stagnant growth since the financial crash) argues that despite the advent of the digital age and their adoption by some leading companies there is a very long tail of poorly productive firms across all sectors. He calls for the development of, for example, online tools that will speed the process of technological diffusion to the long tail. The development of the underpinning digital technologies for the purposes of developing DEAS is the key research challenge adopted by this Network+.",,"Our aim is to develop a community of industry and scholars around the theme of Digitially Enhanced Advanced Services. This includes research topics such as: AI techniques, data analytics, HCI, associated technologies, and service business model innovation within the context of manufacturing, transport and insurance sectors. 

We have a number of direct pathways to impact through our pilot studies and the engagement of our project partners. Specifically, for manufacturing there are a considerable range of potential SME beneficiaries including: Clevedon Fastners; MNB; Nicklin and UV Lights. There are also larger manufacturers including: Aggregate Industries (providing &pound;40k in cash for an early stage pilot), Rolls Royce and GE. For transport our potential pilot studies include Resonate, Axis Fleet Management and Microlise. For insurance, we have potential cases with Royal Sun Alliance, Legal and General and smaller brokers through the support of Lloyds Corporation. More widely we will be linking our analytics and HCI insights with practitioners such as Gartner and the emerging data analytics business at Interroute (world's 5th largest cloud services provider). 
As we have stated in the Case for Support we will develop a taxonomy and formal repository of successful DEAS use-cases to provide a platform for scale up research and practice. This will be made available to a wider group through the support of Sam Turner and HMV Catapult, AFRC (Glasgow) and AMRC (Sheffield), Lloyds Banking Group manufacturing advisors, Knowledge Transfer Network (KTN), the Road Haulage Association, British Vehicle Renting and Leasing Association, Society for Motor Manufactures and Traders and in insurance through the Lloyds Corporation and Huntswood. We also have strong links to the Association of British Insurers and will pursue those as the project progresses. 
In addition, through our three major research groups (ASAP, ASG and CODE) we have a total database of over 250 companies with regular events that bring together large communities of users. In the later stages of the Network, we will introduce opportunities for extending funding into other sectors e.g. the wider professional services where we have an Innovate UK grant (AUTTO - Intelligent Micro-Automation). This links strongly to the recommendations of the forthcoming Blackett report on services."
9,9D5AC1E7-7F89-434F-BC7E-F8EF70408AA0,Business Model Innovation for Intelligent Automation: Unpacking the Productivity Paradox,"Productivity growth has been slowing down in the last decade in major economies as well as in emerging markets despite the prevalence of digital technologies. This phenomenon is widely known as the productivity paradox. The productivity growth slowdown is particularly acute in the UK compared to other major economies. Moreover, industries that are the most intensive users of Information and Communication Technologies (ICT) appear to have contributed most to the slowdown in productivity. One of the main reasons for this productivity slowdown could be due to the limited redesign of business processes and business models following the adoption of new digital technologies by firms. Through the research programme Dr. Velu will provide a better understanding the relationship between business model innovation and productivity improvements following the adoption of intelligent automation technologies. Dr. Velu will build a digital tool for management information and decision support systems for assessment of productivity of business models in order to enable rapid and sustained improvements in productivity within firms following the adoption of digital technologies. In doing so, the Dr Velu aims to propose a new framework for productivity reporting for national income accounting.

Dr. Velu will conduct historical analysis of firms that have implemented intelligent automation technologies in order to learn and develop the criteria for productivity measurement of business models. This will include analysis from historical publically available data as well as within firm analysis of a number of selected sectors such as manufacturing, distribution and the sharing economy. In addition, the research will conduct longitudinal in depth analysis of firms in similar sectors as the historical analysis in order to build a digital tool that will identify business model innovation opportunities following the adoption of intelligent automation technologies. This will involve working with the senior management team of a selected number of firms in these sectors in order to define the data requirements, draw-up the technology specification, develop the software programme, populate and test the digital tool with data and propose ways to embed the digital productivity tool within existing management reporting systems. The research will benefit firms as it will provide the basis for a systematic evaluation of the need for business model innovation opportunities following the implementation of intelligent automation technologies. The research will also benefit policymakers by defining good quality and appropriate data in addressing the challenges of measuring productivity in the digital economy.",,"The research will provide key benefits to the stakeholders across the managerial, policy and academic communities as follows: 

(1) Supporting Business Model Transformation
The digital productivity tool will help managers in established and start-up firms to decide how to assess the impact of the intelligent automation technologies that they implement in order to fully leverage the productivity benefits through appropriate business model innovation. Moreover, technology transfer organisations (such as the Digital Catapult, the Advanced Manufacturing Catapults and Innovate UK) will benefit as they would be better able to provide advice to firms testing and implementing the latest intelligent automation systems using the Catapults facilities to gain the full productivity benefits. Moreover, the involvement and support of IBM as well as The Conference Board will ensure that the digital productivity tool is disseminated widely among firms implementing digital technologies.

(2) Influencing National Policy
Policymakers such as the Department for Business, Energy and Industrial Strategy (BEIS) and Office for National Statistics (ONS) will benefit significantly from defining good quality and appropriate data in addressing the challenges of measuring productivity in the digital economy. The research will feed into initiatives related to the recent Industrial Strategy Green Paper. This will result in more effective policymaking on industrial strategy. Policymakers will be involved from the start of the research programme in order to ensure sustained interactions. Policy Fellowships will be organised through our partnership with the Centre for Science and Policy (CSaP) at the University of Cambridge ensuring strong policymaker engagement and dissemination of findings.

(3) Consumers and the Economy
Consumers and the economy will benefit from new digital technologies such as intelligent automaton becoming available sooner as a result of appropriate business model innovation. Moreover, addressing a key element of the productivity paradox by enabling quicker and more effective business model innovations will stimulate faster economic growth.

(4) Further Academic Investigations and Research Capability
Research findings generated during the project will be disseminated at international conferences and through leading academic journals. Dr.Velu will plan a special issue with one of the leading journals on the topic. These will provide valuable routes to publish developments in intelligent automation and its impact on business model innovation and productivity in an accessible manner to academics and practitioners alike. The involvement of the visiting professor who are all experts in digital technologies will support this strand of activity. Moreover, the research programme will complement existing research on productivity such as the Economic Statistics Centre of Excellence (ESCoE) on measuring the modern economy and the Centre for Economic Performance's (CEP) productivity programme and to the forthcoming ESRC Network Plus on Productivity. Finally, the Research Associates will acquire unique skill sets in combining social science with engineering to address productivity issues. These skills will be disseminated widely through research training in order to help train other researchers."
10,0F22A335-6DBB-413A-B47E-05D00195788F,SPHERE - A Sensor Platform for HEalthcare in a Residential Environment (IRC Next Steps),"The UK currently spends 70% of its entire health and social care budget on long term (&quot;chronic&quot;) health conditions. These include diabetes, dementia, obesity, depression, COPD, arthritis, hypertension and asthma. 
We need to be better at:
-- Understanding the cause of these illnesses
-- Helping a person to avoid developing them
-- Creating new treatments
-- Helping the patient self-manage their conditions

All these require working with a patient over months or years, outside of a traditional hospital environment. In a very real way, we need healthcare to go where the patient goes; the single place that most people spend most of their time is their home. Consequently, SPHERE project is seeking to develop non-intrusive home-based technologies for measuring health related behaviours at home over long periods of time.

The requirements for these technologies are:

-- They should require little or no action from the patient, since our daily lives are busy; being ill is distressing and time-consuming; and when the benefit may take months or years to achieve, there is often not much day to day motivation to be bothered with measurements or devices.
-- They should work reliably in the home; a home is not a hospital or a laboratory - it is smaller, full of furniture, pets and people, often not brightly-lit and often challenging to get wireless network coverage everywhere. This poses lots of problems for researchers.
-- They should be acceptable; bringing healthcare home with us doesn't mean we want to turn our homes into hospital and it definitely doesn't mean we want people spying on us!

Since 2013 this has been the SPHERE vision and we have worked with scientists, doctors, engineers and more than 200 members of the public to achieve the project's initial goal of creating a cheap sensor system that can be installed in a home. More than 30 people have had the experience of living with the sensors over periods from days to months and, by the end 2017 we expect more than 200 people will have had SPHERE sensors in their own home, in many cases for months.

Although the first-generation system was only completed in late 2016 and at the time of writing is still under test in the first &quot;pilot&quot; homes, the system is already moving into real patient applications - we are applying for ethical permission from the NHS to use SPHERE for patients recovering from surgery. Later in 2017 we will be applying for ethical permission to use SPHERE with a group of dementia patients.

The initial testing of the sensor system has gone well but, especially as we start to think about large scale use of the SPHERE system across potentially hundreds or thousands of people, the team have learnt a lot from the early pilots and have some priorities for significant improvements:

1. The SPHERE video system needs to be better at evaluating the quality of someone's movement, such as getting out of a chair, even when the view of the person is blocked by items of furniture. Evaluating quality of movement is important in physical and mental health conditions.
2. The SPHERE wristband lasts for over a month on a single charge, however we want to remove as far as possible the need to charge it at all, because the more ill someone is, the less likely they are to do this.
3. Digital data gathered from sensors needs to be turned into understanding for doctors; this is especially difficult in a home environment because every home and every household is different. 

These are major research issues and will be the focus of the technology parts of the SPHERE programme, while the clinical parts move forward with patient populations.

The NHS itself has recently said: &quot;if the UK fails to get serious about prevention then recent progress in healthy life expectancies will stall, health inequalities will widen, and our ability to fund beneficial new treatments will be crowded-out by the need to spend billions of pounds on wholly avoidable illness.&quot;",,"Like many developed nations, the UK faces huge challenges dealing with long term health conditions such as diabetes, dementia, depression, COPD, arthritis and asthma. Whether seeking to understand the mechanisms, trying to avoid onset, creating new therapies, or supporting self-care of these conditions, we need non-intrusive technologies able to capture data on causes, symptoms and exacerbations over long periods of time.

SPHERE has been developed in partnership with health professionals, specifically with the intention of producing a sensor system that can be used by the NHS. Early SPHERE work is already funded with clinical cohorts e.g. to characterise recovery after Orthopaedic (Bristol) and Cardiovascular (Sheffield) surgery. There are on-going discussions with leading centres on subjects such as personalised behaviour change programmes, physical activity interventions in patients with COPD and new forms of free-living gait analysis. 

Local government has a statutory responsibility for public health and SPHERE has close engagement with Bristol City Council, as described in its letter of support.

SPHERE was designed from the outset as a tool for clinical research, helping us understand what causes these illnesses and develop new treatments. SPHERE is already collaborating with many health researchers but as it now demonstrates its applicability through its 100 Homes trial, it will increasingly be working for impact in partnership with major UK institutes (UK Biobank, the Dementias Research Institute, Inst. for Biomedical Informatics) and continuing discussions with GSK around digital clinical trials.

SPHERE has a stated objective to engage with policy-makers e.g. with Public Health England (the Director spoke at their annual conference) and one of its Directors sits on SPHERE's advisory board. 

Many health issues are global concerns, and indeed the SPHERE director has been invited to speak at the British Embassy in Tokyo in Feb 2017 to an audience of Japanese policy makers and academics on the subject of dementia (
countries such as Japan and Singapore have the world's &quot;oldest&quot; populations). 

With Europe's 2nd largest medical technology industry, UK companies, large and small, are well-placed to address this need. SPHERE has met many SMEs (including Cascoda, Cascade3d, Pumpco, Folium Optics and Loc8tor) to assist them in their approach to this market. SPHERE will partner with organisations such as the Digital Catapult (see their letter of support) to help many more UK companies understand how to address those needs with new technologies, how to develop more reliable in-home wireless networks, develop analytics capabilities for domestic data, etc.

SPHERE is also developing relations with corporates including IBM, Toshiba, BT, Sony, TI, Dyson, BT &amp; Jaguar Land-Rover. UK employers such as Loc8tor (see letter), Janssen Healthcare Innovation, McLaren Technology Group, IBM &amp; Toshiba all have on-going research or stated intentions for collaborative research with SPHERE. 

SPHERE will also work with standards bodies such as IETF (see letter from the 6TiSCH working group chair) to influence the evolution of wireless network standards for health applications. While the potential health benefit, the market and the industrial potential are all quite apparent, the adoption of pervasive in-home technology for health delivery runs the risk of creating a range of new issues, including around security, trust and privacy. By conducting its research in close partnership with the public (SPHERE was 1 of 3 finalists in the UK Public Engagement Awards), SPHERE will seek to disseminate the issues to the public in an open way - this indeed is one of the fundamental purposes and responsibilities of University research. Furthermore, within the programme proposed herein, SPHERE will particularly address the extent to which some populations might be differentially advantaged or disadvantaged by this type of health delivery"
11,3EBA1FF2-7163-44C2-9044-EF032CB5484B,Citizens 3.0 - Who should control the powerful technologies that govern our lives?,"Technologies that have the capacity to seriously impact upon individuals and democratic politics, through processes such as surveillance, big data and artificial intelligence, develop apace. However, protections against the potential harms brought by such technologies have not yet provided a coherent defence. Most particularly it seems that privacy may no longer be the most accurate paradigm to understand the harms of these technologies. The aims of this project are to communicate a new and important analysis of such powerful technologies, one which provides a language with which to justify making citizens the meaningful authors of the rules
that govern these technologies. 

For example, combining mass data collection and big data analysis, it is alleged that Cambridge
Analytica, a private data firm, recently interfered in the US presidential election of 2016 (Grassegger,
Krogerus, 2017, Cadwalladr, 2018). Other less well publicised companies operate within this area as well (Polonski, 2017). The extent of the influence these firms have is not yet clear. However, what is clear is the capacity to gather, process and analyse personal data on a staggering scale - Cambridge Analytica has psychological profiles of 230 million US citizens' data (Cadwallr, 2018) - and to use deep algorithmic networks to then manipulate the information people receive in order to influence their democratic choices (Polonski in Thorsen et al 2017). Manipulation is not new to politics, but what is new here is the cumulative and unrestrained power that these technologies have, and the opacity with which, and by whom, they are operated. 

Into this current scenario comes a rapidly developing Artificial Intelligence (AI) sector, driven primarily by private companies like Google's DeepMind and Facebook's AI, that has the capacity to programme its own algorithms, which can, as an example, conduct heuristic analysis of our street behaviour, or behavioural modelling of our personal relationships, in order to produce results beyond the comprehension of human operators (Bostrom 2014). Not only will we be unsure who is collecting our data in order to influence elections, model our behaviour in the street, or analyse our online relationships, but it may be unclear just what is doing this, and why it is doing it. Crucially, the harms potentially caused by this do not hinge on whether they intrude upon our privacy. Rather, what seems important now is whether we as individuals and as a society have any say in, or understanding of, the rules which govern technologies that can be so influential on our democracies and our behaviour. Drawing on a conception of freedom which considers this type of power as harmful to freedom and democracy, my project will provide a coherent framework to justify democratically controlling and harnessing the power contained in these technologies. Contemporary republicans thinkers (Pettit 1997, Skinner 1998, 2008) have in recent decades promoted an alternative view of freedom. Drawing on neo-roman republican thought, they have shown a different understanding to the 'non-interference' conception of freedom which underpins the individualised and rights based understanding of liberty and privacy. Instead, they tell us, if we do not have a proper understanding of the laws that govern us, we do not fully know the consequences that our behaviour may bring from authorities, and we are not in some way the authors of the rules that control our lives, then, we are exposed to arbitrary power in a way that is 'dominating'. In this situation, regardless of our formal rights, we are 'unfree'. My post-doctoral fellowship project will apply this conception of freedom, self-government and power to technology, and posit the problem of technology from a new angle, beyond privacy and data protections, to ask: if we as individuals are not the authors of the rules that govern these remarkably powerful technologies, are we really free?",,
12,C471AF13-6690-499C-A2A4-E62F4D51589C,Developing synergies between transient astronomy and early medical intervention,"The application of artificial intelligence (AI) to healthcare will revolutionise medicine and patient care over the coming decades. The wealth of data being collected by medical professionals keeps growing and they need significant help to quickly understand this information for their key diagnostic tests. In particular, can this wealth of digital data be used to automatically detect possible problems early, and thus alert doctors for closer inspection?

Astronomy is also experiencing a data revolution driven by STFC experiments like LIGO and LSST. These experiments are opening up the time domain in astronomy and in the near-future astronomers will be deluged by millions of transient events a day; LSST will produce on average hundreds of transients per second! The prioritisation of so many events will be key to our success and astrophysicists are already developing and applying AI technology to help find the most interesting events within the ocean of &quot;ordinary&quot; transients. 

This project brings these two problems together by applying the knowledge and expertise developed in transient astronomy to the early automated diagnosis of medical problems. This project is at the forefront of interdisciplinary research bring world-leading cardiologists from King's College Hospital together with the best transient astrophysicists in the UK. Together, we hope to find early diagnostic signatures in detailed time-series patient data e.g. ECGs. This pilot activity will be used to prime-pump future investigations and grow the number of scientists engaged in such impactful research.",,"This proposal will provide a range of impacts:

1. Development of interdisciplinary research on two cardiology data science projects to create early AI diagnostic tests for cardiac arrest. While still at an early stage, we would hope the outcomes of these projects would inform and influence long-term healthcare providing quicker alerts for doctors to make time critical decisions. Such societal impact is at the heart of the UK Industrial Strategy which seeks to put the UK at the forefront of AI in healthcare.

2. We will engage with a larger range of scientists, doctors and industrialists across the southeast to grow research and innovation capacity for precision healthcare and early diagnosis. These innovators will then be encouraged to explore their ideas through two dedicated hackathons provide rapid prototyping of possible solutions. These solutions will form the basis for future proposals and investment, gain both societal and commercial impact."
13,C340D031-10B4-4652-A9DD-0491CD7F2B75,Novel optimization framework for real-time automated radiation therapy,"The World Health Organization estimates that over 8 million people die of cancer every year, around 70% of them in low and middle income countries. Radiation therapy (RT), a process whereby x-ray or particle beams are used to kill specific cells in cancer patients, is one of the most commonly used and cost effective ways to help treat cancer patients. It is estimated that over 50% of all cancer patients may benefit from receiving RT during the course of their treatment, either on its own or in combination with surgery, chemotherapy, hormonal therapy, or immunotherapy. However, the delivery of radiation therapy treatment plans is time consuming, involves cumbersome treatment planning systems (TPS), is expensive both in terms of personnel and infrastructure, and can be hampered by inaccurate computational models. This makes the delivery of high-quality and affordable treatment a challenging task globally, but also one which disproportionally affects low and middle income countries. Further, while the availability of RT centres in North America, Europe, Japan and Australia is generally adequate to cover current needs, similar coverage remains poor in Africa (34% of estimated need covered) and in the wider Asia-Pacific region (61%). As a consequence, the majority of the global population does not have sufficient access to appropriate cancer treatment. Unless addressed, this situation is expected to worsen further given that cancer incidence rates are projected to grow significantly in low and middle income countries over the next decade. As such, increasing the availability of high-quality cancer treatment, and of RT in particular, is recognized as a key global and societal challenge. Within this context, making RT more widely available, increasingly accurate, faster, and more cost effective, will play an important part in addressing this challenge.

The delivery of high-quality radiation therapy relies on accurate treatment planning systems to create appropriate radiation treatment plans (TP) across a spectrum of different cancer types. Optimizing these TP can require considerable computational resources and is often personnel-intensive. In this project we will develop a fully-automated treatment planning system prototype based on advanced optimization techniques and remote supercomputing, thus addressing an important difficulty in deploying RT systems in challenging and remote environments. The system will make use of a range of cutting-edge computational and machine learning techniques to optimize the efficiency and robustness of treatment planning systems, with the view to reduce infrastructure and personnel costs in radiation therapy centres, and to provide more flexibility for use where expertise and large-scale computational resources may not be readily available.",,"This project aims to demonstrate the feasibility of an automated, real-time treatment planing system based on cloud computing for remote, scalable deployment. Our main objective is to address the needs for affordable, robust and high quality treatment planing systems across the globe, but in particular in low and middle income countries, where the availability of such systems is lacking and the population is underserved in terms of cancer treatment therapy. This project is set within the more general theme of addressing the lack of availability of cancer treatment in developing countries, and will serve as a pump-priming demonstrator to enable the authors to participate in the upcoming Global Challenges Research Fund scheme under the umbrella of developing medical LINACs for challenging environments. Within this larger project we aim to address specific software needs related to radiation therapy treatment and remote supercomputing.

Further, the demonstration of a robust and fully-automated treatment planning system will also impact efforts in the UK and in other developed nations in terms of optimizing clinical systems based on the latest information technology to improve patient care and deliver novel adaptive therapy solutions, as well as to help lower the cost of such systems. The cost aspect in particular is important to help address future challenges related to ageing populations and to rising costs in the healthcare sector more generally. As such, we believe this project has a substantial scope to be of impact both economically and from the societal point of view."
14,3E892758-2044-426A-B020-31858AFEFCAA,eNeMILP: Non-Monotonic Incremental Language Processing,"Research in natural language processing (NLP) is driving advances in many applications such as search engines and personal digital assistants, e.g. Apple's Siri and Amazon's Alexa. In many NLP tasks the output to be predicted is a graph representing the sentence, e.g. a syntax tree in syntactic parsing or a meaning representation in semantic parsing. Furthermore, in other tasks such as natural language generation and machine translation the predicted output is text, i.e. a sequence of words. Both types of NLP tasks have been tackled successfully with incremental modelling approaches in which prediction is decomposed into a sequence of actions constructing the output.

Despite its success, a fundamental limitation in incremental modelling is that the actions considered typically construct the output monotonically, e.g. in natural language generation each action adds a word to the output but never removes or changes a previously predicted one. Thus, relying exclusively on monotonic actions can decrease accuracy, since the effect of incorrect actions cannot be amended. Furthermore, these actions will be used to predict the following ones, likely to result in an error cascade.

We propose an 18-month project to address this limitation and learn non-monotonic incremental language processing models, i.e. incremental models that consider actions that can &quot;undo&quot; the outcome of previously predicted ones. The challenge in incorporating non-monotonic actions is that, unlike their monotonic counterparts, they are not straightforward to infer from the labelled data typically available for training, thus rendering standard supervised learning approaches inapplicable. To overcome this issue we will develop novel algorithms under the imitation learning paradigm to learn non-monotonic incremental models without assuming action-level supervision, relying instead on instance-level loss functions and the model's own predictions in order to learn how to recover from incorrect actions to avoid error cascades. 

To succeed in this goal, this proposal has the following research objectives:

1) To model non-monotonic incremental prediction of structured outputs in a generic way that can be applied to a variety of tasks with natural language text as output

2) To learn non-monotonic incremental predictors using imitation learning and improve upon the accuracy of monotonic incremental models both in terms of automatic measures such as BLEU and human evaluation. 

3) To extend the proposed approach to structured prediction tasks with graph as output.

4) To release software implementations of the proposed methods to facilitate reproducibility and wider adoption by the research community.

The research proposed focuses on a fundamental limitation in incremental language processing models, which have been successfully applied to a variety of natural language processing tasks, thus we anticipate the proposal to have a wide academic impact. Furthermore, the tasks we will evaluate it on, namely natural language generation and semantic parsing, are essential components to natural language interfaces and personal digital assistants. Improving these technologies will enhance accessibility to digital information and services. We will demonstrate the benefits of our approach through our collaboration with our project partners Amazon who are supporting the proposal both in terms of cloud computing credits but also by hosting the research associate in order to apply the outcomes of the project to industry-scale datasets.",,"- Economy

The two applications we will focus on in the project, natural language generation and semantic parsing, are key technologies in a variety of commercial products which require generating and understanding language. In particular, personal digital assistants such as Google Now, Microsoft's Cortana, Amazon's Alexa and Apple's Siri are used by millions of users at home or on their mobile devices and are of great importance to these companies since they act as gateways to many of the services and products offered by them. 

- Society

Personal digital assistants and natural language interfaces are used by a large number of users. Thus improving technologies of language generation and semantic parsing through non-monotonic incremental language processing is likely to affect these end users by improving their experience. We will explore this during the research visit of the RA at Amazon and test our approach in the context of Alexa.

- Knowledge

The project aims to address a fundamental limitation in an approach successfully applied to a variety of natural language processing tasks. Thus we anticipate that we will publish our results in high profile natural language processing conferences. Furthermore, we will accompany the paper publications with open source implementation of our approach on the project github repository. 

- People

The project will have a positive impact on the careers of both the PI and the RA. It will enable the PI to build on his success and expertise he has developed in incremental language processing using imitation learning, and thus solidify his position in the field while simultaneously addressing a fundamental shortcoming in the approach. An EPSRC first grant would be of great significance to the PI as it will be his first time proposing and delivering a project on his own, which will provide him with useful experience and strengthen his profile in applying for further funding. Finally, the named RA has been working in language generation throughout his career and most recently with the PI in applying imitation learning to this task achieving state-of-the-art results."
15,C4C85A62-B372-40D3-8276-978F45DDBE64,Creative Media Labs: Innovations in Screen Storytelling in the Age of Interactivity and Immersion,"Storytelling is central to human activity, one of the ways in which we make sense of the world. The screen industries are the latest in a long line of technologies and cultural practices committed to the creation of stories. Film, TV, video, computer games and other interactive media now tell stories digitally. But digital technologies are changing rapidly, enabling new modes of creation, new approaches to storytelling, new experiences for audiences and users. How can the screen industries keep pace with such change? How can they make the most of the new opportunities available to them? How can they develop the skills necessary to engage with these new technologies? How can they create with those technologies in ways that are exciting, commercially viable and capable of generating significant economic growth? These are some of the questions that Creative Media Labs (CML) seeks to answer, as it enables innovations in screen storytelling in the age of interactivity and immersion.
 
The focus of CML is the considerable cluster of screen industry enterprises in the Yorkshire and Humber (Y&amp;H) region. The partnership aims to enable this regional cluster to become the UK centre of excellence for the next generation of digital storytelling. This is an established creative industries cluster that has been earmarked for significant support through Screen Yorkshire's (SY) Growth Plan, backed by the British Film Institute's (BFI) Creative Clusters Challenge Fund, and showcased in the Creative Industries Sector Deal document as one of five &quot;prominent creative industries cluster projects&quot;. SY, the BFI and University of York (UoY) have come together in a collaboration that blends world-leading research on digital storytelling with national strategic vision and unparalleled regional industry nous. 

Clustering is key to the development of the contemporary screen industries, and clusters come in many shapes and sizes. With key initiatives across its five major cities, the Y&amp;H region saw the fastest rate of screen industry growth in the UK in 2009-2015. It is home to one of only three ITV production centres, producing around 500 hours of TV annually; True North, the biggest factual producer in the North of England, now owned by Sky; Warp Films, probably the most important out-of-London film company in the UK, and winners of multiple BAFTAs; Rockstar, one of the largest games developers in the world; Sumo Digital, one of the fastest growing games companies in the UK with over 300 staff; Revolution Software, developers of the hugely successful Broken Sword series; Viridian FX, one of the largest VFX houses in the North of England; and Church Fenton Yorkshire Studios, a major production facility used by Mammoth Screen for ITV's Victoria. There is also a wealth of micro businesses working in the sector. 

Creative Media Labs will build a sustainable, collaborative R&amp;D partnership around this regional screen industries cluster, its numerous MSMEs and branches of large creative enterprises. Its core delivery partners are UoY, SY and the BFI; the key local authorities, enterprise partnerships and universities in the region are on board; so too are investors and several leading national trade associations, organisations and creative enterprises. Co-creation and collaborative working will be at the core of what we do.

UoY has an excellent track record in multi-disciplinary research, with huge investment in creativity, across the arts, humanities and sciences - a combination reflected in the multi-million pound Digital Creativity Labs. There is an extensive pool of research expertise in digital storytelling, from writing, through media embodiment, to development of underpinning technologies. By identifying industry-led challenges, this expertise will be shared with the Y&amp;H screen industries cluster in ways that will enable us to fulfil our ambition to establish the region as the UK centre of excellence for digital storytelling.",,"The Creative Media Labs (CML) Partnership will act as a regional hub through which to invest strategically in R&amp;D that is capable of having a transformational impact to grow the screen industries economy in Yorkshire and the Humber. Our goal is for the Y&amp;H cluster to become the UK centre of excellence for immersive and interactive storytelling, and for that cluster to be economically sustainable and financially healthy. Given the nature of this AHRC programme, CML is all about impact, and we have established a number of pathways to achieving, measuring and demonstrating that impact.

In order to ensure that the products, services and experiences produced through our R&amp;D programme are thoroughly innovative and capable of commercialisation, we have devised a series of funds to support different types of R&amp;D activity in different ways. These will enable the development of both small and large projects, working with specific companies on specific R&amp;D challenges, ranging from student internships, through three-year PhD projects, to collaborations between teams of academic researchers and industry practitioners. Our funds are also designed to enable both slow-burn, extensive R&amp;D developments, and ones that require immediate and swift intervention. 

One fund will support industry partners who identify collaborative R&amp;D challenges on which they want to work; another will support post-doctoral research and knowledge exchange; a third will enable the flexible deployment of researchers to projects that will benefit from their skills and expertise. Some of these funds will come from the AHRC grant, some from the University of York contribution, and some from CML partners. We have not committed large sums of money to long-term Co-Is, preferring a more flexible and dynamic model that can match researchers to challenges as they arise. 

The BFI and UoY will also fund economic and policy analysis and business modelling, and the development of the creative and technical skills necessary for the production of high-quality digital storytelling. The criteria we will apply when evaluating the viability of any proposed partnership activities are designed to ensure that such activities have the capacity to generate both creative innovations and economic growth. That growth potential will be defined in particular in terms of the creation of new skills, jobs and businesses, and the scaling up of existing businesses.

We have developed robust systems for gathering evidence about partnership activities, and monitoring, understanding and evaluating the impact of those activities. We will continue the work of mapping the screen industries economy and analysing the market in the Y&amp;H region begun by our key partner Screen Yorkshire. This work will be led by the 0.2 FTE Co-I based at the BFI, our other key partner. One key task will be to define what we mean by the 'screen industries' (film, TV, games and other digital media), a term in wide use in industry and policy circles, but one that does not map easily onto Standard Industrial Classifications of industrial sectors. In terms of measuring growth, we will develop a bespoke Customer Relationship Management system to track business interactions and company information. 

In terms of measuring success, our overall goal will be to demonstrate the impact of partnership activities on the strength of the screen industries economy in the Y&amp;H region. To do this, we will draw on the analytical categories and reporting processes developed by Nesta et al., to establish a set of Objectives and Key Results (OKRs) for each individual R&amp;D project and the CML Partnership overall. Working with a bundle of OKRs will enable us to measure success in a flexible and balanced manner and make investment decisions based on robust evidence. In terms of reporting, monitoring and evaluation, we have developed a rigorous set of processes involving our Executive Board, Delivery Team and Partnership Steering Board."
16,9D34D1FA-2338-4112-9096-7FEE2EC625E8,Shared Autonomy via Robust Task Planning and Argumentation (SHARPA),"The overarching objective of this project is to endow autonomous systems with advanced decision-making capabilities and collaboration skills. We aim to build artificial systems that, in real-world environments, are capable of reasoning about high-level goals specified by human operators and formulating, in collaboration with them, a course of actions to successfully achieve such goals. Strategic reasoning and fluid teaming are fundamental skills of cognitive systems: they are needed in a variety of situations, from day-to-day tasks such as assisting humans in household chores, to extreme missions, such as space exploration. The techniques that we propose are general and can be used to support both robotic systems and software agents. We choose disaster response operations where unmanned aerial vehicles (UAVs) assist emergency responders as our demonstration arena. In this domain, in fact, it is crucial for the UAVs to think strategically to pursue goals efficiently and to act in concert with the human operators who are ultimately in charge of critical decisions. 

The primary objective of the project is broken down into two strands. The first is to equip artificial artefacts that operate in real-world settings with the ability to reason about themselves and the world around them to determine plans for achieving high-level goals efficiently and robustly. Planning is a key component of intelligence and one of the most traditional fields of artificial intelligence (AI). Planning has achieved impressive results in idealised settings where the world is deterministic, and actions are instantaneous. However, planning in real-world environments in which temporal constraints and uncertainty cannot be ignored remains very challenging. Currently, no single temporal planner exhibits strong performance and, at the same time, handles all the features needed to represent practical problems. This project aims to contribute to filling this gap. On the one hand, we will investigate how different representations of temporal planning problems impact the performances of existing planners and whether there is one representation that facilitates efficient and flexible reasoning. On the other hand, we will formulate efficient algorithms that support advanced features of temporal reasoning such as required concurrency, timed transitions and uncontrollable action durations. 

The second strand of this project emerges from the observation that, in any complex real-world operations, artificial artefacts rarely operate in isolation from humans. For the humans and the agents to team up in a fluidly and trustworthy, it is crucial that the agents' decision-making is intelligible to the human operators and also receptive to inputs from them. In this project, we explore the idea that planning can play a pivotal role in achieving intelligibility in autonomous systems. We consider two different facets of intelligibility: ex-post intelligibility, or explainability, whereby the system can exhibit the information and the logic that it has used to arrive at its decisions; and ex-ante intelligibility, or transparency, whereby the system exposes how it operates to a human operator in such a way that the operator can intervene and negotiate with the system a different course of actions. We investigate how planning and computational argumentation can be blended to achieve both ex-post and ex-ante intelligibility. Argumentation refers to a set of techniques for evaluating claims by considering reasons for and against them through logical reasoning. Argumentation techniques based on planning will empower the agent with the capacity to exhibit arguments in support of its decisions as well as to negotiate with the operator a change in the plan if needed. 

Providing advances in the planning and collaboration skills of autonomous systems would benefit research in planning, AI and robotics and, more crucially, promote their broad adoption in real-world contexts.",,"The goal of this project is to equip artificial artefacts, such as robots and software agents, with the ability to autonomously undertake sophisticated tasks and, when needed, to collaborate with humans in a natural and mutual intelligible way.

The potential for society and economy of robotics and autonomous systems is well understood and has been recently highlighted by many studies. The report &quot;Disruptive technologies&quot; by McKinsey identifies in advanced robotics and autonomous systems two of the main technologies that &quot;have the potential to truly reshape the world in which we live and work&quot; and estimates that advanced robotics could generate a potential impact of $1.9 - 6.4 trillion per year by 2025. The Royal Society report &quot;Robotics and autonomous systems&quot; states: &quot;Robotics and autonomous are of immense societal impact, pervading all areas of society including medicine, transport, and manufacturing.&quot;

The use of both software agents and robotic artefacts in everyday life has risen sharply, and we see increasingly more examples of their use in society with several commercial products already on the market. As interaction with humans increases so does the demand for sophisticated capabilities associated with deliberation and high-level cognitive functions. WP1 of my proposal responds to this need and deals with endowing artificial systems with higher level cognitive functions that enable them to reason and act in complex environments. Planning is, in fact, considered central in the spectrum of capabilities required for autonomy.

The Royal Society report also states: &quot;Fully autonomous robotics can be problematic. As a result, there is a shift from isolated decision-making systems to those that share control, with significant autonomy devolved to robotics and autonomous systems, leaving end-users to make only high-level decisions. Shared autonomy will demand the closing of the semantic gap between human and machine.&quot; WP2 of my proposal deals with shared autonomy and aims at contributing to close &quot;gap between human and machine&quot; by combining planning and argumentation to give rise to fluid and transparent teaming between human operators and artificial artefacts. The use of these two techniques will empower the human part with the ability to enquire into the behaviour of the machine and directly argue with it to better understand its line of reasoning and change it, if appropriate. 

The need for an intelligible AI has gained a lot of traction in academia, industry and the public sector recently, and has been highlighted by several official reports. The House of Lords report &quot;AI in the UK: ready, willing and able?&quot; states: &quot;We believe that the development of intelligible AI systems is a fundamental necessity if AI is to become an integral and trusted tool in our society.&quot; The techniques at the core of this proposal will help in achieving more intelligible autonomous agents, impacting a variety of AI artefacts, from virtual characters that interact with users online to autonomous cars to service robots that assist people at home.

WP3 deals with an application of clear importance for society: supporting disaster response operations via autonomous UAVs. Humanitarian disasters cost lives and can cause huge setbacks. The UK Government's Humanitarian Policy emphases how the use of innovative technology can have a considerable impact on improving the efficiency and reach of humanitarian assistance. When a disaster strikes, UAVs can provide support with risk assessment, mapping, planning and search-and-rescue in the affected region. Currently, two of the main barriers to the use of UAVs for disaster relief are that the availability of expert pilots to teleoperate them and the lack of transparency in the behaviour of these UAVs. This project deals with both these problems as it aims to develop autonomous UAVs that do not require to be continuously piloted and whose conduct is decided in concert with the human operators."
17,68332534-B374-4F07-9146-E56AA18EA394,Learning to Efficiently Plan in Flexible Distributed Organizations,"Teams of robots are expected to revolutionise industry and other other parts of society. However, decision making in such so-called multiagent systems (MASs) under uncertainty is computationally very complex. The decentralized partially observable Markov decision process (Dec-POMDP) framework facilitates principled formulation of such decision making problems, but currently there are no scalable solution methods that provide guarantees on task performance. To simplify coordination in MASs, agent organisations assign an abstracted, easier problem to each agent. Typically only the most rigid organisations, which completely decouple the agents, have led to clear computational benefits. However, these come at the expense of task performance: full decoupling means that agents can no longer collaborate to divide the workload. 

This project will focus on flexible distributed organisations (FDOs) for Dec-POMDPs, which restrict considered interactions to spatially nearby agents without imposing full decoupling. Currently no scalable decision making methods with guarantees on task performance exist for FDOs: the main goal of the project is to develop such methods along with the theory that supports their formalisation. To accomplish this goal, it will investigate the use of deep learning techniques to learn representations of 'influence' in FDOs and use those representations to develop novel planning methods. If successful, this will provide the proof-of-concept that learned influence representations can enable principled decision making in large-scale MASs. This will be the basis for a larger research program investigating such influence representations for different forms of abstraction and will spark applied research that investigates deployment of the developed algorithms in real robotic teams.",,"This is a relatively short project that pursues basic research in the field of AI. As such, the expected short-term impact will mostly be in the form of knowledge (developed techniques), the scientific output (articles and software) and the influence on research questions picked up by peers (of which citations to those output are an indication). These academic impacts are a crucial link in the pathways to longer-term impact on society and economy. In particular, the project aims to make simulation-based planning dramatically more efficient, and thus more effective or even feasible in cases where it was not before. This could have a great mid-to-long-term impact since the potential of application of these methods is huge; they are advanced versions of model-predictive control (MPC) methods which are widely applied in industry. The recent application of simulation-based planning in the mastering of the game of Go is likely to attract attention in many areas. 

Another reason that simulation-based planning methods have not seen more application yet is their large computational costs. However, this is precisely what we address for the class of problems that admit flexible distributed organizations. For such problems, the proposed research will have a crucial impact in terms of just making it feasible at all to apply simulation-based planning to these domains. For instance, we expect that this could be the case for robotic teams collaborating in future factories or warehouses, but also for other problems that can be modelled as spatial task allocation problems, such as dispatching of emergency vehicles, or for applications areas such as optimizing traffic control by simulating large traffic networks, optimizing routing policies by simulation of communication networks, optimizing UAV patrolling policies in security domains or for law enforcement, etc. 

Therefore, a large part of the longer term impact will be of the economic kind (new companies, improved products and services) with, given the possible application areas, the potential to improve quality of life. The first companies to adopt these techniques will be logistics and manufacturing companies since these already are starting to use robotic teams. Many of the other aforementioned applications (UAV patrolling etc.) will probably need more time and are likely to be tackled by AI and robotics start-up companies. The discipline of high-performance computing (HPC) is involved in optimizing both hardware and software, e.g., to make simulations maximally efficient. The basic idea put forth in this proposal---that one can mostly use 'local model simulations' that consider only a small subset of the variables considered in 'full model' simulations, while still converging to the same behaviour---could potentially make an impact on this community, and thus may affect the design of computers dedicated to doing simulations. 

I will contribute to the impact of the developed methods by contributing to awareness (interacting with potential industrial partners, as well as with academic peers, and organising a workshop), clarifying potential gains (an expected output of the research), and facilitating adoption by releasing open source software."
18,10DF9A79-3253-48B7-AD3B-6EB10D3900F4,Active Learning for Computational Polymorph Landscape Analysis,"The proposed research will develop advanced computational methods for predicting the possible crystal structures of drug-like molecules. The work is motivated by the importance of anticipating the occurrence of polymorphism, where a molecule can crystallise in more than one crystal structure, depending on the conditions used for its crystallisation. In the context of pharmaceutical materials, we must know when polymorphs exist that we have not yet characterised. These present a risk related to property control; a change in crystal structure can dramatically alter important properties of a crystalline drug, affecting its processing, tabletting and bioavailability. Hence, there has been a huge investment in crystal structure prediction methods. Predicted structures could guide experimental screening - where to focus effort and, in the long run, what experimental variables to vary to maximise likelihood of isolating new structures. 

Structure prediction has progressed impressively but still not made the expected impact on assessing risk. A root cause is the problem of over-prediction. Current methods always predict many competing crystal forms, most of which are never observed. Accordingly all candidate drug molecules appear to have significant uncertainly as to expected extent of polymorphism and this adversely impacts risk analysis. 

The root of the problem is that the underlying lattice energy surface, on which local minima represent possible structures, is extremely complex and current methods for predicting polymorphism do not provide a sufficiently detailed description of this energy surface. We will develop the use of statistical learning methods to guide crystal structure calculations to efficiently map out the global features of lattice energy surfaces in a way that is not possible using current computational methods. 

Two lines of study are proposed: to improve the fidelity of energetic assessment and, more importantly, to map the energy landscape of structures more globally. A starting point is to develop advanced statistical learning methods for correcting approximate computational models that are used for assessing lattice energies of predicted crystal structures. Our goal is to reduce the uncertainty in ranking of predicted structures at a controlled computational cost. We will then move to a completely unexplored problem: learning more detailed features of the lattice energy surface, such as the depth, shape and connectivity of energy basins. Key to this work is the development of multi-fidelity (multiple models of known accuracy and computational cost) and multi-objective Bayesian optimisation approaches to make use of the hierarchical of energy models (a series of approximate energy models with known, ordered accuracy) used in crystal structure prediction.

The objective is to judge the thermodynamic robustness and kinetic accessibility of individual predicted crystal structures and address the polymorphism over-prediction problem. This is completely new in the area and can be transformative in guiding experimental screening.

Thus, the vision is that active learning methods will guide the computer simulations that, in turn, will provide guidance to experimental polymorph screening.",,"The crystal form that a drug molecule adopts has an important impact on its solubility, dissolution rate (and bioavailability), shelf-life and mechanical properties. A complete understanding of the possible crystal forms of a drug is a regulatory requirement for pharmaceutical registration. Currently, pharmaceutical companies rely on high throughput screening of different crystallisation conditions in the hope of identifying all stable polymorphs and making a choice of which to formulate, but with no guarantee of success. A particular risk is a late-appearing stable and hence insoluble solid form, while a change of form to a more soluble material can have toxic effects. In the case of Ritonavir, an HIV drug already on the market where polymorphism suddenly became apparent, Abbott Laboratories had to reallocate over 600 scientists onto the case for more than 1 year. Abbott lost an estimated $250 million in sales as well as hundreds of millions of dollars to recover the original, patented polymorphic form. Consequently, there are now significant requirements concerning polymorphism that are imposed by healthcare regulatory bodies before a solid form drug can be marketed. Around 90% of prescribed drugs are essentially administered in the solid form and so the pharmaceutical industry is actively searching for approaches to accelerating the polymorph screening process. 

Computational methods have been developed to supplement experimental polymorph screening by applying algorithms to find all energetically stable ways that a molecule can be packed into a crystalline structure. These crystal structure prediction (CSP) methods have enjoyed rapid development in recent years, but have yet to transform the field of polymorph screening. This project aims to develop the computational methods that could be transformative in how CSP is used to assess risk of polymorphism of drug molecules by developing statistical learning methods to guide the simulation and exploration of the energy surface that describes all possible crystal structures. The primary beneficiaries of this research are therefore industrial drug preformulators and formulators, and ultimately patients. The goal of developing better drugs whose solid form selection is guided by predictive computational methods, leading to pharmaceutical materials that are readily processed, tableted and consumed, contributes to better public health and a more productive UK.

The results of this feasibility study will benefit all scientists involved with crystallisation phenomena and have practical application in guiding solid form choice in drug formulation. 

Hierarchical experiments are also commonplace in many other areas of science and technology, where computational or physical data can be collected at differing levels of cost and accuracy; for example, drug development through laboratory, pilot plant and manufacturing scales, materials development with multi-scale mathematical modelling, and epidemiological studies with multiple different computational models of disease spread (e.g. compartmental and agent-based). Hence the methods developed on this project for the construction and exploitation of hierarchical statistical learning models will have impact in scientific areas well beyond crystal structure prediction. 

Impacts of the project include: the training of two postdoctoral research scientists in both high-level research skills and multi-disciplinary working; creation of knowledge - new methodologies in statistical learning and new insights into crystal structure prediction; economic impacts for the pharmaceutical industry, through better risk assessment of polymorphs and societal, through faster development and regulatory approval of new medicines."
19,D310E9DE-F0AE-43B4-ACD5-E3F564C1ADC7,"AutoTrust: Designing a Human-Centered Trusted, Secure, Intelligent and Usable Internet of Vehicles","Vehicles are increasingly connected, to each other (vehicle-to-vehicle), to the underlying road and service infrastructure (vehicle-to-infrastructure) and, especially, connected to the people who use them, often via smart devices (vehicle-to-device). This emerging Internet of Vehicles (IoV) offers tremendous opportunities in transforming our transportation system. Real-time data about traffic allows more efficient traffic flows, increasingly autonomous vehicles promise greater safety and apps that seamlessly organise multi-modal journeys enable greener approaches to transportation, including car sharing or ride sharing schemes.

The IoV can be seen as a microcosm for the Digital Economy. However, a key element of the IoV, often overlooked, is the citizen that should be central to the system and the prime motivator for its development. In such an approach, the IoV is focused around the needs of the individual to connect, in person, with a range of entities from families to colleagues to services, where physical distances must be overcome in timely ways to enable these connections. The foundation of the IoV is also, like the web economy more generally, founded on personal data. Data sharing on the Internet is used mainly as a currency in the sense that it could be replaced with money. Within the IoV, however, personal data is far more mission critical to the efficacy of the entire system: using personal travel plans enables improved traffic flows; storing relevant medical records on a vehicle allows better on-scene support during accidents, and learning a driver's interests and routines creates the opportunity for giving relevant contextual information. While this promises better safety, reduced carbon and increased travel efficiency, the IoV's reliance on personal data is also potentially its Achilles' heel. Large-scale sharing of data is constantly shown to be vulnerable to massive identity thefts (eg Sony's user database being hacked) &amp; infrastructure threats (Stuxnet worm). Furthermore, connected devices themselves can be vulnerable to repurposing (eg Mirai DNS Denial of Service attack).

The challenges to design an IoV that is human-centered and as effective and efficient as imagined are complex and multidisciplinary. Our team brings together the best, cross-cutting group of experts in intelligent automation and services, safety and security and human computer interaction research. Our approach is to use the platform to develop the UK's IoV thought leaders of the future by having them lead rapid, agile and responsive pilot projects that are co-created with our social science, legal and industrial partners who are committed to work with us from co-creation through co-design to technical and policy translation. In particular, the Platform approach allows us the flexibility necessary to connect this robust interdisciplinary expertise through our network to appropriate stakeholder groups to co-create and rapidly prototype and pilot ideas both for scientific and applied insights of value across our DE communities. 

To guide this co-creation, we have developed four x-cutting research strands, vital to framing a human-in-the-center IoV: services, interaction, automation and security. For example, open research challenges include: what is the least amount of personal data required to run a service/infrastructure safely? Can this balance be dynamically responsive to detected risk situations? How can greater transparency of data-use help incentivise citizen participation where personal data is required? How to design agents and interactions to intelligently assist both citizen and service to negotiate data use agreements so people will not feel the need to fake the system to protect their privacy? By using this platform to support interdisciplinary research leadership towards co-creation and delivery of novel, human-centered approaches to the IoV, the UK will lead IoV design to support better quality of life for all.",,"This platform will enable an internationally leading step change in our ability to create a successful, trusted and resilient Internet of Vehicles (IoV). This result will be achieved by an unprecedented consolidation of expertise within the field of cyber-social systems because of our dual site approach between Warwick and Southampton.

A strong assumption of this bid is that, unlike approaches that split research into discipline-specific, siloed work packages, a new way of working is needed that brings disciplines much closer together, blending them within close-working teams to develop each member into a multi-disciplined deep-generalist researcher. This platform grant will enable us to do this, creating a new path for others to follow if it is successful.

The platform will create impact in the short term by involving industry, policymakers and citizens from the start in identifying key human-cyber IoV research challenges and addressing them within the scope of the platform. This will be done through RF-led workshops, which we have used successfully in previous projects and which involve stakeholders and citizens through a process of co-design: citizens are not simply informed but act as co-contributors with our partners to test approaches with the goal to deliver technology that empowers them.

However, considerable impact will be achieved beyond the timescales of the platform, through follow-on grant applications, by building a national network of leading IoV researchers and by informing future research.

More specifically, the key beneficiaries of this platform are the following:

INDUSTRY: We will help industrial partners design solutions for the IoV that are robust, resilient and trusted by users. By empowering users and engendering trust in the IoV, this will also increase participation and enable services that use accurate data from users where this is mutually beneficial.

POLICYMAKERS: We will identify key challenges that currently face the IoV and this will help in the development of future policies to both safeguard the privacy of citizens, but also to enable the benefits to be reaped from future IoV systems.

CITIZENS: When implemented successfully, the IoV promises more efficient, cleaner and safer transportation for all. Our work will address obstacles that are currently impeding its progress. Citizens will not just benefit from a successful IoV, but the platform will enable this to be done in a manner that is safe, secure and empowers citizens to own and control access to their own data.

EARLY-CAREER RESEARCHERS: The RFs working on the platform will learn important research skills, including managerial and bid-writing skills, and this will enable them to become future leaders of the field. Some of our RFs will eventually move to other institutions or organisations. This will help them pursue independent research careers while also establishing a national network of IoV experts far beyond the initial two host institutions of this platform.

ESTABLISHED RESEARCHERS AND WIDER ACADEMIC COMMUNITY: The exploratory and high-risk research carried out within this platform will define a research landscape and agenda for the IoV, and this will give rise to more specialised follow-on grant applications in the future. Some of these will be led by the investigators involved in this platform, but our work will also set the agenda for colleagues at other institutions, both nationally and internationally.

A strong assumption of this bid is that, unlike approaches that split research into discipline-specific, siloed work packages, a new way of working is needed that brings disciplines much closer together, blending them within close-working teams to develop fine-grained, multi-disciplined researchers. This platform grant will enable us to do this, creating a new path for others to follow if it is successful."
20,FB8B9B50-1DC9-4621-A9F8-D90232484E82,Analysing the Motion of Biological Swimmers,"Consider a deformable object moving in some medium. In general, one cannot infer the internal deformation of the body based on surface shape alone: it requires detailed knowledge of the mechanical properties and forces operating at every point of the object. And yet, such inverse problems are of considerable practical and fundamental interest in soft matter physics, material science and engineering. We propose that the relatively simple and constrained geometry of the worm C. elegans, and our extensive knowledge of its anatomy and material properties makes the inverse problem tractable. A new and promising approach to the solution of inverse problems is provided by deep learning, which has already transformed performance on standard tasks from across artificial intelligence, particularly in the areas of language and vision, and increasingly now in robotics. The proposed research is to explore the feasibility of using deep learning to understand the internal and external forces acting on the body of C. elegans from video footage of the worm swimming in three dimensional complex fluids.",,"The research will begin to shed light on the way in which a simple biological swimmer generates propulsion in a liquid medium, and the associated interaction between neural and mechanical systems. This could have a major impact in soft robotics, providing new ways to think about propelling a deformable (soft) robot through water, and the associated control systems. The area of soft robotics is highly promising in creating a new generation of flexible robots that are highly agile, resilient to unpredictable external forces, low-powered, compact and low-cost. Such robots have many potential applications, for example, they could be used in monitoring chemical manufacturing processes; they could be used in remote underwater environments to monitor the safety of water supplies; and they could be used in the maintenance of infrastructure.

More generally, the research will contribute to the international effort to gain a full understand of a single biological life form - C. elegans. Such an understanding will shed light on biological movement control in general, with potential impact on human medicine, and all of robotics."
21,70338EF4-1482-45F5-B382-B3A6F4B53EF2,"Strategic Priorities Fund - AI for Science, Engineering, Health and Government",please see attached business case.,,Please see attached business case
22,813BCDB0-9C96-4D71-870C-4F77343BDAF5,ACTION on Cancer,"Death from cancer is typically both slow and painful, and few families have been spared its scourge. Cancer is also one of the world's greatest killers (13 million deaths and 22 million new cases per year by 2025), and it is estimated that every second person on the planet will develop cancer at some stage of their life. 

Over the last 30 years our knowledge about cancer has increased enormously, and now, for the first time, we understand the fundamental nature of the disease(s): malfunctioning in the way that cancer cells process information. All the cells in our bodies process information about their internal state, and communicate with their neighbours, and when this goes wrong cancer can occur.

As everyone's cells are different, and there are very many different ways that this information processing can go wrong and cause cancer, it is not possible to design a single treatment for cancer, or even for a sub-type of cancer such as breast cancer. Instead what is needed are personalised treatments tailored to each patient's cancer. However, such personalised treatments are very expensive to design, and the expertise to do so is limited. In addition, it is often necessary to execute custom designed experiments to better understand what is the best treatment. Therefore the only way to make personalised cancer treatment available to everyone is through laboratoryautomation, and the use of artificial intelligence (AI).

In this project we will develop ACTION, which will be a prototype AI system for the design of personalised cancer treatments. ACTION will focus on chemotherapies - design of drug cocktails. Given initial information about a cancer ACTION will extract all the relevant knowledge it can find about the cancer, both from databases and computational models of cellular information processing that scientists have developed. ACTION will rationally integrate this knowledge, and infer what extra knowledge is required to make the best decision on how to treat the cancer. ACTION will then automatically execute custom designed experiments using laboratory robotics to determine the missing information. Finally, using all the knowledge it has gathered, ACTION will decide on the best chemotherapy.

We will evaluate ACTION using different types of cancer cells grown in the laboratory. This avoids the ethical complexities of working with patients, and is much cheaper and faster. If the development of ACTION is successful it will then move to testing with patient derived cancers.",,
23,2E00F109-E239-4D66-97CB-8F66B5D73765,Automatic repair of natural source code,"Fixing software defects is a time-consuming and costly activity. One of the main reasons why software debugging is so expensive is that it still remains mainly a manual activity. Fixing a bug is a complex process, which consists of many different steps including finding and understanding the underlying cause of the bug, identifying a set of changes that address the bug correctly, and finally verifying that those changes are correct. Automating this process (or parts of it) can potentially reduce the time, cost and effort it takes to fix bugs, and therefore the quality of the produced software.

Automatic Program repair (APR) is defined as the activity of removing a software defect during software maintenance without human intervention. The most popular approaches to automatic program repair rely on test suites as proxies of the behavioural specification of the system under repair. Although these approaches have demonstrated promising results, such as the synthesis of a fix for the famous Hearbleed bug, they face serious challenges. The main one is the correctness of the generated patches. Test suites are an imperfect metric of program correctness. Therefore techniques, which rely on test suites to evaluate the correctness of the generated patches, tend to overfit the test suite. Additionally, test-driven APR techniques can have scalability limitations in terms of execution time. The space of possible patches is vast and correct patches occur sparsely. In test-driven APR approaches the search time is mainly dominated by the validation of the patches, i.e. by the execution of the test suite. Existing approaches try to address this issue by employing various strategies such as parallel execution of test suites or the use of just a subset of the test suite (sample of positive test cases and all the negative ones). Although, such strategies improve the execution time, scalability of APR approaches remains a challenge.

The motivation of the project is to address overfitting and scalability limitations of APR techniques. The main hypothesis is the following: Statistical properties of source code can improve the state-of-the-art generate-and-validate techniques for APR in terms of patch correctness and execution time.

Statistical properties of source code (i.e. naturalness of source code) can help address the challenges of APR techniques in the following ways:

1) Heuristics based on the statistical properties of source code can be used to rank the results of fault localisation algorithms, and therefore augment their ability to correctly identify faulty lines of code.
2) Execution time of test-driven APR techniques can be reduced by discarding unnatural patches without executing the test suite.
3) Correctness of candidate patches can be improved by using the naturalness of code as part of the fitness function.

To measure the naturalness of source code, this project will mine existing software repositories such as GitHub and it will develop language models, similar to those used in the Natural Language Processing (NLP) domain. These models will be used to assess the naturalness of proposed patches.",,"The potential impact of automatic program repair is substantial. Automatic repair of bugs has the potential to lower the cost of software development and improve the quality of the produced software. In the long run the automatic repair of software will likely be a necessity. History suggests that the systems built are becoming more and more complex and interconnected, and as a result finding and correcting defects in them becomes more and more challenging.
The impact of the project can be decomposed into four different areas:

- Economy: Engineering high quality software at low cost can be a potential competitive advantage for the UK software engineering sector. The results of the project will contribute towards this goal, and therefore they will be disseminated to companies developing software in the UK. The industrial collaborator of the project, is expected to use the developed tool internally in projects. Moreover, the results of the project will be disseminated to other industrial collaborators of the DEIS research group. Finally, we plan to expand the industrial user-base of the approach, by publishing the results in journals, which target an industrial audience. To strengthen the industrial impact of the project, we plan to make a number of visits to interested companies in order to promote the project and train engineers on the developed tools and techniques.

- Society: Software is transforming various domains of everyday life. For example, autonomous vehicles can provide mobility for individuals, who cannot normally drive; home automation enables ageing population to live independently for longer; and smart medical devices can improve the life of patients. Approaches which automate the removal of software defects can potentially bring value to society by reducing the overall cost of software development and by increasing the reliability of software. To increase awareness of software quality and debugging, we plan to reach the general public by publishing the results of the project in appropriate outlets, such as The Conversation website. Moreover, the results of the project will be used to establish future research in the important area of reliable software through funding bids, and research projects.

- Knowledge: We expect that the results of the project can enhance the research capacity, knowledge, and skills of businesses and organisations in software debugging. In order to ensure the impact of the proposed project several steps will be taken. First, close collaboration with engineers from the industrial partner (IBM) will enable us the dissemination of the produced knowledge to them. Moreover, to achieve wider dissemination of the results, the prototypical implementation, relevant datasets, and a set of demonstrators of the techniques will be made available via a public website. Moreover, a workshop on software debugging, which will bring together interested academics and commercial organisations, will be held at the end of the project. In addition to the workshop, the results of the project will be published in high-impact venues such as the ASE conference, and tutorial sessions will be organised at these events.

- People: One of the main aspirations of the project is to further develop the skills of engineers industry-wide by making them familiar with novel software engineering techniques and tools. To this end, the investigators will propose talks and presentations at local and national engineering user-groups such as SoCraTes UK."
24,1A8E5DB9-E9CD-4D56-99E1-BF66165BA9F6,Enhancing Machine Learning with Physical Constraints to Predict Microstructure Evolution,"De-mixing is one of the most ubiquitous examples of self-assembly, occurring frequently in complex fluids and living systems. It has enabled the development of multi-phase polymer alloys and composites for use in sophisticated applications including structural aerospace components, flexible solar cells and filtration membranes. In each case, superior functionality is derived from the microstructure, the prediction of which has failed to maintain pace with synthetic and formulation advances. The interplay of non-equilibrium statistical physics, diffusion and rheology causes multiple processes with overlapping time and length scales, which has stalled the discovery of an overarching theoretical framework. Consequently, we continue to rely heavily on trial and error in the search for new materials. 

Our aim is to introduce a powerful new approach to modelling non-equilibrium soft matter, combining the observation based empiricism of machine learning with the fundamental based conceptualism of physics. We will develop new methods in machine learning by addressing the broader challenge of incorporating prior knowledge of physical systems into probabilistic learning rules, transforming our capacity to control and tailor microstructure through the use of predictive tools. Our goal is to create empirical learning engines, constrained by the laws of physics, that will be trained using microscopy, tomography and scattering data. In this feasibility study, we will focus on proof-of-concept, exploring the temperature / composition parameter space for a model blend, building the foundations for our ambition of using physics informed machine learning to automate and accelerate experimental materials discovery for next generation applications.",,"From an economic perspective, the most immediate beneficiaries are those companies that focus on multiphase materials discovery, development and optimisation with applications ranging from personal care products to aerospace and energy. The UK has a strong tradition of hosting the research and development activities for such knowledge-based companies, many of which have their own in-house modelling groups. Our program of research will open up new approaches to materials discovery by enabling traditional materials modelling techniques to be enhanced with the power of machine learning. By improving and refining models with learning from prior experimental data, we will enable acceleration of materials development, thus contributing to the need for industry to innovate quickly in order to remain competitive. 

From the perspective of public engagement, the beneficiaries will be physical scientists who may not have previously considered a career in artificial intelligence as a viable option. We will focus activity, through the Sheffield University Diversity in the Cultures of Physics - International Summer School, which is aimed at women. This is a group that is under represented in the rapidly growing AI branch of the tech industry. We will use the School as an opportunity to introduce women undergraduates to machine learning and enable them to explore, through involvement with the research, how a background in physical sciences provides a legitimate background for a future career in AI. We also aim to use our research as a springboard to engage in conversation with the public about the future role of AI within society. Given that the media is naturally focussed on the highest profile, and often controversial, developments in AI, we are keen to broaden the discussion to include the impact on society of research developments that are traditionally off the radar of non-specialists"
0,FF79BDC1-1531-413D-9833-40CB076DF619,The 'risk of risk': remodelling artificial intelligence algorithms for predicting child abuse.,"Child protection in the UK relies heavily on risk prediction, an area of growing interest in the UK since the late 1980s (Browne &amp; Saqi 1988, Creighton 1992). It is generally taken as an axiom that child abuse can and should be detected via risk prediction to identify vulnerable and risky families whose children may become abused or neglected. The purpose of identifying such families at an early stage is to target early intervention towards them to reduce the risk of abuse. To service this need, individual local authorities commission algorithmic risk prediction systems from profit making providers. The question this proposed project addresses is whether such systems are 'fit for purpose' given the concerning longitudinal data showing poor accuracy in child protection outcomes and an unacceptably high number of false positives and false negatives in risk prediction. This concern was recently highlighted by the President of the Family Division (Munby 2016).

This proposed project addresses the issue by looking at the possibilities for a new method of predicting risk in a more realistic way that provides a better means for child protection systems to be supported by them, rather than have to work potentially inaccurate data. It sets out a new and transformative means of collating, assessing and extracting consistent information from previous studies and testing them in a consistent and reliable way. The potential exists for scoping a new system which moves algorithmic risk prediction into new territory; existing systems do not 'learn' from these errors so the technology stalls at the stage of algorithmic prediction rather than developing into evidenced-based, reliable and responsive artificial intelligence (AI).

The key research questions/objectives are:
- What is a normalised confidence limit(s) in existing risk prediction studies in child protection;
- To develop a new method of calculating risk, and design for its application in child protection;
- To assess the possibility of designing a model for a new, GDPR-compliant, AI model of risk prediction suitable for use in pre- and post-proceedings child protection work.

This study's methodology is transformative, bringing together a mix of traditional and pioneering methods. Each stage of the methodology has been assessed for the level of potential transformation in either its approach and/or outcome. The team will start the proposed project by creating the first, comprehensive and re-usable database of previous relevant studies. The creative and new methods employed by the rest of the study is higher risk, but if successful will yield a correspondingly high reward. Having created the database of studies, the team will analyse their characteristics, size, scope and methods to apply a consistent means of calculating their power ratio, creating a comparative analysis including strengths, weaknesses and confidence limits. These results will be analysed using Bayesian statistics in the context of Eggleston's work in respect of the use of probability in fact finding processes (Eggleston 1983). Bayesian networks provide a novel means of establishing criteria for weighting of evidence for social and technical problems including reasoning (using the Bayesian inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, finding explanations for datastreams, and helping systems to analyse processes over time. Used in this context, we will provide a consistent measure of confidence across risk-factors and measure of their evidential probity. This core transformative element of our methods will enable scoping of a risk prediction system to take account of strengths and weaknesses, including identifying gaps, providing a reliable legal indicator to courts as to the appropriate weighting as a project outcome.",,"The impact activities fit into a well-established existing programme already in place. Interim results will be communicated via evidence briefings, reports and workshops. The team work with the Ministry of Justice and the Department of Education (the two government departments collectively responsible for child protection) so will introduce this project into their pre-existing dissemination strategy amongst policy heads. The project's findings will also be directly disseminated across all 152 Local Authorities in England. The team are experienced media commentators with extensive media contacts; two are ESRC media trained. Their previous work is featured in the Times, the Telegraph, the Guardian and on BBC and Channel 4 news and the ESRC's Society Now.

Impact will be maximised through focus on stakeholder engagement during and after the project. The team have a track record of engagement with a large and diverse network having previously successfully run funded research in the field. The team's contacts comprise a range of interlinked academics including authorities in the area such as Professors Judith Masson (Bristol), Judith Harwin (Lancaster), Karen Broadhurst (Lancaster), Viv Cree (Edinburgh), Sian Pooley (Oxford) and Lucy Bowes (Oxford). The team have been invited to work with the Family Policy Team at the Ministry of Justice in relation to research, strategy and policy to reduce the number of children requiring child protection systems, and to advise on how the system operates at each stage.

The project will create a new inter-disciplinary group via three workshops bringing together stakeholders who may not usually have the opportunity to work together to discuss and debate the key themes of the project as it develops. Following completion of the project and publication of the results the group will be well established and will continue as a new network of expertise to maximise dissemination. This is an important element of the planned impact as the research aims to establish and open dialogue in relation to the need for inter-disciplinary perspectives on this topic. An open-access website and social media activities are planned to give regular updates and information about developments in the research, leaflets and information packs will be distributed to interested groups and key government stakeholders who will be invited to comment and engage with the final report.

A primary aim of stakeholder involvement is to bring together existing information, but consider it in a novel and transformative manner to create a new way of considering the research questions. Findings will be of use to those involved in economic policy as well as those concerned with welfare policies. Findings are potentially of global significance in relation to any jurisdiction using a similar child protection system. Given the nature of the project and its potential national and global economic and social impact the aim of a range of partner involvement throughout the project will increase networking and knowledge exchange opportunities.

- Creation of an Ethical Charter for the use of AI in child protection.

- Academic dissemination: At least one 3* and one 4* paper will be produced and a related conference paper.

- Practitioner dissemination: The external engagement activities including the workshops will include an appropriate variety of stakeholders, invited to express their views and be involved throughout the project.

- Public sector and NGO dissemination: Key figures in the MoJ and DfE, the third sector, the judiciary and local authorities will be invited to be involved at all stages of the research, and will receive regular updates and a copy of the final report. The team hold an annual public lecture, hosted at London South Bank University and will disseminate findings via this medium to a wide public audience"
1,E640CCA3-E6FC-464A-8507-8BFA22C0A76D,ReEnTrust: Rebuilding and Enhancing Trust in Algorithms,"As interaction on online Web-based platforms is becoming an essential part of people's everyday lives and data-driven AI algorithms are starting to exert a massive influence on society, we are experiencing significant tensions in user perspectives regarding how these algorithms are used on the Web. These tensions result in a breakdown of trust: users do not know when to trust the outcomes of algorithmic processes and, consequently, the platforms that use them. As trust is a key component of the Digital Economy where algorithmic decisions affect citizens' everyday lives, this is a significant issue that requires addressing. 

ReEnTrust explores new technological opportunities for platforms to regain user trust and aims to identify how this may be achieved in ways that are user-driven and responsible. Focusing on AI algorithms and large scale platforms used by the general public, our research questions include: What are user expectations and requirements regarding the rebuilding of trust in algorithmic systems, once that trust has been lost? Is it possible to create technological solutions that rebuild trust by embedding values in recommendation, prediction, and information filtering algorithms and allowing for a productive debate on algorithm design between all stakeholders? To what extent can user trust be regained through technological solutions and what further trust rebuilding mechanisms might be necessary and appropriate, including policy, regulation, and education? 

The project will develop an experimental online tool that allows users to evaluate and critique algorithms used by online platforms, and to engage in dialogue and collective reflection with all relevant stakeholders in order to jointly recover from algorithmic behaviour that has caused loss of trust. For this purpose, we will develop novel, advanced AI-driven mediation support techniques that allow all parties to explain their views, and suggest possible compromise solutions. Extensive engagement with users, stakeholders, and platform service providers in the process of developing this online tool will result in an improved understanding of what makes AI algorithms trustable. We will also develop policy recommendations and requirements for technological solutions plus assessment criteria for the inclusion of trust relationships in the development of algorithmically mediated systems and a methodology for deriving a &quot;trust index&quot; for online platforms that allows users to assess the trustability of platforms easily. 

The project is led by the University of Oxford in collaboration with the Universities of Edinburgh and Nottingham. Edinburgh develops novel computational techniques to evaluate and critique the values embedded in algorithms, and a prototypical AI-supported platform that enables users to exchange opinions regarding algorithm failures and to jointly agree on how to &quot;fix&quot; the algorithms in question to rebuild trust. The Oxford and Nottingham teams develop methodologies that support the user-centred and responsible development of these tools. This involves studying the processes of trust breakdown and rebuilding in online platforms, and developing a Responsible Research and Innovation approach to understanding trustability and trust rebuilding in practice. A carefully selected set of industrial and other non-academic partners ensures ReEnTrust work is grounded in real-world examples and experiences, and that it embeds balanced, fair representation of all stakeholder groups.

ReEnTrust will advance the state of the art in terms of trust rebuilding technologies for algorithm-driven online platforms by developing the first AI-supported mediation and conflict resolution techniques and a comprehensive user-centred design and Responsible Research and Innovation framework that will promote a shared responsibility approach to the use of algorithms in society, thereby contributing to a flourishing Digital Economy.",,"In terms of knowledge, key communities across AI, computer science and the social sciences will benefit from our research. ReEnTrust will develop new technical insights into the capabilities of advanced AI techniques to support the process of rebuilding trust in online platforms by assisting mediation and conflict resolution. By bringing together techniques from different areas of AI, we will produce novel models that are expected both to provide each of these areas with new applications for their methods, and to open up new avenues for research into trust rebuilding methods. 

This core technical work will be embedded in an extensive programme of human factors and Responsible Research and Innovation work that will encompass extensive empirical work with users and stakeholder groups and address the policy and education dimensions of improving the trustability of online algorithm-driven platforms. The results of this work will benefit social science disciplines and applied computing research by providing new methodologies for the responsible design of algorithmically driven systems. They will also benefit government, NGOs, professional and regulatory bodies by providing case studies, design principles, and policy guidelines that can be used to raise awareness and shape their future strategies and activities. 

In terms of economic impact, trust rebuilding technologies not only provide important opportunities to de-risk future uses of AI-driven online services, but also open up new directions to exploit untapped business opportunities around socially responsible AI and &quot;trustability services&quot;. Work on a &quot;trust index&quot; as specified in our programme of work will be an essential part of this, as it provides an accessible way to communicate our work to industrial stakeholders, but may also open up new business opportunities surrounding trust certification. In order to enable business leaders and future entrepreneurs to benefit from these opportunities, we will use Horizon's network of over 200 commercial partners to disseminate findings and encourage participation in relevant events. We will also explore opportunities for engaging in commercial spin-off activities of our research ourselves.

In terms of societal impact, a major outcome of the project will be new insights on how trustable AI-powered systems should be &quot;collectively engineered&quot; in the future. These outcomes of ReEnTrust will not only improve wellbeing by helping people address and resolve trust breakdown situations, but also provide a mechanism for collective reflection about values and conduct of both platforms and users, which will foster a culture of accountability and shared responsibility. Apart from the reported anxiety and uncertainty, feelings of disempowerment, defeatism, and loss of faith in articulating societal demands through regulatory and legal institutions, there is currently a real threat that loss of trust in algorithms turns into adversarial behaviour toward platforms and their providers. Our research will benefit society at large by offering new solutions to rebuild and enhance trust in AI algorithms that will help prevent the emergence of an &quot;us against them&quot; culture, and thus contribute to a healthy, resilient Digital Economy."
2,858968FC-BA8C-48F0-9EB1-C90E7C7A1918,Artificial Intelligence Tools For Automatic Single Molecule Analysis,"*What will we do?*
Develop and distribute an &quot;artificial intelligence&quot; application to allow fellow scientists to analyze a type of data (&quot;single molecule data&quot;) difficult to analyse with existing methods. 

*Machines can learn, but they take a lot of training*
Machines can recognise speech in devices from Amazon's &quot;Alexa&quot; to call centres using artificial intelligence (&quot;AI&quot;). As an example, just ask Alexa what &quot;AI&quot; is, and she will tell you. This has only been possible recently as computers have become sufficiently powerful. The technologies to do it are collectively called &quot;machine learning&quot;. One advantage of such &quot;machines&quot; is that they can answer questions unsupervised by people. One limitation is that they require enormous labelled datasets to &quot;learn&quot; in the first place. This is called &quot;training data&quot;. We have devised, a &quot;trick&quot; to generate massive datasets, by playing simulated data into recording apparatus and then recording back the resulting signal. Because we control the entire process the data is inherently &quot;labelled&quot; in the way necessary to train intelligent machines. With this technique together with the use of Google Brain's freely available &quot;TensorFlow&quot; AI library, we can create applications that analyze data for us.

*Why &quot;Single Molecules&quot;*
Many molecules found in animal cells behave as switches. Their individual &quot;on&quot; or &quot;off&quot; state can then be measured as either pulses of light or electrical current giving real-time mechanistic insight. They are important throughout biology with the estimated Global market for drugs targeting one family of these molecular switches (ion channels) alone being $11.5bn. The flip-side is that experiments measuring these &quot;switches&quot; generate big datasets that are difficult and laborious to analyse.

*Could single molecule biology contribute to tackling diseases of age, climate change, anti-bacterial resistance and global terrorism?*
In short &quot;Yes&quot;: Ion channel malfunction in particular, is responsible for many age-related diseases. Interest from Pharma is enormous because they are targets for many drugs from sedatives to heart medicines. Certain insecticides act via their ion channels, but these are toxic to people too. One such agent, the nerve toxin &quot;VX&quot; hit the news when it was used in the assassination of Kim Jong-Nam. Even the relatively safe insect repellent citronella repels mosquitoes by activating ion channels. Permethrin-resistant mosquitoes are resistant because they have a specific mutation in an ion channel creating a real problem in malaria control. Plants too express a range of ion channels with critical roles including salt regulation. Since climate change is increasing the salination of many agricultural regions, there is keen interest in whether biological modification of root ion channels could promote survival of crops in salt-rich soils. Ion channels have also been studied in synthetic biology because they can be activated by chemicals at concentrations far lower than that of other sensors. Many uses have been proposed for these, such as detection of explosives, biological weapons, narcotics or certain diseases. A recent discovery is that bacteria also &quot;talk&quot; to each other by an ion channel dependent biofilm communication network that is necessary for their survival. This has raised the possibility that ion channel blocking drugs could constitute a new generation of antibacterials that are less susceptible to resistance. So indeed single molecule biology could contribute to study of several grand challenges in society. In each case, a limiting factor is currently the time required to study the large datasets generated by these molecules. 

*How can we help?*
We will create a simple to use AI-based analysis application to allow rapid analysis of these data. These will be especially useful to industrial partners who produce large data sets during drug development but have few tools available to analyze this fully.","Despite recent advances in the analysis of 'omic data, there have been few equivalent advances in the analysis of the large time-series datasets created by functional single molecule experiments. We will address this by developing machine learning tools to allow a novel &quot;Big Data&quot; approach for the analysis of complex single-molecule FRET (smFRET) and patch-clamp data from any cell type. Whilst the technologies for recording such data improve year-by-year, the procedures for analysis have not changed significantly over the past 20 years. Such analysis is slow and laborious, requiring full expert supervision. In native cells, the data are difficult to interpret objectively, and it takes many times longer to analyse than to collect. Furthermore, recent new high throughput recording machines have been created, but there are no software solutions that fully cope with this volume of data. Big Data Analytics are the key to solve the problem. Deep learning, a recent machine learning development, has brought what has been referred to as the 'artificial intelligence (AI) spring'. Deep learning can innovatively solve complex and ambiguous problems and is ideal for solving the data analysis challenges posed by single-molecule research. In this project, we will produce two different families of deep learning kernels for single molecule analysis. The first will be developed using long-short-term memory architecture, a recurrent neural network with back-propagation through hidden layers (to retain sequence information) and the other is an adaptation of our convolutional neural network models which we currently use for image analysis. We train the network initially with semi-synthetic data acquired through regular acquisition hardware and finish training with small expert labelled training sets. We will produce, validate and distribute a GUI faced AI-based package to analyse single molecule data of either smFRET or patch-clamp origin, reading popular data formats directly.","Single molecule research and AI are of great strategic relevance to industry. There have been numerous reports, including the Economist's article **&quot;Artificial intelligence: Million-dollar babies&quot;** discussing a series of huge recent investments and buy-ups in/of AI technologies and a brain drain to Silicon Valley. Furthermore, the Global Market Report (from Reuters) puts the ion channel modulator market alone at $11.5bn/year. This project will boost this strategic priority area by creating a UK bridge-head for Artificial Intelligence approaches to drug discovery.
Timelines for all the following activities are included in the Full Impact Statement attachment.

*Training*
The BBSRC's (politely termed) &quot;Vulnerable Capabilities Report&quot; recently highlighted that the UK has an alarming and increasing lack of skills in 5 areas amongst many of our young biologists. These included &quot;(ii) Maths, statistics and computation&quot; skills. This may follow the hard bits being removed from UG programs as courses try to increase student marks and feedback scores? The predominance of kit based science may then have also contributed to other vulnerable skill deficits identified in the report such as lack of &quot;(iii) Physiology [and pathology]&quot; and &quot;(i) Interdisciplinary&quot; skills. This project will, therefore, address the BBSRCs top THREE &quot;vulnerable skills&quot; by embedding either a young mathematician/computer scientist or biologist into a multidisciplinary group. A biologist employed to lead this project would receive specific training in (1) Matlab programming, (2) Python programming and (3) machine learning. A mathematician already with those skills would receive single molecule theoretical training and interact in group meetings with active single molecule researchers on a weekly basis.

*Industry*
The strategic relevance of AI approaches to biotechnology (&quot;Data-Driven Biology&quot;) are clear and the focus of our industrial enterprise is software support for high-throughput (HTS) ion channel drug discovery technology. Hardware system outputs greatly now exceed the ability of existing software to cope. We have started working with Molecular Devices and Nanion to develop a suite appropriate for HTS. Our ultimate goal is for this software to be shipped with the hardware. We have Letters of Support from both Europe and the USA's largest manufacturers of HTS, from Pharma and from major UK software developers CED. 

*Animal Welfare*
In the short term, this project will pilot technology without the need of the animal tissue that would normally be used in this type of study and by releasing models initially trained on synthetic data, we will reduce the need for others to use animals. Furthermore, the AI approach will recover molecular events more efficiently than the non-AI approach and so the numbers of animals needed for any of these studies is reduced.

*Dissemination*
We will disseminate through, but also, in order of perceived importance: (i) A workshop hosted at the Physiological Society &quot;main&quot; meeting (summer 2019). (ii) Link with the Physionet website (hosted by MIT-USA) to share our data and we will run a Kaggle.org competition for informaticians to extract idealized records for our most complex synthetic data. The cash prize this competition will offer will entice non-biologists and hobbyists to also become engaged. (iii) Our social media outlets (several thousand followers). Additionally, as with all projects, our early progress will be disseminated at US Biophysical/Experimental Biology and international AI conferences and by continued publication in high impact international journals. 

*Outreach and Public Engagement*
RBJ set-up the first Meet-The-Scientists event in Liverpool a few years ago, but the success of this has led to a regular series of events at the Liverpool World Museum. This project team will pioneer the use of illustrative cartoon examples of machine learning specifically designed for kids."
3,2C0CA77B-0821-4FAC-8289-56405C405DEE,Technology Driven Change and Next Generation Insurance Value Chains (TECHNGI),"Financial and insurance services contribute more than 7% of UK national income and over one million jobs, with around one third of this employment in insurance. London is the leading global centre for specialist commercial (re)insurance broking and underwriting, with a wide range of specialised skills that supported the writing of &pound;60 billion of gross written premium in 2013. Exports of insurance and pension services were &pound;17.6bn in 2016, some 7% of total services exports.

Our project investigates the opportunities and challenges for the UK insurance industry arising from the application of the new Ai technologies, including machine learning, distributed ledger, automated processing and the explosion of available data for business analytics and modelling including from social media and the connections emerging to the 'internet of things'.

We will explore the implications for the insurance industry of this wave of new digital technologies, with the support of many of the UK's leading insurance companies. We will identify and map the range of opportunities for AI based innovation in business processes and business models, across underwriting and risk analytics, claims processing and customer engagement. We will examine, through engagement with industry on business opportunities and challenges and through a range of case studies, the barriers to adoption and the enablers of change. 

We will examine these barriers and enablers both from organisational and industry wide perspectives. At the organisational level we will examine the requirements for successful innovation (&quot;critical success factors&quot;) and develop organisation wide assessments of their readiness for adoption. At the industry level we will examine new emerging ways of providing insurance services, including the possibilities for transformative change in insurance value chains, for example with separation of risk and underwriting from customer engagement, and the potential for sharing of services and data. 

Finally from these organisational and industry investigations we will develop strategic and policy analysis and recommendations, identifying the steps required from firms and from policy makers to support the adoption of AI technologies and ensure that these support automation and efficiency gains in UK insurance industry and benefit insurance customers. 

A distinctive feature of our project is our deep industry engagement, offering us the opportunity to engage with practitioners across the full range of business functions: in strategic roles; in specific business areas across product lines and operational processes; in risk analytics; and in technology. These contacts will support a range of case studies of the deployment of AI in insurance and also interview, survey and forum style empirical investigations to achieve the full understanding from both these organisational and industry perspectives.",,"WHO MIGHT BENEFIT FROM THIS RESEARCH?

Our research is designed for several different user groups: 
(a) Staff working across the insurance value chain. Successful technology based business innovation involves changes in both business process and business models and hence (Stephenson 2018 ch 7) requires a combination of expertise: technological (AI and data science), business (responsible for engagement with customers and suppliers) and analytical (e.g. underwriting and risk pricing). Our research - the Delphi interviews in Phase A, the case studies in Phase B, the frameworks and solutions in Phase C - will directly support these staff by providing relevant knowledge and understanding.
(b) Techinsur startups or in new competing technology based challengers, e.g. major internet platforms, who need understanding the insurance value chain and the potential for AI and data technologies. They will particularly benefit from our Phase A mapping and our detailed Phase B case studies. 
(c) Senior management in the industry need to shape an appropriate strategic response to this new emerging landscape. Our integrative workpackages in Phase B (WP4.1, WP4.2, WP5.1, WP5.2) together with our Phase C workpackage 6.3 on implementation frameworks are addressed to their needs.
(d) Regulators, compliance officers and policy makers want to support these technological changes, and where possible use them to better achieve regulatory objectives such as customer protection at less cost to the industry.

HOW MIGHT THEY BENEFIT FROM THIS RESEARCH?

Business line and technology staff in UK insurance can gain insight into the organisational and business challenges arising from the application of AI to their products and business processes. They will be able to learn best practice and the factors for successful innovation from our case studies and research based empirical investigations. Risk analytics staff may also benefit from better understanding the opportunities for data-based improvements in risk modelling and underwriting.

Insuretech entrepreneurs can gain from a better understanding, especially from our case studies, of the business requirements for successful new technologies.

Senior management can also gain strategic insight, in particular into changing business models, into the possibilities for co-operative effort to develop standards and supporting technology and for negotiating the potentially transformative impact of technology on the industry as a whole.

Policy makers and line staff in regulatory bodies can benefit from improved understanding of insurance business models and how they are affected by technological change; on recommendations and opportunities for better alignment of regulations with the new technologies; on developing policies for regulation of new business products and technology supported business models; and from understanding and supporting opportunities for using technology to support regulatory compliance

We offer all of these different user groups practical options for promoting the application of AI and data technologies in the UK insurance industry, in the short to medium term at the level of specific business process and over the longer term by influencing the strategic and policy decisions that shape future business models and support industry transformation. 

Our research strategy, based on close engagement with our industry partners and co-production of research, provides assurance that our range of research outputs will be relevant to these targeted user groups and, if we do our job well, have the desired medium and long term impact on the future of the UK insurance industry."
4,FF745D53-1942-4187-B14E-78B807EA9BF9,Using artificial intelligence to share control of a powered-wheelchair between a wheelchair user and an intelligent sensor system.,"Research will focus on the novel use of sensors and inventing new shared control systems and artificial intelligence (AI) to significantly and positively impact on the lives of both current and potential powered-wheelchair users.

Recently developed sensors will be digitised and then used in novel ways with AI to assist people with driving a powered wheelchair. This will allow some people to use a wheelchair by themselves for the first time, and will make driving and steering easier for many others. That will reduce the need for carers, improve health outcomes and give disabled people an opportunity for more independent mobility. For some it will provide mobility for the first time.

Access to independent mobility is important for self-esteem and a feeling of wellbeing. Natural independent mobility such as crawling and walking are usually acquired in the first two years of life; if this does not happen then people can find it difficult to acquire the skills later. Currently a wheelchair can provide some self-initiated mobility but it cannot be introduced unless a person has the spatial awareness, physical ability and cognitive skills to understand the concept. Being able to transport oneself has a positive effect on general development that cannot be underestimated. This research will provide that opportunity.

Research at the University of Portsmouth has already resulted in analogue collision avoidance and effort-reduction systems, so that people can drive for longer. Work at the Chailey Heritage Foundation created track systems to guide wheelchairs and novel systems that can follow a path parallel to a wall and sensors to safely detect the environment. All the devices will be redesigned as digital systems to connect them to expert systems for improved control. The new digital versions will interface to microcomputers. The new systems will interpret hand movements and tremors to improve control further. That will allow end-users to steer their powered wheelchairs without needing helpers and provide a greater sense of accomplishment and freedom, whilst simultaneously helping to reduce carer costs. 

The abilities of the wheelchair user will be constantly assessed so that control gains can be automatically set for the sensor system and the human driver. This will be achieved by calculating a self-reliance factor depending on ability, tiredness, recent driving performance etc. An intelligent avoidance-factor will depend on obstacle proximity, a safety-factor will denote the ability of the driver and an assistance-factor will depend on time spent driving and tiredness. The sensor system will influence the motion of the wheelchair to compensate in those areas. This is the first time that this has been attempted.

Different AI systems will be used for different tasks to capitalise on their separate distinct strengths in diverse circumstances. An original hierarchy based upon the structure of Artificial Neural Networks will be used to integrate them. At least three AI techniques will be used to select courses of action for a wheelchair and a new Decision Making System (DMS) will be created to determine a best course of action by considering and comparing the outputs from the different artificial systems and the requirements of the human user. Each system will provide a level of confidence for a potential course of action, for example turn left, stop etc. The DMS will determine the action to take.

This EPSRC project will produce both new devices and new ways of integrating devices into wheelchairs to ensure safe navigation and personalized assistance with general low cost but automatically adjustable solutions that make the systems bespoke and adaptable in real time. This will help to ensure users achieve maximum functionality. The devices can be added to existing wheelchairs, providing a cost-effective way of improving quality of life and independence.",,"Research will benefit people with Multiple-sclerosis, Arthritis, Stroke, Paraplegia, Orthopedic-impairment, Cerebral-palsy and Diabetes, especially if blind or with missing or damaged-limb(s). Initially, disabled community groups, carers, users and families who have used the analogue systems are expected to adopt the new systems. They and their families are keen to hear about the research and are already engaging; they are keen to give their opinions to help guide the research.

Health
Work will directly benefit disabled and older users and their quality of life will be significantly enhanced. New systems will allow people with limited dexterity to use wheelchairs; users will drive for longer and more safely. The digital and AI systems will lead to new, faster and responsive processes that will replace some of the older systems in schools and institutions, hence further improving lives. It will make a significant positive difference by giving disabled individuals more confidence, independence and freedom, especially people with limited spatial awareness or cognitive ability. The techniques could also contribute to the digital hospital component of the Research Council's UK Digital economy programme involving real-time data fusion and patient tracking, and to the Healthcare Technologies theme.

Professional services
Professional UK guidelines and training were informed by previous analogue research, and new technical standards and clinical protocols were introduced. The new systems will do the same, prompting changes in professional practices in that powered-wheelchairs will be considered as an option, even for blind children, and new technical standards and clinical protocols will need to be introduced. Health outcomes will improve because of the new systems and the work will lead to new methodologies for therapists to teach people how to drive.

Cost Reduction
The research will introduce some autonomy and reduce the need for carers. Further, the costs of the systems should be significantly reduced due to digitization. Health outcomes will be further improved because of the availability of the new systems and the new ways that people will be trained on them and improvements to mental health and wellbeing will reduce NHS costs.

Schools, NHS, and institutions
Beneficiaries will gain directly through links to CHF and other institutions; time between discovery, research and use will be short as new theoretical knowledge and systems will be passed to CHF for testing, use and immediate impact. Systems will be quickly proved in schools where they will have an immediate beneficial effect. With the help of CHF, they will quickly move on to be used by the NHS, disabled community groups, in private homes and by individuals. This project also provides predictive tools to determine system use over time. That will support clinicians to assess intervention needs. Developing such techniques and technology is of key interest to powered wheelchair specialists in the medical and healthcare industries.

UK Industry
Wheelchair manufacturers will have access to new systems that can easily be added to their existing wheelchairs and software companies will have access to the new software. The UK is short of rehabilitation engineers and this project will help address that by producing two more postdocs working in rehabilitation technology. Car manufacturers will have access to the new technology for driverless vehicle research. The work will be of immediate use to Dynamics Controls and Penny &amp; Giles and of interest to the defence and automotive industries (tracking, driving, navigation) and any innovation in this field is quickly adopted. The research could result in a new start-up to produce some of the new systems. Research in sensor fusion and AI will feed forward (and across) to Delphi UK who are developing automotive software and sensors, and Jaguar Land Rover who want to deploy self-driving cars on British roads."
5,65A5E759-B1B4-4567-9BDE-C741D8FCD3FF,IRC Next Steps Plus : OPERA - Opportunistic Passive Radar for Non-Cooperative Contextual Sensing,"Physical activity and behaviour is a very large component in an array of long-term chronic health conditions such as diabetes, dementia, depression, COPD, arthritis and asthma, and the UK currently spends 70% of its entire health and social care budget on these types of conditions. All aspects of self-care, new therapies or management conditions, require novel non-intrusive technologies able to capture the salient data on causes and symptoms over long periods of time.
The OPERA Project - Opportunistic Passive Radar for Non-Cooperative Contextual Sensing - will investigate a new unobtrusive sensing technology for CONTEXUAL SENSING - defined as concurrent physical activity recognition and indoor localisation - to facilitate new applications in e-Healthcare and Ambient Assisted Living (AAL). The OPERA platform will be integrated into the &quot;SPHERE long term behavioural sensing machine&quot; to gather information alongside various other sensors around the home so as to monitor and track the signature movements of people.
The OPERA system will be built around passive sensing technology: a receiver-only radar network that detects the reflections of ambient radio-frequency signals from people - in this case, principally, the WiFi signals in residential environments. These opportunistic signals are transmitted from common household WiFi access points, but also other wireless enabled devices which are becoming part of the Internet of Things (IoT) home ecosystem.
The project will make use of cutting-edge hardware synchronisation techniques, and recent advances in direction finding techniques to enable accurate device-free (non-cooperative) localisation of people. It will also employ the latest ideas in micro-Doppler radar signal processing, bio-mechanical modelling and machine/deep learning for automatic recognition of both everyday activities e.g. tidying and washing-up, to events which require urgent attention such as falling. OPERA is expected to overcome some of the key barriers associated with the state-of-the-art contextual sensing technologies. Most notably non-compliance with wearable devices, especially amongst the elderly, and the invasion of privacy brought about by the intrusive nature of video based technologies.",,"UK faces substantial challenges dealing with chronic health conditions such as diabetes, dementia, depression, COPD, arthritis and asthma. Whether seeking to understand the mechanisms, trying to avoid onset, creating new therapies, or supporting self-care of these conditions, we need non-intrusive technologies able to capture salient data on causes and understand symptoms. At the most fundamental level this project aims to understand the potentials of opportunistic sensing technology to address health problems. The proposed research programme is wide ranging covering advanced radar and wireless synchronisation and detection methods, AI / deep learning and statistical signal processing, modelling and activity simulation. There is also a significant systems engineering and embedded processing aspect involved in the device synchronisation work, and developing the in-home activity recognition demonstrator. Outside of the health domain, the technology is also expected to find a set of other applications ranging from everyday pattern of life monitoring to enable intelligent and predictive smart homes and cities, right through to applications in retail shopper tracking, safety and security."
6,C606F38C-B3B0-4623-979C-EFF24DE5E032,Accelerated Discovery and Development of New Medicines: Prosperity Partnership for a Healthier Nation,"GSK is a global healthcare company that discovers, develops and manufactures medicines to treat a range of conditions including: respiratory diseases, cancer, heart disease, epilepsy, bacterial and viral infections (such as HIV and lupus), and skin conditions like psoriasis. GSK makes over 4 billion packs of medicines each year, with the goal of playing its part in meeting some of society's biggest healthcare challenges.

Alongside a mission to provide transformative medicines to patients, GSK continually seeks to improve the efficiency and sustainability of our processes across the discovery, manufacturing, and delivery components of our supply chain. Indeed, GSK are committed to ambitious sustainability goals by 2050 that can only be achieved by making existing and future medicines via better routes, driving innovation all the way from the first design of the molecule through to patients in the clinic.

This Prosperity Partnership aims to build on existing vibrant collaborations between GSK and the Universities of Nottingham and Strathclyde. The strengths of each partner will be leveraged to deliver a new suite of methods and approaches to tackle some of the major challenges in the discovery, development, and manufacture of medicines. Our vision is to increase efficiency in terms of atoms, energy, and time; resulting in transformative medicines at lower costs, reduced waste production, and shorter manufacturing routes. 

Key challenge areas, or themes, covered in our partnership include:

1. The development and application of Artificial Intelligence (AI) and Machine Learning to the efficient identification of next generation medicines: in Drug Discovery, many hundreds of candidate structures are designed, prepared, and tested to find the molecule with the right profile to take into the clinic. The development of AI informed decision making has the potential to deliver huge savings by minimising the number of compounds that need to be made at this stage. The software developed will incorporate green chemistry principles with the goal that the chemical methods employed are as efficient and sustainable as possible.

2. Next generation catalysis and synthesis: Chemists seeking to discover new medicines need new reactions that will allow them to make and investigate structures that are currently difficult, or even impossible, to make. A key objective of this proposal will be to develop new reagents, catalysts, and reactions to facilitate the more efficient preparation of drug-like molecules to accelerate drug discovery. Similarly, we will develop new ways of performing some of the most common chemical transformations in the synthesis of medicines whilst avoiding the use of carcinogenic reagents.

3. Sustainable processes that deliver efficiency and transition to scale-up from grammes to kilogrammes. Currently under-utilised approaches, such as electrochemistry, will be explored for their ability to catalyse reactions with cheaper and less environmentally impactful metals, such as replacing palladium with nickel.

4. A new Digital Design toolset for equipment will enable Digital Manufacturing of novel pharmaceutical processing equipment. Current development relies on existing traditional vessels and flow reactors that compromise our ability to deliver processes that operate at optimal performance. The research will couple advanced process models, state-of-the-art experimentation, and 3-D printing/additive manufacturing technologies to revolutionise how we develop, scale up, and operate chemical processes to supply new medicines. 

Integration of the projects and the expertise from the three partner institutions, and the successful prosecution of our research objectives, will make a major contribution to the wider pharmaceutical sector and, indeed, GSK's mission of discovering and developing transformative medicines faster to help people do more, feel better, and live longer.",,"This Prosperity Partnership will have positive impacts in the following areas: 
PEOPLE. Ultimately, the delivery of better medicines using smarter methods with shorter lead times will have a notable impact upon the health, wellbeing, and prosperity of the nation and worldwide. On a more local level, this Partnership will facilitate and drive the training of a cadre of highly skilled scientists who will share technical excellence and the wider impacts of their science both locally and globally.
The pharmaceutical industry constantly needs cutting edge science to deliver transformative medicines, increased sustainability, and better manufacturing processes. This drives a continuing demand for highly qualified and skilled scientists to lead innovation and manage change in these areas. As part of our Partnership we will deploy an inclusive cohort-based training network, spanning all 3 partners, which will provide industry-ready scientists with advanced technical competencies and leadership to ensure that the sector retains its primary position in innovation and productivity.
ECONOMY. A healthier nation is a more productive nation. Enhanced delivery of better medicines, envisioned by this proposal, allows people to do more. Drug discovery, development, and manufacturing are immensely challenging processes, particularly when sustainability is included. Addressing these challenges, this Partnership will deploy machine learning approaches to enable data-driven decision making in target generation and synthetic route definition. We will expedite optimal syntheses by using responsive digital methods to deliver energy and material resilient production to secure continued growth through innovation. Successes from this Partnership will be applicable across both the pharmaceutical and wider chemical industries, as many of the processes will be readily transferable.
SOCIETY. The diverse range of products manufactured by the pharmaceutical and chemical industries is vital to maintain the prosperity of the UK. Our Partnership will have a direct impact by ensuring the supply of trained people and new knowledge for increased health and sustainability for all. Health awareness and &quot;green and sustainable&quot; agendas are now fixed in the public consciousness, and there is an increased expectation for the pharmaceutical industry to deliver accordingly. Recent developments in AI and machine learning offer new tools for particular application in the discovery of new molecular entities through medicinal chemistry and early development. This Partnership will also seek to deploy novel chemistries, better catalysis, and state of the art processing opportunities to deliver potent materials with vastly increased sustainability. Being able to deliver these outputs in such an energy-efficient fashion will engender a sustainability ethos unique to the UK.
Engagement is also a crucial component of this Partnership; we will invite input and discussion from the public via lectures, showcases, and exhibition days. The Carbon Neutral Laboratory at Nottingham and Strathclyde's Engage Week will form hubs for technology open days and will provide key interfaces to give school pupils and young adults the opportunity to view science from the inside. We will broaden the impacts of our science by wider dissemination not only to the GSK/pharmaceuticals community but also across any who, directly or indirectly, have a shared interest in the prosperity of this sector.
KNOWLEDGE. In addition to the supply of highly trained people, this Partnership will have a major impact on knowledge. Our PDRAs and PhD candidates will tackle challenges at the forefront of sustainable pharmaceutical chemistry and through our links to GSK we will apply this knowledge to industry, whilst also filing patents and publishing in high impact journals. Our knowledge-based activities will drive innovation and economic activity, realising impact through creation of new jobs and securing our future."
7,9C5CCF7F-1E60-4655-A5B6-FFEEC3A8AE05,Realising Accountable Intelligent Systems,"Intelligent systems technologies are being utilised in more and more scenarios including autonomous vehicles, smart home appliances, public services, retail and manufacturing. But what happens when such systems fail, as in the case of recent high-profile accidents involving autonomous vehicles? How are such systems (and their developers) held to account if they are found to be making biased or unfair decisions? Can we interrogate intelligent systems, to ensure they are fit for purpose before they're deployed? These are all real and timely challenges, given that intelligent systems will increasingly affect many aspects of everyday life.

While all new technologies have the capacity to do harm, with intelligent systems it may be difficult or even impossible to know what went wrong or who should be held responsible. There is a very real concern that the complexity of many AI technologies, the data and interactions between the surrounding systems and workflows, will reduce the justification for consequential decisions to &quot;the algorithm made me do it&quot;, or indeed &quot;we don't know what happened&quot;. And yet the potential for such systems to outperform humans in accuracy of decision-making, and even safety suggests that the desire to use them will be difficult to resist. The question then is how we might endeavour to have the best of both worlds. How can we benefit from the superhuman capacity and efficiency that such systems offer without giving up our desire for accountability, transparency and responsibility? How can we avoid a stalemate choice between forgoing the benefits of automated systems altogether or accepting a degree of arbitrariness that would be unthinkable in society's usual human relationships?

Working closely with a range of stakeholders, including members of the public, the legal profession and technology companies, we will explore what it means to realise future intelligent systems that are transparent and accountable. The Accountability Fabric is our vision of a future computational infrastructure supporting audit of such systems - somewhat analogous to (but more sophisticated than) the 'blackbox' flight recorders associated with passenger aircraft. Our work will increase transparency not only after the fact, but also in a manner which allows for early interrogation and audit which in turn may help to prevent or to mitigate harm ex ante. Before we can realise the Accountability Fabric, several key issues need to be investigated:

What are the important factors that influence citizen's perceptions of trust and accountability of intelligent systems? 

What form ought legal liability take for intelligent systems? How can the law operate fairly and incentivize optimal behaviour from those developing/using such systems?

How do we formulate an appropriate vocabulary with which to describe and characterise intelligent systems, their context, behaviours and biases?

What are the technical means for recording the behaviour of intelligent systems, from the data used, the algorithms deployed, and the flow-on effects of the decisions being made?

Can we realise an accountability solution for intelligent systems, operating across a range of technologies and organisational boundaries, that is able to support third party audit and assessment? 

Answers to these (and the many other questions that will certainly emerge) will lead us to develop prototype solutions that will be evaluated with project partners. Our ambition is to create a means by which the developer of an intelligent system can provide a secure, tamper-proof record of the system's characteristics and behaviours that can be shared (under controlled circumstances) with relevant authorities in the event of an incident or complaint.",,
8,D2069419-4A42-4319-B01E-776B79619513,Augmented Humanity: Does the Human Enhance the Machine or the Machine Enhance the Human?,"The project will support fellowships that aim to support:
To support the career development of talented early career researchers and nurture future leaders

To support the broader skills development of high-calibre recent doctoral graduates or early career post-doctoral researchers in the art and humanities, particularly in relation to working with creative economy partners to support the wider impact of research.

To support projects which will contribute to the Creative Economy 
To support research which is cross-disciplinary, collaborative and innovation-orientated.",,See case for support
9,FF65FE43-95F4-4102-845E-A9BD3F778EEB,A Robot Chemist,"Eve is an artificially-intelligent 'Robot Scientist' designed to make drug discovery faster and much cheaper. She has already discovered that a compound used in soap and toothpaste might also be used in the fight against drug-resistant malaria, demonstrating its success. The proposal is to now give Eve the ability to do chemical reactions and to synthesise new compounds.

Eve inhabits an enclosure 2.5 meters in length, 2 metres wide, and 93 meters high. It consists of two robot arms, surrounded by equipment regularly found in laboratories for dispensing liquids into a large number of wells lined up on plastic plates, then incubating and testing them. But by integrating together instruments usually separated into different departments, Eve can do tests and interpret the results, and go on and use that knowledge in further tests faster. We will now give Eve the power to design and make her own, new compounds before testing their potential for drug discovery.

For the majority of medicines available today, scientists view drug molecules as nanometre-scale keys that slot into similarly sized protein or enzyme locks in cells in our bodies. Drug screening tests put these locks using biological systems that trigger a signal, such as a fluorescent flash, when a molecule fits into it like a key. 

While pharmaceutical industry screening can identify positive signals known as hits, Eve is also independently able to follow up and check if the hits were true prospects, known as leads. But simply screening and following up hits is not where Eve's greatest promise for drug discovery lies. Instead, by learning from the results from those tests, Eve is able to do what it currently takes teams of chemists and biologists many months to hammer out. Drug researchers currently already use software that employs 'machine learning' to take screening results and create a 'quantitative structure-activity relationship'. This is a mathematical function that relates the composition, shape and properties like fattiness and electrical change of the molecules, to how good drugs they are likely to be. Using such models scientists choose which molecule to make and test next. 

Currently, Eve can only learn to predict which out of a large set of ~15,000 compounds would be hits. The proposal is to add to Eve the ability to also synthesise novel compounds. In particular, we will program Eve to be able to carry out a chemical process known as 'late stage functionalization'. This is the introduction of a medicinally-relevant chemical group to existing drug-like molecules in Eve's library. This will enable Eve to make new chemical entities and to form an extended collection of drug-like molecules. We will program Eve to use machine learning to (1) Learn how to best to design drugs using late stage functionalization, and (2) learn which molecules are most likely to undergo successful late stage functionalization.

An important goal of our project, therefore, is for Eve to develop, optimize and 'road test' a new and important chemical process that will be of great use to molecule-makers around the world. However, the main project goal is to make drug discovery cheaper and faster. This will enable the development of treatments for diseases currently neglected for economic reasons, such as tropical and orphan diseases, and generally increase the supply of new drugs, and so potentially improve the lives of millions of people worldwide.",,"Impact Summary
The proposal is an ambitious one with a high potential for significant technological, medical, economical, and societal impact.


Recognised National Importance - AI
The importance of AI research to the UK is recognised at the highest level: On 22.11.17 The Chancellor announced in his Autumn Budget &pound;75M for AI research and development; On 27.11.17 The UK government's &quot;Industrial Strategy: building a Britain fit for the future&quot; white paper was published. The goal is to &quot;propel Britain to global leadership of the industries of the future - from artificial intelligence and big data to clean energy and self-driving vehicles.&quot; Prime Minister.

Recognised National Importance - Chemistry
As molecules are prepared and manipulated in the day-to-day work of academic scientists and scientists in established (pharmaceutical, agrochemical, contract research organisations) and emerging (organic electronics, biotech /biopharmaceutical) industries in the UK, our studies aiming to develop the first Robert Chemist are aligned with the needs of UK industry (and academia). 


Technology Readiness
The proposal aims to take the concept of a Robot Chemist from TR2 (Technology concept and/or application formulated) to TR4 (Early proof of concept demonstrated in the lab).


Commercial Opportunities 
The integration of AI with Drug Design is currently a &quot;hot topic&quot;. RDK has close ties with Exscientia, with whom he collaborated on a previous EPSRC grant. Since then the Dundee based start-up have done numerous large deals with big Pharma. The most notable of these is a deal worth potentially Euro 250M with Sanofi. The leading UK company in the area of AI and drug design is Benevolent AI. They are a 'Unicorn' company now worth &pound;2Billion.

In addition to a number of successful academic collaborations, DJP has active funded collaborations with industrial teams (AstraZeneca x 2, Lilly UK). DJP has received 30 grants from UK Industry (AstraZeneca, Lilly UK, GlaxoSmithKline, Syngenta, Pfizer, Avecia, Novartis, Celltech, MSD, OSI Pharmaceuticals, SAFC-Hitech, Pentagon Chemicals Ltd), received Pfizer and AstraZeneca Strategic Funding, and has consulted for three companies (Antabio, Syntor Fine Chemicals, PZ Cussons). Knowledge exchange in the PI's team has also been facilitated through the delivery of invited Industrial courses for AstraZeneca (2006, 2010) and GSK (2009, 2013-2016). Knowledge Transfer Partnerships (ACAL Energy, Pentagon Fine Chemicals Ltd) have successfully implanted DJP's synthetic expertise into two companies. Finally, medicinal chemistry teams at AstraZeneca (UK) and Theravance Inc. (biopharmaceutical company, California USA) have used DJP's processes to generate lead compounds during industrial projects in important therapeutic areas. 

Follow-on Funding
The proposal is for an initial 'feasibility study'. To continue the research we will require follow-on funding. We will investigate opportunities for both grant and commercial funding. Potential sources of grant funding are the EPSRC (especially a possible 'Digital Chemistry' call - see the recent EPSRC workshop), H2020, the Welcome Trust, DARPA, etc. Potential commercial sources of funding are large Pharmaceutical and Health product companies, and new AI/Chemistry companies that are starting to appear, e.g. Exscientia, Benevolent AI, etc. We will also approach our VC contacts, as it is our experience that they are willing to invest in such early stage developments.


The evidential support to produce a high quality follow on funding application will come from two main sources: publications, and Intellectual Property (IP)

Research capacity building 
The project will train two PDRAs in key areas of future science and technology: AI, machine learning, laboratory automation, synthetic organic chemistry."
10,17B1B72C-8084-4E6A-B7C3-15933738887C,PETRAS 2,"Rapidly developing digital technologies, together with social and business trends, are providing huge opportunities for innovation in product and service markets, and also in government processes. Technology developments drive socioeconomic and behavioural changes and vice versa, and the rate of change in these makes tracking and responding to high-speed developments a significant challenge in public and private sectors alike. Agile governance and policy-making for emerging technologies is likely to become a key theme in strategic thinking for the public and private sectors.
Particular trends that are challenging now, and will increasingly challenge society include developments in technologies on the outskirts of the internet. These include Artificial Intelligence, not just in the cloud but in Edge computing, and in Internet of Things devices and networks. Alongside and in conjunction with this ecosystem, is Distributed Ledger Technology. Together this ensemble of technologies will enable innovations that promote productivity, like peer-to-peer dynamic contracts and other decision processes, with or without human sight or intervention. However, the ensemble's autonomy, proliferation and use in critical applications, makes the potential for hacking and similar attacks very significant, with the likelihood of them growing to become an issue of strategic national importance.
To address this challenge, and to preserve the immense economic and productivity benefits that will come from the successful deployment and application of digital technologies 'at the edge', a focused initiative is needed. Ideally, this will use the UK's current platform of experience in the safe and secure application of the Internet of Things. The contributors to this platform include PETRAS partners, and several other centres of excellence around the UK.
It is therefore proposed to build an inclusive PETRAS 2 Research Centre with national strategic value, on the established and successful platform of the PETRAS Hub. This will inherit its governance and management models, which have demonstrated the ability to coordinate and convene collaboration across 11 universities and 110 industrial and government User Partners, but will importantly step up its mission and inclusivity through open research calls for new and existing academic partners. PETRAS 2 will maintain an agile and shared research agenda that views social and physical science challenges with equal measure, and covers a broad range of Technology Readiness Levels, particularly those close to market. It will operate as a virtual centre, providing a magnet for collaboration for user partners and a single expert voice for government. User partner engagement is likely to be strong following the successes of the current PETRAS programme, which has raised over &pound;1m in cash contributions from partners during 2018.
The new PETRAS 2 'Secure Digital Technologies at the Edge' methodology will inherit the best of PETRAS, including open calls to the UK research community and a partnership-building fund that allows a responsive approach to opportunities that emerge from existing and new user and academic partnerships. PETRAS 2 will be driven by sectoral cybersecurity priorities while retaining a discovery research agenda to horizon-scan and develop understanding of new threats and opportunities. The scope of projects and the associated Innovate UK SDTaP demonstrators, spans early to late TRLs and aims to put knowledge into real user partner practice. Furthermore, the development of many early career researchers through PETRAS 2 research activities should lead to a step change in our national capability and capacity to address this highly dynamic area of socio-technical opportunity and risk.",,"The overarching aim of the proposed PETRAS 2 Centre is to build the flexible national capacity for creating a comprehensive &amp; systematic understanding of the opportunities and threats arising when edge computing nodes are deployed, migrating AI/ML technologies to the periphery of the internet and local IoT networks. To deliver this aim, the PETRAS 2 Centre will reach beyond academia and engage with the public, private and civil society sectors in the UK and internationally. The key impact objectives for the Centre will therefore include, but not be limited to, the following;

1. To provide strategic advice and policy insight, becoming the go-to resource on SDTaP issues for the public and private sector: PETRAS 2 will provide a nexus between the two communities, academic and user, matching a base of existing published work and research agendas that can be executed with PETRAS funding. Critically, this provide a coherent, expert, trusted view which decision-makers can rely upon to be responsive to their needs. Given there is a proliferation of organisations and initiatives dealing with the challenges and opportunities that data and AI present, both within the UK and internationally, the Centre will work closely with these existing organisations (for example, the Ada Lovelace Institute, Alan Turing Institute, the Centre for Data Ethics and Innovation, the Office for AI, the Information Commissioner's Office and the AI Council), to provide a unique, impactful contribution at the focal point of IoT, AI and Cyber security .

2. To functionally improve the capacity of government to rise to the challenge of the 'arms race' of ever more complex sociotechnical systems and threats: We will produce a cohort of post-doctoral experts with experience of cross-disciplinary working and challenge-oriented thinking. They will feed the growing need for cybersecurity expertise in government and industry. We recognise the need to draw from all areas of society and will encourage those with talent from outside academia to engage, and where relevant, embark on academic careers. In this way, we will ensure diversity is a key consideration in our pathways to impact activities.

3. To increase the early adoption of new methods and technologies by our industrial, service and government User Partners: We will achieve this through translating and evaluating many of our research outcomes to real environments. This process will be facilitated by the incorporation at governance-level of the Innovate UK SDTaP Demonstrator and Commercialisation programmes. This will also increase the confidence of our industrial, service and government User Partners, thereby promoting the early adoption of new methods and technologies. 

Our key organisation design objective is to create an inclusive Centre that can attract and closely connect experts and organisations, together forming a strategic national research capability in the cybersecurity of devices and networks at the edge of the internet. We will deliver this objective by using innovative processes for the synthesis and communication of the research programme which will ensure that the unique benefit of the PETRAS socio-technical approach is translated effectively to industry, government and broader non-technical audiences. Each year, in addition to an IET partnered conference and project level activity, the Management Team will run at least four 'Centre level' events, designed to fully exploit the value of the collective insights and expertise of the Centre."
11,DD5FF82D-B5A0-45CE-BD91-FCF597A0EF57,Realising Accountable Intelligent Systems (RAInS),"Intelligent systems technologies are being utilised in more and more scenarios including autonomous vehicles, smart home appliances, public services, retail and manufacturing. But what happens when such systems fail, as in the case of recent high-profile accidents involving autonomous vehicles? How are such systems (and their developers) held to account if they are found to be making biased or unfair decisions? Can we interrogate intelligent systems, to ensure they are fit for purpose before they're deployed? These are all real and timely challenges, given that intelligent systems will increasingly affect many aspects of everyday life.

While all new technologies have the capacity to do harm, with intelligent systems it may be difficult or even impossible to know what went wrong or who should be held responsible. There is a very real concern that the complexity of many AI technologies, the data and interactions between the surrounding systems and workflows, will reduce the justification for consequential decisions to &quot;the algorithm made me do it&quot;, or indeed &quot;we don't know what happened&quot;. And yet the potential for such systems to outperform humans in accuracy of decision-making, and even safety suggests that the desire to use them will be difficult to resist. The question then is how we might endeavour to have the best of both worlds. How can we benefit from the superhuman capacity and efficiency that such systems offer without giving up our desire for accountability, transparency and responsibility? How can we avoid a stalemate choice between forgoing the benefits of automated systems altogether or accepting a degree of arbitrariness that would be unthinkable in society's usual human relationships?

Working closely with a range of stakeholders, including members of the public, the legal profession and technology companies, we will explore what it means to realise future intelligent systems that are transparent and accountable. The Accountability Fabric is our vision of a future computational infrastructure supporting audit of such systems - somewhat analogous to (but more sophisticated than) the 'blackbox' flight recorders associated with passenger aircraft. Our work will increase transparency not only after the fact, but also in a manner which allows for early interrogation and audit which in turn may help to prevent or to mitigate harm ex ante. Before we can realise the Accountability Fabric, several key issues need to be investigated:

What are the important factors that influence citizen's perceptions of trust and accountability of intelligent systems? 

What form ought legal liability take for intelligent systems? How can the law operate fairly and incentivize optimal behaviour from those developing/using such systems?

How do we formulate an appropriate vocabulary with which to describe and characterise intelligent systems, their context, behaviours and biases?

What are the technical means for recording the behaviour of intelligent systems, from the data used, the algorithms deployed, and the flow-on effects of the decisions being made?

Can we realise an accountability solution for intelligent systems, operating across a range of technologies and organisational boundaries, that is able to support third party audit and assessment? 

Answers to these (and the many other questions that will certainly emerge) will lead us to develop prototype solutions that will be evaluated with project partners. Our ambition is to create a means by which the developer of an intelligent system can provide a secure, tamper-proof record of the system's characteristics and behaviours that can be shared (under controlled circumstances) with relevant authorities in the event of an incident or complaint.",,"Issues of accountability regarding automated and intelligent systems touch all parts of society. Therefore, in broad terms, our work on providing the means for articulating, interrogating, validating and assessing intelligent systems and their behaviour brings great benefits to: 

* Individuals

* Public sector organisations

* Government and policy-makers - in terms of those 
 - developing the regulatory frameworks around emerging technology (AI, autonomous systems, etc);
 - using intelligent systems as part of policy implementation.

* Business (including SMEs), including both:
 - those active in the autonomous systems, AI, smart technology marketplace; 
 - users of intelligent systems to achieve business aims.

How will they benefit from this research? 

Broadly, individuals will benefit from this work, as it brings transparency and the means to challenge automated systems affecting their lives. Specifically, members of the public will benefit from their direct involvement in the research - through their participation in activities (including user workshops) which explore issues of accountability - and their ability to directly shape the research agenda. The wider public will be exposed to these issues via a series of public engagement activities (organised under the Alt-AI [Accountability-Liability-Transparency] banner) - our aim being to stimulate debate about the future of intelligent systems and society.

Public organisations will gain greater understanding of the challenges associated with future technology deployments, and models for system accountability. Importantly, increased accountability and explainability of systems will work towards the public acceptability of such technology, while working to address public-sector concerns regarding safety, fairness, bias, etc, thereby encouraging the benefits of data-driven policy implementation.

Government and policy-makers at local, devolved and national levels will be able to access evidence drawn from real user scenarios, as well as the opinions of citizens and members of the legal profession. We will provide useful resources both for legislators and for courts considering how such technologies should be used, as well as for public authorities and policy-makers more generally in establishing public trust in the use of such systems. At a technical-level, devising novel approaches for both capturing evidence on how intelligent systems operate, and by making this auditable, we provide the means for producing the evidence for proper (governmental/judicial) oversight over intelligent systems. Further, technical means will work to shape regulatory frameworks (e.g. which might embed &quot;accountability by design&quot; principles, as has been done for 'privacy/security by design').

Technology businesses will gain access to a range of solutions necessary to enhance transparency and accountability of future intelligent systems. This is crucial for the industry, as otherwise the public concern regarding such issues will hinder adoption. Our approach will be accessible through a range of open source software prototypes and frameworks, promoted through academic and industrial forums and through an online presence. Through preliminary conversations with IBM who are leaders in the intelligent system (cognitive computing space) there is clear evidence of interest in our proposals. 

In terms of industry in general, businesses see much value in automating a range of processes, to bring about innovation and efficiency. Again, by tackling issues of accountability, this work directly works towards increasing public acceptability - to best ensure the full economic potential for the technology is realised."
12,B2FA570F-0F1F-4BC7-BC3F-CF0EA11E9413,Realising Accountable Intelligent Systems (RAInS),"Intelligent systems technologies are being utilised in more and more scenarios including autonomous vehicles, smart home appliances, public services, retail and manufacturing. But what happens when such systems fail, as in the case of recent high-profile accidents involving autonomous vehicles? How are such systems (and their developers) held to account if they are found to be making biased or unfair decisions? Can we interrogate intelligent systems, to ensure they are fit for purpose before they're deployed? These are all real and timely challenges, given that intelligent systems will increasingly affect many aspects of everyday life.

While all new technologies have the capacity to do harm, with intelligent systems it may be difficult or even impossible to know what went wrong or who should be held responsible. There is a very real concern that the complexity of many AI technologies, the data and interactions between the surrounding systems and workflows, will reduce the justification for consequential decisions to &quot;the algorithm made me do it&quot;, or indeed &quot;we don't know what happened&quot;. And yet the potential for such systems to outperform humans in accuracy of decision-making, and even safety suggests that the desire to use them will be difficult to resist. The question then is how we might endeavour to have the best of both worlds. How can we benefit from the superhuman capacity and efficiency that such systems offer without giving up our desire for accountability, transparency and responsibility? How can we avoid a stalemate choice between forgoing the benefits of automated systems altogether or accepting a degree of arbitrariness that would be unthinkable in society's usual human relationships?

Working closely with a range of stakeholders, including members of the public, the legal profession and technology companies, we will explore what it means to realise future intelligent systems that are transparent and accountable. The Accountability Fabric is our vision of a future computational infrastructure supporting audit of such systems - somewhat analogous to (but more sophisticated than) the 'blackbox' flight recorders associated with passenger aircraft. Our work will increase transparency not only after the fact, but also in a manner which allows for early interrogation and audit which in turn may help to prevent or to mitigate harm ex ante. Before we can realise the Accountability Fabric, several key issues need to be investigated:

What are the important factors that influence citizen's perceptions of trust and accountability of intelligent systems? 

What form ought legal liability take for intelligent systems? How can the law operate fairly and incentivize optimal behaviour from those developing/using such systems?

How do we formulate an appropriate vocabulary with which to describe and characterise intelligent systems, their context, behaviours and biases?

What are the technical means for recording the behaviour of intelligent systems, from the data used, the algorithms deployed, and the flow-on effects of the decisions being made?

Can we realise an accountability solution for intelligent systems, operating across a range of technologies and organisational boundaries, that is able to support third party audit and assessment? 

Answers to these (and the many other questions that will certainly emerge) will lead us to develop prototype solutions that will be evaluated with project partners. Our ambition is to create a means by which the developer of an intelligent system can provide a secure, tamper-proof record of the system's characteristics and behaviours that can be shared (under controlled circumstances) with relevant authorities in the event of an incident or complaint.",,
13,C1F6824F-80BB-4E61-8663-DE640C57AE2D,Unlocking the Potential of AI for English Law,"The proposed research will explore the potential and limitations of using artificial intelligence (AI) in support of legal services. AI's capabilities have made enormous recent leaps; many expect it to transform how the economy operates. In particular, activities relying on human knowledge to create value, insulated until now from mechanisation, are facing dramatic change. Amongst these are professional services, such as law.

Like other professions, legal services contribute to the economy both through revenues of service providers and through benefits provided to clients. For large business clients, who can choose which legal regime will govern their affairs, UK legal services are an export good. For small businesses and citizens, working within the domestic legal system, UK legal services affect costs directly. Yet unlike other professions, the legal system has a dual role in society. Beyond the law's role in governing economic order, the legal system is more fundamentally a structure for social order. It sets out rules agreed on by society, and also the limits of politicians' ability to enact these rules. 

Consequently, the stakes for AI's implementation in UK legal services are high. If mishandled, it could threaten both economic success and governance more generally. Yet if executed effectively, it is an opportunity to improve legal services not only for export but also for citizens and domestic small businesses. Our research seeks to identify how constraints on the implementation of AI in legal services can be relaxed to unlock its potential for good. 

One major challenge is the need for 'complementary' adjustments. Adopting a disruptive new technology like AI requires changes in skills, training, and working practices, without which the productivity gains will be muted. We will investigate training and educational needs for lawyers' engagement with technology and programmers' engagement with law. With private sector partners, we will develop education and training packages that respond to these needs for delivery by both universities and private-sector firms. We will investigate emerging business models deploying AI in law, and identify best practice in governance and strategy. Finally, we will compare skills training and technology transfer in the UK with countries such as the US, Hong Kong and Singapore, and ask what UK policymakers can learn from these competitors. To the extent that these issues are also faced by other high-value professional services, these parts of our results will also have relevance for them.

However, the dual role of the legal system poses unique challenges that justify a research package focusing primarily on this sector. There are constitutional limits to how far law's operation can be adjusted for economic reasons: we term this second constraint 'legitimacy'. We will map how automation in dispute resolution might trigger constitutional legal challenges, how these challenges relate to types of dispute resolution technology and types of claim, and use the resulting matrix to identify opportunities for maximum benefit from automation in dispute resolution. 

A third constraint is the limits of technological possibility. AI systems rely on machine learning, which reaches answers by identifying patterns in very large amounts of data. Its limitations are the size of the datasets needed, and its inability to provide an explanation for how the answer was reached. This poses particular difficulties for law, where many applications require or benefit from reasons being given. We will explore the possibility for frontier AI technologies to deliver legal reasoning. 

The research will involve a mix of disciplinary inputs, reflecting the multi-faceted nature of the problem: Law, Computer Science, Economics, Education, Management and Political Economy. Working closely with private-sector partners will ensure our research benefits from insights into, and testing against, real requirements.",,"The project goal is to unlock the power of AI in UK legal services to ensure the future of these services for the UK and global digital economy. The proposal has been co-developed with a core group of partner organisations and a broader user group, represented in our Advisory Panel, to whom the outputs of our research will be relevant. 

The work packages (WPs) will enable our partners and broader user group to identify and implement ways to relax constraints on AI adoption in legal services. In particular, by identifying (a) complementary investments to facilitate adoption of AI in legal services: successful firm structures for AI business models (WP1) and skills and training necessary to maximise value from AI technologies (WP4 and WP5); (b) the nature of legal constraints (legitimacy) on the deployment of emerging AI applications in dispute resolution (WP2) and (c) ways to improve AI's functionality for legal reasoning (WP3).

More broadly, we anticipate the WPs will benefit the following user groups:
(i) Legal services firms (incumbent law firms, alternative legal service providers, in-house legal teams) who will benefit from understanding the business models they might adopt (WP1); ways to deploy emerging dispute resolution technology (WP2); advances in AI technology for law (WP3); and how best a range of skills might be coordinated to work together (WP4 and WP5).
(ii) Users of legal services (organisations and individuals), who will benefit from increased efficiency in legal services, and in particular through more direct access to justice, as explored in WP2, and new technological developments in WP3. 
(iii) Educators and Academics in law and technology, who will benefit in particular from the outputs of WP5.
(iv) Students of Law and Computer Science and other professional services that will be disrupted by AI, again, in particular through WP5, but also WP2 and WP3.
(v) Professional Associations, Regulators and Policymakers such as the Solicitors Regulation Authority, the Bar Council, the Law Society, the Ministry of Justice, HM Courts and Tribunals Service, and the Law Commission, who will benefit from the holistic coverage provided by all the WPs in examining what is being done (WP1), what can potentially be done (WP3), how this might translate directly into digital justice (WP2), how skills can best be coordinated (WP4) and how lawyers and computer scientists of the future might best be trained, both in their own fields and in co-working (WP5). This will help these bodies understand their role in supporting the adoption of technology in a way that is trusted and safe, but also enables the UK to maintain its status as a global market leader. In order to achieve this we will participate in initiatives such as the Law Commission's LawTech Policy Commission and the Ministry of Justice's new LawTech panel.
(vi) User groups corresponding to (i)-(v) in other high-value service sectors, including the Institute of Chartered Accountants of England and Wales, the Financial Reporting Council, the Association of British Insurers and the Prudential Regulation Authority. These face similar challenges and can therefore use the understanding gained through our WPs in the legal context to inform their own industries.

In addition, the project will engage with key opinion formers - columnists, bloggers, social networkers, legal affairs correspondents, think-tanks, industry associations and conferences (eg Legal Geek), and the legal media (eg Legal Week, The American Lawyer) etc. This will enable the further dissemination of the results of all the WPs, as well as engaging and informing the public in the UK and elsewhere."
14,849D99B0-F348-4376-9D4D-3E151C17635F,PredictinB statg Tus of dairy cows from mid infra-red spectral data using machine learning,"Bovine tuberculosis (bTB) is a chronic, infectious and zoonotic (i.e., it can be transmitted to humans) disease endemic in the UK and other countries, and presents a significant challenge to the UK cattle sector particularly in the south west of England and south Wales. The Department for Environment, Food and Rural Affairs (DEFRA) lists bTB as one of the four most important livestock diseases globally. The continued spread of bTB among cattle in England and Wales has been a socioeconomic disaster for over 40 years, causing catastrophic and devastating damage to farming businesses both large and small. In 2017 the number of animals in the UK slaughtered due to bTB was in excess of 43,500. The disease has proven difficult to completely eradicate using techniques that are socially acceptable and at a cost acceptable to the UK taxpayer. Current costs are estimated at over &pound;175 million per year with an average cost of &pound;34,000 per bTB outbreak per farm. The continued polarised debate on the role of wildlife as a farmed cattle disease reservoir is making progress slow. This project seeks to develop a non-invasive tool created from routine milk recording of dairy cattle to predict bTB status from milk analysis (by spectrophotometry) by exploiting state of the art Deep Learning techniques. Deep learning is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms (an algorithm is a process followed to solve calculations). Deep learning works by imitating the way that the human brain works and involves feeding a computer system a large volume of data, which it can use to make decisions about other data. This method of analysis has been successfully deployed by our group to predict pregnancy status in dairy cows with high accuracy and hence expectations are high that bTB leaves a signal in milk that can be detected with Deep Learning applied to MIR spectral data. 

The involvement of a commercial partner (NMR, National Milk Records) that is extensively active in the bTB area ensures that results can be rapidly applied to maximise impact in the short term. Furthermore, NMR has a long history of supporting dairy farmers in herd management (including disease) and so the results of this project will be exploited in a familiar context for dairy farmers ensuring its widespread uptake.","Using a Deep Learning approach we will develop a computer pipeline for routine prediction of bTB status from milk MIR spectra, a by-product of routine milk recording. 

Individual cow records from multiple sources will be collated into a central database at SRUC. Records will include animal and herd identification, lactation information, pedigree, bTB skin test status, date of birth/death, movements and MIR spectra. Currently, after routine predictions for milk fat and protein have been carried out, the spectral data are stored for prediction of other important traits such as fatty acids and body energy.

Deep learning (a sub class of Machine Learning) will be utilised to analyse historic national bTB test results and milk MIR spectral data. Data will be modelled using deep convolutional neural networks following a supervised approach and validated in the first instance using SRUC's officially TB-free Langhill herd. Further validation will come from applying the model to a variety of different datasets set up to test predictions both between and within herd. Thereafter, the prediction pipeline will used to investigate whether the milk MIR can be used to determine the point at which a cow became infected with bTB. If successful this would offer the potential to significantly reduce the length of bTB breakdowns by allowing the removal/isolation of infected cows from the herd sooner than is the case currently.

The computing system will be constructed so as to allow for immediate deployment by NMR, the UK's largest milk recording organisation, in a commercial setting with on-going support from SRUC. A number of options will be considered including the use of a specialised GPU powered server vs. Cloud based server for model training or localised offline model training followed by real-time prediction. The objective is to construct a set of processes that allow real time predictions of bTB status at a cost that will be economically realistic and feasible for NMR.","The impact of the proposed work is expected to be multi-faceted as detailed below: 

Milk recording companies and other organisations involved in livestock breeding (e.g. breed societies, levy boards, breeding companies): A successful outcome of the project is expected to increase the return on investment in milk recording by using the same milk sample to predict a range of additional traits; explicitly bTB status. The industrial partner (NMR), in particular, will further benefit from being able to immediately and directly commercialise the outcomes in the form of additional services to farmers and to create new and novel services to assist dairy farmers in managing animal disease. Recording also allows monitoring bTB prevalence and so the efficacy of the testing and culling scheme. 

Government: bTB is expensive for both government and taxpayer. It consumes finance that could be better utilised in other areas to generate more income (and tax). The potentially reduced incidence of major disease outbreaks will create a more vibrant and efficient dairy system by allowing reduced restrictions and less risky trading of cattle. The removal of infectious animals earlier is expected (but not yet known) to lower the general level of infection and potentially, the cow to wildlife transmission thereby altering the dynamics of bTB spread. 

Farmers: At present over 3,700 herds have a breakdown status and in the last year (2017) alone over 43,500 animals were slaughtered as a result of testing positive for bTB. This has profound implications for farmers not only in a business sense but also psychologically. No farmer wants to have rampant bTB on their farm and so any initiative that helps in the early identification and rapid removal of potentially infectious animals from the dairy herd will find widespread approval. Most importantly, the optimal utilisation of genetic resources in short term selection will enhance the long term sustainability of the supply chain. Currently, high values animals are being lost to the selection pipeline through this disease. 

Consumers: The successful implementation of the proposed testing framework will enhance the efficiency of the dairy industry and the profitability of the sector. The benefits of improved efficiency and robustness of agricultural systems has benefits right across the entire supply chain as seen recently when supply chains were disrupted due to extreme weather. Consumers will benefit through tax revenue being utilised for alternative initiatives. 

The UK science base: The methods developed and project data produced will contribute to the increased research capacity within the UK (and beyond). The scope of the project is aligned with the BBSRC Strategic Research Priority 1 (Agriculture and Food Security) and successful completion will contribute to the competitiveness and excellence of the UK science base as well as its positioning at the frontiers of delivering novel tools to address the challenges of global agricultural production. Of particular note is the use of Deep Learning with large scale agricultural animal data to gain new insights to improve food production. 

Training: The proposed research will feature in training courses that the applicants are regularly invited to present e.g. farmer training days, &quot;Vetnomics&quot; and undergrad teaching. The PDRA working on the project will have the opportunity to be trained in a cutting edge area of research on the use of Deep Learning to predict new disease phenotypes from milk mid infra-red spectral data, while interacting with other scientists in a world-leading research environment as well as with a leading commercial partner intent on application and making a difference. 

Policy: The reduction of bTB in the dairy herd is a vital objective of DEFRA and the outcomes of this project have the potential to contribute to the 25 year TB Eradication Strategy without compromising long-term competitiveness of the dairy herd."
15,EB80621E-3FBF-493B-8A6D-C1E9DB3FED42,"Art, Artifice &amp; Intelligence: A UK-Japan partnership exploring art and AI","Technologies of artificial intelligence are an increasing part of our everyday lives. Neural networks and deep learning find application in vast areas such as the financial markets and weather prediction. We are told that many traditional kinds of jobs may be in danger of being replaced by automation. Artificial intelligence can emulate human decision making, and software programmes can now beat chess masters. AI research in the UK and Japan are at the forefront of international advancement in the field. Programmes by London companies like DeepMind take the chess example to the next level of complexity by playing the ancient Japanese game of Go. In Japan, advanced robots that take on humanoid form have been deployed in healthcare settings such as minding older people.

While AI and automation might better human chess players or risk to replace jobs, they can be seen as a partner in dialogue with human activity. This is seen in a compelling way in the world of art. While computer programmes have been created to generate visual images and algorithmically compose music, it is in partnership with a human artist that new forms of art can emerge. If an AI algorithm becomes part of the creative palette of an artist, what kinds of new work emerge? For an artist to be able to harness these advanced technologies, how do they need to be configured? Is the human artist a partner, master, or mere operator? How do these new techniques change our aesthetic sense and challenge what might constitute a work of art or piece of music? 

The Art, Artifice &amp; Intelligence project brings together leading research labs from the UK and Japan to foster new partnerships to explore the creative potential of artificial intelligence in art and music. The Embodied Audiovisual Interaction (EAVI) unit from Goldsmiths, University of London, will lead the project in partnership with the SACRAL artificial life laboratory of the University of Tokyo and the Faculty of Design at Kyushu University. The project will facilitate exchange of academics, young researchers, and students between the UK and Japan to share knowledge and best practices in harnessing AI technologies in creative settings. The project activities will take place in a series of workshops - two in Japan and one in London - and in a 3-month residency for a young UK researcher to develop a new project in Japan. The results of the project will be presented to general audiences in public exhibition/performance events in London, and in Japan. We will work with cultural institutions such as the Barbican in London and the Yamaguchi Center for Art &amp; Media (YCAM). The project will be a trigger for future large scale collaborations in art and AI between the UK and Japan, and bring to the public eye the rich histories, technologies, and critical perspectives that underpin our present-day fascination with artificial intelligence.",,"The project results will have wide ranging impact on the creative sector and the general public. It will first be of interest to artists, musicians, and designers whose creative palettes mifght be extended by the AI technologies demonstrated in the project. Members of the popular press will have historical, technological, and cultural context to inform their reporting. Amongst the general public, the project will be of interest to young, technology savvy professionals who are at ease with the technologies of a digital society and who may work in the technology sector. For them to apprehend the artistic possibilities of AI will be of keen interest. Finally, to an art-going public interested in new forms of cultural production will gain access to information on the ways AI technologies become creative tools. This impact will be realised by working with cultural sector actors like YCAM and the Barbican."
16,D602D842-CF11-4B66-8C31-3006F35E6F20,Using Epigenetically-Inspired Connectionist Models to Provide Transparency In The Modelling of Human Visceral Leismaniasis,"Biologically inspired connectionist models are made up of multiple interconnected units which are designed to mimic biological processes in nature which give rise to emergent phenomena. Typically, connectionist models are used as computational tools which are capable of learning by example, for instance predicting the next days activity on the stock exchange by learning from previous months data. Epigenetically inspired connectionist models (EICMs) are a particular type of biologically inspired connectionist model which allow for the activation and deactivation of their interconnected units whist they are solving a task. These models have been shown to break complex tasks down into smaller sub-tasks autonomously, with certain interconnected units being applied to certain sub-tasks, and other interconnected units being applied to other sub-tasks.
 
Biologically inspired connectionist models in general are difficult to interpret. Their decision making processes are an emergent property of their interconnected units, from which it is very difficult to provide an explanation as to why specific decisions have been made. Because of this, deriving confidence from the decisions they make is difficult. Having confidence in the decision making process is of importance especially when the tasks they are applied to are in domains which are considered &quot;high risk&quot; such as medical simulations and financial forecasting.
 
To address these issues, this work aims to develop a set of techniques which allow for EICMs to provide a rationale for their decision making process, essentially making its decisions transparent. This will be achieved by analysing the way the model breaks down complex tasks, which of its units are active at any given time and then correlating this with the behaviour of both the network and the task. 
 
We apply the EICMs and the techniques developed in this project to improve the understanding of the often fatal disease human visceral leismaniasis (HVL). The immune response to HVL is a significant indicator of patient outcome and is the product of the interplay between multiple interacting cells, macrophages and specific cytokine responses. The project partner Simomics, a world leading disease modelling company, has a comprehensive data set which describes changes to the immune response in reference to HVL over varying timescales, and has provided it for use during this project.
 
The overall development of HVL and the immune response to it is not well understood. The techniques developed in this work which are able to provide a rationale for their decision making process, will be applied to learn the interplay and interactions between these processes. This will allow for model to provide an explanation of what processes are most important in the immune response over the duration of HVL infection. 
 
By contributing to the field of biological modelling, which places a strong emphasis on transparency and confidence in results, other fields will be able to adopt the models developed in this work to provide transparency in other domains.",,"The proposed research aims to develop connectionist models and computational tools which, inspired by epigenetic functionality in biology, allow for robust computational performance whilst allowing for transparent execution so that an explanation can be provided of its decision making processes. 

Biological modelling impact

In general, when connectionist models are used in biological modelling, its rationale for decisions being made are hidden from view. In addition to understanding the immune response to human visceral leismaniasis, this work aims to provide a platform for biologists, immune modellers and other &quot;high risk&quot; fields to be able to use the tools and models developed in this work. By sharing the software developed in this project we aim to give other domains the ability to interact with this work with no prior training and no expertise in computer science.
 
Bio-inspired impact

The work in this project takes a different approach to the classical form of bio-inspired computational models, namely that architectures can change their topology during execution. By showing that this leads to the novel transparency where a rationale is provided for the decisions it is making, this could generate a lot of interest in the scientific community for such models. In addition, these ideas could be used in other areas of bio-inspired computation other than what is prescribed in this work, as the benefits might be transferable.
 
Economic impact

This project has potential to stimulate a wide range of economic areas, as understanding the rationale of connectionist architectures is imperative to instil confidence in the results they provide. Working with Simomics in this project, we will provide a free lightweight version of the software developed in this work. This is to be utilised for independent researchers and small businesses. A more comprehensive commercial piece of software will also be developed with support for more large scale businesses which have complex and specific needs.
 
Impact on society

The open source tools developed throughout this work will be designed to be used by both experts and non-experts in computer science. It will be made as simple as possible for users to work with these novel technologies, to re-visit previous work and to generate a better understanding how it has been interpreted by other models. In addition, the tool itself can also be used to highlight new areas of investigation which have been previously unknown. This, twinned with the simplicity of use could have a profound effect on a number of different groups, fields and industries."
17,63F56B26-0636-4296-BADC-B965F2C3B5B3,Museums and Artificial Intelligence Network,"As far back as the mid 2000s museums were talking about Artificial Intelligence (AI), however while these technologies have become increasingly pervasive in wider society from voice activated systems such as Alexa to the promise of Tesla's self driving cars, they are only beginning to be explored, in a museum context. With the National Gallery (UK), The Metropolitan Museum of Art (US), American Museum of Natural History (US), MoMA (US), Cooper-Hewitt (US) all beginning to explore the potential of AI this network will bring together a range of senior museum professionals and prominent academics to develop the conversation around AI, ethics and museums.

AI technologies including machine learning, predictive analytics and others, bring exciting possibilities of knowing more about visitors and collections. However, it raises important challenges to ethics. With the increasing awareness and regulations about data usage, museums, must approach AI with both caution and fervour. To successfully achieve this senior museum professionals need to be provided with the opportunity to examine what this model might look like, and the wider impact that museums can have when it comes to advocating for new ethical standards. 
 
This research project will bring together museum professionals and scholars to discuss the cost, and indeed skills required to successfully adopt the true possibilities of AI. Cost and skill, have thus far acted as a barrier for museums, however with these technologies becoming more pervasive and skills more in demand, this is a timely moment for museums to explore the possibilities and ethical challenges of AI across their work from visitors to collections. As such this network seeks to challenge this known issue - ethics as an afterthought - by embedding it into the conversation on AI in Museums at this critical moment. A conversation that will help to inform funders and senior managers about the opportunities and challenges this technology poses for the sector.",,"The network will have important impacts for policy making across the museum sector, and will support funders particularly in the UK to decide what funding priorities and criteria should be used to catalyse ethically sound, creative and sustainable approaches to the development of AI in museums.

The network will engage with key sectoral bodies including the American Alliance of Museums and the Museums Association, and draw policy recommendations on professional standards and AI, standards that already exist around many other areas of museum practice. Both of which have demonstrated an interest in this nascent area of museum practice, with AAM recently holding a workshop on AI and MA publishing a number of short articles on the topic in their print publication Museums Journal.

The network will support the development of transatlantic interdisciplinary partnerships, that will develop the work of 1) Museums 2) Academics 3) Digital companies. For museums this will mean greater collaborative working, across the sector, and a move away from siloed work culture. For academics this will enable research that is grounded in critical praxis, and responds to current sector challenges. For digital companies, and freelancers attendance at the network will create opportunities for new markets, clients, and business, crucially these opportunities will provide international opportunities. 

Skills development is central to this network, and core network participants, and those that attend our workshops in London, New York or San Diego will develop new strategic, and management skills in relation to the development of AI technologies. These skills will ensure that museums and academics are able to write better project briefs in turn ensuring more relevant, ethical, and effective applications of these technologies are created within museums. This will have a wider impact on the museum institution and will help to shape new ways of working, and support the development of more robust and sustainable technology projects. 

The work of this network will have wider impacts beyond the museum sector, in that its work will also add to similar conversations in the third sector, namely the charity sector and civil society organisations. 

Through dissemination of this networks work, and the public event to be held at the Barbican, this network will open up a new conversation around AI and data distribution, that will help the wider public to become more conscious in how they permit public institutions to collect and use their data."
18,EB5C5719-7126-4F5E-A029-818535E903CA,Engineering Transformation for the Integration of Sensor Networks: A Feasibility Study - 'ENTRAIN',"There is a need to make use of new digital data analysis techniques to improve our understanding of the environment. Data from a new generation of environmental sensors, combined with analyses based on Artificial Intelligence, has the potential to help us understand from human influences and long-term change are affecting the environment around us. Artificial Intelligence approaches enable computers to identify trends and relationships across different streams of data, often picking out patterns that would be too difficult or time-consuming for humans to identify manually.

To realise these benefits, data from diverse sensor networks must combined and analysed together. Currently many sensor networks are operated individually, and data are not readily combined due to differences in the way measurements are made (e.g. between weekly river samples and sub-second measurements of gases in the atmosphere). In addition, to combine these data in an automatic way without human intervention requires much finer and more consistent descriptions of the contents of data streams, so that machines can understand the content sufficiently. Links between sensors in space are also important, and machines will need an understanding of these links, not just in the sense of coordinates, but for example how sensors are linked along rivers. We can construct a digital representation of rivers in order to enable this.
We will describe the various elements of a future environmental analysis system that will be required in order to achieve these benefits, and addressing some of these currently missing components. We will look at technologies, from databases to data transfer mechanisms, to understand how a system could be built.

We will use data from 3 NERC sensor networks measuring environmental variables from the atmosphere to river water quality, and show how this data can be automatically integrated in such a way that machines would be able to analyse it automatically.
A significant issue when monitoring with high-resolution sensors is how to handle problems in the data, which could include missing data, and erroneous values due to sensor failure. There is too much data for humans to manually view and check, and so automated approaches are needed. Currently these are often simple checks of individual data values against expected ranges, but again there are opportunities for artificial intelligence to improve this. AI approaches can look across multiple sensors, identify relationships, and find subtle changes in data signals, and this can be used to both identify data problems and to fix them through infilling. We will enhance the 3 NERC networks by testing and applying such approaches to data quality control.

We will investigate some fundamental limitations of high-resolution monitoring, the transfer of large amounts of data from the field site to the data centre, the security of such systems, and whether more processing could be done on the instruments themselves to reduce data transfer volumes.

We will meet with the public, with policy-makers, with industry and with researchers to discuss where there will be most to be gained from development of AI approaches to analysing environmental sensor data. We will develop ideas for future work to realise these gains, and will promote the benefits of an integrated system for environmental monitoring. These stakeholders are likely to include the Environment Agency, SEPA, Natural Resources Wales, Defra, Water companies, sensor network developers, and public organisations with an interest in the environment, including the National Trust, the Rivers Trusts, and local community groups.",,"The Digital Environment programme will benefit from ENTRAIN's foundation work, providing requirements, methods, best practice advice and recommendations for integration and data modelling across multiple sensor networks and other datasets, such as EO. This information and techniques will benefit other areas of science and industry, as well as public engagement.

Environmental practitioners, regulators, government, consultants, the water industry, agribusiness, insurance, and many others can benefit substantially from the joined-up evidence that ENTRAIN will start to generate.

The public, schools and colleges will benefit from access to meaningful data in a spatially aware context. There is strong public interest in environmental issues, and yet beyond weather forecasts, weather data and perhaps more recently air quality readings, there are few accessible data which have tangible meaning to the layperson.

The Earth Observation (EO) community will benefit from better access to connected in situ data for retrieval algorithm validation &amp; development. E.g. flooding extent, soil moisture and land cover products etc. and from new automated Phenocam greenness products output by ENTRAIN. Other Big Data projects (e.g. Data Labs) will benefit from a greater and easier connection to datasets, with common spatio-temporal linking requirements and proper metadata description already done.

Data modellers, and environmental informatics will benefit from improved sensor metadata schemes, sensor registers/catalogues, and data structuring for interoperability of a network of networks. We will disseminate the results of the ENTRAIN feasibility study also through an environmental informatics paper, describing the proposed methods and advances made in data modelling and data provenance, to ensure wide impact and uptake of these methods.

Observational scientists and regulatory observers will benefit from our website case studies, webinars, training workshops and online video tutorials to disseminate best practice and training in realtime data collection, cyber security, data vocabularies, metadata schemes and new deep learning QC techniques. These studies will benefit the global environmental, and wider, data communities such as the Committee on Data for Science and Technology (CODATA). Harmonisation of data networks to increase or facilitate interoperability, and production of spatio-temporally connected observations across environmental domains (e.g. Digital Rivers), will benefit the British Geological Survey (BGS), CEH, UK MetOffice, the Environment Agency (EA), the Scottish Environmental Protection Agency (SEPA), Defra, water companies (and other utilities such as the power grid), argi-business, insurers, and public health. Other NERC and EPSRC funded programmes (e.g. ASSIST, Natural Flood Management, HydroJULES, Internet of Food Things) and Defra Air Quality Monitoring Networks will all benefit through higher data quality, more complete data and efficiency gains in analysing data across sensor networks, and by using developed data structures in other environmental monitoring and food chain domains. The spatially connected integrated data visualisations will be demonstrated at both academic and industry events and conferences, where practitioners can benefit from e.g. improved water quality alerts (e.g. algal blooms, nutrient levels etc.), and crucially see the connected drivers of those trends, and get decision support information. Similarly, this has the potential to provide interconnected, cross-discipline, environmental management information that can give government the evidence chain to implement, monitor and evaluate policy.

We will monitor and evaluate the success of our impacts by recording attendances at events, the number of website visits, and use Twitter to promote activities, whilst recording the number of followers and retweets. We will also log email enquiries, webinar and web video views."
19,2F509632-D357-4422-B52B-C54F6C1A25BE,"CORSMAL: Collaborative object recognition, shared manipulation and learning","CORSMAL proposes to develop and validate a new framework for collaborative recognition and manipulation of objects via cooperation with humans. The project will explore the fusion of multiple sensing modalities (touch, sound and first/third person vision) to accurately and robustly estimate the physical properties of objects in noisy and potentially ambiguous environments. The framework will mimic human capability of learning and adapting across a set of different manipulators, tasks, sensing configurations and environments. In particular, we will address the problems of (1) learning shared autonomy models via observations of and interactions with humans and (2) generalising capabilities across tasks and sites by aggregating data and abstracting models to enable accurate object recognition and manipulation of unknown objects in unknown environments. The focus of CORSMAL is to define learning architectures for multimodal sensory data as well as for aggregated data from different environments. A key aim of the project is to identify the most suitable framework resulting from learning across environments and the optimal trade-off between the use of specialised local models and generalised global models. The goal here is to continually improve the adaptability and robustness of the models. The robustness of the proposed framework will be evaluated with prototype implementations in different environments. Importantly, during the project we will organise two community challenges to favour data sharing and support experiment reproducibility in additional sites.",,n/a
20,4B13ADA0-4814-43C3-81A1-545E25EE2DAD,UK-China Agritech Challenge: CropDoc - Precision Crop Disease Management for Farm Productivity and Food Security,"CropDoc seeks to exploit existing research on Potato disease identification and outbreak management in the domain of precision agriculture, agriculture digitisation &amp; decision management support. It will harness cutting-edge technologies (i.e. IoT, mobile devices, crowd sourced data, big data analytics and cloud computing). It will build a decision support system that generates insight from multiple data collected from remote sensing above the fields and IoT ground sensing within the fields for monitoring &amp; prediction of disease in real time. CropDoc will base its data service and analytics platform on open standards and will allow interoperability through open APIs. This will ensure an end-platform ecosystem can emerge that consumes the data &amp; analytics service, allowing farmers to use their platform of choice, while allowing central authorities to identify and manage sector outbreaks. 

The initial focus will be on potato late blight disease, one of the most devastating crop diseases in China. In a typical blight pressure season crop protection chemicals cost the industry an estimated $10-20bn per annum. Late blight has been referred to as a 'community disease', due to its ability to spread rapidly from field to field under the right weather conditions. Asexual spores travel easily on the wind when the weather is cool and moist, and can rapidly infect neighbouring fields. As such, understanding the symptoms of the disease and what to do when it is detected are essential to preventing an outbreak from rapidly turning into an epidemic.",,"The potential beneficiaries can be broadly classified into two groups including specific users and wider users.

1.1 Specific users
1) Our collaborators from both China and UK, Plant disease control and management agencies, pathologists, and farmers/homegrowers. They will benefit from the project by using our decision support system to assist diagnosis of plant diseases and outbreak management.
2) Researchers from Bioscience, Remote Sensing, and Computer Science. This project will advance new practices for such researchers by delivering an ICT-based solution using data-driven approaches (new algorithms and software). These new algorithms can be applied and adapted to other bioscience related problems (e.g. not just crop diseases but also other plant diseases). This project will complement and strengthen the current research activities in the area of Food Security and Traceability in both host and the partner institution and will contribute to the international standing of UK research in this area and beyond. Most importantly, the output of this project will provide a solid work to attract further funds from different sources (e.g. Horizon 2020).
3) Skilled researchers, fluent in crop disease detection, image processing, remote sensing, machine learning, big data processing and analytics will be trained by the unique interdisciplinary nature of this project which brings together researchers from Computer Science and Bioscience.
4) Research Councils and policy makers, who will be able to draw on the outcomes of the project to advise policy development and decision makers on appropriate strategies for future investment on enhancing plant disease management and improving food safety and traceability.
5) Industry practitioners/farm companies who provide solutions for plant disease control and prevention. Our system could be potentially commercialised and transformed to new products for disease diagnosis and prevention. Additionally, our platform is a standard, open platform, which can be easily plugged into existing plant disease control and management systems.
6) The users from education sector. Students and lecturers in Bioscience, Remote Sensing, and Computer Science will be able to utilize the algorithms and the user-friendly tool for learning and teaching in relation to plant health science and image pattern recognition related topics. 

1.2 Wider users
We anticipate that the project will generate wider user interest, in particular from the general public. Our ICT-enabled intelligent solution will increase operability, measurability, visibility for the general public who have little knowledge about the crop diseases. One potential impact of this research will be to educate broader communities and raise public awareness and understanding of plant pathology, and to support the future of plant pathology. 

We will adopt different impact activities to ensure all the potential beneficiaries have the opportunity to benefit from this research including:
1) The dissemination of project deliverables and software through the project website hosted at MMU, research publications in prestigious journals and appropriate conferences (e.g. IEEE Transactions on Automation Science and Engineering, Precision Agriculture, The British Society for Plant Pathology, etc.).
2) Seminars and workshops for researchers across multi-disciplines, end users and industry partners.
3) Public engagement activities for general audience by distributing flyers, posters and involving in outreach activities (Science Festival, etc.). 
4) Dissemination through both internal publications (e.g.ManMetLife) and external publications to gain wider coverage of the project."
21,D7756D2C-4536-44C5-8DDB-CD033D430912,"Immersive VR and Interactive Machine learning: a London-Shanghai Collaboration in art, education, &amp; AI","Three activities proposed will be led by Goldsmiths experts in the area of virtual reality, immersive media, AI, art, education, and computer games, with the supports from other UK creative industry and Chinese organisations.

First, a 360 Video in Chinese will be produced by a team of UK-based and Chinese directors, led by UK academics and facilitated by a local Chinese school. The production of a proof-of-concept 360-degree video promotes the use of VR in language learning with Chinese students in London, and explores new ways in using VR in education.

Second, a one day London-Shanghai Art and Tech Partnership Development Event will be hosted at Goldsmiths this June. Supported by keynotes and demos (e.g., immersive VR demos, 360 videos), we will identify and invite 30 key partners to attend this event to develop a close conversation around London-Shanghai partnership and network development through a panel discussion and a workshop. One-to-one meetings will be organised for the following day to explore concrete future collaborations.

Third, a one week Shanghai trip will be co-organised with the Chinese partners, when UK academics will visit current our Chinese partners and any other shanghai- based partners developed throughout this project. PI and co-Is will prepare a presentation summarising themes and challenges identified in the Goldsmiths event and will also prepare separate talks closely related to their own research area (immersive art, VR, interactive ML and AI) where they see potentials for future collaboration.",,
22,326C118D-0ED4-4945-B07B-DE440BABCC18,AI for Music in the Creative Industries of China and the UK,"Artificial Intelligence is changing in the Creative Industries from creation and production, protection, distribution, to consumption. The Music Industry is a leading example of a Creative Industry sector embracing AI, and its use of AI impacts and foreshadows other Creative Industries, providing a vibrant and rich ecosystem in which to examine the use and implications of AI. The size of the global Music Industry and the substantially different landscapes of digital music, AI, and culture between the UK and China provide significant opportunities for interdisciplinary long term collaboration building on each countries' different yet complementary strengths.

Two research-industry workshops will be held in London, UK, and Shanghai, China, to examine the increasing role and potential of AI for music in the Music Industry and the Creative Industries in China and the UK. The workshops will build partnerships leading directly to the development of future substantial collaborations between the UK and China in AI and music in the Creative Industries. To achieve this the workshops will map the current landscape of AI for music in the Creative Industries of UK and China, and examine questions including:
- What can be learnt from AI for music across Creative Industries;
- How data might be shared across sectors, countries, and cultures;
- How IP and business models are affected by AI;
- What skills are needed for AI in Creative Industries;
- How the impact of AI on Creative Industries might be measured.",,"Bringing together researchers and industry stakeholders from the UK and China will provide state of the art insights into the current landscape for AI and music. This will provide context for answers to the remaining questions such as what can be learnt from AI for music in the Music Industry and other Creative Industries. The workshops will prioritise which remaining questions have the greatest potential for future partnerships and impact e.g. whether future partnerships would be in the ethics of AI and music, or new business models for AI, etc. This prioritisation of questions leads to prioritisation of which challenges are to be addressed, and which opportunities to be taken. For example, the question of how might large data sets be shared directly responds to the opportunity of sharing data to improve AI creativity, and would involve tackling the challenge in differing regulation and cultural contexts."
23,4CA62BA0-1307-43C5-8474-B1F9F2013636,RoboTIPS: Developing Responsible Robots for the Digital Economy,"This fellowship will to bring together a variety of people from different walks of life, including academics, industry, civil societies, policy makers and members of the public, in order to create new ways of developing and managing technological innovations. There is often a tension between the economic needs for increasing technological innovation and the ways in which these innovations may be developed responsibly - that is in a manner that is societally acceptable and desirable. We will develop an approach that aims to anticipate not only the positive outcomes but also the potentially negative consequences of technological innovations for society. We will draw on this and an understanding of people's lived rights and obligations to provide creative resources and methods for designers to develop responsible and accountable new technologies. Responsible Innovation lies at the heart of technologies in the Digital economy that aim to promote trust, identity, privacy and security. 

Although it has been drawn on in other scientific domains, as yet we have no complete example of how responsible innovation can be successfully applied in the DE sector. The fellowship will consider a motivating example to develop responsible innovation in action. We will look into one particular domain of technology and develop an agile process which will take account of the views of a wide range of people in a fast-changing context, in order to have some influence over the trajectory of an innovation. We will focus on the domain of social robots, those which interact with people and make decisions about what to do on their own accord. Because they make their own decisions in order to perform actions, we need to be able to recover what they did and why they did it, when things seem to go wrong. We will develop an ethical black box (EBB) through which the social robot will be able to explain its behaviour in simple and understandable ways. The development of the EBB will be an example of responsible innovation. We will test this out in particular accident investigations as a social process and we will do this in 3 different study domains. In the final stages of the fellowship, we will show the outcomes of the technological development and the investigations through a variety of means, including through the web and a final public showcase event. This will be to a variety of people including the general public, policy makers, and developers.",,"Science: 
Key scientific communities will benefit from the knowledge generated by the fellowship and its responsible innovation (RI) approach. This includes fields across robotics, human robot interaction, robot ethics, ethical design, value sensitive design, Human-Computer Interaction, Artificial Intelligence and the social sciences. In particular, the development of the ethical black box (EBB) will form a motivating example of RI that is both human centred and based on a Trust, Rights and Relational approach, and that can generalised to other domains and technologies. The findings of the programme of work and its unique approach will be disseminated through high quality publications and conferences. 
Industry: 
The fellowship will develop a unique transparency mechanism that that can make robotic decision making open and accountable and thereby foster user trust in social robots across society. This fellowship work will therefore enhance responsibility in industry and advance the acceptance of social robots. The development of the EBB will benefit industry in the specific domains studied in phases 1 and 2 and will also reach a broader range of settings through later activities that will generalise the study findings and RI approach. Impact will be fostered through a series of stakeholder workshops and designer studies and will be supported by the fellowship's industrial partners.

Designers and innovators: The fellowship will benefit designers and innovators by fostering a reflective and inclusive innovation process and developing a Trust, Rights and Relational based approach to design which acknowledges the lived experiences of users and the networks and ecosystems they inhabit. Research activities involving design and innovation stakeholders will deepen understanding of how these professionals currently practice. This understanding will then be drawn on to produce mechanisms through which designers and innovators can embed responsibility within the development of technologies for the digital economy (DE). These mechanisms will include a responsibility toolkit.

Policy: The fellowship will engage critically with the concept of responsibility in the DE and will explore and engage with new forms of anticipatory governance. The perspectives of policy stakeholders will be elicited across the study and the outcomes of this engagement will be combined with other project activities to develop a responsibility toolkit and specify an agile governance approach for social robots. This fellowship will thereby benefit policy makers seeking to identify novel and effective means to regulate the use of social robots and, through the generalisation of the study findings, other innovative technologies. 

Society and the general public: The EBB will benefit the wellbeing of citizens in the DE by fostering accountability in the use of social robots and a culture of transparency and responsibility in their use. It will also produce broad societal benefit through the development of a Trust, Rights and Relational approach which will elicit the concerns of citizens and consider how to address them in order to increase trust and forge better relationships to maximise the potential of new digital technologies. The fellowship will include dedicated public engagement and communications activities - to include educational materials, a video animation and participation in public understanding of science events."
24,799215BF-FC06-4003-849D-08E1007DA39C,Novel techniques for stochastic modelling of time-dependent multivariate relationships with application to primary visual cortex,"Advances in data acquisition technologies lead to the availability of ever more complex datasets. Often, the gathered variables have fundamentally different statistics, some being continuous while others are discrete. In many domains, the relationships between the recorded variables are of particular importance and also changing in time. One such domain is computational neuroscience where it was recently shown that even in early sensory brain areas, neural responses to stimuli are modulated by behavioural context. The precise functional interactions underlying this modulation are currently unknown but nonetheless important for understanding how the amazing versatility of sensory processing comes about. From an analytical point of view, understanding the complex interactions between neural activity, behaviour and task variables, all being subject to different statistics and timescales, is a major challenge.

In this project, we will address the general problem of assessing probabilistic descriptions of time-dependent relationships between elements with mixed statistics as motivated by the context-dependent sensory processing problem encountered in neuroscience. To join mixed elements, we will use parametric copula models embedded in a Bayesian framework for time-varying parameters. For model fitting, we will use an inference scheme based on Expectation Propagation in conjunction with Gaussian Process priors, the latter being naturally suited to take into account different timescales. Contrary to other commonly applied methods, this approach will make stochastic relationships explicit and generate interpretable joint models of elements with strikingly different statistics.

In order to investigate neural response modulation and in particular context-dependent visual processing, we will apply our analysis framework to data already recorded by project partner Dr Nathalie Rochefort. The data consist of fluorescence changes in large populations of neurons as recorded from primary visual cortex of awake behaving mice using two-photon calcium imaging. The data also include concurrently recorded behavioural and task variables gathered from a virtual reality environment. Our analysis will deepen our understanding of functional relationships in primary visual cortex that make the visual system so versatile, thereby providing new system state characterizations as well as improved sensory decoders.

The development of versatile time-dependent relationship models will be driven by the particular neuroscience application to understand context-dependent relationships in primary visual cortex, but will be more broadly applicable to many other domains where stochastic relationship analysis is of importance.",,"This project will have a wide range of impacts on data science and computational neuroscience communities, industry and the general public.

The methods for analysing time-dependent relationships that we are proposing are fairly general and will be broadly applicable in industrial domains such as information flow in computer networks and trading in micro- and macro-markets.

The application part of the project addressing context-dependent relationships of neural activity will deepen our understanding of the role of primary visual cortex in context-dependent visual processing. This will contribute knowledge about normal brain function and might in the long term aid in finding new treatments for major brain diseases.

Understanding context-dependent feedback processing in the brain might also suggest ways to mimic the same principles in artificial neural networks. Currently, the most successful deep learning architectures are feed-forward networks. Our findings might lead to more versatile architectures that could find applications in tasks such as object recognition and control.

We will develop two Free and Open Source Python software packages that will benefit practitioners from various fields in academia and industry. The first package will be a general implementation of the time-dependent relationship models and associated inference and information estimation procedures and will target practitioners in machine learning and statistics. The second package will build on the general purpose package and will add an easily applicable framework for analysing specific context-dependent relationships in neuroscience. This package will primarily target the computational neuroscience community. We will release these packages on multiple channels to foster widespread dissemination.

To reach the general public, we will participate in events such as the Brain Awareness Week, the Edinburgh Science Festival and the European Researchers' Night. The models we are proposing as well as the underlying neuroscience data can be appealingly visualised, allowing us to raise public awareness in data science and in neuroscience research."
0,2CAB74A4-3EA8-47DF-9C7A-AB41D926081A,Making Satellite Volcano Deformation Analysis Accessible,"This project aims to improve the ability of ODA countries to forecast and mitigate volcanic activity, by using satellite data to improve volcano monitoring. According to the UN Global Assessment of Risk, over 90% of the total global volcanic threat is in developing countries, but 25% to 45% of historically active volcanoes were found to have no ground-based monitoring. Space-based methods offer a means to bridge the monitoring gap. Our survey of ODA volcano observatories identified two main activities that would improve uptake: 1) accessible automatic processing and 2) training in interpretation. This proposal brings together NERC-funded research on the application of Earth Observation data to volcanic processes with EPSRC-funded research in image analysis to develop automated processing and analysis systems. Our project partners, with whom we have long-standing relationships, represent an LDC (Ethiopia) and an UMIC (Ecuador) and will work with us to develop an accessible web platform to disseminate appropriate products. The proposal consists of three objectives: 1) to develop a suitable web platform to disseminate automatically processed satellite imagery; 2) to build capacity in ODA countries to access and interpret satellite data and 3) implement and refine algorithms to flag volcanic unrest and develop an alert system. The products will be developed with our project partners and launched globally at the Cities on Volcanoes conference 2020.",,"This proposal is applicable to UN Sustainable Development goals 1, 11, 13 and 17 with the aims of reducing vulnerability, deaths and economic losses, increasing capacity and resilience through disasters. The project also addresses knowledge sharing and cooperation for access to science and technology, and increasing scientific capacity, particularly in developing countries. 
Target 1.5 &quot;build the resilience of the poor and those in vulnerable situations and reduce their exposure and vulnerability to ... economic, social and environmental shocks and disasters&quot; 
Target 11.5 &quot;significantly reduce the number of deaths and the number of people affected and substantially decrease the direct economic losses ... caused by disasters ...&quot; 
Target 13.1 &quot;Strengthen resilience and adaptive capacity ... natural disasters in all countries&quot; 
Target 17.6 &quot;Enhance ... international cooperation on and access to science, technology and innovation and enhance knowledge sharing, ... including through ... through a global technology facilitation mechanism&quot; 

Improved volcano monitoring increases the resilience of communities and countries reduces disaster-related socio-economic losses. Forecasts of eruptive activity can be improved by stronger monitoring capacity, and in turn, mitigation measures and exclusion zones can be put in place and lives saved. Economic losses may be reduced as unnecessarily premature or extended evacuations are avoided. However, most volcanoes lack sufficient ground-based monitoring, particularly in ODA countries. We seek to equalise access to satellite data globally, permitting un- or under- resourced observatories to measure processes that may otherwise go undetected, putting them on a par with well-resourced observatories, more typical of developed countries. 
The planned activities are designed to increase uptake of satellite data for decision making during volcanic crises. They have been co-designed a) directly with our project partners in Ethiopia and Ecuador and b) through a survey of needs distributed to all ODA countries with volcano observatories and c) incorporating many years experience working in ODA countries. The outputs of the project will include a sustainable and accessible website for delivering near-real time and background satellite deformation imagery including 1) background deformation monitoring data for all ODA volcanoes, including automated annual summaries. 2) a near-real time system for monitoring deformation during volcanic unrest and eruption. and 3) automated alerts to flag unusual signals that have been identified as potential deformation.

For our project partners (IGSSA, Ethiopia and IG-EPN Ecuador), direct outcomes will be:
- co-produced pilot studies of background and current volcanic activity using system test data.
- increased capacity in the analysis and interpretation of satellite deformation through training of embedded individuals.
- experience developing automated analysis systems.
- training and demonstration materials adapted to local needs.

For decision makers in ODA countries around the world (e.g. volcano observatories, local authorities, civil defence and local communities), outcomes will include increased capacity to incorporate satellite monitoring into decision making through
- Increased awareness of the capabilities of satellite monitoring 
- Improved access to satellite data through an accessible and sustainable system.
- Accessible tools for interpreting satellite imagery without relying on international partnerships (e.g. automated flagging and decision trees)."
1,827043DA-D688-45F3-96EE-44D994A305CA,Robot In-hand Dexterous manipulation by extracting data from human manipulation of objects to improve robotic autonomy and dexterity - InDex,"Humans excel when dealing with everyday objects and manipulation tasks, learning new skills, and adapting to different or complex environments. This is a basic skill for our survival as well as a key feature in our world of artefacts and human-made devices. Our expert ability to use our hands results from a lifetime of learning by both observing other skilled humans and ourselves as we discover how to handle objects first hand. Unfortunately, today's robotic hands are still unable to achieve such a high level of dexterity in comparison to humans nor are systems entirely able to understand their own potential. In order for robots to truly operate in a human world and fulfil the expectations as intelligent assistants, they must be able to manipulate a wide variety of unknown objects by mastering their capabilities of strength, finesse and subtlety. To achieve such dexterity with robotic hands, cognitive capacity is needed to deal with uncertainties in the real world and to generalise previously learned skills to new objects and tasks. Furthermore, we assert that the complexity of programming must be greatly reduced and robot autonomy must become much more natural. The InDex project aims to understand how humans perform in-hand object manipulation and to replicate the observed skilled movements with dexterous artificial hands, merging the concepts of deep reinforcement and transfer learning to generalise in-hand skills for multiple objects and tasks. In addition, an abstraction and representation of previous knowledge will be fundamental for the reproducibility of learned skills to different hardware. Learning will use data across multiple modalities that will be collected, annotated and assembled into a large dataset. The data and our methods will be shared with the wider research community to allow testing against benchmarks and reproduction of results. More concretely, the core objectives are: (i) to build a multi-modal artificial perception architecture that extracts data of object manipulation by humans; (ii) the creation of a multimodal dataset of in-hand manipulation tasks such as regrasping, reorienting and finely repositioning; (iii) the development of an advanced object modelling and recognition system, including the characterisation of object affordances and grasping properties, in order to encapsulate both explicit information and possible implicit object usages; (iv) to autonomously learn and precisely imitate human strategies in handling tasks; and (v) to build a bridge between observation and execution, allowing deployment that is independent of the robot architecture.",,Not Applicable
2,504E583F-86CF-483C-B388-7094D8CCDCC5,HEAP: Human-Guided Learning and Benchmarking of Robotic Heap Sorting,"This project will provide scientific advancements for benchmarking, object recognition, manipulation and human-robot interaction. We focus on sorting a complex, unstructured heap of unknown objects --resembling nuclear waste consisting of a set of broken deformed bodies-- as an instance of an extremely complex manipulation task. The consortium aims at building an end-to-end benchmarking framework, which includes rigorous scientific methodology and experimental tools for application in realistic scenarios. 
Benchmark scenarios will be developed with off-the-shelf manipulators and grippers, allowing to create an affordable setup that can be easily reproduced both physically and in simulation. We will develop benchmark scenarios with varying complexities, i.e., grasping and pushing irregular objects, grasping selected objects from the heap, identifying all object instances and sorting the objects by placing them into corresponding bins. We will provide scanned CAD models of the objects that can be used for 3D printing in order to recreate our benchmark scenarios. Benchmarks with existing grasp planners and manipulation algorithms will be implemented as baseline controllers that are easily exchangeable using ROS. 

The ability of robots to fully autonomously handle dense clutters or a heap of unknown objects has been very \textit{limited} due to challenges in scene understanding, grasping, and decision making. Instead, we will rely on semi-autonomous approaches where a human operator can interact with the system (e.g. using tele-operation but not only) and giving high-level commands to complement the autonomous skill execution. The amount of autonomy of our system will be adapted to the complexity of the situation. We will also benchmark our semi-autonomous task execution 
with different human operators and quantify the gap to the current SOTA in autonomous manipulation. Building on our semi-autonomous control framework, we will develop a manipulation skill learning system that learns from demonstrations and corrections of the human operator and can therefore learn complex manipulations in a data-efficient manner. To improve object recognition and segmentation in cluttered heaps, we will develop new perception algorithms and investigate interactive perception in order to improve the robot's understanding of the scene in terms of object instances, categories and properties.",,nA
3,F91ADBCF-D8FA-429C-B671-876EC53C0951,Resilient Path Coordination in Connected Vehicle Systems,"The deployment of connected and autonomous vehicles presents us with transformational opportunities for road transport. As the standardization of inter-vehicular communications progresses, vehicles will soon be wirelessly connected, enabling coordinated driving strategies. By enabling vehicles to jointly agree on optimal maneuvers and navigation strategies, coordinated driving promises to improve overall traffic throughput, road capacity, and passenger safety. However, coordinated driving in connected and autonomous vehicle systems suffers from one key limitation: all vehicles in the system are assumed to be cooperative. This is an issue, since automated vehicle control systems are susceptible to numerous failure conditions, ranging from internally triggered faults (e.g., hardware, software, or communications failures) to externally triggered faults (e.g., environmental disturbances or malicious tampering). When cooperation breaks down due to these faults, coordinated driving strategies lead to negative unanticipated consequences, compromising passenger safety and traffic fluidity.

The goal of this project is to develop a resilient coordinated driving method for multi-vehicle systems that are potentially non-cooperative and unreliable. The issue of providing resilience in the face of non-cooperation (e.g., faulty, byzantine or adversarial agents) has received considerable attention within the domain of distributed network control. However, it is not until very recently that we have started to tackle the question of how to deal with failures and misbehavior when connected agents (vehicles or robots) are mobile. Although preliminary results are promising, the methods deal with control (without planning), and cannot handle discrete workspace constraints (i.e., lane topographies) nor kinodynamic constraints (i.e., car-like motion primitives). Consequently, they do not lend themselves to the problem of coordinated driving. There lies a gap between what we know about resilient network control, and what we know about resilient path coordination. The main contribution of this research programme is to fill this gap by providing methods for resilient planning of trajectories for car-like vehicles with lane constraints. 

Our methodology is based on a juxtaposition of centralized and decentralized path planning: we will leverage decentralized planning to guarantee collision-free paths at all times and in all circumstances, and couple this with centralized planning that optimizes global objectives whenever possible. The aim is to develop an adaptive algorithm that slides between the distinct modes as a function of real-time factors that define the level of cooperation in the multi-vehicle system. Such sliding mode architectures have yet to be established within the context of connected multi-vehicle systems, where vehicles cannot be assumed to be cooperative at all times. The proposed research will build upon the expertise of the PI in the field of resilient control, inter-vehicular coordination, and optimization.

The implications of this research are expected to contribute to the theory of multi-vehicle path planning and control, with direct applications to connected and autonomous vehicles. This will ultimately contribute to the improvement of future road transport systems, addressing both safety as well as efficiency.",,"The real-world installment of V2V and V2I infrastructure is still in a preliminary stage - we have a window of opportunity to develop reliable and resilient coordinated driving solutions, before large-scale deployment of connected (and autonomous) vehicles takes place. 

The benefits of resilient coordinated driving methods are: 
(a) gains in road capacity, traffic throughput, and fuel efficiency, without compromising passenger safety; this will be facilitated by our sliding mode path coordination algorithm; 
(b) creation and maintenance of user trust in coordinated driving systems, with direct implications on the commercial aspects of involved industries; this will be facilitated by formal guarantees and empirical validations of our methods; 
(c) drive the generation of further knowledge; this will be facilitated through our principled study of failure modes in coordinated multi-vehicle systems, and through the publication of benchmark tests."
4,CCF4AF99-B306-4507-9C7E-8B01A6C41158,Perception-guided robust and reproducible robotic grasping and manipulation,"This research aims at developing and testing perception and manipulation strategies that will allow a robot to grasp and manipulate objects from a complex scene, e.g., an unstructured self-occluding heap of reflective metallic parts in manufacturing environments, or a heap of unknown/un-modeled and/or deformable waste materials in nuclear decommissioning or mixed waste recycling. The project addresses the key challenges mentioned in the call, namely, the grasping and manipulation of objects by robots using novel, hardware-independent, robust techniques, composed of modularisable subtasks. General strategies will be developed that will be reproducible on different hardware configurations. Indeed, from the outset, the project focuses on robustness and reproducibility, which are key concepts that connect all project objectives. The fundamental scientific questions addressed in the project can be summarized as follows: 1) Robust visual data collection, segmentation and production of sets of graspable features in complex and difficult real scenes, 2) Grasp planning based on grasping visible features (instead of object models) and hardware independent implementation of the grasping strategies, 3) Grasping or re-grasping strategies, to best enable desired post-grasp actions, and also based on extrinsic dexterity, namely the exploitation of the environment or the robot's dynamic capabilities, and 4) Integration of all project components into an operational scheme that will be implemented in the laboratory settings of all participants. Regarding visual data collection and analysis (item 1), algorithms capable of working in unstructured environments associated with uncertainty will be developed. The project will tackle difficult environments, which are characteristic of a variety of industrial applications. We note that industrial benchmark datasets are comparatively few in the vision and robotics research communities, despite their clear economic importance and also significant intellectual complexity. In item 2, the concept of graspable features will be developed and used to devise novel grasping strategies. Means of evaluating the performance of the manipulation strategies will also be developed in order to assess the quality of the results obtained. By managing the perception-action loop using the detection of graspable features, the project will also provide tools for potentially handling unknown objects in unknown environments. Item 3 follows the concept of graspable features since a graspable feature may yield a proper temporary grasp but may require re-grasping depending on the task to be performed. Finally, the integration of the project components will also raise issues of implementation, real-time constraints and other practical limitations. 
Experiments will be conducted in all participating research groups, initially using identical or similar equipment and then using different set ups and configurations in order to demonstrate generalisation, reproducibility and robustness. The perception aspect of the work will focus on visually complex, noisy and cluttered scenes. The manipulation aspect of the work will focus on generality and reproducibility, based on searching for graspable features rather than relying on object models. Finally, the project will generate a large amount of data, which will be logged, shared and made available to the international robotics research community as a set of public benchmark challenges, including training and testing data.",,"The project will yield significant advances in the area of robotic perception / recognition and grasping / manipulation. By decomposing the challenging task of grasping and manipulating objects in difficult environments into a series of modularisable subtasks, it is expected that the techniques developed will greatly extend the range of applications of autonomous robotic grasping. Using a perception-based approach rather than a recognition-based approach for grasp planning is expected to yield a paradigm shift of the problem, which will lead to highly innovative scientific results. Also, difficult realistic environments remain a challenge for industrial applications. The solutions proposed in the literature tend to be rather specific or apply to limited data sets in clean environments. In the context of Industry 4.0, general grasping and manipulation strategies are in high demand in a very large variety of applications. Given the track record of the participants and the recent advances that they have produced, the potential of this project to yield novel effective and transferrable methods is very high. Therefore, the potential impact of this research on industry is very significant. Also, the large amounts of data generated and made available to the research community for validating and reproducing experiments will be a significant contribution of the project. 
Industrially relevant new datasets and benchmark challenges, will directly enable the international research community to develop and test their algorithms and approaches on societally and economically useful problems. Furthermore, the extreme challenges of industrial objects and scenes will push the academic research community to develop increasingly robust new methods."
5,BA3EC168-0132-4E62-A78E-892C6D7F0F0E,Brain--inspired disinhihbitory learning rule for continual learning tasks in artificial neural networks,"Machines are achieving near-human performance at learning tasks such as image categorisation or speech recognition, but most of the state-of-the-art solutions excel in fixed environments. Systems deployed in real-world scenario, on the other hand, need to be able to learn in changing environments. Why is this a challenge? Every time a typical learning system encounters a new task, it overwrites the solution to previous tasks by what it learns on the new one. Imagine a robot used for elderly care: After two months of training it to carry the person up and down the stairs, there are renovations in the house, and the robot learns to transport the person with a temporary lift. It would be silly if the robot would thereby unlearn its skills for navigating stairs and would have to relearn the stair condition for two months after the renovation. Current state-of-the art machine learning algorithms have this limitation, a challenge called continual learning, or life-long learning.

For this EPSRC Fellowship, we plan to develop a brain-inspired learning algorithm and test it in artificial neural networks solving a continual learning task. So, let's look at how the brain might solve continual learning. Humans have the fascinating ability to adapt to their environment and memorise experiences; both require memory. We can learn quickly and remember for a long time, but this leads to a dilemma: In order to learn quickly, the brain needs to change very easily i.e. be plastic, but in order to remember for a long time, the brain must not be too plastic. The basis of learning and memory at the neural level are changes in the connections between neurons, called synaptic plasticity. Scientists have worked thoroughly to characterise synaptic plasticity, focusing on excitatory neurons, while mostly neglecting the role of inhibitory neurons. We suggest that instead of learning equally across all experience, the solution to the dilemma is to regulate which memories to learn, therefore avoiding unnecessary overwriting of important memories. We propose that inhibition is the key to regulating learning, in that lowering inhibition opens a gate for learning. To test this hypothesis, in this EPSRC Fellowship we will investigate the interaction of excitatory and inhibitory plasticity using computational models in recurrent networks. We will then test whether and how inhibition gates synaptic plasticity and therefore learning. Finally, we will test the performance of our brain-inspired learning rule in a continual learning task of navigation under a reinforcement learning framework.",,"*** Providing tools for the experimental neuroscience community. Our computational models will benefit the experimental neuroscience scientific community by providing models that can be used to test scientific hypotheses before performing any experiments; this will therefore reduce animal usage and provide novel tools to speed up science by increasing the number of possibilities that can be tested. To this end, we will publish our codes on a standard database in the field (ModelDB) along with an easy-to-use graphical interface.

***Technological impact: This work could benefit current and future developers of smart technologies, since the learning rules developed for this fellowship will develop new machine learning algorithms for continual learning. In particular, we will use our industry collaboration with Google Deepmind, London, to ensure that our learning rules can refine their state-of-the-art deep learning networks, and be another stepping stone toward general-purpose artificial intelligence (CC consults for Google twice a month), as well as our industrial contact at Qualcomm - USA, Eugene Izhikevich, to ensure that our work flows into marketable neuromorphic chip design technology. 

***Health impact: On a longer timescale, this work will benefit people with diseases related to learning and memory such as Alzheimer's disease. It will also benefit people with diseases related to connectivity disorders such as autism and schizophrenia. Finally, it will benefit the growing ageing population in the UK and around the world, since this work will set a reference point for the amount of plasticity seen in the adults with natural or biased input statics. It has been shown that the synaptic turnover is increased in the aged brain. Therefore our work can be a starting point for studying the behaviour and the impact of synaptic plasticity in an aged brain.

***Educational impact: We will train a new generation of scientists by training the staff in my laboratory and by teaching at summer schools. But more importantly, we will train a new generation of non-academic workers in the UK by through CC's computational neuroscience course in the Bioengineering Department at Imperial College London, teaching a solid skillset for working in pharmaceutical, biotechnological, or engineering companies such as high-tech companies using machine learning or robotics, but also in banks and insurances that use artificial neural network techniques.

***Public engagement: We aim to sensibilise the broader public to the discoveries of neuroscience and communicate the great scientific challenges of the future. To this end, we propose to work with the outreach manager of my Department to set up a number of activities, such as press releases after publication, maintenance of a webpage with breaking news, stands at two different outreach activities of Imperial College London (Imperial Festival and Imperial Fringe) as well as giving talks to prospective students during the open days of Imperial."
6,920C4CA6-4918-424A-B952-CA46A57B389D,Crowdsourcing and Machine Learning for Disaster Relief and Resilience,"This project builds on a strong history of successful, impactful STFC-supported research, applying this research within the world-leading Zooniverse citizen science platform to humanitarian and disaster management issues in countries that require Official Development Assistance. The Planetary Response Network is a partnership led by the Zooniverse, the Machine Learning Group at the University of Oxford, and the response and resilience charity Rescue Global. Since 2015 the PRN has deployed crowdsourcing projects to classify multiple kinds of damage following major natural disasters in Nepal, Ecuador, and multiple Caribbean nations including Dominica and Antigua &amp; Barbuda. This project seeks to improve on the successes of those projects by incorporating feedback from ground-responders partnered with Rescue Global and from a recent multi-agency report which clearly articulated the unique needs of crowdsourced projects in humanitarian response applications. Thanks to STFC support, the Zooniverse has well-established platform infrastructure that can fully address these needs; the modest additional support requested in this project will bring high value for money by adding targeted high-impact features to the Zooniverse platform. These features include a pipeline to rapidly process pre- and post-event satellite images into classifiable &quot;subjects&quot; for the crowd, application of STFC-supported machine learning research to pre-classification of images, incorporation of STFC-supported advanced algorithms for real-time human-machine classification, and intuitive visualisation of consensus results so that decision makers and responders on the ground can easily interpret damage maps and maximise situational awareness, leading to better allocation of resources and aid, faster restoration of infrastructure, and a significant positive impact on societies preparing for and recovering from natural disasters.",,"This project benefits both academic researchers and society as a whole by building on the products of past STFC research and applying these products to the humanitarian sphere. The deployments enabled by this project are intended to provide rapid, accurate, and ongoing high-value information about evolving conditions before, during, and after a major crisis, especially in countries that require Official Development Assistance. Local, regional and national responders and decision makers benefit from the significantly improved risk awareness and situational awareness provided by the Planetary Response Network damage maps. The additional information can then be used to more efficiently assess risk, allocate aid and resources, and both preserve and more rapidly repair infrastructure. These actions can save lives, and have additional benefits such as helping the local economy recover more quickly after a disaster. 

Classifications produced by the Zooniverse have proven to be of high value to the wider academic community in a variety of disciplines; this is anticipated to continue with this project. In particular, machine learning researchers may find both the raw classifications and the processed consensus damage maps extremely useful in pursuing further advances in human-machine computation problems.

Additionally, the project's results will be directly applicable to researchers in other fields using geo-tagged data, especially those will benefit from rapid, real-time classification of geo-tagged data, such as conservationists. As the software work in this project will be added to the open-source Zooniverse platform, researchers may use it to build crowdsourcing projects that capitalise directly on the tools created in this project. The open availability of these tools within the public Zooniverse platform maximises the long-term impact of the project."
7,72C58ECE-ABAC-4A62-BCAA-2231E29F0261,Automation and Contemplation for Model Adaptation in Multiagent Interactions,"An agent is a computer system that acts intelligently given its sensory input from the environment. Agent technologies have proved to be effective and reliable solutions in many practical applications and will continue to play a major role in modern society. For example, the eBay buyer agent recommends good deals for people in an e-market. The Google self-driving car operated by an autonomous agent has successfully navigated thousands of miles on the road. A smart meter controlled by an intelligent software agent helps optimize energy consumption for a household. 

In many such applications, an autonomous agent (namely a subject agent) is expected to make a rational decision by predicting behaviors of other agents in a common environment. The decision quality relies on building decision models of the other agents and then solving the models to understand how the other agents will behave in the environment. When the subject agent's model is deployed in a real-world application, it may fail since the subject agent may receive unexpected observations incurred by other agents. Hence the challenge is about the prediction of other agents' behavior and the interpretation of model failure so as to adapt the subject agent's model for successful interactions. 

The goal of this project is to improve the subject agent's adaptation by automating the model construction of other agents and revising its own decision model when the model fails in the execution. This project will propose scalable learning algorithms to build decision models of other agents upon historical data of agents' interactions. The algorithms will also facilitate the model construction in a new problem domain that will be likely larger and more uncertain in practice. To interpret failures of the subject agent's decision model, this project will search for a novel reasoning technique to identify the most probable reasons behind the failures, and accordingly revise the model so that the subject agent's decisions can be adapted to the other agents' behaviors in their real-time interactions. This project will implement all the proposed techniques in a toolkit and conduct comprehensive tests to evaluate practical utilities of the toolkit. Real-world applications on personalized learning and intelligent computer game AI engine development will be extended through our industrial collaborators. 

The broader impact of this research will be to enable individual agents to act rationally in complex multiagent environments. This is a crucial step toward the integration of autonomous agent technology within society that will support humans in tasks such as disaster response, energy distribution and security operation.",,"The proposed research will benefit a wide range of industrial stakeholders that either develop their core technologies on AI or improve their services through AI related products. It would particularly facilitate the development of intelligent agent technologies in a real-world application where the agent needs to optimize its decisions when interacting with other agents. The agent could be either a piece of software, a computer system, or even a human being. 

This project will build a data-driven multiagent decision making toolkit and test the practical utilities of the toolkit in the Starcraft gaming systems using real-word game replay data. The integration of the toolkit into the gaming systems will provide insights to computer game sectors on using data-driven approaches in a game AI engine, which has not been much explored in the game industries. 
As a data-driven approach does not demand much effort from problem domain experts on the modeling task, it can be easily adopted by many industrial sectors that do not have a good background in intelligent technologies. 

This project will focus on two particular applications through our existing collaboration with external industrial partners. The first application is to facilitate the development of a personalized learning platform in the TWI training centre where the learning data will be used to construct a student model so as to configure learning pathways for individual students. This application will help students to improve their learning outcomes while reducing the workload of trainers on preparing a diverse set of training materials. 

The second one is to develop intelligent non-player characters (NPCs) in a game AI engine in the mobile game company - SinceMe. The proposed research will exploit existing game data to learn typical behaviors of human players. The learned behaviors will inform a better design of NPCs so that the NPCs can adapt their strategies to how human players act in the changing game environment. The application will not only enhance players' experience in the gameplay but also improve the game development efficiency in SinceMe. 

We are also seeking for more industrial collaboration in order to extend the impact of our research outcomes in this project. For example, in a healthcare application, the proposed agent technologies could be used to build a human-like assistant robot that provides daily services for the elderly. The PI of this project, as a co-lead of the University Grand Challenge in Health and Wellbeing, will drive the development of such an application through the Tees Healthcare Innovation Partnership between Teesside University and South Tees Hospitals NHS Foundation Trust. 

The toolkit will be available in a project web site and its utility will be demonstrated at conferences, e.g. the prestigious IJCAI and AAAI conferences, where potential industrial partners are present. We will communicate our research findings with relevant stakeholders through traditional mass media, like newspapers, television, radio and popular magazines. This will be organized through our partnership with Native Consultancy, which supports the targeted dissemination of the University's research, and has recently worked with the PI. Social media platforms like Twitter, YouTube and LinkedIn, will be also used to further enhance the outreach of the project and specifically target and connect end-user audiences. While giving regular lectures in University open days and delivering public talks organized by Digital Catapult Centre North East &amp; Tees Valley or others, we expect that the successful applications of our research outcomes will also raise public awareness of AI technologies and increase their acceptability in a modern society."
8,0DD481E0-B734-4686-9663-28DEA098BB3B,Robotics &amp; Autonomous Systems: EPSRC UK-RAS Network,"The EPSRC UK-RAS Network (http://uk-ras.org) was established in 2015 with the aim of bringing together academic centres of excellence, industry, government funding bodies and charities to strategically grow the UK-RAS research base, acting as a portal to interface with industry and deliver technological advances with the potential for translational uptake. Since its foundation, the UK-RAS Network has helped transform the research landscape of Robotics and Autonomous Systems in the UK, bringing cohesion to the UK-RAS research base, enhancing capital facilities across the country, supporting education programmes and public engagement activities at all levels. 
 
The UK-RAS Network has successfully strengthened the research landscape and interdisciplinary community of RAS by facilitating synergistic research and education programmes across the UK. It has also significantly raised the awareness of RAS nationally through the UK Robotics Week, strengthened the interaction between the research community and end-users, enhanced the linkages of CDTs, provided coordinated investment to state-of-the-art capital facilities, and exerted significant influence on future direction and national policies of RAS and AI. However, there are further gap analysis activities and cross-disciplinary developments in RAS research need to be made, and, as research moves toward technology departments, engagement programmes offering knowledge and training for marketisation and commercialisation need to be addressed, through strategic industrial partnerships, cross-promoting existing programmes and facilitating further access. 
 
This grant is to build the success achieved, expand the current UK-RAS portfolio as identified by the recent EPSRC RAS Theme Day Report, strengthen collaboration with parallel industrial activities, both nationally and internationally, and support the government's industrial strategy challenge fund (ISCF) as Robotics and Autonomous Systems is ubiquitous in all the major themes identified. will continue to grow the RAS community, through increased university membership, Innovate UK KTNs, The UK Automatic Control Council (UKACC), EPSRC Robotics Hubs, and the RAI-SIG. New activities of the Network will include entrepreneurial programmes, support SMEs and a vibrant UK-RAS eco-system, industrial fellowship schemes, management of national RAS assets, STEM ambassador programmes in RAS, establishment of UK RAS &amp; AI topic groups, as well as planned network events including annual UK Robotics Week, Robotics Challenges, public engagement events and exhibitions.",,"The main objectives of the continuation grant is to build on the existing success of the UK-RAS Network, expand its remit to have stronger inter-disciplinary and inter-technology connections (e.g. materials, control, biology and environment) to address future grand challenges of RAS and foster collaboration across key RAS centres in the UK and internationally, supported by a coordinated effort in doctoral training, building a vibrant RAS eco-system with direct industrial involvement and effective pathways for knowledge exchange and commercial exploitation. The expected impacts include: 
 
- Shared infrastructure for research in RAS developing strong industrial connections and provision of experimental facilities relevant to key sectors and challenges; 
 
- Closer collaboration among leading UK research groups on joint funding opportunities, cultivating our relationships with our European partners post Brexit, as well as building deeper international links and to accelerate the exploration, technological development and applications of RAS; 
 
- The creation of a vibrant environment for exchange of knowledge, flow of people and promotion of collaboration between UKRI funded projects and stimulating and supporting new larger-scale projects at higher Technology Readiness Levels involving multiple institutions; 
 
- Enhanced public awareness on how RAS can address the social-economic challenges and help sustain the future growth of the UK through a diverse range of planned public engagement activities; 
 
- Seamlessly linked doctoral training programmes (CDT) with a consistent and coherent process, propagation of best practice, access to pooled resources and secondment opportunities, and the establishment of a thriving environment for interdisciplinary mobility and nurturing new talents in RAS; 
 
- National coordination of real world RAS assets and synergistic usage of the capital equipment across all UK centres supported by resource sharing, knowledge exchange and comprehensive training programmes; 
 
- Increased influence on the future direction and investment of RAS at national, EU and international levels, leading to a sustainable eco-system for the UK RAS to thrive. 
 
The Network will promote Responsible Innovation throughout the UK-RAS communities by anticipating ethical, regulatory, and legal considerations of wider deployment of RAS and those that may arise in the next 5, 10 and 20 years, reflecting on the current RAS programmes of work, develop mechanisms for engaging with the wider community including the public and government policy makers and acting to ensure that RAS research is carried out for the best interest of society. By working with the EPSRC ORBIT for delivering responsible research innovation services, the Network will incorporate the established AREA framework for responsible innovation throughout the Network's activities."
9,CCF52373-EF37-4BC4-BC8D-94675AAF6FE5,Engineering Transformation for the Integration of Sensor Networks: A Feasibility Study - 'ENTRAIN',"There is a need to make use of new digital data analysis techniques to improve our understanding of the environment. Data from a new generation of environmental sensors, combined with analyses based on Artificial Intelligence, has the potential to help us understand from human influences and long-term change are affecting the environment around us. Artificial Intelligence approaches enable computers to identify trends and relationships across different streams of data, often picking out patterns that would be too difficult or time-consuming for humans to identify manually.

To realise these benefits, data from diverse sensor networks must combined and analysed together. Currently many sensor networks are operated individually, and data are not readily combined due to differences in the way measurements are made (e.g. between weekly river samples and sub-second measurements of gases in the atmosphere). In addition, to combine these data in an automatic way without human intervention requires much finer and more consistent descriptions of the contents of data streams, so that machines can understand the content sufficiently. Links between sensors in space are also important, and machines will need an understanding of these links, not just in the sense of coordinates, but for example how sensors are linked along rivers. We can construct a digital representation of rivers in order to enable this.
We will describe the various elements of a future environmental analysis system that will be required in order to achieve these benefits, and addressing some of these currently missing components. We will look at technologies, from databases to data transfer mechanisms, to understand how a system could be built.

We will use data from 3 NERC sensor networks measuring environmental variables from the atmosphere to river water quality, and show how this data can be automatically integrated in such a way that machines would be able to analyse it automatically.
A significant issue when monitoring with high-resolution sensors is how to handle problems in the data, which could include missing data, and erroneous values due to sensor failure. There is too much data for humans to manually view and check, and so automated approaches are needed. Currently these are often simple checks of individual data values against expected ranges, but again there are opportunities for artificial intelligence to improve this. AI approaches can look across multiple sensors, identify relationships, and find subtle changes in data signals, and this can be used to both identify data problems and to fix them through infilling. We will enhance the 3 NERC networks by testing and applying such approaches to data quality control.

We will investigate some fundamental limitations of high-resolution monitoring, the transfer of large amounts of data from the field site to the data centre, the security of such systems, and whether more processing could be done on the instruments themselves to reduce data transfer volumes.

We will meet with the public, with policy-makers, with industry and with researchers to discuss where there will be most to be gained from development of AI approaches to analysing environmental sensor data. We will develop ideas for future work to realise these gains, and will promote the benefits of an integrated system for environmental monitoring. These stakeholders are likely to include the Environment Agency, SEPA, Natural Resources Wales, Defra, Water companies, sensor network developers, and public organisations with an interest in the environment, including the National Trust, the Rivers Trusts, and local community groups.",,"The Digital Environment programme will benefit from ENTRAIN's foundation work, providing requirements, methods, best practice advice and recommendations for integration and data modelling across multiple sensor networks and other datasets, such as EO. This information and techniques will benefit other areas of science and industry, as well as public engagement.

Environmental practitioners, regulators, government, consultants, the water industry, agribusiness, insurance, and many others can benefit substantially from the joined-up evidence that ENTRAIN will start to generate.

The public, schools and colleges will benefit from access to meaningful data in a spatially aware context. There is strong public interest in environmental issues, and yet beyond weather forecasts, weather data and perhaps more recently air quality readings, there are few accessible data which have tangible meaning to the layperson.

The Earth Observation (EO) community will benefit from better access to connected in situ data for retrieval algorithm validation &amp; development. E.g. flooding extent, soil moisture and land cover products etc. and from new automated Phenocam greenness products output by ENTRAIN. Other Big Data projects (e.g. Data Labs) will benefit from a greater and easier connection to datasets, with common spatio-temporal linking requirements and proper metadata description already done.

Data modellers, and environmental informatics will benefit from improved sensor metadata schemes, sensor registers/catalogues, and data structuring for interoperability of a network of networks. We will disseminate the results of the ENTRAIN feasibility study also through an environmental informatics paper, describing the proposed methods and advances made in data modelling and data provenance, to ensure wide impact and uptake of these methods.

Observational scientists and regulatory observers will benefit from our website case studies, webinars, training workshops and online video tutorials to disseminate best practice and training in realtime data collection, cyber security, data vocabularies, metadata schemes and new deep learning QC techniques. These studies will benefit the global environmental, and wider, data communities such as the Committee on Data for Science and Technology (CODATA). Harmonisation of data networks to increase or facilitate interoperability, and production of spatio-temporally connected observations across environmental domains (e.g. Digital Rivers), will benefit the British Geological Survey (BGS), CEH, UK MetOffice, the Environment Agency (EA), the Scottish Environmental Protection Agency (SEPA), Defra, water companies (and other utilities such as the power grid), argi-business, insurers, and public health. Other NERC and EPSRC funded programmes (e.g. ASSIST, Natural Flood Management, HydroJULES, Internet of Food Things) and Defra Air Quality Monitoring Networks will all benefit through higher data quality, more complete data and efficiency gains in analysing data across sensor networks, and by using developed data structures in other environmental monitoring and food chain domains. The spatially connected integrated data visualisations will be demonstrated at both academic and industry events and conferences, where practitioners can benefit from e.g. improved water quality alerts (e.g. algal blooms, nutrient levels etc.), and crucially see the connected drivers of those trends, and get decision support information. Similarly, this has the potential to provide interconnected, cross-discipline, environmental management information that can give government the evidence chain to implement, monitor and evaluate policy.

We will monitor and evaluate the success of our impacts by recording attendances at events, the number of website visits, and use Twitter to promote activities, whilst recording the number of followers and retweets. We will also log email enquiries, webinar and web video views."
10,FCFEF620-5D64-4255-BA2B-155241488C8A,EMERALD - Enriching MEtagenomics Results using Artificial intelligence and Literature Data,"Microbes like bacteria and fungi inhabit diverse environments, including soil, water, and human body sites, such as the mouth, skin and intestine. Ubiquitous in nature, they also show adaptation to extreme environments, such as acid mine drainage or hydrothermal vents. We have appreciated the potential of microbes for a long time - they are important for food and beverage manufacturing (e.g. cheese and beer), and are key players in bioremediation, as demonstrated by their pivotal role in breaking down complex oils following the Deep Horizon oil spill in the Gulf of Mexico. The field of metagenomics offers an exciting opportunity to examine these microbial communities and gain insights into various aspects of their existence, i.e. their interaction with humans and plants, their potential as disease reservoirs, and as sources of novel enzymes with bioremediation or plastic recycling abilities.

Metagenomics studies microbial communities by sampling the environments directly, extracting and sequencing their genetic material (DNA), and applying computational methods to elucidate microbial composition and function. This sampling approach helps to characterise unculturable or as yet uncultured microbes in the laboratory. Metagenomics experimental data are typically large (10-100s of GBs per sequencing run; 100s of runs per project), complex (comprising 100-1000s of different microbes) and variable due to the nature of the underlying experiments and (sub-)sampling of the dynamic populations.

Despite knowledge about fluxes within a microbial community (e.g. time of year or day), metagenomic datasets typically contain poor descriptions (termed metadata) relating to the sample origin or methods used to obtain the DNA and process the sequence data. To help interpret data across experiments and derive meaningful biological conclusions, it is crucial to know whether a difference between two metagenomics datasets is due to differences in underlying experimental techniques or the biological qualities of the sample. The lack of metadata has impeded our attempts to apply machine learning (ML) techniques to interpret new incoming data, and therefore our capacity to find novel biological applications.

To circumvent these issues, our proposal aims to employ different ML methodologies to enrich the currently available metadata and start elucidating new knowledge embedded in the sequence data. The text mining approach will focus on identifying research articles on metagenomics experiments to unearth and extract detailed descriptions which will be used to enrich the metadata associated with the corresponding DNA sequences and generate new or improved classification systems. This dictionary of descriptor terms will also serve as the template for developing methods to discover previously unidentified metagenomics papers. We will train algorithms on this enriched metadata to progressively learn what criteria might be applied to incoming data with inadequate descriptions in order to determine sample origin, processing, as well as decipher which experimental biases affect the results, when comparing similar samples.

ML approaches will also be used for the discovery of new biological functions. Bacteria encode gene cassettes that are responsible for producing compounds of pharmaceutical and agricultural value. Functional descriptions for the genes constituting these cassettes are incomplete, while many cassettes still await discovery. By combining the ML and text mining approaches, we intend to better describe these cassettes and also focus on the detection of novel groups.

Data underpinning this work will originate from key EMBL-EBI databases, namely EBI Metagenomics and Europe PMC, as well as other resources (e.g. MIBiG). Developments aimed at herein will help resolve complexities underlying experimental data, enriching the metadata in the process and also laying the foundation for a new generation of reliable predictive models.","The field of metagenomics is burgeoning as the technique furnishes insights into the sum total of all microbial content within particular biomes. Technological advances in sequencing methods have resulted in a data deluge - while this has afforded us access to hitherto rare microbes, the analysis is often complicated due to inconsistencies in data sampling, lack of metadata specificity, data variability for identical biomes and choice of downstream analyses tools. As datasets from metagenomics experiments are inherently noisy, detecting significant and explicit biological signals becomes challenging. Comparison across similar datasets would help detect meaningful signals, but the paucity of standardized contextual metadata, associated literature and granularity in labelling makes this difficult. 

To overcome these issues, we will apply text mining (NLP) and machine learning (ML) methodologies to enrich and standardize metadata, improve functional annotations, and enhance discovery of novel secondary metabolite gene clusters (SMGC). We will identify metadata-linked terms already present in the EBI Metagenomics portal (EMG) and in full text publications in Europe PMC to develop training sets that will facilitate NLP/ML approaches for finding additional metadata. We will apply ML algorithms based on metagenomics datasets to determine biome-specific tags, enrich metadata and identify outlying datasets. Using a combination of EMG-linked data, biological relationships, and literature, we will also develop ML models that incorporate the complex rules behind cluster evolution and metabolite production. We will enhance existing SMGC descriptions via NLP approaches and use them to develop training sets for the detection of SMGCs. These ML SMGC models will then be applied to assembled metagenomics contigs to find novel SMGCs. We will also investigate clustering tools to enhance our ability to discover novel clusters based on the EMG protein sequence database.","Metagenomics is a rapidly expanding field wherein the depth and breadth of data are constantly increasing. Consequently, the number of published research articles associated with the field is also growing. However, there is often a disconnect between sample, sequence data and publication. The lack of data integration has hampered the production of statistically robust, predictive models. Moreover, datasets from different groups are rarely compared, partly because experimental approaches for investigating different microbiomes are constantly evolving.

In this proposal, we plan to adopt the use of machine learning (ML) algorithms and natural language processing (NLP) to help overcome these challenges by improving metadata and developing predictions based on taxonomic and functional assignments contained within EBI metagenomics (EMG), enhanced by linking to the primary literature in Europe PMC. We will also focus on the use of both ML and NLP to enhance our ability to discover novel microbial secondary metabolite gene clusters (SMGCs) in our metagenomics assemblies. SMGCs are responsible for the production of key products, like antimicrobials and insecticides, both of great agricultural and biotechnological importance, as well as impacting human health.

Due to the widespread use of metagenomics and the position of EMG and Europe PMC, we anticipate the impact of this research to be significant. Metagenomics is widespread in research projects associated with BBSRC strategic priorities- agriculture and food security, industrial biotechnology and bioscience for health; the field represents the epitome of data driven biology. Through the application of NLP and ML, we will demonstrate how these new technologies can be utilised to help research scientists interrogate big data. Whilst we will domain focused, the technical developments within this project will have far reaching impacts, applicable to other fields and analytical disciplines. The 'use cases' in the program will cover a range of cross-cutting themes, demonstrating the general applicability of the techniques to different environments and conditions. Furthermore, the semantically marked up literature, enriched metadata and SMGC annotations will have applications in a wide range of academic and industrial fields, including enzyme discovery, environmental science, diagnostics and animal/human health.

We will ensure impact on all academic and industrial audiences by the publication of software, data, compute containers and peer reviewed articles. To address the skills shortages in the fields of metagenomics, NLP and ML, we will deliver training, webinars and participate in community workshops. Other dissemination routes include the use of networks and collaborations, conferences and social media channels. The public sector will also be engaged, via specific events and through the publication of non-specialist articles and interviews. 

The outputs of the project will be of exceptional value to the commercial sector, and the benefits will eventually feed through to the public. The software and the applications there of, will lead to new discoveries such as new antibiotics for humans and livestock, higher agricultural yields from the understanding of socio-ecological interplay (e.g. food chain microbes) and expanded discovery of novel enzymes capable of operating at extremes, such as psychrophilic enzymes for detergents, or with novel catalytic functionality (e.g. anaerobic digestion pathways in biofuel production).

Combining literature and metagenomic data as in this proposal is pivotal to the notion of One Health- the collaborative effort of multiple disciplines working at national and international levels to attain optimal health for people, animals and the environment. Our proposal encapsulates this philosophy and will impact major UK and international communities, ensuring that the potential of metagenomics data is collectively realised."
11,B58BF490-CF95-434B-8A00-4BE9003142BE,Matter of context: Revealing the circuit architecture of internal brain state influence on behaviour,"My aim is to understand how ongoing internal activity patterns within the brain shape the way it processes information and controls behaviour. Human and animal behaviour is not merely a set of 'automatic' reflexes. Rather, the way we respond to sensory inputs such as the sight of food or the sound of a phone ringing depends on multiple contextual factors such as emotional state, time of day and how satiated or alert we are. Modern neuroscience has made important progress towards understanding the brain systems that report these factors. For instance, the dopaminergic and serotonergic systems that signal reward have been extensively studied due to their importance in shaping normal behaviour as well as psychiatric disorders. However, major challenges remain in terms of understanding how multiple brain pathways act together to modulate sensory processing and behaviour. To a large extent this is due to the size and complexity of the brain which precludes simultaneous measurement of the many brain cells involved. By establishing a new research programme in the Department of Neuroscience, Physiology &amp; Pharmacology at UCL, I plan to take a novel approach to tackle this problem. My strategy combines state-of-the-art imaging in an experimentally advantageous model organism - the larval zebrafish - with data-driven biology and computational modelling: key research avenues identified by the BBSRC.
Zebrafish larvae are particularly well suited for simultaneously tracking activity in multiple brain structures. This tiny animal (3.5 mm long) is almost perfectly transparent, allowing its small brain to be monitored non-invasively using fluorescent microscopy while the fish performs a range of recognizable behaviours such as hunting and avoidance. Importantly, many of these behaviours are influenced by contextual factors such as hunger or alertness, by virtue of brain circuits fish share with all other vertebrates, including humans. To study how distributed brain networks work together to shape behaviour, I will use cutting-edge &quot;light-sheet microscopy&quot; to individually track the activity of each of the zebrafish's 80,000 neurons. While doing so, I will alter environmental factors to manipulate satiety, alertness and other contextual elements. 
Deciphering the resulting dataset will be a complex endeavour, comparable to extracting insights into market dynamics by simultaneously listening to each and every one of the 100,000 finance employees in the City of London. The potential for valuable insights is enormous, but so is the challenge in making sense of the massive amount of data and finding the most informative sources. To meet this challenge, I will use recurrent neural networks - a modern machine-learning algorithm akin to the one that powers automated speech recognition. It will enable me to identify neurons that can predict if the animal is likely to respond to a specific visual cue, even before the stimulus is presented. Such cells are good candidates for signalling contextual information and my computational modelling will resolve how they work together to collectively influence behaviour. To test my hypotheses, I will use advanced &quot;optogenetic methods&quot; to directly control brain activity using light and examine the resulting effects on activity elsewhere in the brain and on the behaviour of the fish.
Ultimately, these findings will shed new light on how neural activity related to context and experience combine to influence fundamental brain function. Because all vertebrates possess the same basic brain plan, my experimental findings are likely to reveal principles that apply to many species, including humans. Thus, in line with the BBSRC's priority of supporting world-class basic bioscience for health, this project will provide a major advance in our understanding of how the healthy brain produces behaviour. In the longer term, this could underpin greater understanding of how brain function is disrupted during disease.","A major outstanding challenge in neuroscience is understanding how the internal state of the brain modulates the transformation of sensory input to behaviour. I aim to address this by revealing the principles of circuit organisation that allow multiple internal state variables to work together and exert control over sensorimotor pathways. I will use larval zebrafish, an animal model that uniquely brings together the complexity of the vertebrate brain with an experimental capacity to simultaneously monitor the range of circuits involved in behavioural modulation. Zebrafish display distinct visually-guided behaviours differentially modulated by arousal, feeding state and stress, indicating that gating of sensorimotor pathways in the larval brain is influenced by multiple internal state variables. However, in no species is it known how these sources of information are integrated and where this occurs in the brain. I will tackle this question by combining cutting-edge approaches in data- and model-driven research with state-of-the-art imaging, in line with BBSRC's 'exploiting new ways of working' enabling theme. First, I will use light-sheet microscopy to record cellular-resolution activity throughout the brain during behaviour. Next, using deep-learning algorithms, I will achieve minimally-biased identification of neurons influencing behaviour, resulting in a functional model of internal state integration. Lastly, I will test the inferred architecture by directly manipulating identified neurons using genetically encoded actuators and integrate functional data flows with underlying neuroanatomy. Overall, I will derive a quantitative circuit model that uncovers the neural basis by which multiple evolutionary-conserved internal state components exert a combined effect on behaviour. This will be a major advance in our understanding of how the healthy vertebrate brain produces behaviour, in accordance with BBSRC's priority of advancing basic bioscience for health.","The proposed project will have a broad impact on several groups of academic and non-academic beneficiaries, in the UK and abroad:

1. Academic beneficiaries in the neuroscience community. I anticipate the project outcomes will signify a major advance in our understanding of the interactions between internal state areas and their influence on behaviour. Both the results, conceptual frameworks and methods are expected to have broad impact in the neuroscience community, which I will maximise by:
i. Academic publications: The project has the potential for several publications in high-impact journals that will disseminate the new results and methods to a wide audience. All publications will be made available through UCL's open access portal, 'UCL Discovery'. 
ii. Scientific meetings: Throughout the project I will communicate my research methods and findings at seminars and conferences within the UK and abroad. 
iii. Workshops: Experimental and analytical tools will be discussed and shared at specialist workshops focussed on interpretation of neural activity data, including a symposium I will organise on the use of deep-learning for this purpose.
iv. Software sharing: I will apply state-of-the-art computational approaches to exploit high-dimensional time-series data - an increasingly important challenge in modern neuroscience. To make my software tools highly accessible, I intend to share them as an open-source software framework, 'NeuroSeqMap'. 
v. Student mentorship: Undergraduate, Masters and PhD rotation students will benefit from supervision and training in cutting-edge systems neuroscience methodologies.

2. The general public. The proposed project bears a lot of potential for public interest as it involves cutting-edge experimental techniques with a unique model organism. I will take the following steps to foster public engagement:
i. Online virtual experiment: I will create an interactive virtual experiment based on my data, where users will be able to observe the movements of the animal alongside neural activity in its brain. This will be made available on online platforms such as The OpenScience Laboratory, making it accessible to users around the world, inspiring exploration projects for students and 'citizen scientists' alike. 
ii. Public outreach activities: UCL's outreach programmes will serve as outstanding platforms for communication of the project. These include A-level school visits, Taster Days and Masterclasses. I will also participate in A-level student visits organized annually by the UCL zebrafish labs, the European Researchers Night annual events and the Brain Awareness Week.
iii. Media and publications. With the help of the UCL press office, I will raise interest in my research and its products in mainstream and new (social) media. I will upload lay 'paper summaries' describing my interests and results to the Bianco lab website and write press releases for the general media.

3. Industry and policymakers. My project will apply deep-learning algorithms in the context of massively high-dimensional time-series data. This may have relevance in such fields as weather prediction, mass transit systems, bandwidth management and epidemiology. Realising this potential may impact the competitiveness of UK companies and increase the effectiveness of public policy. To enhance this potential impact, I will use online platforms to expose potential beneficiaries to the project's dissemination products and invite them to participate in the symposium (above).

4. Clinical research community. Neuromodulatory systems and the interactions between them are increasingly being viewed as playing a major role in psychiatric disease and are primary targets of current treatment strategies. Thus, this project has potential indirect and long-term impact on research directions related to medical research. I will present my work to clinical researchers via events in the UCL/UCLH Biomedical Research Centre and UCL Partners."
12,E9593DA8-F859-46A5-AD92-20167928FBFF,Privacy-Protected Human Identification in Encrypted/Transformed Domains,"Biometrics has been widely utilized in the past two decades in many areas such as healthcare, banking, surveillance, and security control. Given the increased uptake of internet and mobile computing globally, many companies have been turning to biometric privacy and security to ensure secure communication. However, biometric verification over third-party or public network servers may be abusively exploited in an unauthorized way. To protect the privacy and improve the security, it has been advocated to carry out biometric verification in encrypted or transformed domains, where privacy and security can be more effectively guaranteed. 

The basic idea behind the project is that the biometrics in the irreversible encrypted/transformed domains contains exactly the same amount of information as its original one, and hence one can establish a pattern recognition methodology to determine/extract useful information from chaotic signals in encrypted/transformed domains. This First Grant Scheme project aims to investigate how to discover and evaluate the information from chaotic signals for discriminative power, and develop robust pattern recognition schemes for biometric/multi-biometric verification in encrypted/transformed domains. The proposed methods/schemes will be vigorously validated over typical wild face/speech/gait datasets, and two practical demo systems (biometric banking and pedestrian profiling) will be designed and tested in real world environments.

The project will focus on both theoretical understanding of chaotic information and application-specific exploitation of chaotic pattern recognition. Considering multiple data structures hidden beneath a set of given chaotic signals, I will develop a robust way to find out the underlying various data structures for data understanding, clustering and classification. On the other side, given a specific issue such as encrypted/transformed biometric verification, one need to examine the generic theoretic findings in this specific topic and develop a robust scheme for biometric human identification.

The work of this project is within the areas of signal processing, machine learning and pattern analysis. The research on encryted/transformed biometric verification has come from the practical new needs of the UK's emerging new businesses. The project will provide the understanding needed to allow the future development of robust biometric verification methods with novel applications.",,"The rapid advancement in biometric technology has generated huge impact in UK's usual life, ranging from financial service, public security, legal service, immigration control, to daily medical service and healthcare. As a result, biometric industry in UK has experienced drastic expansion in its market. For example, biometric banking has been widely endorsed by major UK banks. Public surveillance is also becoming a thriving market in UK. The value of new biometric markets has achieved $15 billion in 2015 and been estimated to grow further to $45 billion by 2021 (Global Biometrics Market, Markets &amp; Markets, 2015). However, accompanying with the wide spread of biometric uses, people are more and more concerned about its security and privacy issues, especially when Internet-of-Things becomes booming and biometrics are used over public network severs. Developing safe and robust biometric technologies is a key priority for the UK to maintain and reinforce its world-leading role in this new research area and business market. 

A key sector in the UK economy is banking and financial services, which accounts for 10% GDP (Financial services industry of the United Kingdom, Wikipedia) and 33% of the UK's trade balance. Biometric banking is now growing in importance due to the increased popularity and spread of mobile banking. People can easily access their accounts by scanning their face/iris/fingerpring. In the past two years, biometrics such as fingerprints/veins have been utilized by major banks such as HSBC and Barclays. Atom Bank has developed a solution to exploit face and voice recognition, which can be more conveniently integrated with mobile banking. This project will address key security and privacy issues linked to mobile banking, and Atom's involvement will give invaluable insight into industrial requirements for advanced technology exploiting biometric verification in the encrypted domain. I will also develop a mobile-based biometric banking demo with Atom Bank's professionals. Atom bank is a potential partner for future spin-off projects funded directly (under non-disclosure agreements), via InnovateUK (KTPs or otherwise) or with other partners in, for example in EU Horizon 2020 projects. 

Public surveillance is another UK's thriving industry with a market value estimated around &pound;6 billions (Securing the Nation's Future, British Security Industry Association). In this project, I will develop a surveillance demo system in collaboration with Warwick team and their spin-out, which is about pedestrian profiling using scrambled gait/face to provide real-time city monitoring on google map. Potentially, this may lead to a further KTP project via InnovateUK or a SME project via EU Horizon 2020.

The training of the postdoctoral researcher and a university funded PhD student with skills relevant to digital technologies will benefit both the individuals and UK industry. The experience gained will include signal processing, artificial intelligence, vision and image, and security systems. They will receive training in public understanding and engagement and will be involved in outreach work through Think Physics and Digital Living activities and attend exhibitions at Sunderland Software Cities and North East Catapult Centre (Sunderland). This will benefit the individuals and also attract wider interest from the public by promoting the spirit of science and highlighting everyday impacts which arise from this research."
13,738142B5-F0E7-480C-A37A-E5D9B0C6872D,Dynamic modulation of brain states using brain stimulation and neuroadaptive Bayesian optimization,"Like an orchestra that relies on the coordinated efforts of its members, the brain depends on its many regions working together to perform the multitude of cognitive functions that makes us human. These functions allow us to solve problems, retrieve relevant information from memory and select the responses necessary to perform a particular task. In order to do this, the brain must coordinate the interactions between regions located far apart. 

One of the greatest challenges of modern neuroscience is to understand how these interactions occur, and how their occurrence gives rise to efficient behaviour. A tool capable of influencing the interactions between brain regions could help scientists understand better how a particular pattern of brain activity is associated to efficient behaviour, such as being able to retain information in memory or solve a problem. Such a tool could then be applied to neurological and psychiatric conditions, where the interactions between brain regions might be malfunctioning.

The objective of this project is to develop this tool. In order to do this, we will combine functional magnetic resonance imaging (fMRI), non-invasive electrical brain stimulation and machine learning. Each of these techniques brings a critical element to this tool.
FMRI is a technique widely used by neuroscientists to provide images with information about brain function. Non-invasive electrical brain stimulation is a technique that applies low-voltage current through the scalp and can change the activity of neurons without requiring surgery to implant electrodes. This technique has been shown to influence brain function and the interactions between brain regions. Electrical brain stimulation, however, can be applied in many different ways, thereby making it difficult to know what would work for to influence a particular interaction between a set of brain regions. In addition, the results of brain stimulation can vary depending on factors such as a person's age, sex, brain anatomy and genetics. This makes creating a tool capable of identifying the stimulation parameters for each individual like 'finding a needle in a haystack'. This is why machine learning is necessary, where a computer program &quot;learns&quot; to identify which brain stimulation parameters optimally engage brain regions involved in cognitive functions in a time frame that would not be possible using conventional methodologies.

In essence, our tool will use brain stimulation to influence how brain regions interact, fMRI data analysed while the participant is receiving a certain type of stimulation to inform on how the brain reacts to it, and machine learning to select the next stimulation that should be investigated. By the end of the experiment we will obtain a map with the brain's responses to different stimulation conditions, and a prediction of what the optimal stimulation condition to elicit a brain response is.

This tool could then be used in many clinical conditions where inefficient communication between brain regions has been observed, such as psychiatric conditions and during rehabilitation after brain injury.","In order to support cognitive functions, the brain must coordinate the interactions among large-scale networks that cooperate and compete to allow for efficient transitions between brain states. Understanding how these operate, giving rise to different behaviours is one of the greatest challenges facing modern neuroscience. 
The overarching aim of this project is to develop a framework capable of shaping the interactions between brain networks. In order to do this, we will combine transcranial alternating current stimulation (tACS) with a novel approach, neuroadaptive Bayesian optimization.
TACS is a promising tool to modulate brain function. The oscillatory electrical activity imposed by tACS has been shown to result in neural modulations that spread along brain networks. However, there are two main limitations to the application of tACS to modulate brain function: 1) the brain networks targeted by stimulation cannot be verified in the absence of brain imaging; 2) the stimulation parameters vary across individuals, due to a multitude of variables, such as age, sex and genetic factors. Thus, identifying the optimal stimulation protocol that drives a particular brain state in a given individual is like 'finding a needle in a haystack'. To address these fundamental challenges, we propose to use neuroadaptive Bayesian optimization, which uses a close-loop search combining real-time fMRI with machine learning. This approach conducts an automatic and intelligent search across the multitude of tACS parameters in order to identify those that optimally elicit a particular target state. 
This framework has translational potential, as several psychiatric and neurological conditions are associated with impaired function of large-scale brain networks. The results of this project can lead to the development of therapeutic interventions that harness the potential of brain stimulation.","The research proposed in this application has the potential to generate results with substantial scope to have a wider societal and economic impact.

Societal Impact:
The health and wellbeing of the general public will benefit from this research. Several of the most common psychiatric and neurological conditions, including depression, obsessive-compulsive disorder, schizophrenia, stroke and traumatic brain injury, are associated with impaired function of large-scale brain networks. The treatment of such conditions depends on understanding the network organization of the brain. The application of brain stimulation in patient populations is currently largely agnostic to its impact in the function of brain networks and this can result in inefficient and costly treatment programmes. This research will provide important insights into the integration of non-invasive brain stimulation and modulation of the function of brain networks, allowing for more precise and cost-effective definition of treatment programmes. The PI has directly worked with clinical partners and has links to clinicians, hospitals and health providers (Imperial College Healthcare NHS Trust, King's College Hospital NHS Foundation Trust and Royal Surrey County Hospital) that can support the translation of findings into diagnostic and therapeutic interventions. 

Economic Impact:
Direct and indirect costs associated with mental health and cognitive impairment after brain injury have a significant impact on the UK economy. These costs constitute an enormous problem for patients and extend to their families, the NHS and society, as they contribute to job loss, impact caregivers' productivity, can lead to family breakdown and recidivism. Unfortunately, there are limited treatment options for these conditions. The results of this project can lead to the development of therapeutic interventions that can harness the potential of brain stimulation. This project will contribute to enhance UK scientific excellence and competitiveness in brain research, which would lead to welfare benefits for patients and cost-benefits for the health-care system. 

Commercial Impact:
The neurotechnology developed in this proposal could create strong links between academia and industry due to the potential to develop products associated with medical imaging and non-invasive brain stimulation interventions. Neurotechnologies for neuroenhancement using non-invasive brain stimulation devices constitute a rapidly growing industry, with more than a dozen companies selling non-invasive brain stimulation products (e.g. Foc.us, Halo Neuroscience, Caputron, etc.). According to SharpBrain, the brain fitness market is expected to grow to $6 billion by 2020 (Fortune, November 2015). However, there is currently a large gap between the understanding of the effects of these products in brain dynamics and behavior. Our research could contribute to the development of more evidence-informed neuroenhancement technologies which can tap into this market. 

Public Engagement:
The PI has an excellent track record in initiating and organizing public engagement events with patients, in schools and participating in science festivals. We will disseminate the results of our research through public engagement events. These will aim at increasing the knowledge about brain research in humans, the use of neurotechnology and to improve awareness about the use of non-invasive brain stimulation devices for neuroenhancement, which interests a growing, often misinformed, DIY community (Scientific American, 2017)."
14,33630BCF-6E4B-4032-BD14-5B7EB6E1B43B,Business and Local Government Data Research Centre,"An ambition to be the world's most innovative economy is set out in the UK Government Industrial Strategy. Local authorities and businesses possess large amounts of data covering every aspect of their daily activities. While this resource is valuable, the opportunity for transformative change in business models in public and private sectors comes from adopting data science and artificial intelligence techniques and embedding them as an analytical layer in every stage of decision making. Transforming data to knowledge with the help of advanced analytics can provide local authorities and businesses additional information which can help them to design better policies and improve their business operations.
The Business and Local Government Data Research Centre (BLG DRC) aims to enable a step change in the way public sector organisations and businesses make use of and harness the power of their data through advanced analytics. As such, BLG DRC will coalesce a number of research strands in social sciences, data science and AI within University of Essex and will provide the synergetic scope required to make significant new breakthroughs in social sciences and methodological research. The primary stakeholders of BLG DRC will continue to be local authorities and businesses, but we will expand our remit of work and collaborations to the wider public sector as well as internationally.
BLG DRC's mission places stakeholders and users at its heart, and the research programme of work will be user-driven and involve an ethos of co-creation with external partners and stakeholders. The comprehensive integrated outreach programme which consists of training and knowledge exchange activities will ensure an ongoing dialogue with the users and stakeholders of the research and project partners including policy makers and that the project outputs will have lasting impact beyond the life of the Centre. In particular, focusing on the public sector and working with our regional partners, Essex County Council (ECC), Essex Police (EP), Essex Partnership University NHS Foundation Trust (EPUT) and others, BLG DRC will serve as a joined up, system-wide public sector AI and data science hub where the focus of the work will be on improving lives and generating efficiency in public services through embedding of novel data science techniques and AI. We have also partnered with businesses who wish to understand how we can foster and support economic growth, particularly for small and medium enterprises and start-ups. We aim to explore the barriers these businesses face and how data science and AI can help us understand the best means of overcoming these. 
The overall programme of work is divided into two strands which are strongly interlinked: (i) developing new research and interdisciplinary capacity, and (ii) delivering and further expanding our integrated outreach programme. The programme has been designed to maximise the impact of both existing activity and also engage in new research that will have direct and significant impact at the regional and national level. Our users and stakeholders will continue to play a key role in shaping up the programme of work on the one hand benefitting from the research and integrated outreach programme while on the other directly feeding in challenges and problems with respect to socio-economic research and the development of new methods needed to address these problems so that the take-up of our work can be maximised. Building on our successful substantive socio-economic programme of research and methodological research stream, we aim to further develop and undertake new research in three core areas: (i) Support for Vulnerable People; (ii) Supporting Economic Growth; (iii) Methodologies and Techniques for Data Science and AI. The new phase of BLG DRC promises to be an exciting development that will not only advance knowledge but also benefit our community.",,"Non-academic beneficiaries of the Business and Local Government Data Research Centre (BLG DRC) include a range of organisations and individuals, operating at different levels. 
BLG DRC mission and focus place stakeholders and users at its heart, and the research programme of work will be user-driven and involve an ethos of co-creation with external partners and stakeholders. This ensures on the one hand that our research remains relevant and close to the problems as experienced by organisations, while on the other hand it maximises the potential for impact and take-up of our research.
Key beneficiaries of the Centre's work include:
1) Businesses will have access to new techniques which can improve their decision processes and training activities will build their capacity in data science and AI to improve their productivity and decision-making. Research on digital leadership and organisational transformation will be relevant and could have implications for organisational development strategies.
2) Business organisations such as the CBI and Institute of Directors will benefit through better understanding their constituents' needs and priorities and how data science and AI can be used to support business development.
3) Local authorities will benefit from AI-driven transformation of public service delivery that allows them to reduce the administrative burden and deliver services for the benefit of communities under decreasing budgets. 
4) Local government officers, Councillors and the Local Government Association, County, Borough and unitary authorities in the UK are responsible for a number of services addressed. The focus on vulnerable people, health and social care and supporting economic growth are all relevant. We expect considerable impacts on practice in the provision of services.
5) Government department researchers and policy makers. The methodological developments and substantive research will provide a model of relevance for all areas of the UK. The likely impact is significant as new ways of utilising data science and AI are developed.
 6) Government executive agencies, non-departmental public bodies, parliamentary select committees and quangos. Many will have interests in data science and AI, and we expect agencies such as e.g. DCMS, HMRC, DFID, Cabinet Office, Dstl, and DWP to be beneficiaries of the research.
7) The international policy community. Methodological and substantive research will be relevant to other countries and developments around the use of AI and Data Science in the public sector and business contexts. We envisage working with the EU Joint Research Centre on AI aspects of smart specialisation and anticipate considerable international impact.
8) NGOs, think tanks, charities and independent research organisations will benefit including for example the New Economics Foundation, the Institute for Public Policy Research, Age UK, the Transport Research Laboratory, the Child Poverty Action Group, and Demos.
9) The general public will benefit through more informed policy-making and an awareness of the benefits of data science and AI can provide including reduction in administrative burden and more targeted provision of public services. Benefits will accrue through the provision of better quality, more effective and targeted services and through new public or firm level policies for the benefit of community. Fair, transparent, and ethical use of AI for the benefit of society will also build trust and public confidence. We expect findings to be reported in news media and to stimulate public debate.
10) As the BLG DRC positively influences policy across a range of areas at both regional and national levels, society as a whole will benefit through improved quality of life via improved public health and better social support for vulnerable people, economic growth, and technological advances."
15,001C43DE-0140-443B-9780-D5B784C63D8B,Determining cerebrovascular reactivity from the pupil flash response,"For most neurological disorders a diagnosis and intervention is only possible once significant progression has occurred and symptoms are present by which time the prognosis may be poor. It would be preferable to be able to make an early and accurate identification of those people who would be likely to develop such a disease later in life, allowing sufficiently early intervention to prevent it. 
The premise is that with the identification of reliable biomarkers of conditions such as Alzheimer's disease, multiple sclerosis, Parkinson's disease and stroke, early identification of individuals predisposed to developing these conditions could be identified from regular screening within routine healthcare activities. The identification of a suitable biomarker has been the focus of much research. 

One feature of these diseases which has the indications of being a good biomarker is a measure of how good the blood vessels in the brain are at responding to changes in demand or in response to an external stimulus. This effect is called the cerebrovascular reactivity or CVR. CVR is known to be impaired in the majority of brain diseases, and appears to be one of the earliest detectable symptoms that something is wrong. CVR is however quite complex and difficult to measure, requiring specialist, expensive equipment, and so it has not been widely studied in clinical trials of diseases or treatments. It has also been suggested that there are two different forms of CVR dysfunction, one due to a person's intrinsic biology and another due to their lifestyle, with each requiring different treatments. 

The impact of lifestyle on CVR can be estimated from simple questions and physiological measurements but there is currently no simple means of determining the level of intrinsic CVR function. Because of this there is the potential that trials of new treatments could target only one cause but include patients with both types (intrinsic and lifestyle) and therefore be ineffective in a majority of subjects and be incorrectly deemed to be of no use. Being able to readily and cost-effectively determine which of these causes is operative would be highly beneficial to both research and, ultimately, clinical environments.

The hypothesis of this project is that intrinsic CVR dysfunction is caused by a general smooth muscle disorder; as smooth muscle is wrapped around blood vessels to control the flow. One of the few other places that smooth muscle occurs in the body is controlling the iris of the eye. It has been shown that groups known to have impaired CVR also tend to have a higher risk of developing neurological disorders, and the same groups have also been shown to have an impaired response of the pupil to a brief flash of light. As the pupil flash response (PFR) is potentially very cheap, quick and easy to assess it would make an excellent means of testing for intrinsic smooth muscle impairments as an indicator of impaired CVR. 

By determining a range of simple physiological measurements for subjects, along with their responses to a lifestyle questionnaire and a measurement of the pupil flash response, the necessary data could be obtained. By then applying analytical machine learning techniques to the results, we propose that this will allow the development of a protocol to enable an accurate assessment of CVR, and its likely type, to be determined. This measurement could then form part of a risk assessment for a host of neurological disorders and enable early interventions to be implemented or discovered.",,"The potential impact of this research is wide ranging. In the short term, clinical trials would benefit as they would have the ability to easily and quickly stream participants according to the nature of their CVR impairments in order to investigate the relationship between CVR and cognitive decline and other neurological conditions. As the beta amyloid hypothesis has failed to produce any tangible results for dementia suffers it is time to consider alternative theories of the causes of these diseases, however this will require carefully designed studies to ensure the different forms of CVR are appropriately considered. Existing methods of measuring CVR in a way that can distinguish between the types is not feasible for large-scale trials.

If CVR is shown to be an important biomarker, or even a potential treatment target, then a pupillometry-based measure of CVR would enable the creation of a commercial device for this purpose. There are a small number of clinical pupillometers on the market, but these are designed for measuring changes over the short term to monitor brain injury patients in the emergency room and are not suitable for this purpose. The Department of Engineering Science has a world-class record of creating spin-out companies, particularly in the biomedical sector and thus is ideally placed to be able to facilitate the creation of such a device. Such a device could potentially be used in future clinical trials into a range of neurological conditions, as well as being available in clinical settings such as a GP surgery. Testing a patient's PFR could become as commonplace as measuring their blood pressure.

As it is not yet known which if any aspect of the PFR correlates sufficiently well with CVR for diagnostic purposes it is not possible to predict what the outcomes will be, however a best-case scenario would be that some useful data can be obtained from very simplistic measures of PFR in combination with some basic physiological and lifestyle data (age, height, weight, gender, etc). If a smartphone app can be created to collect this data then it creates the possibility for a huge dataset to be collected through people downloading the free app. This data could then be made freely available researchers around the world.

The evidence to support the hypothesis is mainly through anecdotal correlation, however the only way to test it is with a study of this scale. Without sufficient variation in CVR and PFR it is impossible to test for a genuine relationship between the two. However, if there is one, and it can be used to diagnose a smooth muscle disorder that would impair CVR then it would be a revolution in the field."
16,CEF50708-922F-4574-8EDA-4EA4BE4ACCA7,"Perceiving, Modelling and Interacting with the Object-Based World","&quot;Perceiving, Modelling and Interacting Autonomously in a Dynamic Object-Based World&quot;

The Dyson Robotics Lab at Imperial College was founded in 2014 as a collaboration between Dyson Technology Ltd and Imperial College. It is the culmination of a thirteen-year partnership between Professor Andrew Davison and Dyson to bring his Simultaneous Localisation and Mapping (SLAM) algorithms out of the laboratory and into commercial robots, resulting in Dyson's 360 Eye vision-based vacuum cleaning robot in 2015 which can map its surroundings, localise and plan systematic cleaning pattern. Our success in working together made it clear that computer vision is a key enabling technology for future robots. This proposal aims to fund the Lab to push the forefront of visual scene understanding and vision-enabled robotic manipulation into new and more demanding application areas.

The research activity we are outlining in this Prosperity Partnership complements the large internal R&amp;D investment that Dyson is making to to created advanced robotic products. The aims of this partnership are to invent and prototype the breakthrough robot vision algorithms which could truly take us to next generation capability for advanced robotics working in unstructured environments, and to transfer this technology into the long-term product pipeline of Dyson as they aim to open up new product categories.

Dyson has now been working on robotics for nearly 20 years, a period during which the emergence of real consumer robotic products has happened alongside astounding progress in academic research in the broad field of AI. At the present time, floor cleaners are still the only category of mass-market robot which have achieved significant commercial success. This can be put down simply to the greater difficulty of the other more complex tasks and chores that a consumer might want an autonomous product to achieve. These tasks place much larger demands on a robotic system to understand and interact with its complicated 3D surroundings and the objects they contain. This programme will focus on creating the research breakthroughs needed to enable this next generation capability.

There are scene perception and modelling competences which underly all of these use cases, and these will be our research focus as we develop the algorithms behind next-generation object-based SLAM systems by combining all of our knowledge in state-based estimation and machine learning. We will also work more specifically on the methods for training learning systems; methods for advanced vision-guided manipulation; and the frameworks needed for practical, contextual human-robot interaction. The core scientific work will be forward-looking and academic, but always with a strong guidance from our partners at Dyson.",,"Domestic robotics has been forecast to be a major global growth sector (&pound;1B currently to &pound;20B by 2025), and Dyson is well-positioned to be a key driver of this growth with its investments into personnel, research and facilities over the past five years. It is often stated that the UK should aim at leadership in AI, but Dyson is one of the very few UK companies aiming seriously at making that happen at scale with real robot products already on the market and sold worldwide. 
Robotic vacuum cleaners and lawnmowers have barely scratched the surface of the possibilities that domestic robotics represent, and the research programme presented in this proposal tackles key fundamental challenges that need to be addressed in order to produce robots that can perform useful functions in the real world alongside humans. Furthermore, specific provisions have been made to ensure a continuous pipeline of technologies from low TRL research at the Laboratory all the way through to high TRL commercial deployment by Dyson and as such Prosperity Partnership funding will allow us to provide a much better flow from technology to industry. 

Besides direct wealth creation, the existence of a centre of knowledge and excellence in the field of vision-enabled robotic manipulation will act as a nexus to draw (and retain) much needed skills, capabilities and investment into the UK. The Laboratory already has a good track record of attracting some of the top research talent in the world, and Prosperity Partnership funding will greatly aid in maintaining this attractiveness. The close involvement of Dyson engineers with the Laboratory will provide researchers with grounding in real-world challenges besides giving them a flavour of life in industry should they be interested in pursuing non-academic careers - Dyson most certainly will require many more robotics engineers. 
SLAM and its evolution into general robotic spatial awareness remain key areas of interest in the academic disciplines of robotics, computer vision and AI, and we intend to make fundamental and high impact published scientific contributions during the project. Our track record in consistently encouraging and helping students and PDRAs to publish at this level speaks for itself. Doubtless, the research outputs of the Laboratory will also find application in a whole host of different sectors, for example medical robotics, construction, disaster relief, assisted living and manufacturing, all of which require robots that can interact in real-time with complex, dynamic environments. 

From a societal point of view, the advent of domestic robots capable of performing a whole range of tasks around the home can be expected to improve the quality of life by reducing the amount of time that is devoted to performing household chores. This will certainly have the largest impact on homemakers and, given that a greater majority of homemakers are still women, the widespread adoption of domestic robotics might also be expected to have beneficial knock-on effects on gender equality issues and household wage demographics. A further corollary to the beneficial effects of domestic robotics is the great potential to extend the independence of an ageing population.

Finally, Dyson and Imperial College have a strong relationship with technical media outlets including The BBC, The Times, The Guardian and Wired Magazine, which will allow us to disseminate our results to a large and worldwide audience. We will also ensure that the key technical demonstrators are showcased at multiple events. Imperial Festival attracts over 5,000 people and generates considerable media interest. Through these engagements with the public, we aim to inspire the next generation of engineers and scientists and generally getting people excited about the potential behind robotics. Lastly, the Lab maintains an open website and this project will have its own space which will be continually updated with the latest developments."
17,507503DD-0CC5-4EEF-88C0-92B7A24B97F0,Interactive Perception-Action-Learning for Modelling Objects,"Manipulating everyday objects without detailed prior models is still beyond the capabilities of existing robots. This is due to many challenges posed by diverse types of objects: Manipulation requires understanding and accurate model of physical properties of objects such as shape, mass, friction, elasticity, etc. Many objects are deformable, articulated, or even organic with undefined shape (e.g., plants) such that a fixed model is insufficient. On top of this, objects may be difficult to perceive, typically because of cluttered scenarios, or complex lighting and reflectance properties such as specularity or partial transparency. Creating such rich representations of objects is beyond current datasets and benchmarking practices used for grasping and manipulation. In this project we will develop an automated interactive perception pipeline for building such rich digitization.

More specifically, in IPALM, we will develop methods for the automatic digitization of objects and their physical properties by exploratory manipulations. These methods will be used to build a large collection of object models required for realistic grasping and manipulation experiments in robotics. Household objects such as tools, kitchenware, clothes, and food items are not only widely accessible and in focus of many practical applications but also pose great challenges for robot object perception and manipulation in realistic scenarios. We propose to advance the state of the art by including household objects that can be deformable, articulated, interactive, specular or transparent, as well as shapeless such as cloth and food items. 

Our methods will learn physical properties essential for perception and grasping simultaneously from different modalities: vision, touch, audio as well as text documents such as online manuals and will include the following properties: 3D model, texture, elasticity, friction, weight, size and grasping techniques for intended use. At the core of our approach is a two-level modelling, where a category level model provides priors for capturing instance level attributes of specific objects. We will exploit online available resources to build prior category level models and a perception-action-learning loop will use the robot's vision, audio, and touch to model instance level object properties. In return, knowledge acquired from a new instance will be used to improve the category-level knowledge. Our approach will allow us to efficiently create a large database of models for objects of diverse types, which will be suitable for example for training neural network based methods or enhancing existing simulators. We will propose a benchmark and evaluation metrics for object grasping, to enable comparisons of results generated with various robotics platforms on our database.

The main objectives we pursue are commercially relevant robotics technologies, as endorsed by the support letters of several companies. We will pursue our goals with a consortium that brings together 5 world-class academic institutions from 5 EU countries (Imperial College London (UK), University of Bordeaux (France), Institut de Robotica i inform&agrave;tica Industrial (Spain), Aalto University (Finland), and the Czech Technical University (Czech Republic), assembling a complementary research team with strong expertise in the acquisition, processing and learning of multimodal information with applications in robotics.",,"We expect impact in several directions from IPALM. We will create an open access dataset,
together with the evaluation data and metrics in order to help build momentum beyond
the project partners. Large part of the project is devoted to development of objective
benchmarks and evaluation strategies for facilitating research in relevant domain.
We will disseminate project outcomes across different scientific disciplines to strengthen the
community involved in addressing the challenges in object modelling and manipulation. Our
consortium includes experts from a broad range of disciplines needed to tackle this topic e.g.
robotic manipulation, computer vision, embodied cognition, and performance
evaluation.
At the 2015 Amazon Picking Challenge, Prof. Henrik Christensen from Georgia Tech noted
that perception was the dominating factor separating the winners from the rest of the
field and that &quot;90% of all robots today don't use sensors&quot;. Thus, the functionality of
perceiving objects in everyday tasks will act as multiplier for creating large impact to markets
and society.
Domestic service robots are foreseen as a key technology to meet the challenge of the
ageing society, enabling enhancing the quality of life by providing personal assistance in
smart homes. IPALM website will provide information to the general public (target: 10 000
views).
International Federation of Robotics study from 2016 showed a 24% annual increase in
number of service robots sold in both professional and personal markets. The trend is
continuing and if capabilities will be developed in Europe through projects such as IPALM,
Europe will be able to capture significant share of the growing market.
As we demonstrated in our ImageCLEF data challenge in 2015, many researchers can be
attracted to a scientific area if given the data and focus. IPALM benchmark will provide this
for object manipulation in domestic scenarios. Our aim is to become a leading research
project in the area of object modelling and manipulation. Released open-source software and
open-access datasets will promote reproducible research allowing other researchers to build
on the advances.

Beyond individual benchmarks, IPALM framework will provide a general-purpose tool for
object modelling and manipulation, which can be used to construct applications in robotics
and beyond.
Scientific advances in object modelling by combining innovations in robotics, computer
vision and machine learning will contribute to new scientific knowledge with wide
implications. The scientific impact will be enhanced by vigorous dissemination activities
through top ranking journal and conference publications in respective fields. In terms of
academic impact, the robotics, computer vision and machine learning research communities
will benefit directly from our methodological innovations, which will be published in top
venues in these areas, including high impact journals (IEEE TPAMI, IJCV, IEEE T-RO, IJRR,
IEEE T-CDS) and conferences (RSS, IROS, ICRA, ICCV, CVPR, ECCV). All consortium
partners have a track record in publishing in these venues.
The scientific achievements will impact on education and training through the partner
institutions' research led postgraduate degree programmes and their continuous professional
education offerings. It will also contribute to training highly skilled researchers and engineers
for the European economy. The outreach element of IPALM will contribute to attracting new
generations to careers in science and engineering. PDRAs at the partner institutions will
participate in Postgraduate Researcher Development Programmes, which support early-
career researchers in the development of research skills to enhance their employability
through career and personal development. PDRAs can for a teaching assistant position,
which will allow them to contribute to a delivery of a relevant course and give them
experience of various aspects of teaching."
18,1A61D1AA-14DD-4D78-857E-C7EAB2CAF918,PLEAD: Provenance-driven and Legally-grounded Explanations for Automated Decisions,"Algorithms and Artificial Intelligence play a key role nowadays in many technological systems that control or affect various aspects of our lives. They optimise our driving routes every day according to traffic conditions; they decide whether our mortgage applications get approved; they even recommend us potential life partners. They work silently behind the scene without much of our notice, until they do not. Few of us would probably think much about it when our credit card application is approved in two seconds. Only when it is rejected, do we start to question the decision. Most of the time, the answers we get are not satisfactory, if we get any at all. The spread of such opaque automated decision-making in daily life has been driving the public demand for algorithmic accountability - the obligation to explain and justify automated decisions. The main concern is that it is not right for those algorithms, effectively black boxes, to take in our data and to make decisions affecting us in ways we do not understand. For this reason, the General Data Protection Regulation requires that we, as data subjects, be provided with &quot;meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing.&quot; Likewise, consumers should be treated fairly when receiving financial services as per financial services regulations and algorithms should be free of discrimination as per data protection, equality and human rights laws. However, as laws and regulations do not prescribe how to meet such requirements, businesses are left with having to interpret those themselves, employing a variety of means, including reports, interactive websites, or even dedicated call centres, to provide explanations to their customers.

Against this background, provenance, and specifically its standard PROV, describes how a piece of information or data was created and what influenced its production. Within recorded provenance trails, we can retrace automated decisions to provide answers to some questions, such as what data were used to support a decision, who or what organisation was responsible for the data, who else might have been impacted. While provenance information is structurally simple, provenance captured from automated systems, however, tends to be overwhelming for human consumption. In addition, simply making provenance available to a person does not necessarily constitute an explanation. It would need to be summarised and its essence extracted to be able to construct an explanation addressing a specific regulatory purpose. How we do this is unknown today.

PLEAD brings together an interdisciplinary team of technologists, legal experts, commercial companies and public organisations to investigate how provenance can help explain the logic that underlies automated decision-making to the benefit of data subjects as well as help data controllers to demonstrate compliance with the law. In particular, we will identify various types of meaningful explanations for algorithmic decisions in relation to their purposes, categorise them against the legal requirements applicable to UK businesses relating to data protection, discrimination and financial services. Building on those, we will conceive explanation-generating algorithms that process, summarise and abstract provenance logged by automated decision-making pipelines. An Explanation Assistant tool will be created for data controllers to provision their applications with provenance-based explanations capabilities. Throughout the project, we will engage with partners, data subjects, data controllers, and regulators via interviews and user studies to ensure the explanations are fit for purpose and meaningful. As a result, explanations that are provenance-driven and legally-grounded will allow data subjects to place their trust in automated decisions, and will allow data controllers to ensure compliance with legal requirements placed on their organisations.",,
19,D8395A4B-EA3B-4091-8E4F-F407E6903FA6,"SPRITE+: The Security, Privacy, Identity, and Trust Engagement NetworkPlus","SPRITE+ is a NetworkPlus that will deliver a step change in engagement between people involved in research, practice, and policy relevant to trust, identity, privacy, and security (TIPS) with a focus on digital contexts. SPRITE+ will deliver a coherent, coordinated, multi-disciplinary approach, with strong stakeholder relationships at the centre. Collectively, we will identify and address key research challenges. 

Our activities will be centred around 'Challenge Themes', which will be broad, future-focused, and important to a wide range of stakeholders, where issues of security, privacy, identity, and trust are all relevant, and where an interdisciplinary approach is essential to fully addressing the Challenge. Examples might be Responsible innovation; Automation, autonomy, acceptability; Usable Security; 'Super-connectivity'; Risk, resilience, and recovery; Digital Identities.

Over the lifetime of SPRITE+, Working Groups will explore each Theme, producing comprehensive, cross-disciplinary understanding of key themes and making recommendations for future research priorities. Members will have the opportunity to bid to our &pound;400K research fund via sandpits at which they will co-create proposals with users, e.g., for events, feasibility studies, and sprint reviews. 

SPRITE+ is led by a Management Team (the PI, 4 co-Is), working closely with Project Partners from across industry, government, third sector and academia. A cadre of Expert Fellows will complement the Management Team's expertise and will help SPRITE+ develop a multidisciplinary approach to realising its vision. Fellows will provide intellectual leadership, take a leading role in Working Groups, and help bridge the gaps between diverse cognate groups and networks. A Strategic Advisory Board will review and develop SPRITE+'s performance. Membership will be open to all with an interest in research on security, privacy, identity, and trust. Members will receive a newsletter, access to online resources, and opportunities to attend events and bid for funds. 

The outcomes of our activities will be (a) a vibrant collaborative community, with strong collaborative relationships and increased industry investment in new research; (b) an expanded academic TIPS community, that includes researchers from humanities, behavioural and social sciences, and from other areas of 'security science'; (c) a community of Early Career Researchers who understand users and have the skills and knowledge to deliver high quality impactful research in their future careers; (d) mutual support and understanding between cognate groups and networks; and e) a set of roadmaps that shape future research investment priorities.",,"Research on issues of security, privacy, identity, and trust have the potential for broad-ranging impact, with benefits to academia, companies, the critical national infrastructure, security and law enforcement professionals, government, civil society organisations, and citizens. The enormous potential economic and social benefits of technological advances will only be realised if security vulnerabilities are well-understood and addressed, and if public confidence and trust in new technologies is built and maintained. SPRITE+ will explore these challenges, synthesise existing knowledge and stimulate new approaches to addressing gaps in our understanding, delivery and engagement. Collaborative relationships that address end user concerns are the core of SPRITE+ activities, and we will include stakeholders in shaping our activities from the outset. 

In addition to academic impact, SPRITE+ activities will seek economic and societal impact via the following mechanisms: 

(a) Deep understanding of user concerns through close collaborative working with Project Partners and other users to establish their key concerns and challenges relating to security, privacy, identity, and trust. 

(b) Research focused on end users. Our non-academic stakeholders will advise on priority research gaps. Feasibility studies and short research projects funded from the SPRITE+ Research Fund will be co-created with end users via sandpits and the Dragons' Den selection panel will draw on diverse viewpoints. Project Partners will also play a crucial role in evaluating the outcomes of funded research and, where appropriate, will fund follow-on research. We believe these steps will maximise the likelihood of research take-up.

(c) Prioritised research translation and communication. We will develop a website (SPRITEHub) that will host on ramping training materials, research outputs (including reports of events), and a comprehensive set of links to cognate groups and organisations. We will provide training for ECRs to help them translate and apply their research to make it usable by stakeholders, and create opportunities for researchers at all levels to talk with Project Partners at 'round table' meetings.

(d) Public Engagement. We will encourage involvement in public science events (e.g., Festivals and university open days), media engagement (e.g., via The Conversation), and public debates (e.g., evidence to Parliamentary committees). 

(e) Outreach to Enterprise and SMEs. We will engage businesses beyond Project Partners through an outreach programme."
20,3600F5C2-92BA-4D5E-A22F-DB815F229866,A Platform for Responsive Conversational Agents to Enhance Engagement and Disclosure (PRoCEED),"The way in which individuals interact with technology is rapidly evolving, as users increasingly expect fast, reliable and accurate information. In order to deliver systems capable of meeting these expectation both businesses and government departments alike are turning to conversational agents (or chatbots). These conversational agents are capable of interacting and engaging with users, answering user queries and even providing advice and guidance as required. This research considers how this technology can be optimised to provide a more effective method of communication, while also focusing on the implicit trust that a user has with a conversational agent. 

As part of this research we will investigate the nature of sensitive information and how the context of the information can play a role in its perceived sensitivity. This will be achieved using a range of experiments to better understand the public's perceptions of personal information, and how those perceptions relate to the classification of the information. 

In order to fully understand the use of conversational agents it is essential to properly understand the nature of personal, sensitive information and also their perceived trustworthiness. We will examine how different facets of a conversational agent's humanity, personality and appearance can be used to affect an individual's perceptions and trust in that agent. 

We will focus on the use of conversational agents across three key sectors: healthcare, defence and security and technology. These three areas have been selected as they are significant users of conversational agents and all deal with potentially sensitive and personal information, as well as being areas of significant public spending. Our research will understand how these interactions between humans and computers can be optimised to deliver a bespoke conversational agent tailored to meet the expectations and needs of the individual. This in turn will increase the trust and confidence in these digital services.",,"The proposed research will generate outputs that bring together trust, identity, privacy and security with human-computer interaction. Ultimately this research will aim to deliver a new way of measuring how individuals interact with conversational agents, which in turn will provide greater insight and understanding into their operation. This work has the potential to drive change across a range of user communities, providing a different way of thinking about interaction with conversational agents. This will result in a number of key changes: 
1. Provide a deeper understanding of the nature of sensitive information and the affect that context has on the sensitivity of this information. 
2. Bring together social scientists and computer scientists to consider this as a broader multi-disciplinary research problem and increase understanding of how social science methods can drive forward the usability and effectiveness of current technologies 
3. Define the situational nature of information disclosure to illustrate how the context of a discussion can inform the sensitivity of the data. 
 
This project will aim to deliver impact into four main beneficiary groups: end-users, owners of conversational agents, designers and developers of conversational agents and academia. 
 
End-users: This is a significant societal impact as there is an increasing reliance on technology and a push towards digital services. This can often be alienating or difficult for a wide selection of the population and as such this work will focus on increasing trust, usability and acceptance of conversational agents. This will be achieved by developing a platform capable of dynamically identifying the conversational agent that is best suited to a specific user. This in effect will deliver a bespoke experience for each user, to ensure that they feel secure in giving honest and complete disclosures. For example, when considering our work with healthcare it is clear that honesty is imperative. Prof. Helen Dawes will provide excellent access to this community to deliver vital impact into the field. 
 
Owners of conversational agents: The shift towards conversational agents allows organisations (commercial or government) to engage with more users than ever before. Our research will benefit a range of stakeholders in this group by defining methods to increase the trust and acceptance of these new technologies as well as improving the quality and depth of interactions between user and conversational agent. For example, we will work with our project partners such as Crest (see letter of support) and Prison Voicemail (see letter of support) to engage with a range of stakeholders. 
 
Designers and developers of conversational agents: This work is intended to help drive forwards the current generation of conversational agents. At present the accepted way of deploying these agents is a 'one size fits all' approach. Our work will deliver significant impact to this community by creating a platform capable of adapting the personality of the conversational agent to maximise engagement and disclosure. This impact will be achieved using the team members links to the technology community and also through our project partners in the field Velmai Ltd (see letter of support). We aim to enable organisations, across a range of sectors in the digital economy, to better engage with their clients/customers, as well as substantially increasing user experience with agents.

Academia: Our inter-university consortium will support the training of highly skilled researchers and facilitate the generation and transfer of new methods of research synthesis. We will engage with our project partners Crest who have strong links throughout the academic community to ensure impact is delivered to a wide range of potential users."
21,A1E1F676-AF77-4350-9BD4-26EEB987E4B1,A Platform for Responsive Conversational Agents to Enhance Engagement and Disclosure (PRoCEED),"The way in which individuals interact with technology is rapidly evolving, as users increasingly expect fast, reliable and accurate information. In order to deliver systems capable of meeting these expectation both businesses and government departments alike are turning to conversational agents (or chatbots). These conversational agents are capable of interacting and engaging with users, answering user queries and even providing advice and guidance as required. This research considers how this technology can be optimised to provide a more effective method of communication, while also focusing on the implicit trust that a user has with a conversational agent. 

As part of this research we will investigate the nature of sensitive information and how the context of the information can play a role in its perceived sensitivity. This will be achieved using a range of experiments to better understand the public's perceptions of personal information, and how those perceptions relate to the classification of the information. 

In order to fully understand the use of conversational agents it is essential to properly understand the nature of personal, sensitive information and also their perceived trustworthiness. We will examine how different facets of a conversational agent's humanity, personality and appearance can be used to affect an individual's perceptions and trust in that agent. 

We will focus on the use of conversational agents across three key sectors: healthcare, defence and security and technology. These three areas have been selected as they are significant users of conversational agents and all deal with potentially sensitive and personal information, as well as being areas of significant public spending. Our research will understand how these interactions between humans and computers can be optimised to deliver a bespoke conversational agent tailored to meet the expectations and needs of the individual. This in turn will increase the trust and confidence in these digital services.",,
22,4A54086F-812F-476D-B398-49FFB58DBCBE,A Platform for Responsive Conversational Agents to Enhance Engagement and Disclosure (PRoCEED),"The way in which individuals interact with technology is rapidly evolving, as users increasingly expect fast, reliable and accurate information. In order to deliver systems capable of meeting these expectation both businesses and government departments alike are turning to conversational agents (or chatbots). These conversational agents are capable of interacting and engaging with users, answering user queries and even providing advice and guidance as required. This research considers how this technology can be optimised to provide a more effective method of communication, while also focusing on the implicit trust that a user has with a conversational agent. 

As part of this research we will investigate the nature of sensitive information and how the context of the information can play a role in its perceived sensitivity. This will be achieved using a range of experiments to better understand the public's perceptions of personal information, and how those perceptions relate to the classification of the information. 

In order to fully understand the use of conversational agents it is essential to properly understand the nature of personal, sensitive information and also their perceived trustworthiness. We will examine how different facets of a conversational agent's humanity, personality and appearance can be used to affect an individual's perceptions and trust in that agent. 

We will focus on the use of conversational agents across three key sectors: healthcare, defence and security and technology. These three areas have been selected as they are significant users of conversational agents and all deal with potentially sensitive and personal information, as well as being areas of significant public spending. Our research will understand how these interactions between humans and computers can be optimised to deliver a bespoke conversational agent tailored to meet the expectations and needs of the individual. This in turn will increase the trust and confidence in these digital services.",,
23,FEF5A07F-BD6E-48E4-8413-E7D9333141CB,A Platform for Responsive Conversational Agents to Enhance Engagement and Disclosure (PRoCEED),"The way in which individuals interact with technology is rapidly evolving, as users increasingly expect fast, reliable and accurate information. In order to deliver systems capable of meeting these expectation both businesses and government departments alike are turning to conversational agents (or chatbots). These conversational agents are capable of interacting and engaging with users, answering user queries and even providing advice and guidance as required. This research considers how this technology can be optimised to provide a more effective method of communication, while also focusing on the implicit trust that a user has with a conversational agent. 

As part of this research we will investigate the nature of sensitive information and how the context of the information can play a role in its perceived sensitivity. This will be achieved using a range of experiments to better understand the public's perceptions of personal information, and how those perceptions relate to the classification of the information. 

In order to fully understand the use of conversational agents it is essential to properly understand the nature of personal, sensitive information and also their perceived trustworthiness. We will examine how different facets of a conversational agent's humanity, personality and appearance can be used to affect an individual's perceptions and trust in that agent. 

We will focus on the use of conversational agents across three key sectors: healthcare, defence and security and technology. These three areas have been selected as they are significant users of conversational agents and all deal with potentially sensitive and personal information, as well as being areas of significant public spending. Our research will understand how these interactions between humans and computers can be optimised to deliver a bespoke conversational agent tailored to meet the expectations and needs of the individual. This in turn will increase the trust and confidence in these digital services.",,
24,2C19C3E6-D0E2-4EBA-B500-666255D4F487,Environment and Listener Optimised Speech Processing for Hearing Enhancement in Real Situations (ELO-SPHERES),"Although modern hearing aids offer the potential to exploit advanced signal processing techniques, the experience and capabilities of hearing impaired listeners are still unsatisfactory in many everyday listening situations. In part this is because hearing aids reduce or remove subtle differences in the signals received at the two ears. The normally-hearing auditory system uses such differences to determine the location of sound sources in the environment, separate wanted from unwanted sounds and allow attention to be focused on a particular talker in a noisy, multi-talker environment.

True binaural hearing aids - in which the sound processing that takes place in the left and right ears is coordinated rather than independent - are just becoming available. However practitioners' knowledge of how best to match their potential to the requirements of impaired listeners and listening situations is still very limited. One significant problem is that typical existing listening tests do not reflect the complexity of real-world listening situations, in which there may be many sound sources which may move around and listeners move their heads and also use visual information. A second important issue is that HI listeners vary widely in their underlying spatial hearing abilities. 

This project aims to understand better the problems of hearing impaired listeners in noisy, multiple-talker conversations, particularly with regard to (i) their abilities to attend to and recognise speech coming from different directions while listening through binaural aids, (ii) their use of audio-visual cues. We will develop new techniques for coordinated processing of the signals arriving at the different ears that will allow identification of the locations and characteristics of different sound sources in complex environments and tailor the information presented to match the individual listener's pattern of hearing loss. We will build virtual reality simulations of complex listening environments and develop audio-visual tests to assess the abilities of listeners. We will investigate how the abilities of hearing-impaired listeners vary with their degree of impairment and the complexity of the environment. 

This research project is a timely and focussed addition to knowledge and techniques to realise the potential of binaural hearing aids. Its outcomes will provide solutions to some key problems faced by hearing aid users in noisy, multiple-talker situations.",,"The key components of this project will have significant impact on the understanding of hearing impairment and on the development of more effective binaural hearing aids. Work on understanding the spatial characteristics of a listening environment will have value in modelling the influence room acoustics has on hearing and in building better VR simulations of those environments. Work on multi-modal hearing assessment using VR will lead to a better understanding of the scientific characterisation of hearing impairment and its consequences in difficult listening situations. Work on optimising spatial audio processing techniques to render auditory scenes matched to the listener will lead to scientific advances in modelling attention and speech intelligibility and to technological advances in spatial signal processing able to cope with moving sources and listeners. The outcomes of the project will encourage new research into spatial hearing, the clinical assessment of hearing impairment, the fitting of binaural hearing aids and the training of HI listeners to make best use of their aids. The VR simulation of hearing impairment will have significant impact in the public awareness of hearing disability."
0,ECB63580-F7BB-4599-B432-FF2985776F94,Measuring outcomes from a peer-led social communication skills intervention for adults following brain injury,"Acquired brain injury is a principal cause of life-long disability worldwide. Chronic cognitive and communication problems can follow injury, adversely impacting previous relationships and the ability to build new ones. Reduced social acceptance and social isolation are commonly reported problems. Intervention for social communication skills is recommended as a practice standard in programmes of rehabilitation, but the evidence of gain from more traditional models of intervention is mixed. In particular, automatic transfer of skills learned in the training setting to everyday life cannot be assumed. Community re-integration entails the ability to build relationships with other people independently of family members and professionals. In order to play a valued social role, the person needs to be able to engage as an equal partner in conversation, and have an ability to speak on their own behalf.

My PhD research developed and investigated the efficacy of a new approach, training a peer with a severe ABI to facilitate communication in an expert discussion group. Outcomes were compared to a control group (a traditional staff-led social activity group for individuals with severe ABI). Existing outcome measurement tools are designed to evaluate change in paired conversations rather than groups, so a new digital measurement tool (the INT) was developed and tested, using social network methodologies, to quantitatively measure change in group social participation over time.

Results showed that a trained peer can successfully facilitate group interaction without direct support from a neurotypical communication partner. The intervention group showed an improved ability to socially connect and participate collaboratively over time. Conversely, there was no change over time in the staff-led group. Overall, these findings show initial evidence of benefit in independent relationship-building within complex encounters and in new social networks with multiple communication partners, consistent with the communication demands of everyday life.

Developing findings from this PhD research has the potential to inform clinical practice in the following ways. It meets current recommendations for new interventions that address communication needs in real-world environments and a fully powered study is now required to increase confidence in these initial findings. A key objective for this fellowship is to produce a grant application for a multi-centre trial. Measurement of change in group social interaction is a new field of investigation and the INT meets current recommendations for new tools to measure social participation outcomes. My first mentor, Professor Joseph Devlin, as Vice Dean of Innovation and Enterprise is well placed to advise me on networking opportunities and the formulation of a contact plan to develop the INT. The measure also requires validation to demonstrate reliability and responsiveness with increased volumes of data, and to test its application to wider clinical contexts and groups. My secondary mentor, Dr Joshua Stott, brings experience of managing grant funded projects in dementia and mental health conditions",,
1,4DBEFB05-9F9A-433D-905F-02E0AFC0DA70,Machine Learning for Space Physics,"Machine learning is a very hot topic in computer science these days. As a world we are generating ever greater volumes of
data, and we need to find effective ways to gather and analyse that data, often by searching for regular patterns in data
sets. The human eye is very good at picking out patterns either from images or from simple time series graphs. However,
the human eye comes with its own biases: if you are trying to pick out blips in a single line trace on a screen your selection
may not always be the same, but may depend on what has come before. Reproducibility is a huge issue here and one
which impacts any kind of data science: if we are to do an experiment, or pick out interesting features from data, we want to
make sure we get the same result every time given the same initial input. Furthermore, as our input data streams get
bigger and bigger, it is extremely time consuming (and a bit boring!) to look through all the data by eye to pick out the kind
of features that we want. This is where the extremely powerful tool known as machine learning can help. In this work we
propose to use machine learning to pick out particular signatures from large catagloues of Space Physics data - but the
computer analysis methods that we will develop will be applicable across multiple disciplines.

The Space Physics problem we are interested in is called magnetic reconnection: it is a very energetic process which can
take place when two oppositely directed magnetic field lines meet, come together, and break. Right before reconnection
happens the field lines are holding lots of energy, but as soon as they break this energy can be released into multiple forms
including kinetic energy and thermal energy (heating). The field lines change shape after they break and these newly shaped
field lines can &quot;ping&quot; away from the site of reconnection, much like an elastic band that has been snapped. The
field lines also carry with them charged particles, and these particles can heat up or change their flow direction as a result
of the transfer of energy.

In the solar system everything happens on a giant scale, and magnetic reconnection can involve the magnetic field lines
and plasma of the Sun and of several magnetised planets, including, but not limited to Mercury, Earth, Jupiter and Saturn.
Spacecraft flying through the solar system have instruments which can measure magnetic fields and plasmas, and thus
can sample any changes associated with reconnection.

The changes in the shape and orientation of magnetic fields and in the temperature and flow characteristics of charged
particles can be observed by spacecraft. When scientists examine spacecraft data to search for evidence of this
reconnection process, they know what they are looking for in the field and plasma data. There is a huge amount of
spacecraft data: years and years' worth, with measurements taken several times a second. Reconnection can happen
every few minutes at some planets. It would be impossible for a human being to look through all the data and pick out
every time reconnection happened in our enormous catalogue.

The purpose of this research is to teach the computer what reconnection signatures look like to a human eye, and to train
the computer to pick these signatures out itself. This technique is called machine learning, and it has many advantages,
because computers can be taught to work more quickly than humans, to give the same answer every time, and to not show
biases.

The ultimate goal at the end of this project is to have trained the computer to select reconnection signatures, and to be able
to roll out this technique on multiple data sets from the solar system. This will be particularly useful for scientists who want
to conduct large studies of the behaviour of magnetic fields and plasma across the solar system, under different conditions
and over multiple years.",,"Several different groups will benefit from this research. As mentioned in the Academic Beneficiaries section, the machine
learning algorithms that will be developed will be of use to those interested in studying reconnection and those who wish to
conduct statistical studies of field and plasma fluctuations in planetary magnetospheres and the solar wind. The work may
feed directly into improved knowledge of Space Weather.

Furthermore, the economic and social beneficiaries of this research will primarily include industrial and governmental
groups linked to space technology and space weather. Such groups include:
UK Space - trade association of the UK space industry http://www.ukspace.org/what-we-do/
The Cabinet office and National Security and Intelligence, who maintain the National Risk Register, of which Space
Weather is a part: https://www.gov.uk/government/collections/national-risk-register-of-civil-emergencies

Modern society is increasingly reliant on space-based technologies for their everyday lives and reconnection events which
transfer huge amounts of energy have consequences on modern technological infrastructure in the Space and Energy
sectors. These include damage to satellites, especially from surface charging, and disruptions to satellite communications
and navigation due to ionospheric absorption and scintillation, to electricity supply due to electrical currents induced in the
ground from ionospheric currents, and to oil and mineral prospecting due to geomagnetic field fluctuations. Such so-called
'space weather' hazards are now considered to be sufficiently important to have been included in the latest UK Government
National Risk Register. Providing information directly relevant to predictive space weather modelling efforts is the first step
towards providing advance warning for low-frequency, but high-consequence events such as those identified by the top UK
and US Science Advisors Holdren and Beddington who warn &quot;The potential total cost of an extreme Space Weather event
is estimated as $2 Trillion in year 1 in the U.S. alone, with a 4-10 year recovery period&quot;.

The UK Meteorological Office is responsible for providing space weather predictive capability and any large statistical
studies, enabled by the use of machine learning algorithms such as those we propose, will feed into this predictive
capability. More generally, the effects of space weather can be felt in all activities that use space-related assets. For
example, during the October-November 2003 geomagnetic storms the effects of space weather were included in a US
National Weather Service report for the first time.

The research and professional skills that the PDRA will develop during this project will be in
computational programming, processing large datasets, and scientific reasoning, with written skills in the form of reports
and publications. All are applicable to many employment sectors."
2,1D141C15-4089-4083-B363-6A9D71FAF3A7,Environment and Listener Optimised Speech Processing for Hearing Enhancement in Real Situations (ELO-SPHERES),"Although modern hearing aids offer the potential to exploit advanced signal processing techniques, the experience and capabilities of hearing impaired listeners are still unsatisfactory in many everyday listening situations. In part this is because hearing aids reduce or remove subtle differences in the signals received at the two ears. The normally-hearing auditory system uses such differences to determine the location of sound sources in the environment, separate wanted from unwanted sounds and allow attention to be focused on a particular talker in a noisy, multi-talker environment.

True binaural hearing aids - in which the sound processing that takes place in the left and right ears is coordinated rather than independent - are just becoming available. However practitioners' knowledge of how best to match their potential to the requirements of impaired listeners and listening situations is still very limited. One significant problem is that typical existing listening tests do not reflect the complexity of real-world listening situations, in which there may be many sound sources which may move around and listeners move their heads and also use visual information. A second important issue is that HI listeners vary widely in their underlying spatial hearing abilities. 

This project aims to understand better the problems of hearing impaired listeners in noisy, multiple-talker conversations, particularly with regard to (i) their abilities to attend to and recognise speech coming from different directions while listening through binaural aids, (ii) their use of audio-visual cues. We will develop new techniques for coordinated processing of the signals arriving at the different ears that will allow identification of the locations and characteristics of different sound sources in complex environments and tailor the information presented to match the individual listener's pattern of hearing loss. We will build virtual reality simulations of complex listening environments and develop audio-visual tests to assess the abilities of listeners. We will investigate how the abilities of hearing-impaired listeners vary with their degree of impairment and the complexity of the environment. 

This research project is a timely and focussed addition to knowledge and techniques to realise the potential of binaural hearing aids. Its outcomes will provide solutions to some key problems faced by hearing aid users in noisy, multiple-talker situations.",,
3,FE392217-6E6B-4AB8-9BF5-2690AF58561C,Explainable AI for UK agricultural land use decision-making,"Agricultural land use dynamics and their associated driving factors represent highly complex systems of flows that are subject to non-linearities, sensitivities, and uncertainties across spatial and temporal scales. They are therefore challenging to represent using traditional statistical modelling approaches. Existing process-based modelling has enabled advances in understanding of individual biophysical processes underpinning agricultural land use systems (e.g. crop, livestock and biogeochemical models). However, these tend to focus on individual processes in detail or link a limited number of processes at large scales, thereby mostly ignoring the complex interdependencies between the multiple interacting biophysical and socio-economic components of land use systems. Artificial intelligence (AI) techniques offer great potential to complement such modelling approaches by mining the deep knowledge (e.g. farming patterns and behaviours) encapsulated in 'big' data from ground-based sensors (such as frequently used for precision farming) and Earth Observation satellites. This will deliver enhanced insight on the past and current state and spatio-temporal dynamics of agricultural land use system flows and how they can be influenced by decisions on agricultural policies and related farm management practices. 

Our proposal aims to develop a novel explainable AI framework that is transparent, data-driven and spatially-explicit by using probabilistic inference and explicit &quot;if-then&quot; rules. We will demonstrate proof-of-concept for two pilot regions of the UK (Oxfordshire and Lincolnshire), and the framework will be set up in a way that can be readily expanded to the whole UK. Specifically, we will draw on time-series of agricultural land use and production datasets (in-kind support from industry project partner SOYL) to identify the key socio-economic and environmental driving factors that have led to historic agricultural land use changes in the pilot regions. We will then establish explainable AI-rules for the characterisation of these agricultural land use changes and refine them within the framework through machine learning and parameter optimisation.

We will demonstrate and test the potential of the explainable AI framework for providing a new and robust method for predicting changing patterns of agricultural land use in the two pilot regions. This will include testing the ability of the AI framework for improving understanding of past and present agricultural land use dynamics across multiple temporal and spatial scales from 'big' data. It will also assess the potential for continually updating the predictions of land use dynamics in real-time using data from sensors. This could provide early warning when certain driving conditions are triggered or used to repeatedly refine short-term projections of land use change and their estimates of uncertainty.",,"We will engage with four major types of beneficiaries to maximise the societal impact of this project:

1. UK Government, Devolved Administrations and Policy-makers: 
Our project will provide decision-makers with an innovative and integrated knowledge base supporting agricultural land use decision-making. We will engage with policy-makers including Defra, Natural England (a project partner), the Environment Agency, and the devolved administrations through consultations on the development of the explainable AI system to ensure it meets a broad set of decision-making needs. We will also demonstrate the final AI framework at a 1-day policy workshop that explores the potential of AI approaches for supporting landscape decision-making alongside other modelling and information products, including defining future needs beyond the remit of this proposal. This will include exploring how the framework can help inform the objective in the 25 Year Environment Plan to deliver a clear evidence base to promote precision agriculture and land management.

2. Farming Industries and Agri-business: 
Through our strategic partnership with SOYL, the leading precision crop production service provider in the UK (see letter of support), we will have access to a unique 'big' dataset of in-situ and EO-based agricultural land use statistics to establish the explainable AI framework. The methodological innovation in the project will be co-created with SOYL (and our other project partner, Natural England) through four meetings throughout the one-year project lifetime. This will ensure that our outcomes, disseminated through a one-week training event at SOYL, are fit-for-purpose in informing the decision-making needs of agri-businesses in order to catalyse business change and innovation.

3. AI Industries and the Economy: 
The AI innovations developed in this project will support the socio-economic development of the UK by promoting automation as part of the Industry 4.0 revolution. The AI framework is fully transformative and scalable, such that it can be elaborated or enhanced as required to support decision-making within a growing UK economy that aims to balance economic, environmental and societal aspects of the UK agricultural sector. Continued technological innovations in the agricultural sector are expected through precision farming and Earth Observation, creating an increasing demand for AI and big data-driven models.

4. Land owners and Trusts: 
The knowledge and explainable AI rules derived in the system will support the decision-making of land owners and national trusts, by reducing environmental hazards (e.g. pesticides), enhancing ecosystem services and natural capital, and promoting smart and profitable agriculture, as well as building an inclusive society that allows people and nature to thrive. We will engage with relevant land owners and trusts through Natural England (see letter of support) via dissemination meetings and communications (e.g. media).
The 'Pathways to Impact' document details the activities we will undertake to deliver these impacts."
4,6F5BE570-41D5-4324-A72E-05C77BBD7726,PLEAD: Provenance-driven and Legally-grounded Explanations for Automated Decisions,"Algorithms and Artificial Intelligence play a key role nowadays in many technological systems that control or affect various aspects of our lives. They optimise our driving routes every day according to traffic conditions; they decide whether our mortgage applications get approved; they even recommend us potential life partners. They work silently behind the scene without much of our notice, until they do not. Few of us would probably think much about it when our credit card application is approved in two seconds. Only when it is rejected, do we start to question the decision. Most of the time, the answers we get are not satisfactory, if we get any at all. The spread of such opaque automated decision-making in daily life has been driving the public demand for algorithmic accountability - the obligation to explain and justify automated decisions. The main concern is that it is not right for those algorithms, effectively black boxes, to take in our data and to make decisions affecting us in ways we do not understand. For this reason, the General Data Protection Regulation requires that we, as data subjects, be provided with &quot;meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing.&quot; Likewise, consumers should be treated fairly when receiving financial services as per financial services regulations and algorithms should be free of discrimination as per data protection, equality and human rights laws. However, as laws and regulations do not prescribe how to meet such requirements, businesses are left with having to interpret those themselves, employing a variety of means, including reports, interactive websites, or even dedicated call centres, to provide explanations to their customers.

Against this background, provenance, and specifically its standard PROV, describes how a piece of information or data was created and what influenced its production. Within recorded provenance trails, we can retrace automated decisions to provide answers to some questions, such as what data were used to support a decision, who or what organisation was responsible for the data, who else might have been impacted. While provenance information is structurally simple, provenance captured from automated systems, however, tends to be overwhelming for human consumption. In addition, simply making provenance available to a person does not necessarily constitute an explanation. It would need to be summarised and its essence extracted to be able to construct an explanation addressing a specific regulatory purpose. How we do this is unknown today.

PLEAD brings together an interdisciplinary team of technologists, legal experts, commercial companies and public organisations to investigate how provenance can help explain the logic that underlies automated decision-making to the benefit of data subjects as well as help data controllers to demonstrate compliance with the law. In particular, we will identify various types of meaningful explanations for algorithmic decisions in relation to their purposes, categorise them against the legal requirements applicable to UK businesses relating to data protection, discrimination and financial services. Building on those, we will conceive explanation-generating algorithms that process, summarise and abstract provenance logged by automated decision-making pipelines. An Explanation Assistant tool will be created for data controllers to provision their applications with provenance-based explanations capabilities. Throughout the project, we will engage with partners, data subjects, data controllers, and regulators via interviews and user studies to ensure the explanations are fit for purpose and meaningful. As a result, explanations that are provenance-driven and legally-grounded will allow data subjects to place their trust in automated decisions, and will allow data controllers to ensure compliance with legal requirements placed on their organisations.",,"Socially-sensitive decisions are made on a daily basis for a wide range of purposes, including credit scoring, insurance and hiring. Increasingly, decision-makers are relying on more advanced automated decisions based on an ever-expanding assortment of personal data. This is because the move towards automated decision-making has various benefits, e.g. the greater speed in which decisions are made or the enrichment of the decision-making process. However, automated decisions are likely to have significant impacts for the individuals who are subject to them (e.g. being refused a mortgage). PLEAD therefore seeks to produce a major contribution in the field of data governance, through the development of techniques and tools exploiting provenance logs, which will allow different types of stakeholders to either produce or receive fit-for-purpose and meaningful explanations each time a decision is automated. As a result, the project will generate a variety of impacts, especially economic and societal impacts. 

At a high level, over the long term, the societal impact will be twofold. First, through offering the means to reach a greater degree of transparency and accountability when automated decisions are generated, PLEAD will contribute to improving the quality of life of individuals interacting within smart environments. For instance, when a loan or a mortgage is refused, individuals will have the means to understand the motives of the decision. The same will hold in other sectors such as health, when a decision to put a patient on a waiting list is taken, or justice, when an automated triage system is implemented. Second, PLEAD will also contribute to enhancing the quality of public services, by supporting decision-makers across sectors, including decision-makers operating within public bodies. 

The economic impact will benefit both small and medium enterprises and bigger organisations, offering them solutions at the cutting edge of research in order to build ethical and effective governance frameworks for the management of data. This will allow them to have a competitive advantage when investing in innovation and research and when involved in data sharing with partners. Legal compliance will be made easier and thereby cheaper. Customers will be more confident, as they will have the means to remain in control and exercise their right to be heard or invoke human intervention. 

At a more granular level, over the short to medium term, the involvement of the industrial partners Roke (law enforcement) and Experian (financial services), and the strategic partnership Southampton CONNECT (smart cities) means that the techniques developed by PLEAD will be tested across different sectors and will feed into the creation of novel services and/or products, benefiting a wide range of stakeholders. Law enforcement agents will be able to test cutting-edge data management tools, intended to enhance auditability and thereby transparency and accountability as well as reliability. Individuals with credit scores will be able to better understand the process of quantification of such scores and either contest or intervene to improve them. Moreover, organisations operating within urban environments concerned about growth and social responsibility such as the city of Southampton will be incentivised to contribute to the setting of an overarching data governance framework promoting transparency and accountability. Citizens will directly benefit from these evolutions through better information and more empowerment. 

Ultimately, these experiments will pave the way for the production of best practice, which will be shared with other sectors, and, disseminated to regulators, to inform the release of guidance on the governance of automated decision pipelines."
5,C8F39651-0073-43D4-85F6-2E6B904638CB,Human-computer collaborative learning in citizen science,"This project explores the potential for collaborative learning between humans and machines within the framework of environmental citizen science. The term `citizen science' encompasses public participation in science and scientific communication to the public. Although not new, citizen science has gained renewed attention because of the opportunities arising from citizens' access to digital technologies in terms of data collection and annotation. While the vast majority of citizen science projects are aimed at data gathering, we instead propose a transformational shift to a new citizen science in which the public and technology are regarded not just as sensors or data recorders, but as a collective and empowered human--artificial intelligence that can help each other in science learning.

We will focus on the task of species identification from images. Citizen science projects such as iSpot invite the public to submit photos of wildlife. These are identified to species level and verified before being contributed to science. We will explore artificial intelligence as a means to automatically identify species in images. While this can save human effort, we are concerned about impact this might have on nature lovers. The introduction of technology is often associated with concerns of de-skilling. For naturalists, the honing of species identification skills is a key motivator of the recording activity. Hence, designing technology that provides opportunities for learning for both citizens and machines is essential, as is co-creating the technology to ensure that it is not only user friendly but responds to their motivations. Our approach will involve citizens collaborating with AI to arrive at 
 a species identification. AI will narrow down the choices and inform the citizen about how to distinguish the options. The citizen in turn will through providing an identification help the machine in its learning. We will study this learning interplay with respect to collaborative species identification, but will also explore technologies that foster wider science learning, environmental consciousness and data literacy through better communication of complex citizen science data. For this we will develop technology for Natural Language Generation that can communicate complex data through language. 

Our proposed work programme seeks to bring about quantifiable benefits to (a) science, e.g., through the production of new knowledge and through monitoring key scientific processes at challenging temporal-spatial scales; (b) diverse stakeholders including the citizens themselves, e.g., through meaningful science learning for sustainability in formal and informal education contexts; and (c) wider society, e.g., through better societal understanding of current sustainability issues, leading to individual and societal action in support of the environment.",,"We hope to achieve the following impacts through this research:

1. On Primary Schools and Secondary Schools

Citizen science practice, and notably technology, has been minimally promoted within formal education. We will unfold how the proposed collaborative learning technologies can be applied to citizen science in formal education settings and quantify the benefits to students and teachers.
We wish to see more schools take up citizen science and outdoor learning as means to enthuse students about science and technology. We expect this to lead to better learning outcomes for schools, and a more involved and engaged student community.

2. On Universities

We will demonstrate how citizen science can enhance STEM teaching in universities, and encourage its uptake within our universities. Citizen science is increasingly being viewed as a complementary approach to traditional science learning and research and offers several benefits, such as opportunities for students to work with their local communities, engage with technology, and involve themselves in designing and testing tools. It also allows academics to integrate their research with their teaching, which makes for a more stimulating student experience.

3. On Students

Through taking part in citizen science, students will develop the most important Science, Technology, Engineering and Mathematics (STEM) skills, including intellectual curiosity, problem solving,creativity, statistics and data-driven decision making. We will in particular engage with students in primary and secondary schools. While secondary school students can learn science and data skills in greater depth, it is important to get primary school students enthusiastic about science and technology. We are particularly conscious about the take-up of STEM subjects by girls and are keen to reach out to younger students who still need to decide which subjects to retain or drop in secondary school.


4. On Society and the Environment

The 2030 Agenda for Sustainable Development Goals (SDGs) explicitly argues for the need to ``take urgent and significant action to reduce the degradation of natural habitats, halt the loss of biodiversity and, by 2020, protect and prevent the extinction of threatened species''.
Yet, the most recent State of Nature report concludes that the UK has lost significantly more nature over the long term than the global average and is among the world's most nature depleted countries.
In the decades that nature has been in decline, so too has our connection with it. Fewer than a quarter of British children regularly use their local patch of nature and many suffer from `Nature Deficit Disorder', impacting education and physical and emotional health. 
 Research that promotes science learning through increased interaction with nature thus has multiple benefits to society. It contributes to health and learning outcomes for individuals and the development of a scientific temperament and pro-environmental attitudes in society.

Or research promotes actionable citizen science, whereby individuals, schools and communities can maintain and repair their habitats in the context of pollinating species. Using our developed technologies, students and teachers will build the knowledge and skills required to collect data, enhance habitats and gain the confidence to become passionate environmental stewards. We will encourage students to, through creative campaigns, share their knowledge about insects, the scientific process and planting for pollinators with members of their local and online communities.

The school-based campaigns will also build the capacity to collect high-quality data about changing pollinator populations and the availability of high-quality habitats."
6,71120FF5-6A55-4BAB-9038-AC603028B359,BURG: Benchmarks for UndeRstanding Grasping,"Grasping rigid objects has been reasonably studied under a wide variety of settings. The common measure of success is a check of the robot to hold an object for a few seconds. This is not enough. To obtain a deeper understanding of object manipulation, we propose (1) a task-oriented part-based modelling of grasping and (2) BURG - our castle of setups, tools and metrics for community building around an objective benchmark protocol. 

The idea is to boost grasping research by focusing on complete tasks. This calls for attention on object parts since they are essential to know how and where the gripper can grasp given the manipulation constraints imposed by the task. Moreover, parts facilitate knowledge transfer to novel objects, across different sources (virtual/real data) and grippers, providing for a versatile and scalable system. The part-based approach naturally extends to deformable objects for which the recognition of relevant semantic parts, regardless of the object actual deformation, is essential to get a tractable manipulation problem. Finally, by focusing on parts we can deal easier with environmental constraints that are detected and used to facilitate grasping. 

Regarding benchmarking of manipulation, so far robotics suffered from incomparable grasping and manipulation work. Datasets cover only the object detection aspect. Object sets are difficult to get, not extendible, and neither scenes nor manipulation tasks are replicable. There are no common tools to solve the basic needs of setting up replicable scenes or reliably estimate object pose. 

Hence, with the BURG benchmark we propose to focus on community building through enabling and sharing tools for reproducible performance evaluation, including collecting data and feedback from different laboratories for studying manipulation across different robot embodiments. We will develop a set of repeatable scenarios spanning different levels of quantifiable complexity that involve the choice of the objects, tasks and environments. Examples include fully quantified settings with layers of objects, adding deformable objects and environmental constraints. The benchmark will include metrics defined to assess the performance of both low-level primitives (object pose, grasp point and type, collision-free motion) as well as manipulation tasks (stacking, aligning, assembling, packing, handover, folding) requiring ordering as well as common sense knowledge for semantic reasoning.",,N/A
7,F2CC930F-19D5-41E4-AAF7-1FFDFA94BCB0,"ActiveAI - active learning and selective attention for robust, transparent and efficient AI","We will bring together world leaders in insect biology and neuroscience with world leaders in biorobotic modelling and computational neuroscience to create a partnership that will be transformative in understanding active learning and selective attention in insects, robots and autonomous systems in artificial intelligence (AI). By considering how brains, behaviours and the environment interact during natural animal behaviour, we will develop new algorithms and methods for rapid, robust and efficient learning for autonomous robotics and AI for dynamic real world applications.

Recent advances in AI and notably in deep learning, have proven incredibly successful in creating solutions to specific complex problems (e.g. beating the best human players at Go, and driving cars through cities). But as we learn more about these approaches, their limitations are becoming more apparent. For instance, deep learning solutions typically need a great deal of computing power, extremely long training times and very large amounts of labeled training data which are simply not available for many tasks. While they are very good at solving specific tasks, they can be quite poor (and unpredictably so) at transferring this knowledge to other, closely related tasks. Finally, scientists and engineers are struggling to understand what their deep learning systems have learned and how well they have learned it. 
 
These limitations are particularly apparent when contrasted to the naturally evolved intelligence of insects. Insects certainly cannot play Go or drive cars, but they are incredibly good at doing what they have evolved to do. For instance, unlike any current AI system, ants learn how to forage effectively with limited computing power provided by their tiny brains and minimal exploration of their world. We argue this difference comes about because natural intelligence is a property of closed loop brain-body-environment interactions. Evolved innate behaviours in concert with specialised sensors and neural circuits extract and encode task-relevant information with maximal efficiency, aided by mechanisms of selective attention that focus learning on task-relevant features. This focus on behaving embodied agents is under-represented in present AI technology but offers solutions to the issues raised above, which can be realised by pursuing research in AI in its original definition: a description and emulation of biological learning and intelligence that both replicates animals' capabilities and sheds light on the biological basis of intelligence.

This endeavour entails studying the workings of the brain in behaving animals as it is crucial to know how neural activity interacts with, and is shaped by, environment, body and behaviour and the interplay with selective attention. These experiments are now possible by combining recent advances in neural recordings of flies and hoverflies which can identify neural markers of selective attention, in combination with virtual reality experiments for ants; techniques pioneered by the Australian team. In combination with verification of emerging hypotheses on large-scale neural models on-board robotic platforms in the real world, an approach pioneered by the UK team, this project represents a unique and timely opportunity to transform our understanding of learning in animals and through this, learning in robots and AI systems. 

We will create an interdisciplinary collaborative research environment with a &quot;virtuous cycle&quot; of experiments, analysis and computational and robotic modelling. New findings feed forward and back around this virtuous cycle, each discipline informing the others to yield a functional understanding of how active learning and selective attention enable small-brained insects to learn a complex world. Through this understanding, we will develop ActiveAI algorithms which are efficient in learning and final network configuration, robust to real-world conditions and learn rapidly.",,"We will combine expertise in insect neuroscience with biomimetic robotic control to gain a functional understanding of how active learning and selective attention underpin rapid and efficient visual learning. Through this, we will develop ActiveAI algorithms, reinforcement learning methods and artificial neural network (ANN) architectures for robotics and AI that learn rapidly, are computationally efficient, work with limited training data and robust to novel and changing scenarios.

Industrial impact
Our novel sensing, learning and processing algorithms offer impact in robotics/autonomous systems and AI generally. AI problems are currently resolved by increasing computational and training resources. We take a fundamentally different approach using insects as inspiration for efficient algorithms. Here we will develop smart movement patterns which combine with attentional mechanisms to aid information identification/extraction, reducing computational and training loads. We foresee two immediate problem domains in RAS: those where learning speed is highly constrained (e.g. disaster recovery robots, exploration, agri-robotics); and those where computational load and energy usage are limited (e.g. UAVs, agritech, space robotics). Longer term we foresee applications in general AI where a new class of highly efficient and thus scalable ANNs are required to realise grand challenges such as General Intelligence. 

We will ensure tight coupling to industrial needs using established industrial members of the Brains on Board advisory board, comprising Dyson, Parrot, NVidia and Google DeepMind as well as collaborators for robotic applications (Harper Adams University, GMV and RALSpace) and Sheffield Robotics contacts (e.g. Amazon, iniVation, Machine With Vision) as well as leveraging new opportunities through both UK and Australian Universities commercialisation operations (Macquarie University's Office of Commercialisation and Innovation Hub; Sussex Research Quality and Impact team, Sussex Innovation Centre; Sheffield Engineering Hub, Sheffield Partnerships and Knowledge Exchange team). Where possible, we will seek to commercialise knowledge through IP licensing, and university supported spin-outs. We already have experience doing so, in particular: optic flow commercialisation through ApisBrain (Marshall); and sensor commercialisation through Skyline Sensors (Mangan). In Sussex, support will be provided by the.

Impact on the team
PDRAs will receive cross-disciplinary training from the UK team in GPU computing, neural simulations, biorobotics and bio-inspired machine learning - very active and rapidly expanding areas and sought after skills in AI and robotics - as well as training from the Australian team in cutting edge neuroscientific methods (electrophysiology and pharmacology combined with virtual reality-enabled behavioural experiments). This will prepare them for careers across academia and industry. In addition, UK co-I Mangan, as an early career researcher, will benefit from support and advice from the senior investigators (UK + Aus), supporting his development as an independent researcher.

Advocacy + general public
We firmly believe in the benefit of ethically aware technology development through responsible innovation. We have already created an ethical code of conduct for the Brains on Board project and engaged with government consultations. We will extend this work and, by promoting and adhering to this philosophy, we will have impact on policy through advocacy and on the general public, through continuation of our extensive public engagement activities e.g. regular public lectures (Cafe Scientifique, Nerd Nite, U3A etc), media appearances (BBC, ABC radio, BBC Southeast) and large outreach events (e.g. Royal Society Science Exhibition 2010, British Science Festival 2017, Brighton Science Festivals, 2010-2018).

Academic Impact
We will impact AI, Robotics and neuroscience (see Academic Beneficiaries)."
8,4D8C8C23-E0C3-4240-A31F-45CED3DB1638,Learning to Communicate: Deep Learning based solutions for the Physical Layer of Machine Type Communications [LeanCom],"With the advent of the Internet of Things (IoT), machine type communications (MTC), cloud computing and many other applications, the wireless network will become far more complex, while at the same time far more essential than ever before. 
 Given the above exponential growth in both connectivity and complexity of the wireless systems and the unprecedented demands on latency, capacity, ultra-reliability and security, the network is becoming analytically intractable. Naturally, human-driven physical layer (PHY) design approaches rooted on mathematical models of communications systems and networks which drive today's network architectures are being surmounted by the sheer complexity of the emerging network paradigms. Hardware imperfections, that are inevitable with the employment of low-cost MTC sensors and transmitters, will drastically increase the volatility of the network, and theoretically driven solutions typically relying on generic and highly inaccurate models cannot address this as they are highly sub-optimal in practice. The above challenges necessitate new data-driven approaches to the design of communications systems, as opposed to traditional system-model driven designs that are becoming obsolete.

Towards the diverse communication paradigms of MTC of the future, there is an urgent need to address reliable and adaptive links detached from mathematical models, and instead based on data-driven approaches. This visionary project will address these fundamental challenges by developing new Neural Netowrk architectures tailored for wireless communications, and new transceiver architectures based on data-driven training. Our research will address the development of a) a communications specific DL framework, b) DL-inspired PHY solutions and, c) proof-of-concept verification of the proposed solutions.

LeanCom will be performed with Huawei, NEC Europe, Duke University, The Digital Catapult and CommNet and aspires to kick-start an innovative ecosystem for high-impact players among the infrastructure and service providers of ICT to develop and commercialize a new generation of learning-based networks. The implementation, experimentation and testing (within WP3) of the proposed solutions serves as a platform towards commercialisation of the results of LeanCom, aiming towards an impact of a foundational nature for the UK's digital economy.",,"The explosive growth of industrial control processes and the industrial IoT applications, provides unique opportunities for the creation of impact through MTC research. This project promotes a fundamental paradigm shift to communications system design, and therefore holds the potential for high impact research of a foundational nature for the UK's digital economy. Impact will be measured in the number of products and commercialisation activities, consultancies and patents filed, by any follow-up industry funded collaborations, and through the success of public engagement activities. 
To foster the economic competitiveness of the UK, this project will develop novel, low-cost solutions suitable for the MTC ecosystem. Direct beneficiaries are: (1) researchers in wireless communications, machine-type communications, supported by the Internet-of-Things (IoT), and researchers in the general areas of machine learning and in other engineering fields, (2) users of MTC applications, as well as, government departments and private sector companies (from small firms to big enterprises) with interest in emerging MTC (e.g. industry 4.0 and IoT), (3) policy making regulators, such as Ofcom and. 
Commercial and societal impact: The growing reliance of society on wireless technologies, along with the emergence of industrial applications, prominently Industry 4.0, that rely on MTC solutions, and others such as smart cities and connected cars, provides a unique platform for achieving commercial impact through the proposed work. By focusing on practical implementation and hardware efficiency, along with proof-of-concept testing, we will ensure the promised massive connectivity targets are brought to practice. By enabling highly complex and adaptive communication scenarios, the proposed technological shift will be able to tackle the data deluge in the upcoming decades. It is anticipated that the outputs of this research project will be used by telecom/electronics manufacturers (e.g., Huawei, National Instruments, Nokia, Samsung), telecom business operators (e.g., Vodafone-UK, BT) and data stakeholders (e.g. Facebook, Google) within the UK and abroad to build machine-type networks with massive connectivity. The strategic placement of our Partners Huawei, NEC, Digital Catapult at the centre of the 5G ecosystem provides direct means for commercialization through IP exploitation of the project's solutions and promotion of the developed solutions to their product lines. 
 Overall the project lies at the centre of EPSRC's artificial intelligence strategy, and aligns with the following EPSRC's cross-ICT priorities (2017-2020) : Future Intelligent Technologies - learning-based transmission is at the centre of this portfolio and our MTC solutions open new horizons for high-reliability communication between different objects, machines and humans, fully aligned with the EPSRC vision to '' promote research which aims to develop intelligent, adaptive or autonomous systems that can learn, adapt and make decisions without the need for human control''. Information and Communications Technology (ICT) networks and distributed systems - by proposing disruptive solutions for future dense MTC, we will facilitate their rollout after 2025. 
 
New experts: The research training within the project, involving visits for hands-on training to our industrial partners Huawei and NEC will develop the research profile, expertise and man power of the world class research group in UCL and foster a team with excellence in DL-based communications and MTC, by producing new experts in the field of communications-tailored learning architectures, learning-based transmission, machine-type communications, and wireless communications in general.

The commercialization, exploitation, outreach and dissemination activities to realise the above impact are detailed in the Pathways to Impact section."
9,190C4646-386F-401B-8850-C471593EE4A4,Cardiff University - EPSRC Capital Award for Core Equipment,"Cardiff University will use the EPSRC's Capital Award allocation to purchase three packages of multi-user equipment as part of a broader strategy to support the development and maintenance of its world class laboratories. Such laboratory facilities are a key component of the University strategy to support high quality research and ensure the long-term competitiveness of its research community. 

The three proposal are linked around the theme of advancing methods for neuroscience research, building on key strengths already in place at Cardiff, within the Cardiff University Brain Research Imaging Centre (CUBRIC). 

The equipment has been proposed by research teams spanning the University, including physicists, engineers, computer scientists, psychologists, geneticists and psychiatrists. An important outcome of this work will be increased collaboration amongst these teams of people - this is essential for making the next advances in our understanding of the brain, its development from infancy through to old age, and what is happening in both health and disease.

The first package of equipment will extend CUBRIC's capabilities so that, using a technique called magnetoencephalography (MEG), we can measure the brain's electrical activity in both adults and young children, potentially while they are moving - this has been impossible up to now. The interdisciplinary team mentioned above will be needed to fully implement this system and optimise its potential.

We will also build a suite of equipment that allows more sensitive and controlled investigations of how the brain processes both touch and pain. This is technically challenging because the equipment must work within our brain scanning environments and the multidisciplinary team of scientists we have brought together are necessary to solve these challenges. However, it is important, because many conditions, such as autistic spectrum disorder (ASD), have, at their core, a deficit in sensory processing, including touch. In addition, many people suffer from chronic pain and this equipment will allow a more detailed investigation of how the brain processes touch and pain in this group.

Finally, many of our research scientists are developing novel, more complex, analysis tools for neuroscience data analysis. The new algorithms, including machine-learning approaches, can make the best use of the opportunities afforded by CUBRIC's brain imaging facilities and the complex data it generates, but need state-of-the-art computer processing systems. The third package of equipment is such a system, using advanced GPU processors to enable work to be done in hours that we would have to wait weeks for on our current system.",,"This Equipment will enable collaborative research between academics across the University sector, including Physics, Engineering, Computer Science, Psychology, Medicine and Biological Sciences. We will also liaise with academic and commercial researchers designing clinical trials regarding ways to incorporate novel neuroimaging acquisitions, and advanced machine-learning analysis tools, into their protocols.
We will disseminate our findings at international conferences and by publications. We will encourage further replication by other groups (by making our designs, protocols and datasets available) to promote the goal of meta-analytically validated imaging markers that can enter use in clinical diagnosis. 

Through methodological improvements across many future research studies, the equipment will have significant impact outside academic research. Ultimately, we envisage that the major non-academic beneficiaries will be: 
1) Patients suffering from a wide range of neurological disorders. 
2) Mental health professionals involved in the care of patients 
3) Pharmaceutical companies with interest in developing drugs for psychiatric and neurological disorders 

Ultimately, the equipment here will help deliver enhanced outcomes in this area in the following ways:
1) Provide the ability to non-invasively study sensitive markers of brain function, such as oscillatory dynamics and connectivity, in a wider range of people, including infants to old age, in more comfortable and dynamic real-world environments and at a much cheaper cost. 
2) Provide more precise and controllable ways of mapping all aspects of sensory function, include vision, auditory and, in particular, high-accuracy mapping of the somatosensory and pain systems in volunteers and patients.
3) Enhance our ability to perform complex data processing via machine-learning algorithms that have, until now, been computationally prohibitive. 

All of the above will enhance our capabilities to deliver large cross-lifespan normative datasets and then test clinical groups against this, improving early detection, stratification of diagnosis and treatment planning.

Ultimately the major beneficiaries will be the patients suffering from these disorders, their carers, and professionals working in the healthcare industry. 

Impact on the pharmaceutical industry 
The development of new neurological drugs is extremely expensive and all of the major companies are attempting to reduce costs by streamlining this process. A particular focus is the development of early biomarkers of drug action both within the patient group and in terms of predicting an individual's response. The methodological improvements that will result from this equipment, and its integration into novel research designs, is therefore likely to have significant impact on this industry. Cardiff University has strong links to large pharmaceutical companies as well as ongoing projects in developing methodology to develop new pharmacological agents via its flagship Medicines Discovery Institute. These links and projects will be further enhanced by the present proposal. 

Outreach activities beyond patient populations 
All of the research partners on this bid have strong outreach programmes that are aimed towards making the results of scientific studies accessible to a broad non-academic audience. This is achieved via university open days, local media and, in order to reach younger people, researchers delivering presentations in local schools. We will support outreach activities by making the results of our collaborative research available for use at such events. Where appropriate we will continue to distribute results via press release."
10,47E31530-610A-46A0-87B8-1C593C6B75C5,Explainable AI for UK agricultural land use decision-making,"Agricultural land use dynamics and their associated driving factors represent highly complex systems of flows that are subject to non-linearities, sensitivities, and uncertainties across spatial and temporal scales. They are therefore challenging to represent using traditional statistical modelling approaches. Existing process-based modelling has enabled advances in understanding of individual biophysical processes underpinning agricultural land use systems (e.g. crop, livestock and biogeochemical models). However, these tend to focus on individual processes in detail or link a limited number of processes at large scales, thereby mostly ignoring the complex interdependencies between the multiple interacting biophysical and socio-economic components of land use systems. Artificial intelligence (AI) techniques offer great potential to complement such modelling approaches by mining the deep knowledge (e.g. farming patterns and behaviours) encapsulated in 'big' data from ground-based sensors (such as frequently used for precision farming) and Earth Observation satellites. This will deliver enhanced insight on the past and current state and spatio-temporal dynamics of agricultural land use system flows and how they can be influenced by decisions on agricultural policies and related farm management practices. 

Our proposal aims to develop a novel explainable AI framework that is transparent, data-driven and spatially-explicit by using probabilistic inference and explicit &quot;if-then&quot; rules. We will demonstrate proof-of-concept for two pilot regions of the UK (Oxfordshire and Lincolnshire), and the framework will be set up in a way that can be readily expanded to the whole UK. Specifically, we will draw on time-series of agricultural land use and production datasets (in-kind support from industry project partner SOYL) to identify the key socio-economic and environmental driving factors that have led to historic agricultural land use changes in the pilot regions. We will then establish explainable AI-rules for the characterisation of these agricultural land use changes and refine them within the framework through machine learning and parameter optimisation.

We will demonstrate and test the potential of the explainable AI framework for providing a new and robust method for predicting changing patterns of agricultural land use in the two pilot regions. This will include testing the ability of the AI framework for improving understanding of past and present agricultural land use dynamics across multiple temporal and spatial scales from 'big' data. It will also assess the potential for continually updating the predictions of land use dynamics in real-time using data from sensors. This could provide early warning when certain driving conditions are triggered or used to repeatedly refine short-term projections of land use change and their estimates of uncertainty.",,"We will engage with four major types of beneficiaries to maximise the societal impact of this project:

1. UK Government, Devolved Administrations and Policy-makers: 
Our project will provide decision-makers with an innovative and integrated knowledge base supporting agricultural land use decision-making. We will engage with policy-makers including Defra, Natural England (a project partner), the Environment Agency, and the devolved administrations through consultations on the development of the explainable AI system to ensure it meets a broad set of decision-making needs. We will also demonstrate the final AI framework at a 1-day policy workshop that explores the potential of AI approaches for supporting landscape decision-making alongside other modelling and information products, including defining future needs beyond the remit of this proposal. This will include exploring how the framework can help inform the objective in the 25 Year Environment Plan to deliver a clear evidence base to promote precision agriculture and land management.

2. Farming Industries and Agri-business: 
Through our strategic partnership with SOYL, the leading precision crop production service provider in the UK (see letter of support), we will have access to a unique 'big' dataset of in-situ and EO-based agricultural land use statistics to establish the explainable AI framework. The methodological innovation in the project will be co-created with SOYL (and our other project partner, Natural England) through four meetings throughout the one-year project lifetime. This will ensure that our outcomes, disseminated through a one-week training event at SOYL, are fit-for-purpose in informing the decision-making needs of agri-businesses in order to catalyse business change and innovation.

3. AI Industries and the Economy: 
The AI innovations developed in this project will support the socio-economic development of the UK by promoting automation as part of the Industry 4.0 revolution. The AI framework is fully transformative and scalable, such that it can be elaborated or enhanced as required to support decision-making within a growing UK economy that aims to balance economic, environmental and societal aspects of the UK agricultural sector. Continued technological innovations in the agricultural sector are expected through precision farming and Earth Observation, creating an increasing demand for AI and big data-driven models.

4. Land owners and Trusts: 
The knowledge and explainable AI rules derived in the system will support the decision-making of land owners and national trusts, by reducing environmental hazards (e.g. pesticides), enhancing ecosystem services and natural capital, and promoting smart and profitable agriculture, as well as building an inclusive society that allows people and nature to thrive. We will engage with relevant land owners and trusts through Natural England (see letter of support) via dissemination meetings and communications (e.g. media).
The 'Pathways to Impact' document details the activities we will undertake to deliver these impacts."
11,27284E23-36D3-4F01-A0E1-62F8EF34EC3D,NSF-EPSRC:ShiRAS. Towards Safe and Reliable Autonomy in Sensor Driven Systems.,"Modern data-driven algorithms trained over enormous datasets have revolutionised contemporary autonomous systems with their accurate predictive power. However, due to technical limitations, it is a challenge to integrate large-scale data from many different and complex sensors. Capturing the confidence of these algorithms also remains a challenge. 

In response to this demand, ShiRAS will develop pioneering approaches that will introduce autonomy at different levels in sensor-driven systems. The main focus is on machine learning methods with quantified uncertainty of the provided solutions. 

Within the field of machine learning, deep learning approaches have resulted in the state-of-the-art accuracy in visual object detection, speech recognition and translation, and many other domains. Deep learning can discover intricate structure in large data sets by using multiple levels of representation, where each level is a higher, more abstract representation of the data. However, a rigorous mathematical framework for uncertainty propagation and update in machine learning models has been largely underexplored. Most current deep learning techniques process the raw data in a deterministic way and do not capture model confidence or trust. Uncertainty can emanate from the noise in the raw data and the parameters of the approach and this impact is a critical part for any predictive system's output.
 
By representing the unknown parameters using distributions instead of point estimates and propagating these distributions from the input to the output of the system, we propose promising machine learning methods able to handle uncertainty in a unified way.",,"The project is multidisciplinary and strongly aligned with the EPSRC priorities for a Prosperous Nation, aiming at informed and connected nation. The project will allow two research teams from the UK and USA to collaborate on a highly interdisciplinary area, together with industrial partners such as QinetiQ, Cisco, Valerann Ltd. and other companies. It will enable them to focus on research and to generate scientific innovations that have the potential to make a difference in key areas with strong demands of autonomy - especially in intelligent transport systems and surveillance. The technologies are aimed to be modular and scalable with respect to the time, space and to large volumes of data. The project will achieve holistic impact in three main areas: (i) Academic, (ii) Societal and (iii) Impact on industrial sectors in accordance with the UK governmental overarching strategies.

As such, ShiRAS will generate considerable impact for a wide range of academic and non-academic beneficiaries, principal amongst whose are:

1) The machine community including academia and industry;
2) Our collaborating industrial partners, directly and other companies;
3) The research community, particularly in the areas of engineering; statistics, computer science
4) The project personnel: the NSF and EPSRC funded postdoctoral researchers
5) The general public, schools and the society in general.

The developed technology can be also beneficial for individual users, the general public, for city councils, policy makers and first responders that need different levels of autonomy, and especially for processing the data provided by multiple sensors. 

We will undertake a number of specific activities: 
- Visits of the researchers and staff from the UK and USA partners.
- Annual workshops to bring together the consortium and wider users group to exchange information and update users on progress.
- A dedicated web site will be developed to enable access to latest results and progress. Latest news and significant advances will be considered for release to the wider media as appropriate.

Future exploitation: A successful outcome of the project will lead to new projects both via collaboration with our business partners, via Innovate UK, EPSRC/UKRI and other funding sources. Where appropriate we will seek the advice of our respective Research and Enterprise teams to support the uptake of the technology via Industrial, Knowledge Transfer Network and other follow-on funding initiatives. The project will afford new partnerships to be formed based on the EPSRC funding. Since the partners are actively involved in many external organisations and networks we will be able to link strongly with these organisations to strengthen the research base.
Hence, the outcomes of this project will have broad scientific, social and economic impacts. We expect also the team of this project to generate further collaborative contributions based on this project, at national and international level through the US collaboration. 
Overall, the project results will shape the future of areas of machine learning and Artificial Intelligence, so that the UK-USA collaboration increases and strengthens the world leadership position in these areas."
12,28FF0153-286B-4B28-8E23-BDC85B6A840B,Explainable AI for UK agricultural land use decision-making,"Agricultural land use dynamics and their associated driving factors represent highly complex systems of flows that are subject to non-linearities, sensitivities, and uncertainties across spatial and temporal scales. They are therefore challenging to represent using traditional statistical modelling approaches. Existing process-based modelling has enabled advances in understanding of individual biophysical processes underpinning agricultural land use systems (e.g. crop, livestock and biogeochemical models). However, these tend to focus on individual processes in detail or link a limited number of processes at large scales, thereby mostly ignoring the complex interdependencies between the multiple interacting biophysical and socio-economic components of land use systems. Artificial intelligence (AI) techniques offer great potential to complement such modelling approaches by mining the deep knowledge (e.g. farming patterns and behaviours) encapsulated in 'big' data from ground-based sensors (such as frequently used for precision farming) and Earth Observation satellites. This will deliver enhanced insight on the past and current state and spatio-temporal dynamics of agricultural land use system flows and how they can be influenced by decisions on agricultural policies and related farm management practices. 

Our proposal aims to develop a novel explainable AI framework that is transparent, data-driven and spatially-explicit by using probabilistic inference and explicit &quot;if-then&quot; rules. We will demonstrate proof-of-concept for two pilot regions of the UK (Oxfordshire and Lincolnshire), and the framework will be set up in a way that can be readily expanded to the whole UK. Specifically, we will draw on time-series of agricultural land use and production datasets (in-kind support from industry project partner SOYL) to identify the key socio-economic and environmental driving factors that have led to historic agricultural land use changes in the pilot regions. We will then establish explainable AI-rules for the characterisation of these agricultural land use changes and refine them within the framework through machine learning and parameter optimisation.

We will demonstrate and test the potential of the explainable AI framework for providing a new and robust method for predicting changing patterns of agricultural land use in the two pilot regions. This will include testing the ability of the AI framework for improving understanding of past and present agricultural land use dynamics across multiple temporal and spatial scales from 'big' data. It will also assess the potential for continually updating the predictions of land use dynamics in real-time using data from sensors. This could provide early warning when certain driving conditions are triggered or used to repeatedly refine short-term projections of land use change and their estimates of uncertainty.",,"We will engage with four major types of beneficiaries to maximise the societal impact of this project:

1. UK Government, Devolved Administrations and Policy-makers: 
Our project will provide decision-makers with an innovative and integrated knowledge base supporting agricultural land use decision-making. We will engage with policy-makers including Defra, Natural England (a project partner), the Environment Agency, and the devolved administrations through consultations on the development of the explainable AI system to ensure it meets a broad set of decision-making needs. We will also demonstrate the final AI framework at a 1-day policy workshop that explores the potential of AI approaches for supporting landscape decision-making alongside other modelling and information products, including defining future needs beyond the remit of this proposal. This will include exploring how the framework can help inform the objective in the 25 Year Environment Plan to deliver a clear evidence base to promote precision agriculture and land management.

2. Farming Industries and Agri-business: 
Through our strategic partnership with SOYL, the leading precision crop production service provider in the UK (see letter of support), we will have access to a unique 'big' dataset of in-situ and EO-based agricultural land use statistics to establish the explainable AI framework. The methodological innovation in the project will be co-created with SOYL (and our other project partner, Natural England) through four meetings throughout the one-year project lifetime. This will ensure that our outcomes, disseminated through a one-week training event at SOYL, are fit-for-purpose in informing the decision-making needs of agri-businesses in order to catalyse business change and innovation.

3. AI Industries and the Economy: 
The AI innovations developed in this project will support the socio-economic development of the UK by promoting automation as part of the Industry 4.0 revolution. The AI framework is fully transformative and scalable, such that it can be elaborated or enhanced as required to support decision-making within a growing UK economy that aims to balance economic, environmental and societal aspects of the UK agricultural sector. Continued technological innovations in the agricultural sector are expected through precision farming and Earth Observation, creating an increasing demand for AI and big data-driven models.

4. Land owners and Trusts: 
The knowledge and explainable AI rules derived in the system will support the decision-making of land owners and national trusts, by reducing environmental hazards (e.g. pesticides), enhancing ecosystem services and natural capital, and promoting smart and profitable agriculture, as well as building an inclusive society that allows people and nature to thrive. We will engage with relevant land owners and trusts through Natural England (see letter of support) via dissemination meetings and communications (e.g. media).
The 'Pathways to Impact' document details the activities we will undertake to deliver these impacts."
13,7A519A57-9EF0-460C-A6D2-E78420284154,Engineering Transformation for the Integration of Sensor Networks: A Feasibility Study - 'ENTRAIN',"There is a need to make use of new digital data analysis techniques to improve our understanding of the environment. Data from a new generation of environmental sensors, combined with analyses based on Artificial Intelligence, has the potential to help us understand from human influences and long-term change are affecting the environment around us. Artificial Intelligence approaches enable computers to identify trends and relationships across different streams of data, often picking out patterns that would be too difficult or time-consuming for humans to identify manually.

To realise these benefits, data from diverse sensor networks must combined and analysed together. Currently many sensor networks are operated individually, and data are not readily combined due to differences in the way measurements are made (e.g. between weekly river samples and sub-second measurements of gases in the atmosphere). In addition, to combine these data in an automatic way without human intervention requires much finer and more consistent descriptions of the contents of data streams, so that machines can understand the content sufficiently. Links between sensors in space are also important, and machines will need an understanding of these links, not just in the sense of coordinates, but for example how sensors are linked along rivers. We can construct a digital representation of rivers in order to enable this.
We will describe the various elements of a future environmental analysis system that will be required in order to achieve these benefits, and addressing some of these currently missing components. We will look at technologies, from databases to data transfer mechanisms, to understand how a system could be built.

We will use data from 3 NERC sensor networks measuring environmental variables from the atmosphere to river water quality, and show how this data can be automatically integrated in such a way that machines would be able to analyse it automatically.
A significant issue when monitoring with high-resolution sensors is how to handle problems in the data, which could include missing data, and erroneous values due to sensor failure. There is too much data for humans to manually view and check, and so automated approaches are needed. Currently these are often simple checks of individual data values against expected ranges, but again there are opportunities for artificial intelligence to improve this. AI approaches can look across multiple sensors, identify relationships, and find subtle changes in data signals, and this can be used to both identify data problems and to fix them through infilling. We will enhance the 3 NERC networks by testing and applying such approaches to data quality control.

We will investigate some fundamental limitations of high-resolution monitoring, the transfer of large amounts of data from the field site to the data centre, the security of such systems, and whether more processing could be done on the instruments themselves to reduce data transfer volumes.

We will meet with the public, with policy-makers, with industry and with researchers to discuss where there will be most to be gained from development of AI approaches to analysing environmental sensor data. We will develop ideas for future work to realise these gains, and will promote the benefits of an integrated system for environmental monitoring. These stakeholders are likely to include the Environment Agency, SEPA, Natural Resources Wales, Defra, Water companies, sensor network developers, and public organisations with an interest in the environment, including the National Trust, the Rivers Trusts, and local community groups.",,"The Digital Environment programme will benefit from ENTRAIN's foundation work, providing requirements, methods, best practice advice and recommendations for integration and data modelling across multiple sensor networks and other datasets, such as EO. This information and techniques will benefit other areas of science and industry, as well as public engagement.

Environmental practitioners, regulators, government, consultants, the water industry, agribusiness, insurance, and many others can benefit substantially from the joined-up evidence that ENTRAIN will start to generate.

The public, schools and colleges will benefit from access to meaningful data in a spatially aware context. There is strong public interest in environmental issues, and yet beyond weather forecasts, weather data and perhaps more recently air quality readings, there are few accessible data which have tangible meaning to the layperson.

The Earth Observation (EO) community will benefit from better access to connected in situ data for retrieval algorithm validation &amp; development. E.g. flooding extent, soil moisture and land cover products etc. and from new automated Phenocam greenness products output by ENTRAIN. Other Big Data projects (e.g. Data Labs) will benefit from a greater and easier connection to datasets, with common spatio-temporal linking requirements and proper metadata description already done.

Data modellers, and environmental informatics will benefit from improved sensor metadata schemes, sensor registers/catalogues, and data structuring for interoperability of a network of networks. We will disseminate the results of the ENTRAIN feasibility study also through an environmental informatics paper, describing the proposed methods and advances made in data modelling and data provenance, to ensure wide impact and uptake of these methods.

Observational scientists and regulatory observers will benefit from our website case studies, webinars, training workshops and online video tutorials to disseminate best practice and training in realtime data collection, cyber security, data vocabularies, metadata schemes and new deep learning QC techniques. These studies will benefit the global environmental, and wider, data communities such as the Committee on Data for Science and Technology (CODATA). Harmonisation of data networks to increase or facilitate interoperability, and production of spatio-temporally connected observations across environmental domains (e.g. Digital Rivers), will benefit the British Geological Survey (BGS), CEH, UK MetOffice, the Environment Agency (EA), the Scottish Environmental Protection Agency (SEPA), Defra, water companies (and other utilities such as the power grid), argi-business, insurers, and public health. Other NERC and EPSRC funded programmes (e.g. ASSIST, Natural Flood Management, HydroJULES, Internet of Food Things) and Defra Air Quality Monitoring Networks will all benefit through higher data quality, more complete data and efficiency gains in analysing data across sensor networks, and by using developed data structures in other environmental monitoring and food chain domains. The spatially connected integrated data visualisations will be demonstrated at both academic and industry events and conferences, where practitioners can benefit from e.g. improved water quality alerts (e.g. algal blooms, nutrient levels etc.), and crucially see the connected drivers of those trends, and get decision support information. Similarly, this has the potential to provide interconnected, cross-discipline, environmental management information that can give government the evidence chain to implement, monitor and evaluate policy.

We will monitor and evaluate the success of our impacts by recording attendances at events, the number of website visits, and use Twitter to promote activities, whilst recording the number of followers and retweets. We will also log email enquiries, webinar and web video views."
14,D48A8506-BB47-42F9-954C-D3EBC220D221,MIMIc: Multimodal Imitation Learning in MultI-Agent Environments,"In UK, we are not allowed to drive a vehicle until we are 17. It is because, driving is a complex and safety critical activity that requires many advanced cognitive skills like recognition of possible threats, anticipation of behavior of other road users and agile reaction to emerging situations. Think about a football player making decisions on field. A good player can sense the opportunities, through anticipating what other players will do, and select an action that will increase the odds of scoring. It takes a long time for humans to develop these advanced cognitive skills, to become an expert at such complex real-world tasks. Artificial Intelligence has made significant progress during the last decade, demonstrated by breakthroughs in cancer detection, computers beating 'Go' masters and intelligent robotics. However, if AI is to live up to its science fictional promises to assist humanity or even supersede human intelligence, it should at least be equipped with cognitive skills such as those possessed by humans. This project aims to develop ground breaking algorithms that equip autonomous systems with human like cognitive skills required to thrive in real world environments.
We are focused on applications that require autonomous agents (e.g. Robot or Driverless car) to interact with multiple intelligent agents in the environment to accomplish a task (known as Multi-Agent Environments: MAEs). Such applications require an agent to anticipate the behaviour of other agents and to select the most appropriate course of actions. Equipping agents with such autonomous decision-making capability is known as policy learning. Compared to policy learning in single agent domains (teaching a robot to walk or a computer to play a video game), the recent progress of policy learning in MAEs has been quite modest. This is due to multiple reasons: 1)Due to agent actions the environment is dynamic 2)multi-agent policy learning suffers from a theoretical limitation known as curse of dimensionality (CoD) 3)Utility functions that capture agent objectives are difficult to define 4)there is a significant lack of adequate multi-agent datasets that allow meaningful research. This project proposes to undertake research in to policy learning in MAEs, by addressing the above limitations. 
Our unique approach to policy learning in MAEs is motivated by how humans thrive in similar settings. Firstly, we perceive the world through multiple senses, (i.e. vision, audition, touch) enabling a rich perception of the world. Secondly, when acting in a MAE, humans do not pay attention to all the stimuli but only to key stimuli e.g. when a football player is attacking the ball, the player pays attention only to the teammates capable of effecting a goal and the key defenders. Finally, the learning paradigm we employ known as imitation learning is an emerging methodology to learn by observing experts, which is a productive approach that we use to learn new skills. Accordingly, we propose to learn realistic policies in MAEs through imitation learning by leveraging multimodal data fusion and selective-attention modelling. Multimodal data fusion allows to capture high dimensional context of the real world and selective attention model allows for allaying the issue of CoD. We have been provided a unique multimodal multi-agent dataset and access to state-of-the-art facilities to capture data, by an elite football club facilitating this ambitious research project.
The project outputs will be subjectively validated as a tool to answer &quot;what-if&quot; questions related to game play in football assisting coaching staff to visualize speculative game strategies, and as a computational benchmark to quantify cognitive skills of football players. The planned impact activities will ensure the project will leave a legacy in AI development benefiting UK PLC through significant contribution in multiple high growth areas, such as driverless vehicles, video gaming, and assistive robots.",,"AI is set to contribute &pound;232Bn in to the UK economy by 2030. Towards facilitating such growth, in alignment with connected nation prosperity outcome of EPSRC (More info in Sec. 3 of CfS), this project develops advanced human-like decision making algorithms. Beneficiaries of this project are anyone who could benefit from advanced decision-making algorithms and assessing skills of humans using AI models. Examples of societal and economic impact from different sectors are given below.

Commercial private sector:
Sports content broadcasters can use MIMIc algorithms can be used to visualize speculative game play with realistic simulations to assist expert commentators to discuss on different game strategies; 
Video gaming industry (worth ~&pound;3Bn to UK economy) can benefit from algorithms capable of learning realistic strategies and player behaviour from real games, enabling creative video games where the strategies are realistic and where gamers can adapt behaviors of professional players. 
Algorithms with human like cognitive skills will benefit autonomous vehicle/collaborative robot design. This enables humans and autonomous systems to coexist. Imitation learning algorithms developed in the project will equip autonomous vehicles/ robots with intelligent control algorithms that are empathetic towards humans. 
Sports analytics industry, set to worth $4.5Bn by 2024, can benefit from the project outcomes through algorithms that fuse multimodal data sources to gain insights to game play, and to gain competitive edge through creative game strategies. 
Project demonstrates the use of AI models as a benchmark to assess human decision making skills. Such methodology can enable industries such as manufacturing/sports/education to identify and develop talent thereby improving the workforce. 

Policy-makers, public authorities &amp; third sector:
Government funded agencies and charities promoting physical activity, and participation in sports, can benefit from this project, by using the demonstrator to raise awareness of sports, and facilitate measuring physical activity. 
Urban planning authorities can gain insights into human behavior in cities through multimodal imitation learning and simulation: e.g. to gain realistic insights of the impact of traffic, or adverse weather conditions; Intelligent infrastructure in smart cities can learn from human interactions to enforce safe and efficient strategies: e.g. road side units can learn policies by analysing safe driving patterns, and then enforce such policies through vehicle-to-infrastructure communication.
The project algorithms can be utilized to assess skills of individuals and teams benefiting selection and training of military personnel; Combination of models with virtual reality, will facilitate realistic virtual training regimes.
Science promotion organizations will benefit from the project demonstrators to show the impact of AI on the society, through a poignant topic of public interest, i.e. football.

Societal impacts 
The public who will eventually encounter robots, such as driverless vehicles, will benefit because the robots will be more aware of human actions. 
Football fans will benefit from novel visualizations of player strategies, which enables intelligible enjoyment;
Coaches in schools who want to use examples from professional players and discuss consequences of actions can benefit from the project demonstrators. 
Video gamers (37Mn in UK) who wish to play games that are realistic and correspond to real game strategies will also be a beneficiary. 
Urban dwellers, city workers, and local councils will benefit from improved city planning to make their journeys enjoyable, reduce pollution and improved safety; 
Finally, it is recognized that improved skills in machine learning, simulation and research methodologies has a critical role in bridging the gap between academia and industry, especially in the fields of multi-agent systems and data driven policy learning"
15,BEF7480E-0B2F-4AEF-9F96-60A15C4CB955,Capital award for Core Equipment,"This proposal addresses a proven demand for enhanced and extended multi-disciplinary core-equipment in materials research and artificial intelligence/machine learning research at Surrey. This is fully in line with the University of Surrey's Research and Innovation Strategy which seeks to conduct world-leading research, providing researchers with access to world-class laboratories and state-of-the-art equipment. 

The selected core equipment items that we would like to fund through this award are as follows:
1. Scanning Electron Microscope (SEM)
2. Thermal Analysis Equipment - Thermogravimetric Analyser (TGA) and Differential Scanning Calorimeter (DSC)
3. Glove Box Facility
4. AI@Surrey High Throughput Compute (HTC) Testbed Facility",,"Scanning Electron Microscopy (SEM) underpins the characterisation of all classes of engineering materials from the classic categories of metal, polymers, and ceramics to hybrids and nanomaterials. The extra capacity and capability afforded by a new SEM, with X-ray analysis, will enhance our research in a variety of areas, including: additive and advanced manufacturing, advanced metrology, aerospace, automotive, defence, electronic, functional, nano and nuclear materials. This improved capability, in spatial resolution and new imaging modes, will increase the quality of data gathered. The additional capacity for our 100+ users will enable more carefully planned and longer experiments, offering improved data quality through greater statistical reliability and better critical design. This will enhance the quality of research outputs, and increase the value of the research for our research partners such as local manufacturing SMEs, NPL, Rolls-Royce, McLaren, DSTL, and CCFE. 

Thermal Analysis Equipment (DSA &amp; TGA) - The new equipment in the Thermal Analysis facility will impact on the economy, people, and knowledge. The equipment will support consultancy and services to industry in support of product development. It will increase the competitiveness of industry (notably aerospace, defence, chemicals and pharmaceuticals) when used in their sponsored research projects. When used in collaborative research projects, the equipment will have an impact on the competitiveness of industry, notably aerospace, defence, chemicals and pharmaceuticals, which all rely on fundamental materials research. Training offered by the facility will build skills in early career researchers (ECRs), including Engineering Doctoral students within the MiNMaT Doctoral Training Centre. Fundamental measurements of phase transitions and thermal phenomena obtained via the facility will contribute to the knowledge base of the properties of condensed matter and materials systems.

Glove Box Facility - Through this grant, the University is aiming to bring together excellence in batteries and materials research from across its engineering community. This will be achieved via the set-up of a co-located glove box facility for the faculty, with two types of glove box to be purchased and made available to a wide user base. A high specification MBraun glovebox (dry box) is essential for the manipulation of air/moisture sensitive chemicals; an inert atmosphere underpins future research of six academic staff, including impact in EPSRC strategic areas (including Fuel Cells, Energy Storage and Catalysis) and grand challenges (Dial a Molecule, Directed Assembly, Energy). This strengthens infrastructure for existing grants in battery research and will have wider impact in the development of industrial/commercial support and deployment. The nitrogen glovebox will support diverse activities including: organic-inorganic and perovskite solar cells, perovskite X-ray sensors, organic semiconductor sensors, Li-ion and Na-ion batteries and supercapacitors. Additional capacity will improve efficiency and expand the operation. 

AI@Surrey HTC - A cross-University HTC compute facility will provide a testbed facility for AI and Machine Learning collaboration from fundamental research through to real-world application. This will provide the essential resource required to catalyse collaboration between domain experts with access to unique datasets and leading experts in AI within CVSSP. Specific application areas of research strength where there is considerable potential for growth in AI research collaboration include digital health and ageing, veterinary science, security and data privacy, 5G communications, environmental sustainability, remote sensing, space and autonomous systems. Further, cross-University collaboration will specifically address the ethics, fairness, regulation and understanding of blackbox AI systems which is essential to their development for the benefit of society"
16,A07F8E0B-E45C-4EB0-AFB7-6C141F4C834E,Streamlining Social Decision Making for Improved Internet Standards,"Many decisions in today's world are made through a complex, dynamic process of interaction and communication between people and teams with different interests and priorities - so called &quot;distributed decision-making&quot; (DDM). For example, many businesses work across multiple geographically dispersed offices and timezones, with teams specialising in quite diverse areas. Each team may have its own goals and reward models, which do not necessarily coincide, and may be spread across multiple organisational units (e.g. different businesses or governments). Communication may happen via several different modalities with very different timescales and properties (e.g. email, instant messenger, and face-to-face meetings).

Unfortunately, although many organisations have started to document these processes and even make records available (particularly governmental organisations e.g. https://data.gov.uk/), we have no way to automatically analyse these records. If we did, we could produce tools to automatically summarise decisions, trace who made them, and why and how they were made (and why other decisions weren't made). From a societal standpoint this would help make these processes more accountable and transparent. We'd also be able to identify collaborative failures, biases and other problems, and thus help improve decision-making in future.

This project will develop these urgently required methods, using a combination of natural language processing and social network analysis. We will collate, annotate and publicly release the first multimodal dataset of real-world distributed decision-making. We will devise techniques to take natural language and semi-structured data to recognise the dialogue and interaction structures in decision making, and analyse those structures to produce summaries and evaluate the efficacy of the decision making process. We will then use the outputs to inform strategic interventions that can streamline and improve decision making. 

Our methods will be suitably generic to span several domains. However, the project will focus on one particular global organisation as its main use case: the Internet Engineering Task Force (IETF). This is an international forum responsible for producing Internet protocol standards - formal documents which specify the languages by which software and hardware &quot;speak&quot; across the Internet. To produce these documents, extensive international collaboration is performed - this spans several modalities including email discussions, collaborative document editing, face-to-face meetings and teleconferencing. Importantly, all of these modalities are documented via transparency reports ranging from public email archives to minutes from meetings. This project has partnered with the IETF to help model and streamline their decision making process. We will borrow from their experience, and employ our methods to extract decision making bottlenecks. We will devise tooling which will provide advice and proposed interventions to relevant parties within the IETF. Amongst many other things, we directly benefit the IETF, and the global Internet standards community, by helping them to uncover biases and help make important decision processes accountable.",,"The project will generate ground-breaking advances in the automated processing of natural language and semi-structured data relating to social decision making. By partnering with the IETF, it will also apply these results to improve the process of developing Internet standards, and to streamline the IETF's decision making process. We envisage impacts from both of these sets of outcomes, and their combination across various themes, discussed below:

Novel research approaches: By bringing together researchers in NLP, social network analysis, computer networks and psychology we expect novel synergy-producing approaches with impact across academic disciplines (see Academic Beneficiaries).

Datasets and tools: By producing and publicly releasing the first large-scale multimodal dataset of real-world distributed decision-making data we will benefit researchers in many areas (see Academic Beneficiaries). We will ensure that all publications, documentation, data and software are made publicly available e.g. via GitHub and the UK Data Archive to encourage uptake and re-use.

Technology transfer: The new methods we will develop will lead to economic and societal impact via technology transfer. Methods for understanding multi-modal human-human interaction (not only in decision-making but also in many social and business environments) will be commercially valuable for building dialogue systems, e.g. chatbots, business and decision support systems (amongst other possibilities). Further, our impact plan will help transfer this to commercial reality via spinout formation and/or licensing to existing UK industry including companies working with our partner organisation, the IETF. This will be supported by QMUL's technology transfer company, QM Innovation Ltd and dedicated mobile app division QApps (www.qappsonline.com); and/or dedicated funding via InnovateUK or similar. The PI has a strong track record of spinout formation, licensing and InnovateUK transfer from NLP research (Chatterbox Labs, IESO Digital Health, Quality Health Ltd).

Impact on the IETF: Our key impact partner is the IETF itself, which will deploy our tools for streamlining and improving their activities. Our research agenda has been coordinated with direct inputs from the IETF, such that the impact can be maximised. Our key long-term impact will come from the improvements to the IETF procedures derived from the project's interventions. Other impacts will be to improve the quality of standards, by better supporting cross-area review, and to help improve community representation by highlighting bottlenecks and biases in the process. To further the impact and extend it beyond the life of this project, we will work towards the integration of our tools with the IETF's datatracker (a web portal, which exposes public data about standards activities).

Impact on industry and society via internet standards: this positive impact on internet standards, and particularly opening them up to inputs from more diverse groups, will feed into better-met industry needs. Since this will have a direct benefit on the technical decisions underpinning how the Internet operates, the impact will eventually extend to most members of society. The UK is a key player in IETF, and we will work with industry to demonstrate how our techniques can improve their effectiveness, ensuring future Internet standards continue to reflect UK interests. Our impact plan therefore focuses on reaching and influencing three main constituencies: (1) organisations/companies interested in standardisation and social decision making (Sky, Ericsson, JISC); (2) the IETF, which has an inherent interest in improving their activities (Eggert, Navarro, Ford, Oever); and (3) the research community interested in exploring social decision making in other domains, as well as those interested in the standardisation process and the interplay of the agents building the Internet (Lascarides, Doty)."
17,3C9CDDB9-3146-493B-BDF2-2E60017BD2E6,Digital Futures at Work Research Centre,"The Digital Futures at Work Research Centre (Dig.IT) will establish itself as an essential resource for those wanting to understand how new digital technologies are profoundly reshaping the world of work. Digitalisation is a topical feature of contemporary debate. For evangelists, technology offers new opportunities for those seeking work and increased flexibility and autonomy for those in work. More pessimistic visions, in contrast, see a future where jobs are either destroyed by robots or degraded through increasingly precarious contracts and computerised monitoring. Take Uber as an example: the company claims it is creating opportunities for self-employed entrepreneurs; while workers' groups increasingly challenge such claims through legal means to improve their rights at work.

While such positive and pessimistic scenarios abound of an increasingly fragmented, digitalised and flexible transformation of work across the globe, theoretical understanding of contemporary developments remains underdeveloped and systematic empirical analyses are lacking. We know, for example, that employers and governments are struggling to cope with and understand the pace and consequences of digital change, while individuals face new uncertainties over how to become and stay 'connected' in turbulent labour markets. Yet, we have no real understanding of what it means to be a 'connected worker' in an increasing 'connected' economy. Drawing resources from different academic fields of study, Dig.IT will provide an empirically innovative and international broad body of knowledge that will offer authoritative insights into the impact of digitalisation on the future of work.

The Dig.IT centre will be jointly led by the Universities of Sussex and Leeds, supported by leading experts from Aberdeen, Cambridge, Manchester and Monash Universities. Its core research programme will cover four broad-ranging research themes. Theme one will set the conceptual and quantitative base for the centre's activities. Theme two involves a large-scale survey of Employers' Digital Practices at Work. Theme three involves qualitative research on employers' and employees' experiences of digitalisation at work across 4 sectors (Creative industries, Business Services, Consumer Services, Public Services). Theme 4 examines how the disconnected attempt to reconnect, through Public Employment Services, the growth of new types of self-employment, platform work and workers' responses to building new forms of voice and representation in an international context. Specific projects include:

1. The Impact of Digitalisation on Work and Employment
-Conceptualising digital futures, historically, regionally and internationally
-Comparative regulation of digital employment
- Mapping regional and international trends of digital technology and work

2. Employers' Digital Practices at Work Survey

3. Employers' and employees' experiences of digital work across sectors
-Changing management processes and practices
-Workers' experiences of digital transformation

4. Reconnecting the disconnected: new channels of voice and representation
- displaced workers, job search and the public employment service
- self-employment, interest representation and voice

Dig.IT will establish a Data Observatory on digital futures at work to promote our findings through an interactive website, report on a series of methodological seminars and new experimental methods and deliver extensive outreach activities. It will act as a one-platform library of resources at the forefront of research on digital work and will establish itself as a focal point for decision-makers across the policy spectrum, connecting with industrial strategy, employment and welfare policy. It will also manage an Innovation Fund designed to fund novel research ideas, from across the academic community as they emerge over the life course of the centre.",,"The impact objectives are to:

1. Generate new co-produced knowledge about digital technology at work to inform the government's Industrial Strategy and business practice.
2. Develop an analytical framework around the concept of the 'connected worker' and the 'connected economy'.
3. Provide a strong career development programme for mid and early career researchers.
4. Develop a Masters programme in HR Analytics.

The mechanisms employed to contribute to a shift in policy and practice will be achieved by building strong networks with relevant businesses, policy and civil society communities, through international visiting exchange fellowships, doctoral internships at government departments, with the Low Pay Commission and selected businesses. This impact strategy will enhance the long-term sustainability of the centre strengthening our capacity with a substantial body of mid- and early career researchers and embedding this knowledge in our teaching programme.

Policy makers, such as government departments (DWP, BEIS) and agencies (LPC, ACAS) and bodies such as the CIPD, will benefit from understanding the catalysts and barriers to business adoption of digital technologies at work and their consequences for productivity, job quality and employment effects. We will provide evidence for future policies. Businesses will benefit from understanding developments in their sectors, the facilitators and impediments, pitfalls and advantages of adopting digital technologies. This should lead to the consideration of which practices and government policies would be most beneficial.

Unions will benefit from monitoring trends and their consequences in different sectors. This will help to raise awareness among disparate unionised and non-unionised groups as to how policy could be developed to improve the quality of digital employment.

Community organisations will benefit from understanding the digital skills requirements for young people looking for work and for the self-employed. Our research and engagement will raise awareness amongst organisations of young people about the challenges they face and the skills they need.

International research organisations, such as Pew, Eurofound, ILO, will benefit from sharing knowledge on methods of how to measure and evaluate the consequences of the emerging digitalisation of work.

Immediate beneficiaries will be those who are participating on the advisory board who will be involved in the co-production of the research and interpreting the initial findings. Through their networks and involvement in dissemination, this will have a medium to longer-term impact on their broader communities, both within the UK and internationally. Advisory Board members have been closely consulted in the preparation of the bid and represent a selection of some of the leading businesses and public sector organisations affected by the digital transformation of work, alongside trade unions, key international research organisations and academic advisors. The board will advise on the design of the employer survey, facilitate access to organisations for the sector case studies and help with dissemination of the results. They will annually review and advise on the output from the centre. 

The Impact and Communications Officer will facilitate communications about the centre from its inception. This work will be supported in year 2 onwards by a Media Fellow (professional journalist) who will write policy-focused summaries of our research findings in a widely accessible form. This will attract additional interest from a broader community in the medium to long-term who will be able to register for newsletters and access multi-media resources via the website and the Data Observatory."
18,A5468BF3-B20E-4CB7-B889-6A40B0E13F71,Learning an urban grammar from satellite data through AI,"This project will propose an urban grammar to describe urban form and will develop artificial intelligence (AI) techniques to learn such a grammar from satellite imagery. Urban form has critical implications for economic productivity, social (in)equality, and the sustainability of both local finances and the environment. Yet, current approaches to measuring the morphology of cities are fragmented and coarse, impeding their appropriate use in decision making and planning. 

This project will aim to: 1) conceptualise an urban grammar to describe urban form as a combination of &quot;spatial signatures&quot;, computable classes describing a unique spatial pattern of urban development (e.g. &quot;fragmented low density&quot;, &quot;compact organic&quot;, &quot;regular dense&quot;); 2) develop a data-driven typology of spatial signatures as building blocks; 3) create AI techniques that can learn signatures from satellite imagery; and 4) build a computable urban grammar of the UK from high-resolution trajectories of spatial signatures that helps us understand its future evolution.

This project proposes to make the conceptual urban grammar computable by leveraging satellite data sources and state-of-the-art machine learning and AI techniques. Satellite technology is undergoing a revolution that is making more and better data available to study societal challenges. However, the potential of satellite data can only be unlocked through the application of refined machine learning and AI algorithms. In this context, we will combine geodemographics, deep learning, transfer learning, sequence analysis, and recurrent neural networks. These approaches expand and complement traditional techniques used in the social sciences by allowing to extract insight from highly unstructured data such as images. In doing so, the methodological aspect of the project will develop methods that will set the foundations of other applications in the social sciences.

The framework of the project unfolds in four main stages, or work packages (WPs):

1) Data acquisition - two large sets of data will be brought together and spatially aligned in a consistent database: attributes of urban form, and satellite imagery.
2) Development of a typology of spatial signatures - Using the urban form attributes, geodemographics will be used to build a typology of spatial signatures for the UK at high spatial resolution.
3) Satellite imagery + AI - The typology will be used to train deep learning and transfer learning algorithms to identify spatial signatures automatically and in a scalable way from medium resolution satellite imagery, which will allow us to back cast this approach to imagery from the last three decades.
4) Trajectory analysis - Using sequences of spatial signatures generated in the previous package, we will use machine learning to identify an urban grammar by studying the evolution of urban form in the UK over the last three decades.

Academic outputs include journal articles, open source software, and open data products in an effort to reach as wide of an academic audience as possible, and to diversify the delivery channel so that outputs provide value in a range of contexts. The impact strategy is structured around two main areas: establishing constant communication with stakeholders through bi-directional dissemination; and data insights broadcast, which will ensure the data and evidence generated reach their intended users.",,"This project takes impact very seriously and, accordingly, has planned a careful strategy to maximise the range of actors and the extent to which they will benefit from its outputs. Ultimately, the project will help to study, plan and manage cities in the UK. Its output portfolio includes academic deliverables, such as active participation in world conferences and publication of articles in internationally renowned peer-reviewed journals, as well as a wide range of items, such as engagement workshops, open source software and data products, specifically targeted at non-academic actors. 

There are four key (non-academic) stakeholder groups who will benefit directly and indirectly from this project: first, local governments, which are in charge of shaping policies that affect the way activities are spatially distributed within their boundaries; second, central government, which needs consistent measures across the country to assess both the global evolution of the urban system, and to which extent different cities are changing in different ways following systematic patterns (e.g. north-south divide); third, national data organisations such as the Ordnance Survey and the Office of National Statistics, whose primary mission is to develop evidence and products that collectively inform and measure different aspects of society and environment in the UK; and, fourth, the general public, for the majority of whom cities are their home, and are interested in better understanding how the building blocks that make them up are distributed over space and change over time. Please see the &quot;Academic beneficiaries&quot; section for a detailed list of beneficiaries within the broader scientific community.

The research will be relevant to these beneficiaries through the following three channels:

1) By providing much needed, timely and detailed evidence about the nature of urban form in the UK and its evolution over time. 
2) By driving better decisions about how cities are planned and managed, made possible thanks to the combination of data and insights delivered through appropriate channels to the relevant stakeholders.
3) By enabling a better understanding of the structure and form of cities, as well as how they evolve over time, preparing us to design better future cities.

To ensure relevant stakeholders have the opportunity to benefit from the project, the PI has designed a careful impact plan structured along two main dimensions: data insights broadcasting, and bi-directional dissemination. Through a diverse range of delivery channels (open data products, open source software packages, local government atlases, policy brief), the project will broadcast outputs of the project of direct relevance to non-academic stakeholders, including both direct evidence (e.g. data) as well as insights from analysis carried out that might be of interest to decision making (e.g. local atlases, policy brief). In parallel, a series of events designed to foster interaction and generate collaboration will ensure that findings and research in the project is co-designed with stakeholders, and that there is a constant channel of bi-directional communication that keeps beneficiaries informed and the project relevant."
19,6EDBFBBF-A0B6-4E26-89B4-63F74E716812,HEAP: Human-Guided Learning and Benchmarking of Robotic Heap Sorting,"This project will provide scientific advancements for benchmarking, object recognition, manipulation and human-robot interaction. We focus on sorting a complex, unstructured heap of unknown objects --resembling nuclear waste consisting of a set of broken deformed bodies-- as an instance of an extremely complex manipulation task. The consortium aims at building an end-to-end benchmarking framework, which includes rigorous scientific methodology and experimental tools for application in realistic scenarios. 
Benchmark scenarios will be developed with off-the-shelf manipulators and grippers, allowing to create an affordable setup that can be easily reproduced both physically and in simulation. We will develop benchmark scenarios with varying complexities, i.e., grasping and pushing irregular objects, grasping selected objects from the heap, identifying all object instances and sorting the objects by placing them into corresponding bins. We will provide scanned CAD models of the objects that can be used for 3D printing in order to recreate our benchmark scenarios. Benchmarks with existing grasp planners and manipulation algorithms will be implemented as baseline controllers that are easily exchangeable using ROS. 

The ability of robots to fully autonomously handle dense clutters or a heap of unknown objects has been very \textit{limited} due to challenges in scene understanding, grasping, and decision making. Instead, we will rely on semi-autonomous approaches where a human operator can interact with the system (e.g. using tele-operation but not only) and giving high-level commands to complement the autonomous skill execution. The amount of autonomy of our system will be adapted to the complexity of the situation. We will also benchmark our semi-autonomous task execution 
with different human operators and quantify the gap to the current SOTA in autonomous manipulation. Building on our semi-autonomous control framework, we will develop a manipulation skill learning system that learns from demonstrations and corrections of the human operator and can therefore learn complex manipulations in a data-efficient manner. To improve object recognition and segmentation in cluttered heaps, we will develop new perception algorithms and investigate interactive perception in order to improve the robot's understanding of the scene in terms of object instances, categories and properties.",,nA
20,8938613B-F64A-462E-910E-9D835539504E,Ensuring the benefits of AI in healthcare for all: Designing a Sustainable Platform for Public and Professional Stakeholder Engagement,"Japan and the UK are both investing heavily in national programmes to accelerate the implementation of AI in healthcare delivery (1,2,3). The perceived benefits of this technology include increased efficiacy and cutting costs by using software and algorithms to 'identify patterns too subtle to be detected by human observation, and to use those patterns to generate accurate insights and inform better decision making (4,5). While the use of AI in healthcare promises to be transformative, its ability to approximate conclusions without direct human input using large datasets of personal information, raises a number of concerns about responsibility, transparency and accountability, and public acceptance (6,7). 

The use of AI in clinical settings challenges the principles that underpin healthcare delivery: the patient's trust in a system governed by professional codes of conduct; the belief that the primary concern of healthcare is to safeguard well-being, and the subsequent legal and regulatory safety nets that have evolved (8). Expert reports have identified a number of issues surrounding the use of AI, such as: privacy concerns associated with access to clinical data; how it might influence people's legal right to know how decisions about them are made; and the possibility of discrimination of vulnerable populations due to database and algorithm implicit biases (8,9). In the UK, concerns have already been expressed about whether access to publicly-generated data by AI companies might erode public trust in the National Health Service (NHS), potentially jeopardising wide-spread adoption of AI and undermining the basic tenets of healthcare practice (1). 

For these reasons, a number of reports have recommended that patients and wider public, who will be most impacted by these technologies, are central to the future development of AI and involved in the design, implementation and governance of software for healthcare, as part of a co-design process (8), that will improve success rates and enhance patient-centred care. To date, very few studies have focused on public concerns regarding AI implementation, or the best strategies for engaging not just with patients, but healthcare professionals and wider publics. What is needed is an engagement platform to support a sustained dialogue with a broad range of stakeholders, so the implementation of AI is in accordance with changing public concerns and for the benefit of humanity. 

This research will be situated in research hospitals in the UK and Japan that are pioneering the use of AI systems, to allow us to identify effective models for sustained dialogue and engagement that can inform policy recommendations for future use of AI in healthcare. We will co-design an innovative programme of research to elicit the views of stakeholders, including patients and the public regarding: 1) the current and anticipated use of AI in treatment, diagnostic decision-making and precision medicine 2) the issues that stakeholders perceive will influence the adoption and implementation of AI in healthcare; 3) the types of engagement mechanisms, safeguards and regulatory controls they would like to see in place; and 4) how to develop a platform for engagement that can address issues of trust, responsibility, accountability and transparency, and influence normative practice. This will provide insights for both countries, leading to better sustained public dialogue, as well as informing global policy-making.",,"Our research will have considerable impact as it will identify effective models for sustained dialogue and engagement that will benefit a range of stakeholders by bringing different constituents together to stimulate the creation of an interdisciplinary, multi-sectoral ecosystem.

Innovators and industry - beneficiaries include non-academic researchers, technologists and firms (SMEs and large companies) with an interest in developing, translating and commercialising AI to infer health-relevant information and products and services derived from them. Research findings will have impact by i) informing early stage development of AI technologies by providing insight into the needs and valuation practices of a variety of stakeholders and ii) enabling emergent business models to take into account public views, that will be directly relevant to product design. The mapping of the implementation landscape in the UK and Japan will also provide valuable guidance on the legal and ethical requirements and responsibilities of service providers.

Policy and regulation - beneficiaries will include agencies and regulatory actors responsible for the governance of AI and healthcare systems; lawyers and data protection officers involved in the regulation of personal information online; policy makers promoting digital innovation in healthcare and the potential social and economic benefits of data-driven health research, and politicians and activists involved in public debates about personal health data. Research findings will have an impact by i) providing an evidence-base on the social and organisational implications of current research trajectories, translational pathways and challenges to implementation of AI in health, including the valuation practices that are applicable for different applications of AI, ii) Highlighting significant dimensions of the development and implementation pipeline of AI systems for health, particularly relating to data needed, to inform policy and regulatory decision-making, iii) Informing policy and legislative consideration of the governance of online health-relevant information and its use, iv) Sharing findings and best practice between the countries involved (UK and Japan), highlighting priorities across each country and outlining transnational differences in public preference and legal frameworks, v) Providing an evidence base about the public, legal and ethical underpinnings of a trustworthy and robust governance system for digital sharing of sensitive information.

HCPs and managers - HCPs and health service managers will be represented in expert workshops and data generation phases of the project. Research findings will have an impact by i) ensuring that development of AI systems for healthcare reflect the needs and values of HCPs, ii) encouraging HCP participation in debate about the value and challenges of integrating AI systems to healthcare, iii) informing HCPs about the values at stake when health data is shared with or received from external developers of AI, and the challenges associated with implementation of AI systems. The project will also have impact on HCPs indirectly, through increased public awareness and discussion of the benefits and challenges of utilising data collected outside of healthcare treatment and diagnosis.

Patients, citizens and publics - beneficiaries include patients, charities and patient/citizen organisations. Recruited patients and citizens of both countries will share their perspectives on the use of health-related data in the empirical research. Research findings will impact by i) ensuring that development of AI systems for healthcare reflect the needs and values of patients and citizens variously located in society, ii) informing and encouraging public participation in debate about the value and challenges of integrating AI systems to healthcare and more broadly on permitting data collected outside of healthcare to be used in treatment and diagnosis."
21,7048423A-7F11-4418-810B-97B70810055B,Sparse Bayesian reconstruction for optimal facial recognition,"Near 100% accuracy is a necessary condition for facial recognition to become more widely utilised in security and to be more widely accepted by users. Existing techniques in the field of computer vision have taken us to unprecedented levels of accuracy. This project aims to close the final gap in accuracy using cutting-edge image analysis techniques developed by astronomers.

Traditional facial recognition software acts directly on a computer processed image to determine an identity. Using a direct image as input has bonuses such as simplicity, but drawbacks in that many theoretical and practical questions are difficult to pose on a pixel-based representation. 

In a more established image analysis practice, one traditionally acts on a representation of the image that is more specific to the object of study, in this case the human face. In this compressed representation it becomes easier to ask more specific questions.

This project seeks to apply the cutting edge of such sparse Bayesian representations to the field of facial recognition, using the state-of-the-art in inference algorithms to determine the optimal and minimal unbiased representation of a human face. This project will be able to provide an orthogonal approach to currently applied techniques and could be the missing link in closing the final gap toward 100% accuracy.",,
22,25A77D8D-6732-41BA-8004-1F44C85ED81F,Neuromorphic memristive circuits to simulate inhibitory and excitatory dynamics of neuron networks: from physiological similarities to deep learning,"Why does the human brain not operate as a computer? Both use logic and numbers when operating. Nevertheless, the human memory is much more distributed (pattern-like) in contrast to localised computer bit-memory, has time decay (clogging), changes &quot;wiring&quot; when trained and is designed to process information signals (excitation waves) rather than just store bits in different memory locations as a computer does. Although modern computers significantly numerically outperform the human brain, they still cannot handle tasks requiring guessing and fuzzy logic.

This explains a booming interest in AI systems trained to perform certain tasks infeasible for numerical simulations. Currently AI technology is driven by two distinct goals: (i) technological demand (autonomous vehicles, online trading, AI for medical imaging analysis etc.) and (ii) an attempt to create an electronic brain with the ability to think, feel and interact with humans. Using different learning algorithms, AI has demonstrated abilities comparable to, or even outperforming, the human brain in several strategic and decision-making tasks (e.g., the game of Go). However, it is unclear how/if AI algorithms relate to information processing and the corresponding psycho-physiological processes in the brain. Answering this big question would help in not only making machines with human abilities but also elucidating whether a brain can be reduced to a biologically-wired electric circuit only or it has something beyond simple electric/chemical functionalities.

To truly emulate information processing in the brain (neuromorphic computing), a new generation of computer architecture should be developed. One of the most promising technologies for neuromorphic computing and signal processing is based on memristors, where resistance is tuned (e.g., switching between two states) by total charge passed through the system (e.g., resistance depends on applied electric pulse sequence/history which can encode an information signal propagating through the brain). Using Loughborough's expertise in solid state physics, functional materials, thin films, modelling, and AI, in synergy with a world-leading centre of neuromorphic research (the University of Massachusetts, Amherst) and neuroscience/physiological expertise (Salk Institute for Biological studies), and driven by the demand of UK industrial partners, we intend to develop a prototype of a memristive neuromorphic chipset able to analyse image-streams and to make decisions and choices by mimicking neural process in a brain cortex.

Inspired by biological neuron operation, and the deep learning AI paradigm, we propose to develop an electric circuit operating via two competing processes:
(1) Intermixing or interfering electric signals generated by visual stimuli at different time moments (e.g., subsequent video frames) and;
(2) Transmitting signals from one circuit layer to another in order to extract the main visual features/concepts.
A combination of these processes for image-stream analysis has never been considered before for neuromorphic systems and is the main novelty of the proposed research. 

This allows us to compare image frames in a video and to reduce the complexity of the information towards a binary decision (choice). This neuromorphic two-process concept has a clear brain-functioning analogy: sensory stimuli excite a myriad of receptors generating superimposing signals, an end effect of which can be expressed by a short statement of recognition (&quot;It's my Mom&quot;) or discrimination (&quot;It was a car not a bike&quot;). Cycles of interfering and convolving information followed by binary choice seems particularly well fit to memristor layered architecture where initial complex voltage-pattern encoding image stream reduces to switching or not of a certain memristor (signalling which decision/choice is made) in a deeper layer of the structure. The developed prototype will be the first memristive realisation of a visual cortex.",,"Various independent reports from the year 2017 estimate the effect of the future developments in artificial intelligence (AI) as potentially increasing the UK GDP by 10.3% by 2030, as well as in higher annual economic growth rate by 2035 - 3.9% instead of 2.5%. These ambitious plans can be achieved only if the AI progress is supported by growing AI researcher workforce as well as a capability and capacity in computational power, availability of new hardware and software, and novel/unconventional AI applications.

Capability and Capacity: The UK lags behind the US and China in investments into AI start-ups as well as behind the US and Europe in governmental support for research in AI. Recent hardware advances, such as GPUs, cloud computing and the multi-core CPU, have pushed forward AI and AI applications during the last decade. However, the future AI advancements in image processing, autonomous systems, object tracking, artificial decision-making etc., require a fundamental breakthrough in computational principles with no separation between the processing unit and memory. One of the most challenging directions of AI research is the development of electronic chips that use the architecture and elements mimicking neuron functionality. This offers not only new computational paradigms of information signal processing, but also a tool to study the brain at the level never achieved before. Indeed, due to the complexity of neural tissues, mathematical modelling of neural processes has a rather limited predictive power, while biological experiments face many ethical problems slowing down neuroscience research. Using neuromorphic hardware will enable the technology to control and enhance brain ability as well as a further progress in the understanding of brain disorders (testing, e.g., a hypothesis of excitatory-inhibitory imbalance nature of neurodevelopmental disorders by tuning inhibition balance in memristive circuits), enormously impacting mental health. One of the most promising technologies for brain emulators is based on memristors, thus, transferring this technology to the UK can create a nationwide impact and can enhance the capability for future AI advances in the UK.

Workforce: The project will allow training a new generation of AI-neuromorphic scientists and engineers contributing to future AI researcher labour force. The researchers and students involved in the project will be trained to fully understand AI tasks and market, the postdocs will be able to design their own neuromorphic hardware or to improve existing chips for specific tasks in mind and to programme this hardware in the most efficient way. Thus, the project will fill a gap in the workforce with both AI hardware and software expertise.

UK industry focus: To make our research industry-driven we plan to use the neuromorphic layered (deep learning) memristive chip for:
1. Large size image and video analysis (main industrial partner: ARM)
2. Pattern and motion recognition and object tracking (main industrial partner: ARM)
3. Decision-making for robotics, unmanned aerial vehicles and driverless vehicles (main industrial partner: HP)

Society: The general public will benefit from enhanced awareness about future AI opportunities and challenges. We will disseminate the project results to the end-users. Future AI technology is recognised by the public as a great opportunity to push further the intellectual ability of the whole civilisation and each individual by combining the human brain ability with modern computational electronics. We plan to inform the general public about the progress of the project via maintaining a project website, developing an outreach programme &quot;Computers watching movies&quot; with two public lectures per year, and an online neuromorphic AI contest for A-level and UG students where students will be invited to program neuromorphic chipsets remotely (online coding). This will also help in recruiting the best PhD applicants worldwide."
23,6FF8B38B-1B26-4EAD-A56B-BFC045433BED,Smart Environments Research Facility,"This multifunctional equipment request will contribute to a Smart Environments Research Facility at UEA and builds on EPSRC-facing research within the School of Computing Sciences at UEA and is made in the context of 3 of the Grand Challenges of the UK Industrial Strategy, namely (i) artificial Intelligence and data, (ii) ageing society and (iii) the future of mobility. The equipment requested will enhance the laboratory facilities in direct support of the research activities of established and early-career researchers and PhD students from across the School of Computing (CMP), and in support of projects linking with Norwich Medical School (MED) and the School of Environmental Sciences (ENV).

The equipment requested will enable the cohort of researchers to further develop and leverage their research with organisations such as Turing, BT , Quadram Institute and others linked to existing EPSRC Research Grants and position them to further develop new internationally leading proposals to EPSRC. A value add will be promotion of the Working Together agenda in support or research linked to joint EPSRC-NERC and EPSRC-BBSRC collaborations. In addition, we will also target opportunities under the joint EPSRC-Science Foundation Ireland Programme.

The facilities requested will enable us to generate more complex experiments that require increased multi-core compute, storage and higher speed network connections to generate better quality data and further our understanding of algorithmic performance and prototype models. Our five research laboratory groups in the School will benefit as follows and further examples are provided in the attached Case for Support and Appendices.",,"The equipment we have requested and listed in the JoR will have immediate impact on PhD students and PDRAs working on projects in the School linked to companies such as BT and Aviva. This includes projects to further develop research by ECRs who have been working with the Turing Institute on the &quot;sktime&quot;: A toolbox for data science with time series classification. They will benefit from having access to higher performance systems with increased compute and storage capacity to support multiple parallel experiments that generate higher volumes of data to stress test their algorithms. Across the timeline of key activities as indicated in the workplan we will ensure that early stage impact is achieved by bringing systems on-line in the first 3 months of the project roll-out and then to maintain them beyond the period of the grant.

UEA School of Computing Sciences (CMP)
We have a track record or organising various platforms to promote wider collaboration and knowledge of our research. Going beyond the traditional, CMP (GP) has organised &quot;Birds of a Feather&quot;(BoF) meetings with scientists on the Norwich Research Park who are engaged with large-scale experiments for BBSRC and NERC that generate significant amounts of multi-faceted research data linked to animal, plant and human health. Once our new equipment has been commissioned and becomes operational, we will seek to hold bi-annual BoF meetings to highlight our joint research and also seek news opportunities for further collaboration.

Knowledge, People, and Skills: 
Key skills and insights will be linked to the design of algorithms and ensembles that will be able to cope with increased computational complexity and increased data payloads in real-time. Such insights will also impact on the compute/storage/networking resources required, but they will also include improved understanding of the energy-requirements of such systems.

A direct impact of the cohort's research to date is the training of PhD and PDRAs as the next generation of scientific researchers. Of these, several have already gone on to hold either academic or industrial research positions in companies such as BT, IBM, and Apple, benefitting the UK's knowledge economy and the ICT skills base for our Connected Nation. With enhanced equipment facilities, the cohort can continue to train our future scientists in a broader contact to address key scientific and engineering challenges such as providing trustworthy systems for low-cost and low power universal services provision for Connected-UK, taking into account the need for scalable solutions that are robust to attack and failure. In support of ECRs we will also explore development of specific training courses within the School for Data Science and AI tools that will support PhD and MSc researchers. These will be developed and organised during our operational phase. In particular, students our DTPs and new EPSRC CDT in Agri-Robotics and our new MSc in Cyber Security (developed in collaboration with BT Adastral Park, IBM and the Police) will benefit. 
We will also provide Mentoring to our PDRAs and ECRs to assist with preparing new grant proposals that can leverage the equipment from this award to support industry-relevant research. By introducing such initiatives we will address ICT priorities for the future skills pipeline in areas linked to the needs UK economy that have been identified in the UK Industrial Strategy, e.g. AI Tools and systems, Mobility, 5G, robust software engineering and data science."
24,F53A7F78-C7B2-4AD2-A8E6-C19EBC6A8FD3,"PATH-AI: Mapping an Intercultural Path to Privacy, Agency, and Trust in Human-AI Ecosystems","PATH-AI is a UK-Japan collaboration that will develop and pilot a methodology for inter-cultural co-design of a culture- and human-centric framework for a more ethical and equitable human-AI ecosystem. 

The project will rely on an experienced multidisciplinary international team of researchers to build the first of its kind intercultural co-designed framework of this sort.

Firstly, a comprehensive literature review and interview-based research on the normative (cultural, social, ethnographic) and institutional (legal, economic, political) dimensions of privacy, agency, and transparency (PAT) will clarify the UK- and Japan-specific views on these values. Specifically, the meaning, roles, and interrelationships between PAT within human-AI interactions will be considered, paying special attention to the fundamental role that PAT values have in furnishing other norms, politices, and laws that guide human-AI interaction.

Secondly, by juxtaposing the Anglo-European and Japanese traditions, the researchers will draw a comprehensive view of PAT in human-AI ecosystems, expanding or delimiting these concepts. Challenging the limits of current AI ethics discussions, we will extend AI ethics to a wider landscape of cultures, values, and societies and explore how these have funded the nature and dynamics of the human-AI ecosystems within the UK and Japan. 

Thirdly, the researchers will build an intercultural bridge for communication between British and Japanese citizens through the commissioning and international exhibitions of British and Japanese art, inspired by our comparative analysis of PAT in innovation ecosystems. By exchanging art pieces between the two countries, viewers will be confronted with different views of AI and their reactions and views will be studied. This will conclude Phase One of the project. 

In Phase Two, extensive stakeholder engagement within working groups will be initiated. Working groups will collaborate to co-design a unique culture-informed international governance framework for AI ethics that is based on the enriched UK and Japanese traditions of PAT vis-&agrave;-vis AI. Through this work, we will also identify policy shortcomings and make recommendations. This framework and policy recommendations will be launched at an international conference, setting the start for future UK-Japan AI ethics and governance dialogues.

Our work will benefit key stakeholders. The research carried out through this project will advance knowledge in the cultural, anthropopological fields by deepening understanding of and comparison between Anglo-European and Japanese representations and traditions regarding PAT, both between humans and between humans and inanimate objects. Our work will also bring key insights to political philosophy, law, and economics by reflecting how political, legal, and economic institutional structures adapt to different sociocultural views of PAT in practice. This will chart the path towards intercultural ethical studies, but also will highlight key lessons for future institution- and capacity-building that is to be based on and adapted to local norms and beliefs. 

PATH-AI will also build communication channels between policy-makers, civil society, research institutions, industry actors, and citizens within and between the UK and Japan. Through continuous and multisided stakeholder involvement in the research process, we will both enhance the quality of our research, but we will also build stakeholder connections that will survive beyond the project. Moreover, by clarifying the different, yet completentary British and Japanese cultures, we will build multual understanding and enrichment between the two nations at the level of both citizens and policy-makers. Ultimately, this will not only benefit intercultural dialogues, but will also facilitate future policy-making and regulation that will encourage ethical innovation and economic activity between the countries as well.",,"The research team has extensive interdisciplinary research experience that they will utilise to ensure the research project maximises its impact on multiple levels. A key advantage that our research team will rely on is our extensive experience providing consultation and advice to public authorities both in the UK and Japan. Among those whom we have contacts with are the Japanese Ministry of Internal Affairs and Communications, the UK Department for Culture, Media and Sport, the Centre for Data Ethics and Innovation, and the Office for AI in the UK. 

Policy-makers and civil society as beneficiaries
Cross-national and intercultural investigation into the key identified themes - agency, trust and privacy - is imperative in order to encourage progress towards a wider and more inclusive dialogue on AI governance, ethics, and regulation. Our project will lay the foundations for intercultural dialogue on AI governance by:
- Facilitating mutual understanding on the basis of cleare and accessible information about UK and Japanese culture
- Encouraging the formation of bonds between stakeholders through increased stakeholder engagement and working group co-design of an AI framework
- Highlighting and enriching knowledge on the policy and legal tools that are informed and influenced by privacy, agency, and trust and how these have economic, technical, and social reflections

Citizens as beneficiaries
Citizens will also benefit from our research. Individuals within the UK and Japan will be involved in the project through interviews, receiving the opportunity to be heard and directly contribute to an international effort for AI governance. In addition, citizens will be enriched through the PATH-AI experience by being presented with challenging new (and foreign to them) ideas about what human-AI interactions look like and how agency, privacy, and trust fit within that framework. This will be done by employing innovative interactive research and engagement strategies. This will also increase the awareness and capability of citizens to engage in AI ethics conversations in their private and public lives. Finally, citizens will benefit from the governance dialogues that PATH-AI can launch by having better regulation, policy, and AI-driven services and tools produced and representing their interests.

Artists as beneficiaries
This research will also benefit artists, those who enhance and engage with dialogues and raise awareness about AI in the society through the unique subjective emotional prisms that shape their creativity. Artists will be introduced to a new side of human-AI interaction that would enrich their artistic capacity and worldview by placing the modern technological revolution in the historical context of UK and Japanese cultural traditions, with their philosophical, social, and ethnographic complexities. Beyond this, artists will benefit from the opportunity to contribute directly to the intercultural dialogue between the UK and Japan through their art, adding their individual energy to the debate and enjoying the opportunity to exhibit their art internationally.

Economic actors as beneficiaries
Economic actors will benefit from the PATH-AI project by receiving greater clarity about the culturally-motivated particularities about consumer behavior, desire and demands in both the UK and Japan. By being more aware about what is socially acceptable and desirable in both countries, economic actors will better be able to guide and direct their investments towards innovation. In addition, thanks to the intercultural governance dialogue launched with PATH-AI, the international achievement of clarity with regard to AI regulation, standardization, certification, and even international trade will be facilitated and enhanced, leading to increased economic opportunities at closer timespans for such innovators, manufacturers, and entrepreneurs."
0,F139BA74-53E4-4CB2-8983-5D98CB1FA43F,University of Exeter - Core Capital 2019,"The University of Exeter has an excellent track record of supporting capital equipment on research grants submitted to EPSRC and other Research Councils and has developed internal processes to ensure high-quality applications are submitted to the EPSRC's Strategic Equipment Process (SEP). Our SEP portfolio is currently &pound;5.4M. This Core Equipment Award provides an excellent opportunity to invest in core equipment at Exeter to support the long-term competitiveness of our EPSRC-research portfolio. 
Our strategy for the allocation of funds was to run an open call for expressions of interest based on the criteria provided by EPSRC: Underpinning multi-user equipment; Invest to save; and Co-location of equipment. The scale of the institutional award, &pound;270k, allowed us to consider a breadth of proposals that would not be appropriate for an SEP and have a broader user base than would be expected on standard grants. The opportunity was made available to our full EPSRC-facing community.
A specially convened panel, following an internal call selected 5 items of equipment that will support a broad range of users, underpin science of national importance and that fits with the strategic direction of the host departments.",,"The University of Exeter's EPSRC Strategy Group (SG) panel, selected a suite of investments that covered research in engineering, computing science, and physical sciences. The collective impact of this award will benefit a wide range of researchers. 
The Strategy Group will monitor the set-up of these items and organise an annual showcase event to promote the new equipment and the access points for researchers to get involved. As part of the annual reporting, the Strategy Group will ask for user stats and look for case studies to be developed and shared on the University's website. Experience shows that the promoting of sharing is best supported at the local level, each item has a Principle Investigator to oversee this. 
The impact for each item has been articulated in the Pathways to Impact and the academic beneficiaries section, highlights are summarised below: 
E1. Structured illumination microscopy for fast volumetric imaging
This equipment will support disciplines across physics, engineering, and the biosciences, with an interest in optical microscopy techniques, investigations of biological and synthetic nanostructures. Of specific relevance to the Physics of Life and Healthcare Technologies themes is the need to understand the temporal behaviour of excitable tissues such as the brain and heart. 
E2. Graphical Processing Unit (GPU) cluster for Isca
GPUs vastly surpass traditional CPUs by performing calculations in a massively parallel way. They have become essential for new data science algorithms for handling large amounts of data and for &quot;deep&quot; neural network architectures: recent &quot;breakthroughs&quot; in AI have used GPU clusters and algorithms that could not run in practicable times on conventional computers. This equipment will benefit the AI and machine learning communities. In addition, GPUs are widely used in simulations and the solution of partial differential equations, e.g. flooding, continuum mechanics, molecular dynamics, etc: an Exeter cluster will thus enable new research across the remit of EPSRC.
E3. Wide-Area Nano physical vapour deposition (PVD) system to enhance advanced solar cell fabrication and materials growth facilities
The Exeter user groups solar, marine and mining reflect the impact areas this equipment will have by providing a materials fabrication facility that will complement their specific testing and characterisation techniques. The equipment will accelerate solar energy and energy storage research, through the development of new materials to enhance energy conversion and energy storage efficiency. 
E4. Controlling multistability in vibro-impact systems: theory and experiment
The equipment will underpin research that aims to develop control strategies for improving system performance and energy efficiency. Impact will in Non-Linear Systems, Control Engineering, Structural Engineering, Robotics, and Water Engineering. Examples include energy harvesting for structural health monitoring. 
E5. New pump laser to extend the lifetime of multiphoton system
Current research in biomedical physics and biomedical engineering uses multiphoton microscopy systems such as this to understand the structure and physical properties of biological tissues and cells in health and disease, including the degeneration of intervertebral disc, the progression of osteoarthritis and the response of tumour cells to immunotherapies. Maintaining the multiphoton system at Exeter with this new laser will allow this research to continue and expand into EPSRC research areas such as 'Biophysics and Soft Matter Physics', 'Assistive technology, Rehabilitation and Musculoskeletal Biomechanics' and 'Biomaterials and Tissue Engineering'."
1,A4279AB6-AA19-46A8-B7ED-08B6FFA4AB5F,The Future of Unpaid Work: AI's potential to transform unpaid domestic work in the UK and Japan,"The &quot;Future of Work&quot; has attracted much attention in recent years. Significant efforts are directed at understanding the implications of new technologies for the future of employment and training. In contrast, the future of unpaid domestic work has received little attention. This is although working-age adults in the UK spend about 56% of all paid and unpaid work time on household and care work; the figure for Japan is 38%. AI-powered technology replacing human labour in domestic tasks can potentially free up large amounts of time for men and women of all ages. It also has the potential to make the time spent on the domestic work that is hard to automate, or that people want to do themselves (e.g. interactive childcare or eldercare), more efficient and help consolidate it into fewer episodes. This project will bring unpaid domestic work into the discussion of AI and the future of labour and assess its implications in two socially and culturally quite distinct countries. 
To do this we will consider multiple factors that will influence the diffusion of AI-powered time-saving domestic technology in three distinct Work Packages (WP). In WP1 we will first evaluate the technological likelihood of automatibility of domestic work tasks using a grid of around 50 such tasks identified in the UK Time Use Survey 2014-15, to which we will apply a modified version of the widely used Frey-Osborne approach to measuring automatibility. We will then use an expert panel to assess how quickly AI-powered domestic technologies will become not only technologically possible, but also affordable for households. 
In WP2 we will address social factors affecting the adoption of intelligent machines at home. We will first analyse the case of shopping, commonly carried out offline or online with the help of AI-powered apps. UK and Japan are global leaders in e-commerce, making online shopping in these countries a good test case to analyse how performing a household task with AI-powered technology may differ from performing it without it. How does AI-powered technology affect daily time use and the rates of participation in the activity by different members of the household? We will then carry out an experimental vignette survey to evaluate the acceptability of outsourcing a range of domestic tasks to AI-powered technology. Our vignettes will describe a fictitious family situation in which a given domestic task is performed. Across the vignettes, we will randomly vary a number of core factors such as type of task, family income, who performs the task etc. The vignettes will be supplemented by survey questions asking how decisions to adopt domestic technology are made when family members disagree about it.
WP3 involves simulations of likely changes in unpaid work time for men and women of different ages using the methodology used in National Time Transfer Accounts (NTTAs). NTTAs use time use data to show how people of different age groups and gender produce and consume time spent for various kinds of unpaid work in households, estimating net time transfers between such groups on the national level. We will estimate future scenarios of domestic task automation and evaluate future time transfers across genders and between generations in UK and Japan factoring in expected population change and automation. We will rely on automatibility, affordability and acceptability scores developed in WP1 and WP2. We will also develop a scenario in which automation of domestic tasks will result in changes similar to those brought about by AI-powered shopping technology as analysed in WP2. 
Our analysis will bring invisible domestic labour that today is largely performed by women into the &quot;Future of Work&quot; debate. It will enrich our understanding of how time, the scarcest resource we have, will be influenced by AI-powered technologies at home.",,"Relying on AI-powered technology to replace or facilitate domestic work will reduce the amount of time spent engaged in domestic tasks, and it is anticipated that this will affect woman more than men, given the current gender inequality in the time invested in household tasks. On an individual level, changes in the patterns of domestic work participation may lead to improvements in an individual's quality of life. On the macro level, outsourcing domestic work to AI-powered technology may restart the stalled gender revolution in the domestic sphere, and alleviate the problems associated with ageing societies. This project aims to provide concrete predictions about the ways in which automation in the home will unfold. 

We believe that our research has the potential to impact policy in the areas of labour regulation, social care and gender equality, and therefore is of interest to NGOs, research organisations and policy makers operating in these spheres. Technological innovation is initiated and designed by men, and marketing focus-groups are similarly male-dominated. Through the dissemination of our research we intend to make the female voice heard, especially as females are ordinarily the primary home-makers. Consequently, we anticipate that our research will lead to corporate practice change, particularly in the areas of product design, marketing and distribution.

Though our project team members are based in the UK and Japan, it is hoped that we will be able to extend the reach of our impact beyond these countries and into Europe, where Hertog and Lehdonvirta have collaborative networks of academics and research professionals, as well as contacts with regional NGOs. Hertog's and Lehdonvirta's contacts within the European Commission and the International Labour Organization may be open to receiving presentations and this may pave the way for more widespread dissemination activities throughout Northern Europe. Nagase and Hertog also have extensive contacts with academics, policy-makers, and research think-tanks in East Asia, and therefore there are impact possibilities therein also. 
Finally, our findings have the potential to impact the product strategies and research outlook of AI and technology companies by highlighting the impact of economic and social factors on domestic AI adoption. 

Our pathways to impact are explained in attached documentation, but in summary we intend to:
- Create an impact focus group comprised of gender-balanced corporate, policy and domestic actors, to advise on the execution of impact activities, help direct the tone of policy brief materials, and to act as a sounding board for proposed pathway activities. 
- We will run a number of workshops with NGOs, policy-makers, research organisations and corporate representatives to disseminate our research. These workshops will have evaluation measures built into their composition (see pathways attachment for details).
- Table-top workshop events will be held with local pre-university students, with female students particularly targeted, to instil career aspirations in STEM fields and raise awareness of gender-based technology issues.
- Engage with social media and press to raise awareness of findings and issues to general, non-specialist audiences."
2,E09A922A-6DA3-4E02-BC84-2E9066BCFDD3,Rule of Law in the Age of AI: Principles of Distributive Liability for Multi-Agent Societies,"The UK and Japan appeal to similar models of subjectivity in categorizing legal liability. Rooted historically and philosophically in the figure of the human actor capable of exercising free will within a given environment, such a model of subjectivity ascribes legal liability to human agents imagined as autonomous and independent. However, recent advancements in artificial intelligence (AI) that augment the autonomy of artificial agents such as autonomous driving systems, social robots equipped with artificial emotional intelligence, and intelligent surgery or diagnosis assistant system challenge this traditional notion of agency while presenting serious practical problems for determining legal liability within networks of distributed human-machine agency. For example, if the accident occurs from cooperation between human and an intelligent machine, we do not know how to distribute legal liability based on current legal theory. Although legal theory assumes that the autonomous human agent should take the responsibility of the accident, but in the case of human-intelligent machine interaction, human subjectivity itself is influenced by the behavior of intelligent machines, according to the findings of cognitive psychology, of the critical theory of subjectivity, and of the anthropology of science and technology.
 This lack of the transparent and clear distributive principles of legal liability may hamper the healthy development of society where human dignity and technological innovation can travel together, because, no one can trust the behavior and quality of the machine, that may cause corporal or lethal injury, without workable legal liability regime. 
Faced with this challenge, that is caused and will be aggravated by the proliferation of AI in UK and Japan, an objective of our study is to make the distributive principle of legal liability clear in the multi-agent society and proposing the relevant legal policy to establish the rule of law in the age of AI, that enables us to construct the &quot;Najimi society&quot; where humans and intelligent machines can cohabit, with sensitivity to the cultural diversity of the formation of subjectivity. 
In order to achieve the objective above, we create the three interrelated and collaborative research groups: 
Group 1: Law-Economics-Philosophy group that proposes the stylized model to analyze and evaluate the multi-agent situation, based on dynamic game theory connected to the philosophy of the relativity of human subjectivity, in order to figure out the distributive principle of legal liability and the legal policy for the rule of law in the age of AI, based on both the quantitative data and qualitative data from the other groups, with the support from experienced legal practitioner and policy makers.
Group 2: Cognitive Robotics and Human Factors and Cognitive Psychology group that implements various computer simulation and psychological experiments to capture data on human interaction and performance with as well as attidues and experience of intelligent machines - in this case (simulated) autonomous vehicles. The outputs of this group will examine the validity of the first group's model and provide mainly the quantitative data relating to subjectivity with the first group, leading to help to construct more reliable model and workable legal principles and policies.
Group 3: Cultural Anthropology group that engages in comparative ethnographic fieldwork on human-robot relations within Japan and the UK to better account for the cultural variability of distributed agency within differing social, legal, and scientific contexts. The output of this group will help the interpretation of the quantitative data and allow the first group to keep sensitivities to the diversity.
 By the inherently transdisiciplinary and international cooperation described above, our project will contribute to make UK and Japanese society more adoptive to emerging technology through clarifying the legal regime.",,"Our novel, cutting edge, cross cultural research will have significant social impact domestically, internationally and globally that makes our current society more adoptable to emerging many intelligent technologies. There are multiple pathways to impact:
 1. Publish our academic achievements as articles within high impact influential international journals. Such publications will increase the possibility that our proposal of the behavioural findings, legal system and fundamental theory will be adopted by other researchers and policy makers.
 2. Social promotion of our academic project. To gain the public acceptance and understanding of our project, we will hold public symposia or other social activities aimed at public acceptance. We will frame these activities in such a way that the audience's experience in this activity will help us to promote our project. Some research members of our project have been engaging in the social activity that aims to help public to understand the potential benefit and cost of emerging technology. For example, some participated as a panelist of public forum that treats potential accidents caused by human-machine interaction. Others held a philosophy caf&eacute; or workshop to give citizens a chance to reflect their perception of human and machine and to raise a critical view to emerging technology and society. These activities may not seem to have a huge impact on society, but continuous effort to include public into scientific circle should have considerable social impact in long term. Based on the achievements of our research, we will engage in these activities to make a social impact.
 3. Interactive education with engineers who build intelligent machines in private or public sectors. Our cultural anthropological team has a rich experience to make communications with front line engineers to raise their critical sense to design the interactive machines, that may influence the subjectivity of the users. They also hold a workshop to critically reflect the influence of diverse interaction between human and machine. These activities may influence and change the mindset of engineers on site and lead to potentially huge social impact on the actual practice of building and designing intelligent machines. And we all are sure that our academic achivement of our research project will enlarge and support these activities that will change the whole society more adoptive to the emerging technologies. For instance, the Cardiff group will act via various research centres and groups that they lead/co-lead, have strong industrial connection with over 40 external organisations.
 4. Make a continuously operating international platform between UK and Japan to exchange stimulating ideas among participants. We will make an international platform that may become future foundation of international academic activity and policy making regarding to intelligent machines. Our research team is already inherently international, and further cooperation with international researchers or practitioners may not only enrich our research project but also increase the social impact of our research. We are reflecting the possibility of holding an annual international research conference or workshop in order to invite many international AI and related experts with diverse backgrounds.
 5. Propose concrete policies regarding to the legal regulation and legal system of intelligent machines, initially amongst lawfirms that we are conneceted with such as AXA, Burges Salmon, Nagashima, Nishimura and Asahi partners, Ohno, and Tsunematsu as well as the Japan Ministry of Economy,Trade and Industry, together with many thousands of employees and connections. As we include experienced legal practitioners and policy makers into our project, we can profit from their knowledge to make new laws and workable legal system."
3,E5763518-5369-4CBB-9516-4049F64DF2F4,Legal Systems and Artificial Intelligence,"A World Economic Forum meeting at Davos 2019 heralded the dawn of 'Society 5.0' in Japan. Its goal: creating a 'human-centred society that balances economic advancement with the resolution of social problems by a system that highly integrates cyberspace and physical space.' Using Artificial Intelligence (AI), robotics and data, 'Society 5.0' proposes to '...enable the provision of only those products and services that are needed to the people that need them at the time they are needed, thereby optimizing the entire social and organizational system.' The Japanese government accepts that realising this vision 'will not be without its difficulties,' but intends 'to face them head-on with the aim of being the first in the world as a country facing challenging issues to present a model future society.' The UK government is similarly committed to investing in AI and likewise views the AI as central to engineering a more profitable economy and prosperous society.

This vision is, however, starting to crystallise in the rhetoric of LegalTech developers who have the data-intensive-and thus target-rich-environment of law in their sights. Buoyed by investment and claims of superior decision-making capabilities over human lawyers and judges, LegalTech is now being deputised to usher in a new era of 'smart' law built on AI and Big Data. While there are a number of bold claims made about the capabilities of these technologies, comparatively little attention has been directed to more fundamental questions about how we might assess the feasibility of using them to replicate core aspects of legal process, and ensuring the public has a meaningful say in the development and implementation.
 
This innovative and timely research project intends to approach these questions from a number of vectors. At a theoretical level, we consider the likely consequences of this step using a Horizon Scanning methodology developed in collaboration with our Japanese partners and an innovative systemic-evolutionary model of law. Many aspects of legal reasoning have algorithmic features which could lend themselves to automation. However, an evolutionary perspective also points to features of legal reasoning which are inconsistent with ML: including the reflexivity of legal knowledge and the incompleteness of legal rules at the point where they encounter the 'chaotic' and unstructured data generated by other social sub-systems. We will test our theory by developing a hierarchical model (or ontology), derived from our legal expertise and public available datasets, for classifying employment relationships under UK law. This will let us probe the extent to which legal reasoning can be modelled using less computational-intensive methods such as Markov Models and Monte Carlo Trees.

Building upon these theoretical innovations, we will then turn our attention from modelling a legal domain using historical data to exploring whether the outcome of legal cases can be reliably predicted using various technique for optimising datasets. For this we will use a data set comprised of 24,179 cases from the High Court of England and Wales. This will allow us to harness Natural Language Processing (NLP) techniques such as named entity recognition (to identify relevant parties) and sentiment analysis (to analyse opinions and determine the disposition of a party) in addition to identifying the main legal and factual points of the dispute, remedies, costs, and trial durations. By trailing various predictive heuristics and ML techniques against this dataset we hope to develop a more granular understanding as to the feasibility of predicting dispute outcomes and insight to what factors are relevant for legal decision-making. This will allow us to then undertake a comparative analysis with the results of existing studies and shed light on the legal contexts and questions where AI can and cannot be used to produce accurate and repeatable results.",,"Artificial Intelligence research encompasses a broad-and ever-expanding-array of disciplines including, but not limited to: the computer sciences, mathematics, linguistics, electrical engineering, psychology, neuroscience, economics, and operations research. While we do not profess to make discrete contributions to each of these fields, we believe this project cuts to the core of questions whose answers have implications for the use of AI in law, but for technical research into AI and social-scientific examinations of its societal impact. Specifically, our proposed research examines the central question of how we might identify and define limits or 'red lines' for using AI to replicate core aspects of the legal system, specifically legal adjudication.

With this in mind, our research is likely to have an impact beyond those contexts that we can foresee at the outset. Most immediately, however, we believe that our research will have the most proximate impact on legal scholarship, public policy around the use of AI in law, government innovation and investment strategies, and ongoing regulatory compliance debates. By involving stakeholders from the LegalTech community, intergovernmental organisations, and government ministries we will receive input from relevant stakeholders driving the development of AI, but our research will not be limited to their input. Here we will build on our existing contacts with international organisations including the OECD and ILO, government departments (BEIS and the MoJ in the UK, METI and the MoJ in Japan), and civil society groups.

The transformative potential and promise of AI is a matter of great public interest and concern. As such, we will ensure that our research project includes input and evidence from the public and civil society organisations. Here we will build on a series of public engagement forums hosted by CoI Christopher Markou as part of his Leverhulme Trust postdoctoral fellowship, and supported by the Law Society of England and Wales and Royal Society for the Arts. These events, scheduled for September 2019, will be hosted at law schools across the UK to educate the public on the implementation of AI and Big Data into legal administration and law enforcement. Public sentiment and concerns will then be fed back into a jointly authored report presented to the UK Ministry of Justice, and capped off by a public lecture by Dr Markou for the Cambridge Festival of Ideas in October 2018. This work will be accompanied by a series of op-eds in major newspapers, blogs, and media spots in print, video, and radio that will help raise the public profile of the project and dissemination of its findings.

We will also integrate our dissemination plan with the activities of the Cambridge Trust and Technology Initiative which is run by Co-Is Jat Singh and Jennifer Cobbe. The Trust and Technology Initiative is a 'big tent,' bringing people together, facilitating collaboration, and engaging industry, civil society, government, and the public, across: (i) relationships and interplays between technology and society; the legal, ethical and political frameworks impacting both trust and technology, and innovative governance, in areas such as transport, critical infrastructure, identity, manufacturing, healthcare, financial systems and networks, communications systems, internet of things; (ii) the nature of trust and distrust; trust in technology, and trust through technology; the many dimensions of trust at individual, organisational and societal levels; and (iii) rigorous technical foundations, for resilient, secure and safe computer systems, including data and communications platforms, artificial intelligence, and robotics"
4,437FC5CF-64D4-4184-BBAE-4DBAD8C67139,Detecting soil degradation and restoration through a novel coupled sensor and machine learning framework,"Overview
In this proposal we outline an ambitious cross-disciplinary project focused on detecting soil degradation and restoration through a novel multi-functional soil sensing platform that combines conventional and newly created sensors and a machine learning framework. Our proposed work directly addresses the Signals in the Soil call to 'advance our understanding of dynamic soil processes that operate at different temporal/spatial scales.' Through the creation of an innovative new approach to capturing and analysing high frequency data from in-situ sensors, this project will predict the rate and direction of soil system functions for sites undergoing degradation or restoration. To do this, we will build and train a new mechanistically-informed machine learning system to turn high frequency data on multiple soil functions, such as water infiltration, CO2 production, and surface soil movement, into predictions of longer term changes in soil health including the status of microbial processes, soil organic matter (SOM) content, and other properties and processes. Such an approach could be transformative: a system that will allow short-term sensor data to be used to evaluate longer term soil transformations in key ecosystem functions. We will start our work with a suite of off-the-shelf sensors observing multiple soil functions that can be installed quickly. These data will allow us to rapidly initiate development and training of a novel mechanistically informed machine learning framework. In parallel we will develop two new soil health sensors focused on in-situ real time measurement of decomposition rates and transformation of soil colour that reflects the accumulation or loss of SOM. We will then link these new sensors with a suite of conventional sensors in a novel data collection and networking system coupled to the Swarm satellite network to create a low cost sensor array that can be deployed in remote areas and used to support studies of soil degradation or progress toward restoration worldwide.",,"The work proposed here focuses on an issue of critical global need. The global decline in soil health and the extensive degradation of soils in managed ecosystems has been identified as a major issue by the UN Food and Agriculture Organisation, the Intergovernmental Platform on Biodiversity and Ecosystem Services, the UN Global Resource Panel and others. The work described here explores fundamental research and engineering challenges but does so with an eye toward the eventual use of our approach in early detection of both degradation and recovery of both managed and natural ecosystems around the world. Our proposed research, and sensor development, are designed with this long-term goal in mind, including our focus on transitioning from high cost, high power conventional sensors to low power (or passive) sensors linked to inexpensive networked field stations that have been used by PI Thomas (U of Colorado) to monitor groundwater pumps serving over 1.5 million people in East Africa. Our proposed use of the Swarm Satellite network for data exchange is also developed to support low cost, remote deployment of the sensor packages developed in this project. As our work progresses, we will explore different avenues to scale our technology through market or non-market mechanisms and our team overall has extensive experience in the transfer of technologies into private or non-profit sector application. In the UK, The Government has, through its 25 Year Environment plan, committed itself to improving soil health, and in particular addressing the need to ensure healthier soils by addressing soil degradation and the factors responsible for it. These factors include soil erosion, soil compaction and the decline in accelerated loss of organic matter, all of which are central to this proposal. A key pillar of the UK government's approach is to develop better information on soil health and the work proposed here plays strongly to the UK government's stated desire for research that provides a clearer picture of soil health. In addition, the plan sets out a desire to develop 'cost-effective and innovative ways to monitor soil at farm level' and to utilise technological developments that have the potential to 'revolutionise how we monitor the environment', both areas are clearly aligned to the sensing developments we propose here. Developing new approaches to machine learning that can be applied in multiple sectors has clear links to the UK's Industrial Strategy and particularly the desire to putting the UK at the forefront of the artificial intelligence and data revolution. The US research site is located on lands managed by the City of Boulder Open Space Department to restore the landscape to one that can support agricultural production. The City of Boulder is strongly supportive of this work and would like to use the data generated in this study to communicate the nature and results of the site restoration efforts to the public. To this end, we will develop a public-facing website (including a UK section, see below) that shows the live data flow from the site along with an explanation of what the data illustrates about changing soil function at the site. This site will be developed with the input of city staff and will include broader information on the causes and consequences of soil degradation at the site and the remediation work underway to address those issues. We will build on this experience and our networks to facilitate knowledge exchange between the project team, end users and policy makers. The project website will provide a valuable tool to both demonstrate the advances in soil health monitoring developed in this project and as a platform for communicating issues pertaining to the sustainable soil management, with a focus on grasslands in the UK context and degraded agricultural lands in the US."
5,9F51F2BF-91FD-457F-9A09-FE82DFFB99A8,Deep Discoveries,"Couched in a network of research initiatives under the umbrella theme 'Towards a National Collection: Opening UK Heritage to the World', the Deep Discoveries project aims to contribute to the creation of a unified national heritage collection by creating a transformative image searching platform. Our goal is to design a prototype app enabling cross-collection image linking by harnessing the ability of computer vision and deep learning methods to identify and recognise specific patterns without the need for preliminary integrated descriptive metadata. The project aims to create a radical shift in content discovery within and between our nation's digitised image repositories, allowing users the opportunity to dissolve established physical and virtual barriers between these collections, opening cross-disciplinary modes of research and engagement and generating new and unforeseen connections leading to user-generated, disruptive, and (re)defined notions of 'national' heritage. 
 Through employing the catalytic potential of ever more socially-integrated artificial intelligence (AI) technologies for the benefit of opening up our national heritage collections and radically diversifying our visitor base, the project will ensure that these tools are directed toward the enhancement of the heritage economy and the wider social good. Crucially, the nature of deep learning architectures future-proofs the search platform, as it will be able to evolve and improve as the underpinning dataset grows. If successful, the creation of an image search platform able to continuously integrate new digital image repositories as they are generated by GLAM-sector organisations will have enormous benefits in making collections networked and openly discoverable across our virtual heritage landscape. Such an advancement will demonstrate the UK's commitment to cutting-edge technologies and shift the view of the museum/archive from a historical repository to a space of dynamic and emergent practices, inviting diverse users to weave new narratives from our collections.
 To achieve our ambitious goals, we have formed a cross-disciplinary network of diverse institutional partners: the core team is formed by researchers from the University of Surrey's Centre for Vision Speech and Signal Processing, the Collection Care and Research departments of The National Archives, the Royal Botanical Gardens Edinburgh, and the V&amp;A Research Institute. We are also joined by three Project Partners who will support the project through offering access to parts of their digitised image collections. Our partners include Gainsborough Weaving Studio, the Sanderson Design Archive, and the Museum of Domestic Design and Architecture. We will work together through a series of workshops, some of which will be jointly held with other projects from the Towards a National Collection programme. By surveying the current landscape of digital image users and working with a variety of stakeholders (public, academic, institutional) we will work to iteratively design a visual search platform that truly Opens up UK Heritage to the World.",,"As a Foundational Project within the Towards a National Collection programme, Deep Discoveries offers initial research into a fundamentally transformational and disruptive new framework for networking and discovering digitised image collections across the nation. The project aims to develop a visual search technology able to integrate a variety of image collections, structured or unstructured, breaking down geospatial boundaries and offering both institutional and public stakeholders a new pathway to accessing digitised and born-digital visual collections. As all projects within the programme, Deep Discoveries will employ advances in technology combined with innovative methodologies for assessing user engagement and audience diversification to establish the UK as a leader in networking its heritage collections in the virtual realm. In doing so, the project, and the programme as a whole will work to foster UK innovation in the cultural sector, economic and technological competitiveness, and the deployment of advanced digital methods for social and public services. 
 The project will benefit public end-users of the developed search platform by offering new ways of accessing and discovering collections. Curators will benefit from a technology allowing them to reach new audiences. Both of these outcomes have clear benefits for GLAM sector organisations, by helping to broaden current audiences and make use of digitised content through new and fruitful practices. The cross-disciplinary and diverse network at the core of the project will help to form strategic and lasting relationships between IROs and the higher education sector, key in ensuring continual success of research programmes conducted in organisations like The National Archives, the Royal Botanical Gardens Edinburgh, and the V&amp;A. Furthermore, our project will build bridges with private sector companies which also hold large digitised image collections. Academic researchers interested in both STEM and humanities fields will benefit through the opening up of new avenues for research, whilst education professionals and students will be able to create exciting connections within image collections. 
 A core contribution of the project will be the generation of a final report outlining the State of the Art in Computer Vision Searching for Heritage Collections, which will have significant impact on shaping future research in this area. This report will be used to provide clear evidence-based policy recommendations on next steps in the field. To ensure that all of the mentioned stakeholders benefit from our work, we will develop the search technology platform through a user-centric R&amp;D programme with the aid of a UX consultant. We will share insights and assess the state and availability of currently digitised visual collections, invite diverse stakeholders to test the proof-of-concept technology and feed into its modifications, and share the outcomes of the workshops via the final report, to enable beneficiaries and their associates to action the insights generated. We will also ensure dissemination of the project through open access peer-reviewed publications, presentations at conferences, project updates and blogs for public audiences."
6,2154141B-7872-4378-A2C5-71E01721F8C0,AI-driven biomaterial screening to accelerate medical device development,"Our multi-disciplinary team will leverage the power of artificial intelligence, computer simulation and high-throughput screening approaches to create biomaterials for regenerative medicine in a sustainable, rapid and rational framework. Conventional engineering processes of biomaterials are expensive and laborious that has significantly impeded the translation from bench to bedside. To mitigate this unmet challenge, we will use combinatorial numerical simulations and machine learning models to generate and analyze large-scale and multi-modality synthetic data of cell-biomaterial interactions. Such application will effectively narrow down material choices and lead to hypothesis-driven empirical experiments for model verification. Novel high-throughput arrays containing 3D cell-laden hydrogels will be used to conduct thousands of cell-biomaterial experiments. We will further scale up the AI-informed material design and apply to vocal fold tissue engineering. Lastly, we will perform a cost analysis of this new AI-driven biomaterial design and screening with that of the conventional hypothesis-driven approach. We anticipate that the implementation of AI in biomaterial therapeutics will dramatically reduce the finance, time and labor while accelerate the development of personalized and precise biomaterials for medical applications.",,"This project intersects artificial intelligence (AI), biomaterials, medical devices, high-throughput screening (HTS) and regenerative medicine that are seamlessly aligned with the objective of responsible AI through &quot;sustainable development in the research design&quot;.

Social-Economic Impact: Tissue engineering products for medical applications require a highly complex manufacturing process and evaluation protocol. In biomaterial research, figures have shown that the total costs for preclinical testing, clinical evaluation and medical follow up could reach GBP &pound;602k or CAD $975k per patient. Expensive costs and lengthy cycles in biomaterial product development and empirical testing create a notable barrier in the pipeline of clinical translation. The proposed AI-informed biomaterial design 
will lead to the creation of a computational platform that can be used for performance evaluation of tissue engineering products, e.g., scaffold biomaterials. The combinatorial simulation and machine learning approach can be easily adapted to test other application of tissue engineering, e.g., cell/ drug delivery as well tissue engineering for the replacement of other organs. We anticipate that this computational tool will accelerate the clinical translation of novel engineered products and mitigate some of the high costs associated with animal and human preclinical testing.

Patient Care: This project's population significance is linked to the widespread nature of VF and IVD disorders, documented personal and financial costs for it, and the lack for effective treatments for intractable VF and IVD tissue defects. Specifically, VF dysfunction is a significant public health problem and concern. This chronic laryngeal disease affects almost one-third of the general population in North America. The negative impact on quality of life in patients with defected VF are documented as significant as those of many chronic diseases such as angina pectoris, sciatica and chronic sinus infection. Unfortunately, effective treatments are not available for these perplexing and functionally debilitating vocal conditions. Given the unique anatomical and mechanical properties of VF, tissue engineering-based strategies are needed for defected VF replacement and regeneration. The success of this proposal will help develop biomaterials that are personalized and precise to the desired VF reparative therapeutic outcome. 

Research Training: At least 2 PhD students and 3 post-doctoral trainees will be supported by this proposal. Given the highly cross-sectoral nature of this project, trainees in this research program will develop well-rounded research skills and competency that will contribute to the fields of computer science, material science and tissue engineering. Computational medicine and tissue engineering are growing fields in both academia and industry. Research specialists in these areas are in high demand and will guarantee our research trainees promising academic and career options and in turn contribute to the advancement of technology in the UK and Canada.

Innovation and wealth-creation for the UK and Canada: The foundational academic research of this project will have a significant and wide impact on the growing number of start-ups, SMEs and corporations in the UK and Canada that use AI and biomaterials as part of their research and revenue stream. This will help build relationships with industry to explore IP exploitation and clinical translation. Development of hydrogels for VF and IVD tissue engineering will have wide-ranging impact beyond the project objectives such as: (i) facilitate development of hydrogel coatings for medical implants; (ii) inform new medical devices development, including stents, meshes and support structures that may be derived from hydrogels (iii) enable new product development for pharmaceutical applications, e.g. as biocompatible delivery systems for active components."
7,EBA6D3BD-5B10-42E7-A3BC-AF3F4B1C0C48,"Responsible AI for Inclusive, Democratic Societies: A cross-disciplinary approach to detecting and countering abusive language online","Toxic and abusive language threaten the integrity of public dialogue and democracy. Abusive language, such as taunts, slurs, racism, extremism, crudeness, provocation and disguise are generally considered offensive and insulting, has been linked to political polarisation and citizen apathy; the rise of terrorism and radicalisation; and cyberbullying. In response, governments worldwide have enacted strong laws against abusive language that leads to hatred, violence and criminal offences against a particular group. This includes legal obligations to moderate (i.e., detection, evaluation, and potential removal or deletion) online material containing hateful or illegal language in a timely manner; and social media companies have adopted even more stringent regulations in their terms of use. The last few years, however, have seen a significant surge in such abusive online behaviour, leaving governments, social media platforms, and individuals struggling to deal with the consequences. 
 
The responsible (i.e. effective, fair and unbiased) moderation of abusive language carries significant practical, cultural, and legal challenges. While current legislation and public outrage demand a swift response, we do not yet have effective human or technical processes that can address this need. The widespread deployment of human content moderators is costly and inadequate on many levels: the nature of the work is psychologically challenging, and significant efforts lag behind the deluge of data posted every second. At the same time, Artificial Intelligence (AI) solutions implemented to address abusive language have raised concerns about automated processes that affect fundamental human rights, such as freedom of expression, privacy and lack of corporate transparency. Tellingly, the first moves to censor Internet content focused on terms used by the LGBTQ community and AIDS activism. It is no surprise then that content moderation has been dubbed by industry and media as a &quot;billion dollar problem.&quot; Thus, this project addresses the overarching question: how can AI be better deployed to foster democracy by integrating freedom of expression, commitments to human rights and multicultural participation in the protection against abuse?
 
Our project takes on the difficult and urgent issue of detecting and countering abusive language through a novel approach to AI-enhanced moderation that combines computer science with social science and humanities expertise and methods. We focus on two constituencies infamous for toxicity: politicians and gamers. Politicians, because of their public role, are regularly subjected to abusive language. Online gaming and gaming spaces have been identified as private &quot;recruitment sites&quot;' for extreme political views and linked to off-line violent attacks. Specifically, our team will quantify the bias embedded within current content moderation systems that use rigid definitions or determinations of abusive language that may paradoxically create new forms of discrimination or bias based on identity, including sex, gender, ethnicity, culture, religion, political affiliation or other. We will offset these effects by producing more context-aware, dynamic systems of detection. Further, we will empower users by embedding these open source tools within strategies of democratic counter-speech and community-based care and response. Project results will be shared broadly through open access white papers, publications and other online materials with policy, academic, industry, community and public stakeholders. This project will engage and train the next generation of interdisciplinary scholars-crucial to the development of responsible AI. 
 
With its focus on robust AI methods for tackling online abuse in an effective and legally-compliant manner to the vigour of democratic societies, this research has wide-ranging implications and relevance for Canada and the UK.",,"Main Beneficiaries: 

1) The public: The prevalence of cyber abuse has lead to many government and industry attempts to curb its occurrence through prevention and policy; however, these attempts are hindered by the massive, dynamic volume of online content, as well as impeded by the largely ineffective and time-consuming nature of current abuse moderation methods. The project seeks to address these challenges while also considering issues of content moderation biases that tend to disproportionately tag certain individuals' and communities' language as toxic. These biases affect public dialogue, democratic participation and certain legal rights, such as freedom of expression, equality and privacy rights.

2) Policy makers and NGOs: The results generated by this project will help policymakers (e.g, economic diversification and innovation, justice, privacy, gender and equality) and NGO/community stakeholders (e.g., Amnesty, Reporters without Borders) establish guidelines for addressing online abusive language and inform them of the impacts. It will also provide alternative responsible (effective, unbiased and fair) methods for countering abusive language. Research results will contribute to a more balanced and democratic moderation of political dialogue and engagement while protecting against abuse of politicians and users.
 
3) Technology companies: Companies such as Intel are seeking to work with academics and NGOs to address abuse-prevention, especially as policies and regulatory frameworks are being developed. Gaming is also an important site for the tech industry, with a &gt;4% yearly growth globally. The community of gamers is growing more diverse (~50% women in Canada in 2018). However, gaming can be a very toxic environment in terms of sexism, racism and other discriminatory forms of abuse, which ultimately limits the size of the gaming market.
 
4) Law enforcement agencies and social media companies: The responsible NLP methods
arising from this project could be incorporated in existing tools, helping law enforcement agencies and 
social media companies detect and counter online abuse in real time. 
 
5) Media companies and stakeholders engagement: Through previous projects, we have already established and will leverage collaborations with Buzzfeed, BBC News, ITV, Reuters Institute for the Study of Journalism and Google; and promote research results through the Centre for Freedom of the Media/UNESCO Journalism Safety Research Network. 
 
6) Early career researchers (ECR)/students: the project will help advance emerging scholars' research trajectories by offering training in interdisciplinary research skills, widening collaborations in the UK, Canada, and the USA, and engaging them in cutting-edge research methods with major social impacts and benefits. 

Impact and Outreach Activities: 

To achieve maximum impact, project results will be made open-source. Project results will contribute to more responsible AI methods to detect online abusive language. This in turn contributes to increased users' confidence through platforms' greater compliance with relevant policies, human rights and legal frameworks and reinforces key socio-economic and Digital Economy areas, namely online gaming, social platform companies, digital journalism and content moderation technologies and services. 
 
Policy impact will result from knowledge shared in Canada, the UK, and the US (through AI NOW). We will draw on the UK PI's experience who has just submitted written evidence on online abuse of UK MPs to the UK Parliamentary inquiry on Democracy, free speech and freedom of association and harness the Industrial and Parliament Trust. The Canada PI will share new findings with a network of over 35 collaborating scholars and policy/community/industry partners with the Canada 150 Research Chair/SFU Digital Democracy Group."
8,10982572-470A-44B4-A4C4-A8EC1BA41B04,Nikon-UCL Prosperity Partnership on Next-Generation X-Ray Imaging,"X-Ray Imaging (XRI) has a fundamental role in medicine and security, and is instrumental in the automotive, aerospace, pharmaceutical industries and in manufacturing in general. Cultural heritage relies on XRI, as do materials science, biology, and many other scientific fields. Through our established collaboration between Nikon X-Tek Systems (NXTS, Nikon's UK based x-ray division) and UCL, we are targeting the next paradigm shift in XRI. Our vision is that this will involve the incorporation of phase effects in the image formation process (&quot;Phase-based&quot; XRI) coupled with energy-resolved (&quot;colour&quot;) XRI and new data reconstruction and interpretation algorithms. &quot;Colour&quot; XRI could be seen as the x-ray equivalent of the transition from black and white to colour photography, meaning a much wider spectrum of information can be obtained from the imaged sample. Phase-based XRI enables contrast increases of up to two orders of magnitude, thus allowing the detection of features classically considered &quot;x-ray invisible&quot;.

Our vision is to marry UCL's world-class research and expertise on phase-based XRI, inverse problems and nanofabrication with NXTS's innovation on scatter analysis, image reconstruction and colour x-ray imaging in order to achieve the next step change in XRI technology, with the UK industrial and academic communities firmly at the centre. This will deliver transformative solutions that are practicable in an industrial context and beneficial to a wide user base, while also enabling new science. Our ambition is to replace conventional attenuation based XRI with energy-resolved, phase-based technology combined with scatter retrieval and novel algorithms in most application areas.

At synchrotron facilities, UCL researchers have used phase-based XRI to image rocks, metals, tissues, animals, humans, cells, foams, fabrics, batteries, manufacturing processes, food, and heritage artefacts. They have done this statically and dynamically, in situ and in operando, in vivo and ex vivo, invariably detecting key features that were invisible to other methods. Making this available through standard, lab-size machines would be nothing short of a revolution, leading to economic and societal impact through the multi-disciplinary applications, making NXTS the commercial leader in the field, and cementing UK's leading research status. In our vision this will be strengthen even further by its combination with &quot;colour&quot; imaging, and with new ways of handling scattered radiation such that the &quot;structured&quot; scatter signal leading to additional information is exploited, while the uniform background that limits image contrast and therefore detail visibility is rejected.

We will pursue this vision through a combination of modelling and experimental work. Using experimentally validated simulation software developed jointly by the UCL and NXTS teams, we will model experiments before they are carried out, compare simulated and experimental results, refine models and setups until all discrepancies are clarified, and only then proceed to the next step. This will enable us to develop systems where i) we keep all parameters under control and have full understanding of their effects and implications, and ii) we can steer the design towards effective solutions to specific problems. Cutting-edge nanofabrication methods (available at UCL's Photonic Innovations Lab and London Centre for Nanotechnology) will enable the development of beam modulators that allow the exploitation of phase effects with the conventional x-ray sources routinely used by NXTS.

We will apply the novel technologies to a range of high-impact applications, including non-destructive testing of composite materials and additive manufacturing processes and products, biomaterials and tissue-engineered organs, digital histology, improved detection of concealed explosives and forensics.",,"The areas this partnership will have impact on are wide and diverse. X-Ray Imaging (XRI) has a fundamental role not just in medicine and security, but also in the automotive, aerospace, pharmaceutical industries and in manufacturing in general. These are all key areas of the UK's Industrial Strategy, and indeed advanced imaging underpins many of the themes highlighted by the Industrial Strategy Challenge Fund and by Sector Deals such as those in life sciences, automotive, creative industries. Moreover, cultural heritage relies on XRI, as do materials science, biology, and many other scientific fields. We expect this partnership to deliver impact on all of these fields, and to keep the UK on the forefront of the development of new imaging techniques. Through the close interaction between academic and industrial researchers, it will deliver innovation that can be directly applied to real world problems, with UK Plc set to reap the benefits. It will enable the UK's leading manufacturer of x-ray micro-CT systems to rise from its current position as world's third to world's first, and re-invigorate the entire field of non-destructive testing by opening opportunities to test and detect features which are currently inaccessible. It will create a knock-on beneficial effect on neighbouring industries such as x-ray sensors (with some of the UK's leading companies involved in the programme from the beginning) and microfabrication. This will result in both wealth and job creation. It will also deliver a unique, inter-disciplinary training opportunity for students and early career researchers in this area, and reach out at the general public and policymakers through an intense engagement activity.

Notable example of application areas include:
- Industry, e.g. for assessing and monitoring processes and products in additive manufacturing;
- Medicine, e.g. for the detection of life-threatening diseases such as cancer;
- Energy, e.g. for understanding failure mechanisms and developing fail-safe batteries; 
- Security, e.g. for higher detection rate of threat objects at airports, with fewer false alarms.
The key strategy we will pursue to deliver this impact is through the creation of a consortium around the NXTS/UCL partnership to ensure maximum development, application and ultimately exploitation of the developed technologies. As far as applications to industry and energy are concerned, NXTS already has an impressive customer base, including many leading UK companies such as Rolls Royce and Mc Laren. Recently, NXTS has expanded their range of interests to include significant aspects in biomedicine and in the life sciences in general. This is an area where the UCL team is particularly strong, and has a wide range of collaborators both at UCL and beyond (for a partial list see https://www.ucl.ac.uk/medical-physics-biomedical-engineering/research/groups-and-centres/advanced-x-ray-imaging-group-axim), who will act as &quot;champions&quot; for the various biomedical applications and provide the required, relevant samples. The NXTS and UCL teams have also recently engaged with various Authorities to look into the development of solutions for security, especially as far as the detection of concealed explosives is concerned. This wide range of multidisciplinary contacts will enable us to create a community of stakeholders and opinion leaders, facilitating the realisation of impact in all its forms.

Importantly, industry will not be the only beneficiary. For example in medical applications, clinicians will have access to better diagnostic tools, the NHS and other health services worldwide could reduce costs and provide better healthcare and, ultimately and most importantly, the patients would receive better care leading to improved life quality/expectancy. A similarly &quot;multilayered&quot; impact is expected in most of the other application areas, e.g. through improved security at airports or the availability of better industrial products."
9,E5362C9D-A062-4D8E-9987-AAC0B120E75A,"Machine Learning for Tomorrow: Efficient, Flexible, Robust and Automated","Artificial intelligence systems have recently led to significant advances in the state-of-the-art in downstream fields including computer vision, speech and natural language processing, and game playing. Although impressive, these advances mask a set of fundamental limitations of the underlying machine learning technology that need to be addressed to unlock gains in a wide variety of applications relevant to industry and society. 

These limitations come in four main forms. First current approaches are data-inefficient requiring extremely large and painstakingly curated datasets. Second, they are inflexible solving single tasks that are fixed through time. Third, the current approaches are brittle as performance can degrade catastrophically in the face of noise, missing data or adversarially selected data points. Fourth, the approaches are only semi-automated requiring an expert to design and tune them. These limitations mean that many important application domains are currently out of reach. For example, in medicine we typically have only small and noisy datasets which requires data-efficient and robust machine learning. Providing machine learning as a service requires fully-automated machine learning. 

This Prosperity Partnership will develop machine learning that is data-efficient, robust, flexible and automated by leveraging recently developed technology from the University of Cambridge's Machine Learning Group and deep expertise from Microsoft Research Cambridge. This partnership has identified a unique testbed of impactful application domains: health, enterprise tools and games development. This research programme is central to realising Microsoft's vision to empower every developer, organization and individual to innovate and transform the world with AI. Moreover, this area of immediate and wide-ranging national importance, and provides pathways to impact by partnering with one of the world's largest technology companies.",,"Artificial intelligence (AI) and machine learning (ML) have the potential to transform many industries and society, but current methods have fundamental limitations that prevent this. The basic research tackled in this open academic program will overcome some of these limitations. The program will therefore have large impact in the UK and globally, through and beyond project partner.

Business benefits. 

This partnership will bring global impact to Microsoft. The benefits from unlocking machine learning will be quickly applied and scaled through Microsoft's UK and global customers, partners and billions of users, including through productivity tools (Visual Studio for developers, Microsoft Office and Windows), enterprise services (data, customer relationship management, human resources), hyper-scale cloud computing, hardware, and societal impact (accessibility, global sustainability, humanitarian action, philanthropy).

ML to support tomorrow's computer games: The methods developed in this proposal are particularly useful for enabling and improving AI infused tools via (i) faster personalisation, (ii) integration of more disparate data sources (e.g. jointly from spreadsheets and presentations), and (iii) reliable, robust and fully automated AI.

ML to support tomorrow's healthcare: The project will contribute (i) causal modelling methods that enable data driven decision making for healthcare provider policy, (ii) imaging techniques that can adapt to small numbers of data points, (iii) uncertainty aware, robust methods for medical decision making. 

Democratising ML: The methods developed in this proposal will provide: (i) superior autoML systems, (ii) federated machine learning method for distributed ML which could also help support new AI hardware being developed by MS, and (iii) meta-learning methods that are deployable on low-power devices. 

UK economic. 

Almost every UK organisation uses Microsoft technologies. The fundamental tools developed in the proposal will impact a wide variety of downstream application areas used by consumers, enterprises, SMEs, and public sector organisations. Organisations that are investing in establishing the right approach to AI technology now - specifically, by developing underlying values, ethics, and processes - are shown to be outperforming those that are not by 9%. This research program is necessary to provide the fundamental foundations to ensure that AI and ML technology can be deployed ethically, safely, and at scale in products and services. MSR and the Machine Learning Group have a track record in impactful open source software e.g. Infer.net &amp; GPFlow. The project will train people directly involved in the work. The project's RAs/PhDs/PIs will contribute to University training (undergraduate and graduate ML programmes) and at MSR (e.g. the AI residency and internship programmes). Joint workshops and events will be open to communicate the work and foster the AI community, building on the existing MSR-CUED events. 

Societal. 

The technological advances in secure, private, interpretable and robust AI will have direct impact on society as these are required to protect important principles and values in our society. ML is currently concentrating power in a privileged few, but this will be mitigated by the automation work package that will improve access to machine learning. This program provides an ambitious joint research initiative to focus on key technical blockers and test them in domains of pertinent application, with the opportunity to accelerate research into industrial deployment via Microsoft, and other organisations through open research.

Moreover, there is an urgent need for postgraduates with training in AI/ML. This research programme will train at least 3 PhDs, 1 RA and 2 SRAs and they will help train undergraduates and postgraduates in machine learning and artificial intelligence."
10,16173E55-664B-4B4D-8FE1-2DDBA52F7E94,Shaun the Sheep: Immersive Experience,"Proposed Activity:
Our aim is to deliver a world-first in research innovation (i.e. technical, methodological, creative application) that will transform the delivery of Aardman's high-end animation work (Shaun the Sheep) from a currently linear and pre-rendered end-user product (e.g. TV/cinema screen-based) into a non-linear, real-time, responsive, spatially immersive experience; one that can be enjoyed by a wide (family) audience in China (without the need for headsets, gloves or pre-rendered animation) whilst recognising China's cultural context and values in the realisation of this work. 

Rationale and Research/Industry Context:
The animation market is increasingly challenged by changes in media consumption and modes of distribution. This innovation will focus on the development of shared experiences for the family entertainment market that will turn the passive observers of screen-based storytelling into active participants. We will create a novel digital-physical spatial experience that embodies the spirit, ethos and participatory nature of games, built around interaction, responsiveness and immersion. With the global games audience estimated between 2.2 and 2.6 billion (UKIE), and a significant number of these in China, we believe that this project will catalyse the development of transformative approaches to engaging animation audiences in novel, and more participatory, experiences.

Research Aim:
This research links three significant domains of knowledge in the family entertainment market which can be expressed as a simple equation: Audience + Animation + Space + Games Technology = narrative, interactive, spatial experiences. Facilitating this, the project will combine research and creative practice expertise from the fields of animation, architecture, creative media arts, theatrical arts, engineering, AI and robotics as well as gaming.

Combining immersive spaces with real-time audio-visual content has precedence, predominantly within media art installations, however, our audience-focussed, spatially immersive, and responsive approach to storytelling differs substantially from what we might have called in the past &quot;immersive cinema&quot;. Furthermore, due to the application of a quasi-intelligent system (supported by AI) we are developing new territories for Aardman and the wider animation sector.

By combining the &quot;scalability&quot; of film/animation with the &quot;intimacy&quot; and responsiveness of immersive experiences, we are seeking to redefine the term &quot;mixed reality&quot; (as a family experience with no headsets, no gloves and no mobile phones), providing a scalable concept that could generate significant impact within and beyond the creative industry sector. In the context of this project, we are calling our approach &quot;responsive cinema&quot;.

Research Questions &amp; Methodology:
Our research raises critical practice-based research questions in the following six areas: WP1 Story &amp; Conceptional Realisation; WP2 Dynamic Content Creation; WP3 Game &amp; Interaction Design; WP4 AI &amp; Sensoring; WP5 Target Audience; WP6 Space Integration - all of which will require a uniquely tailored, interdisciplinary methodological approach.",,"1. LINKING RESEARCH &amp; INDUSTRY COLLABORATION 
a. Creation of a new Joint UK-China Lab: 
This project will enable a new collaboration between three academic partners; the University of Liverpool (UoL), Liverpool John Moores University (LJMU) and the Shanghai Theatre Academy (STA) and the Industry partners (Aardman, Media Industry Association and Digital Fun). This links up six research centres/institutes, utilising their complementary expertise for an applied research project with high level industry partners.
 
b. Offering new Business Services:
Our project will develop a series of innovations that will be of value to the wider screen and animation industries, including the use of 'real-time' games engines for immersive animation experiences, the application of artificial intelligence for creating dynamic experiences and the spatial interpretation of screen-based media experiences. 
 
c. Engaging Beneficiary Groups across Industry and Academia
Our publishing and dissemination plan (book; journal articles; film; conference) will be organised in partnership with Immerse UK and its network to ensure the project has impact within academia and the creative industries.
 
d. Market Implementation and Knowledge Transfer
Immerse UK will assist us in reaching out to further UK companies to ensure it has impact for UK creative industry stakeholders and policy makers. They will also help in identifying implementation and knowledge transfer opportunities from our research in the emerging immersive marketplace.
 
2. CREATIVE ECONOMY IMPACT
a. UK Business Impact Abroad
The project offers a global commercial opportunity in line with the UK Government's Industrial Strategy concerning the development and exploitation of immersive content and technologies. Our direct route to economic impact will be through collaboration with Aardman on a piece of research innovation for which, as our previous research (AHRC Network Grant) has shown, there is considerable demand in China. 
 
b. Securing UK Business Investment
We have already established links to large investors in the culture/entertainment/tourism sector (e.g. China Media Capital (CMC); Dingsheng Cultural Investment Company; Shanghai Yorkie Investment Management Co.Ltd.; Zhejiang Cultural Tourism Investment Group; Shanxi Cultural Tourism Group), some of whom already show a commitment to bringing the IP/results of our research to market. 
 
c. UK Family Entertainment Sector Impact
Aardman is a flagship company for a much wider sector in the UK, focusing on stories and animations for children and families - a sector for which the UK is known globally. Operating at an international level with global partners, an ambitious project like this will likely act as a catalyst for other screen-entertainment companies to embrace new forms of storytelling in a spatial environment.
 
3. TECHNOLOGICAL IMPACT and NEW TECH IP
This project enables high level technical and research expertise to innovatively fuse animations, games technology, spatial design, experience design, computer science and theatre practices in order to generate new real-time applications for immersive experiences. It is anticipated that this inter-disciplinary approach will lead to both creative and technical innovation that carries new IPs in ways that are relevant for a wide range of screen-based animation/entertainment companies and audiences. 
 
4. CULTURAL IMPACT
Liverpool has a deeply established relationships and partnerships with Shanghai; they are twinned cities since 1999 and both UoL and LJMU have strategic links at both a teaching and research level (e.g., the UoL's joint university in Suzhou (XJTLU). This project offers an opportunity for these links to be extended into meaningful creative industry collaboration. Our project is also about the translation of British humour and the UK animation industry's distinctive style for a Chinese audience, which has important cross-cultural implications."
11,704F628F-6696-4641-A74D-C4E07E275639,Leveraging the impact of diversity in neurodevelopmental disability by integrating machine learning in personalized interventions.,"Neurodevelopmental disability (NDD), which is an umbrella term for autism, attention deficit, and intellectual and learning disability, affects 13% of the population. It has major economic and quality-of-life impacts on NDD individuals and families, and substantial economic burden on the healthcare system. So far, treatment is aimed only at general symptoms, which often leads to low efficacy and frequent side effects. 
 
The advent of novel genetic testing methods has provided plenty of evidence of the major impact that genes and their regulation have on clinical presentation in NDD. Nonetheless, there is a large diversity among individuals with NDD, even with the same genetic mutation. This is not unique to NDD as it is seen widely in many other medical conditions. The complexity derived from the genetic heterogeneity and the clinical (neuro) diversity has proven challenging to traditional approaches for treatment. 
 
Recent research in the UK and Canada has led to the development of large databases recording detailed information about individuals with NDD. Artificial intelligence (AI) now provides us with the tools to quickly analyze the information in those datasets. In particular, we will use machine learning (ML) to manage complex information, leading to the acceleration and better prioritization of interventions. Also, our project takes a novel view on the understanding of genomic information in NDD. Instead of directing our focus only on exploring data from a single individual or small group of individuals carrying the same gene mutation, our team will apply ML to large databases to identify features (from genes and their biology) correlated with improved clinical outcomes. 
 
In addition, we will use ML to better understand the interdependence between different symptoms to develop treatments that have a globally positive impact. In other words, we would find solutions that improve cognitive skills without impacting sleep negatively or generating more anxiety, as has been seen in previous clinical trials. 
 
We will finish by providing the entire scientific community with an open access portal, including our research findings, which will be integrated with the current Open Targets platform, a partnership between academia and industry in the UK that allows researchers to access linked data on diseases, genes and drugs in a single site. Researchers will be able to provide further information, which will improve the ML model. 
 
To ensure that we accomplish our objectives, we have assembled a team of experts in clinical and genetics of NDD: Dr. Bolduc (Canada); in computer science of genomics, molecular and pharmacological data: Dr. Dunham (UK); bioinformatics: Dr. Droit; machine learning: Dr. Greiner; social sciences, patient engagement and health economic: Dr. Zwicker. Our team has also developed strong links with NDD patient and research organizations in Canada and the UK, which will provide insight throughout the project. We are supported by collaborators involved in family and government engagement, ethics and data management in the UK and Canada. The project will also be a unique opportunity for multidisciplinary international training. 
 
Our project will show how ML can disassemble the complexity and diversity seen in NDD to develop more successful interventions. It will allow us to develop new ML approaches that will be readily applicable to other disorders where personalized interventions have been lagging behind diagnosis. More importantly, it will bring together families, society and scientists into a shared space where more and better information is exchanged. Finally, our project will embrace responsible implementation of data privacy and confidentiality while recognizing the need for data sharing to develop better interventions.",,"Our project will increase awareness of the positive impact of machine learning (ML) in developing treatment informed by patients. While most people associate ML with self-driving cars and facial recognition, its enormous impact in pharma remains largely unknown to the public. Yet ML can help us to quickly process and reliably exchange vast amounts of accurate, relevant and timely information amongst an array of diverse knowledge users. Moreover, the techniques applied here will be immediately transferable to research on the genetic and phenotypic influences on other rare and common disorders. 

Our project will show that ML can enhance expert ability to understand complexity and diversity related to neurodegenerative disability (NDD). Affecting 13% of the population, NDD represents a large group of genetically heterogenous disorders with overlapping clinical features 1,4. Thus, the development of drugs for NDD is extremely slow and costly. Our project will show how ML can process large datasets and identify the important targets for treatment in a maximum number of individuals. 

Our program will showcase that ML allows for a rational and cost-effective prioritization of candidate treatments. The main scientific impact of our project will be to allow researchers to prioritize candidate treatments for NDD using the input of human genomic data, and limit the unnecessary exposure of children with NDD to drugs. In addition, it will avoid the repeated failure of clinical trials that rely too much on trial and error. 

Our project will illustrate how confidentiality and privacy are respected while using ML responsibly. With recent events, including the use of large amounts of data and AI, a negative view of ML has developed with the public about respect of privacy. Our team therefore includes a major focus on ethics and individual privacy. It's a focus that we share with all investigators and knowledge users because we believe that, not only must privacy be ensured, but it must be seen to be ensured. We will also develop protocols, in collaboration with data privacy expert Dr. Mouratidis (Brighton,UK) for harmonization between UK (General Data Protection Regulation-GDPR)4 and Canadian (Personal Information Protection and Electronic Documents Act-PIPEDA) 5 datasets that will serve as a model for future international data sharing. 

Our project will build capacity in our understanding of the genomic basis of NDD. Dr. Dunham, as Director of Open Targets has created a unique platform combining the functional data necessary for this project. In his clinical practice and research lab, Dr. Bolduc has developed a very successful rapport with NDD individuals and their families, especially understanding their needs and conditions. His lab has also developed good relationships with artificial intelligence (AI) experts in searching for data on NDD. By connecting with international colleagues and using machine learning on large databases, Drs. Dunham and Bolduc and other team members will substantially accelerate their capacity building. The result, in both the UK and Canada, will be the training of several highly qualified people, development of better investigative techniques and significant advances in our understanding of NDD.

Finally, our project will build synergies between researchers/clinicians and families of those with NDD and allow for a sustained development of data stored and managed responsibly."
12,091B9CF2-29D8-4896-B7E2-FD1DB9EB49CE,EPI-AI: Automated Understanding and Alerting of Disease Outbreaks from Global News Media,"Disease outbreaks, such as Zika, Ebola and SARS epidemics, are of the greatest importance to the international community and the UK/Canadian governments. Public health organisations need data as early as possible in an outbreak to respond rapidly and prevent human suffering. Traditional bio-surveillance relies on human laboratory networks, but these data are often unavailable in real-time, patchy in geographic coverage, and tuned to specific diseases. Digital disease surveillance (DDS) using Web-based news data overcomes some of these limitations, providing a critical supplement to traditional networks. However, current DDS systems rely to a large extent on manual screening of Web data for events of interest: a skilled and labour-intensive process given the volume, multilingualism, velocity and potential bias of news sources.
Research has shown that there is significant potential to automate DDS. Natural Language Processing (NLP) has been in use since the early 2000s to efficiently detect and track health threats from outbreak news reports. For example, the Canadian GPHIN system, which detected the first evidence of SARS, uses a combination of NLP and human experts to sift through over 20K online news reports each day in nine languages. However, traditional automated approaches are insensitive to context that can help experts to interpret risk factors and fail to take account of possible data biases. 
Our goal in the EPI-AI project is to achieve a step-change in real-time automated DDS. Previous work has tended to take a siloed approach, focusing on Natural Language Processing methods or spatial analysis with little consideration of equality considerations that arise from biases in the data. We will use an interdisciplinary approach, combining expertise from three disciplines - computer science, epidemiology, and bioethics - to develop novel machine learning and statistical models adapted to the complex data and objectives of global epidemic surveillance. 
Benefits that we see include: (i) improved geographic precision and coverage; (ii) improved ability to understand the topical focus of a report; (iii) automated normalisation of risk factors to a standard terminology for integration of evidence across systems; (iv) automated spatio-temporal analysis of reports to update global risk maps and trigger alerts; and (v) provision of contextual information on potential media bias to support interpretation of alerts. 
This fundamentally interdisciplinary research will be closely aligned with key Canadian, UK and global public health stakeholders.",,"The development of novel neural and statistical machine learning (ML) methods for the extraction of structured event data from news media, the understanding of bias in news data, and the integration of event data with baseline data for the purpose of timely risk assessment will have an important scientific and technological impact. This investigation is highly relevant to a several domains such as public health, computer science, life science and medicine.
1. Public health experts performing infectious disease alerting, situation awareness and risk assessment will benefit from being more efficient and access to earlier warnings and greater coverage about health threats, e.g. pandemic influenza, Zika, Ebola and Marburg. In addition to using modern ML techniques to dramatically improve Natural Language Processing (NLP), the proposed technology will also integrate these methods into an advanced spatial analysis framework to support public health analysts in early alerting, tracking and risk assessment. The techniques supplement scarce human expertise (by replacing manual search and de-duplication), bring in evidence beyond national boundaries, cover segments of the population who may not interact with traditional clinical surveillance networks (e.g. patients who may not visit a GP), and incorporate media bias. With respect to risk assessment, the project will provide valuable data to calibrate the parameters of disease transmission models which are often hampered by insufficient data.
2. Life scientists and clinicians involved in translational studies will benefit from having a novel database of epidemiological evidence about infectious diseases and their associations to risk factors such as symptoms, locations and population descriptors with links to existing scientific data infrastructure through standard ontological codes. The database will include region-specific information about potential sources of media bias that influence reporting. 
3. Industry involved in AI technologies and e-science will benefit from open source software tools and publications explaining state-of-the-art ML techniques for text mining and risk alerting that takes account of bias reduction. AI has huge potential in healthcare as a means to support patients and clinicians in making decisions and to reduce administrative costs. The techniques pioneered in this proposal for bias reduction, NLP/Machine Translation and risk analysis are highly relevant to a wide community involved in the development and use of AI in clinical practice, e.g. technology companies involved in R&amp;D on electronic patient records, the pharmaceutical industry looking for online evidence to repurpose drugs, and online patient support networks. 
4. Decision-makers and the public will benefit from having improved AI technologies for early detection of health threats and improved understanding of their benefits and limitations. The main benefit is the potential for the research results to make the world safer from the threat of emerging and re-emerging epidemics by strengthening our global capacity to detect and control such threats rapidly, before they cause extensive human suffering. Public understanding of the project will be aided through the openly available EPI-AI portal, conference publications and demonstrations."
13,00F1A37C-9112-415A-8F8C-21FB00A5B348,BIAS: Responsible AI for Labour Market Equality,"What do we study? BIAS is an interdisciplinary project to understand and tackle the role of AI algorithms in shaping ethnic and gender inequalities in the labour market, which is now increasingly digitized. The project seeks to understand and minimise gender and ethnic biases in the AI-driven labour market processes of job advertising, hiring and professional networking. We further aim to develop 'responsible' AI that mitigates biases and attendant inequalities, by designing AI algorithms and development protocols that are sensitive to such biases. The empirical context of our investigation includes these labour market processes in organisations and on digital job platforms.

Why is it important? Labour market inequalities need to be tackled because they deny and thwart equitable and sustainable socio-economic development. In both the UK and Canada, access and rewards to work remain patterned around social distinctions, like gender, race, and ethnicity. The deployment of AI in labour market processes is known to exacerbate such inequalities through perpetuation of existing gender and ethnic biases hiring and career progression. From the point of view of policy our proposal speaks directly to the following priorities in both countries-the UK's Industrial Strategy, which has 'putting the UK at the forefront of the AI and data revolution' as one of its grand challenges; the UK's AI sector deal that aims to 'boost the UK's global position as a leader in developing AI technologies'; and the Canadian SSHRC's goal of tackling persistent demographic (ethnic and gender) disparities in workforce selection and development.

Why is it unique? Although we know that AI can exacerbate biases in the labour market, we do not know how AI can mitigate these biases. The project develops responsible and trustworthy AI that reduces labour market inequalities by tackling gender and ethnic/racial biases in job advertising, hiring and professional networking processes. It enhances capacity development for responsible AI applications through training of early career researchers, builds on existing and develops new UK-Canada research partnerships, and develops outputs for multiple stakeholders (researchers, companies and policy units). It speaks to multiple objectives of the funding call.

Why is it intellectually original and challenging? The project is interdisciplinary. It integrates and cuts across three distinct streams of research (the first two are from the social sciences and the third from the computational and mathematical sciences) to tackle the research objective through an interdisciplinary approach. The first includes studies on socio-economic antecedents of labour market inequality, which underscore the persistence and prominence of gender and ethnic/racial inequalities in the UK and Canada. The second includes studies from business management (digitalisation, technology/AI adoption and human resource management). It emphasizes that while we know that the use of AI in the labour market processes of job advertising, hiring and professional networking can strengthen ethnic and gender bias in these processes we do not know what those biases are and how AI algorithms can mitigate them, as opposed to merely (re)producing them. The third draws on computational statistics (Bayesian statistic/machine learning) to design new AI algorithms and development protocols that integrate human and machine inputs/outputs.

What is the work plan? Our project comprises two interlinked work packages that respectively (1) understand the different dimensions of bias from a multi-stakeholder perspective (e.g. employer, employee, digital platform developer) through in-depth data mining and qualitative investigations when AI algorithms are used in the labour market processes of job advertising, hiring and professional networking; and (2) test/design new AI algorithms to mitigate them and create protocols for their development and implementation.",,"Potential 'biases' produced by AI technologies may significantly undermine labour market equality and stymy equitable and sustainable socio-economic development. BIAS's objectives speak directly to multiple national priority agendas in both the UK and Canada - gender pay gap, ethnic/racial disparity, digital and industrial strategy. As both the UK and Canada look to embrace digital transformations as part of their national (economic and industrial) strategies, our focus on the implications of such transformations for labour market equalities and our objective to reduce such inequalities through the responsible development and deployment of AI promises a broad range of impacts, which are pertinent to the future of labour relations, economic competitiveness, human resource management, and industrial strategies.

Who are the beneficiaries? 
We expect our project to achieve impact on and through at least six stakeholder groups: (1) data science developers, computer scientists and technicians involved in the designing, testing and maintenance of AI algorithms and platforms for job advertising, hiring and professional networking, (2) international, national and professional associations responsible for regulating the design and deployment of AI and for skill/labour management, (3) platforms (e.g. LinkedIn) that use AI to broker labour market processes, (4) employers that use AI to facilitate hiring, retention and renumeration, (5) job seekers and employees; (6) the general public who are interested in and undergoing digital transformations in society. 

How will they benefit?
The stakeholders will benefit from our co-production of open-source AI development protocol and packages, which promises to set new industrial standards for the workflow of AI development that is sensible to 'biases' and accountable for gender and ethnic inequalities. The project will thus inform policy and regulation developments in the design and use of AI. Our findings will also inform platform developers and owners to operate AI-driven labour market platforms in a transparent and explainable manner to ensure egalitarian labour market opportunities and outcomes. In doing so, employers will be able to combine gender and ethnic equality liability into efficient hiring and human resources management practice in their use of AI platforms. A good understanding of potential AI biases and inequality implications will also empower individual job seekers and employees to devise personalised strategies as they engage with AI. Digital transformations in the labour market and more broadly in society concern all citizens. Our project therefore contributes to citizen digital education in soliciting public opinions to input into AI design and in making the AI workflow explainable to the general public. 

What are our pathways to impact? 
Our anticipated impact will be achieved through a number of channels. First, we will co-produce with research participants (e.g. public sector organisations, governmental departments, the Cabinet Office, platform owners, employers, individual employees, AI developers) and influential industrial stakeholders (e.g. Output, Profusion) an open-source AI development protocol. We have already secured the named stakeholders to form a learning network to take part in the project. Throughout the life of the project, four (international) knowledge exchange workshops will be hosted for the research team and stakeholders to engage with one another. We will also produce regular newsletters, media engagements, vlogs, policy reports, and a short documentary. Members of the project team will work closely with the Cabinet Office through links such as the Lancaster-Cabinet Office Innovation Partnership to partake in the government consultations and development of national strategies. Project members will also speak at AI developer conferences to job fairs felicitate impact among frontline AI practitioners and users."
14,3C72044D-C4B3-4555-8B0F-2C476C0C8373,Using AI-Enhanced Social Robots to Improve Children's Healthcare Experiences,"Children experience pain and distress in clinical settings every day, and the negative consequences of unaddressed pain can be both short-term (e.g. fear, distress, inability to perform procedures) and long-term (e.g. needle phobia, anxiety). In a series of small, innovative studies by project team members, a Nao humanoid robot has been used to deliver cognitive-behavioural therapy-based interventions during needle-based procedures. The results of these early studies have been positive, showing high acceptance among the target population as well as promising initial clinical results. However, these studies were all hindered by a critical technical limitation: in all cases, the robot was remotely operated and employed purely scripted behaviour with limited AI support, diminishing the flexibility and robustness of its behaviour as well as its potential to offer personalised, adaptive procedural support to children. In this project, we aim to address this limitation by developing and evaluating a clinically relevant and responsive AI-enhanced social robot. We believe that interaction with a robust, adaptive, socially intelligent robot can effectively distract children during painful clinical procedures, thereby reducing pain and distress.",,"Who will benefit from this research?

The research proposal has the potential to benefit all stakeholders involved in delivering potentially painful interventions in a children's clinical setting, including:
1. Children undergoing these procedures
2. Parents and caregivers of those children
3. Healthcare providers who carry out such procedures
4. Policymakers and clinical decision makers who determine standards of care 

In addition, the software and guidelines developed during the project represent an opportunity for impact on an additional set of beneficiaries: namely, those who develop and deploy social robots that are designed to interact with children.


How will they benefit from this research?

Achieving the project objectives will deliver numerous practical benefits to all stakeholders involved in the target clinical setting. At one level, as the research plan itself involves children, parents, and healthcare providers throughout the process, ensuring that they are fully consulted throughout every stage and engaged in the development of the robot software and the results of the study. Also, the new, flexible, robust robot intervention has the clear potential to benefit all of the above stakeholders: children will experience less pain and distress in clinical settings; their parents and carers will experience less distress and anxiety as well; while the healthcare providers will find it easier and less stressful to carry out procedures in the short term, and will also not need to deal with longer-term consequences such as needle phobia. Clinical decision makers who determine standards of care in emergency departments will also benefit from the addition of a novel, engaging, clinically proven method of reducing pain and distress in the emergency department and other clinical settings.

Developers of similar social robots will also directly benefit from the work done on this project: not only will they be able to use the publicly released software components, but also -- and perhaps more importantly --- they will have access to a comprehensive, evidence-based set of recommendations and guidelines on how to ethically and responsibly integrate a user-centric approach into their AI and robotics projects.


What will be done to ensure they have the opportunity to benefit?

Our primary channel for delivering impact is through Solutions for Kids in Pain (SKIP), a federally-funded knowledge mobilization network in Canada that seeks to bridge the gap between current treatment practices and available evidence-based solutions for children's pain in Canadian health institutions. Through an in-kind contribution, SKIP will permit us to:
- Disseminate and share information about the research across the SKIP network including in the newsletter, website, and social media.
- Take advantage of knowledge translation activities and opportunities such as media pitches, partner blog posts, and featured articles.
- Have access to 48 Canadian health institutions, including all children's hospitals in Canada, to support data collection and connections with patient, parent, and health professional groups, as well as access to regional knowledge brokers at sites across Canada."
15,042B9AD3-4E5F-4B27-8D25-2B6FC701D1F2,Responsible Automation for Inclusive Mobility (RAIM): Using AI to Develop Future Transport Systems that Meet the Needs of Ageing Populations,"To capture the full social and economic benefits of AI, new technologies must be sensitive to the diverse needs of the whole population. This means understanding and reflecting the complexity of individual needs, the variety of perceptions, and the constraints that might guide interaction with AI. This challenge is no more relevant than in building AI systems for older populations, where the role, potential, and outstanding challenges are all highly significant. 

The RAIM (Responsible Automation for Inclusive Mobility) project will address how on-demand, electric autonomous vehicles (EAVs) might be integrated within public transport systems in the UK and Canada to meet the complex needs of older populations, resulting in improved social, economic, and health outcomes. The research integrates a multidisciplinary methodology - integrating qualitative perspectives and quantitative data analysis into AI-generated population simulations and supply optimisation. Throughout the project, there is a firm commitment to interdisciplinary interaction and learning, with researchers being drawn from urban geography, ageing population health, transport planning and engineering, and artificial intelligence.

The RAIM project will produce a diverse set of outputs that are intended to promote change and discussion in transport policymaking and planning. As a primary goal, the project will simulate and evaluate the feasibility of an on-demand EAV system for older populations. This requires advances around the understanding and prediction of the complex interaction of physical and cognitive constraints, preferences, locations, lifestyles and mobility needs within older populations, which differs significantly from other portions of society. With these patterns of demand captured and modelled, new methods for meeting this demand through optimisation of on-demand EAVs will be required. The project will adopt a forward-looking, interdisciplinary approach to the application of AI within these research domains, including using Deep Learning to model human behaviour, Deep Reinforcement Learning to optimise the supply of EAVs, and generative modelling to estimate population distributions. 

A second component of the research involves exploring the potential adoption of on-demand EAVs for ageing populations within two regions of interest. The two areas of interest - Manitoba, Canada, and the West Midlands, UK - are facing the combined challenge of increasing older populations with service issues and reducing patronage on existing services for older travellers. The RAIM project has established partnerships with key local partners, including local transport authorities - Winnipeg Transit in Canada, and Transport for West Midlands in the UK - in addition to local support groups and industry bodies. These partnerships will provide insights and guidance into the feasibility of new AV-based mobility interventions, and a direct route to influencing future transport policy. As part of this work, the project will propose new approaches for assessing the economic case for transport infrastructure investment, by addressing the wider benefits of improved mobility in older populations.

At the heart of the project is a commitment to enhancing collaboration between academic communities in the UK and Canada. RAIM puts in place opportunities for cross-national learning and collaboration between partner organisations, ensuring that the challenges faced in relation to ageing mobility and AI are shared. RAIM furthermore will support the development of a next generation of researchers, through interdisciplinary mentoring, training, and networking opportunities.",,"There are a number of potential beneficiaries for this research, and the project has established partnerships with policymakers, stakeholders, and industry bodies to ensure impact is maximised. These beneficiaries can be broadly grouped into the following categories:

Local Transport Authorities and Government - Through partnerships established through the project, the local transport authorities in Manitoba and the West Midlands will gain access to project outputs (data, reports, analyses, new methods, and models). These outputs will help inform future policymaking on older population mobility, and promote long-term planning for new travel mode integration. More broadly, the research developed during the project will be of benefit to local transport authorities worldwide, given both the global context of ageing populations, and the broad scope of the research setting that captures both urban and rural needs in two diverse contexts. The project will furthermore contribute towards discourse around the economic value of transport investments in relation to older populations, and highlight the wider benefits of investment to local society, economy, and government spending. Broader links will be achieved through research dissemination in the form of reports, presentations, and events, that highlight the potential for this research. Project partner CUTRIC (Canadian Urban Transit Research &amp; Innovation Consortium) will be central to enhancing the reach of the project findings and outputs.

Local Citizens - The research will benefit older populations in Manitoba and the West Midlands in particular, through the design of a transportation service which meets mobility needs while minimising constraints and barriers to use. The project will explore the economic case for investment in an electric autonomous vehicle (EAV) demand responsive transit (DRT) system, which accounts for more than typical values on economic growth. The project will also provide opportunities for local citizens to learn more about AVs, and assess their potential benefits. More broadly, the adoption of such technology, will have a benefit for all local citizens, by promoting inter-generational interaction, and reducing care burdens on family members. Links to older populations in our regions of interest will be made directly through partnership with local support groups (e.g., Transportation Options Networks for Seniors).

Non-Profit Organisations - In the same way that the research will benefit local populations, RAIM will provide non-profit organisations the evidence and tools needed for campaign effectively for new approaches to older mobility. The research will produce improved methods for meeting the mobility needs of older populations with automation, and provide a framework for making the economic case for public intervention. These benefits will be realised through provision of materials on the project website, and through the final project meeting, to which we will invite wider participation from these groups (via CUTRIC and the University of Manitoba Centre for Ageing), to be held in Winnipeg.

Autonomous Vehicle Manufacturers - By identifying pathways to improving the provision of EAVs to an older population, this research represents a new market for AV manufacturers. In developing a case for intervention, local governments (beyond Manitoba and the West Midlands) will require suppliers of an EAV DRT service, resulting in both vehicle sales and ongoing maintenance contracts. This market presents potentially stronger early economic opportunity than appealing to private ownership, which is currently cost prohibitive. Opportunities for engaging with these groups will be formed through our project partner CUTRIC, an organisation to promote research translation in Canada."
16,66A8767B-36F0-4BA0-A546-F262268B4F91,UMPIRE: United Model for the Perception of Interactions in visuoauditory REcognition,"Humans interact with tens of objects daily, at home (e.g. cooking/cleaning) or outdoors (e.g. ticket machines/shopping bags), during working (e.g. assembly/machinery) or leisure hours (e.g. playing/sports), individually or collaboratively. When observing people interacting with objects, our vision assisted by the sense of hearing is the main tool to perceive these interactions. Let's take the example of boiling water from a kettle. We observe the actor press a button, wait and hear the water boil and the kettle's light go off before water is used for, say, preparing tea. The perception process is formed from understanding intentional interactions (called ideomotor actions) as well as reactive actions to dynamic stimuli in the environment (referred to as sensormotor actions). As observers, we understand and can ultimately replicate such interactions using our sensory input, along with our underlying complex cognitive processes of event perception. Evidence in behavioural sciences demonstrates that these human cognitive processes are highly modularised, and these modules collaborate to achieve our outstanding human-level perception.

However, current approaches in artificial intelligence are lacking in their modularity and accordingly their capabilities. To achieve human-level perception of object interactions, including online perception when the interaction results in mistakes (e.g. water is spilled) or risks (e.g. boiling water is spilled), this fellowship focuses on informing computer vision and machine learning models, including deep learning architectures, from well-studied cognitive behavioural frameworks.

Deep learning architectures have achieved superior performance, compared to their hand-crafted predecessors, on video-level classification, however their performance on fine-grained understanding within the video remains modest. Current models are easily fooled by similar motions or incomplete actions, as shown by recent research. This fellowship focuses on empowering these models through modularisation, a principle proven since the 50s in Fodor's Modularity of the Mind, and frequently studied by cognitive psychologists in controlled lab environments. Modularity of high-level perception, along with the power of deep learning architectures, will bring a new understanding to videos analysis previously unexplored.

The targeted perception, of daily and rare object interactions, will lay the foundations for applications including assistive technologies using wearable computing, and robot imitation learning. We will work closely with three industrial partners to pave potential knowledge transfer paths to applications.

Additionally, the fellowship will actively engage international researchers through workshops, benchmarks and public challenges on large datasets, to encourage other researchers to address problems related to fine-grained perception in video understanding.",,"The fellowship focuses on learning a model for understanding human object interactions, using visual- and auditory-sensors, with novel capabilities. The model will be capable of understanding the actor's hierarchy of goals and predicting upcoming interactions. The model will also be able to map the perceived interaction into a set of steps that could be replicated by a robot, tested within a simulated environment. 

By enhancing the capabilities for computer vision models for recognising human-object interaction, the fellowship has limitless impact on future technologies. The economic and societal impacts are here intertwined where industry would be the prime beneficiary to build new technology, but individuals would be the end users. I summarise the potential through three application areas, impactful on the UK's national capabilities of several industries, and availing opportunities previously unexplored. 

1) Assistive Technologies
Every individual can benefit from assistive technologies of object interactions. For example, reminding a person whether they had added salt to their meal or securely closed a water tap are capabilities of the model UMPIRE. Further assistance specialised for the elderly or people with impairments can be envisaged where alarms are raised in cases of unsafe interactions. Several start-ups have attempted to use assistive technologies in daily interactions. These however rely on specialised sensors to be integrated with every instrument (one sensor per tap to detect running water). Instead, this project promises human-level cognition using general visuo-auditory sensors, not specialised for the action. Through a model that can understand and detect the interaction's consequences and changes to environment (e.g. if water is still pouring then the water source has not been secured), the potential for assistive technologies will be widely enhanced. To realise this impact the fellowship, will engage with the Samsung AI Centre Cambridge, where assistive wearable technologies are under development.

2) Robotics and Beyond
A key capability of the UMPIRE model is actionable perception, i.e. a step-by-step procedure for an artificial agent to replicate the object interaction. This capability will be impactful to people working on vision for robotics. Teaching a robot how to 'open a can' by demonstrating the interaction is a main objective for effective household robotics. In this fellowship, I work closely with NVidia, originators of the open source simulating development kits Isaac and PhysX, to prepare for this impact.

3) Entertainment and Gaming
Virtual and augmented reality games can now integrate a three-dimensional avatar in our home, running around our sofas and tables. However, object interaction perception would enhance the ability to integrate these games with our everyday tasks combining life with fun. Though perceiving object interactions, avatars would be able to simulate opening your kitchen tap and augmented water flowing. Currently, such potential requires hand-coded graphics. Using a model for interaction perception would enable novel entertainment applications.

In this fellowship, I will engage with the first two impact areas, but note gaming as a potential for further exploration. Due to the large commercial potential, the fellowship will have a commercialisation plan, developed through consultation with Ultrahaptics and SAIC towards a spin-out and/or knowledge transfer.

In addition to the economic and societal impact, the fellowship has an impact on integrating two very active research communities, particularly in the UK: cognitively-inspired human behaviour and data-driven computer vision. New research directions can emerge introducing tools for data-driven research to cognitive psychologists."
17,F905D66C-3152-4790-BB8E-EB3CA0DA4363,Self-guided Microrobotics for Automated Brain Dissection,"Neural precursor cells (NPCs) are neural stem cells and their progeny, which are multipotent, self-renewing cells that are responsible for building the fundamental components of the vertebrate central nervous system during embryonic development. Notably, NPCs persist into the mature brain as very rare cells that continue to generate new neurons throughout the lifetime of the organism. Intensive study of these precious cells could lead to a watershed moment in the field of neurobiology and the development of regenerative therapies to treat Alzheimer's disease, dementia, and traumatic brain injury. However, our ability to harness the therapeutic potential of NPCs is limited by the practical difficulty of identifying and collecting them for analysis. Current methods require painstaking manual microdissection of neural tissue followed by cell sorting methods which, in addition to requiring large samples sizes (and thus large quantities of tissue) also often fail to distinguish between NPCs and other cells expressing the same biomarkers.

This highly interdisciplinary project brings together researchers with expertise in artificial intelligence (AI), robotics, engineering and neuroscience to create a new automated microsurgical platform which will be applied to identify and collect NPCs from brain tissues. By exploiting the capacity of deep learning approaches to detect features in complex data sets we will develop two new image-guided microsurgery tools: a microrobotic resector capable of carefully excising the small region of brain tissue (the SVZ) within which NPCs are found and microrobotic cell collector, capable of efficiently harvesting individual cells for further study and analysis using a coordinated array of optoelectronic microrobots. When then aim to integrate these two systems into a single, unique robotic microsurgery platform.

The new system will be applied to collect and analyse NPCs from different brain tissues. In particular, we propose to analyse the sex-dependent differences in NPCs phenotypes, which have been reported in previous research. As well as transforming our capacity for collection of NPCs and likewise supporting the development of new regenerative therapies, we believe this approach will provide a powerful new method for a wide range of microsurgery applications. Altogether, we will use these results to simulate debate among the related engineering, scientific and wider public communities to shape a new international multi-disciplinary network focusing on challenges with regenerative medicine i.e. providing new case studies for govt. policy and further investment for AI-driven healthcare applications.",,"This project addresses the need for improved microsurgical tools in neurobiology through development of a new autonomous micro-robotic concept powered by Artificial Intelligence (AI). The work brings together a diverse range of world leading research expertise across both Canadian and UK institutions in nerobiology, computer science, robotics and engineering. Given the broad range of connected sciences that also include, optics, machine vision, neuroscience, healthcare engineering etc. we expect the results generated to have a wide reach. Importantly, UCL is well placed to support these activities by providing access to aligned investments to facilitate clinical applications and translation through our flagship Wellcome / EPSRC Centre in Interventional and Surgical Sciences (WEISS) and its links to institutionally backed units like the Translational Research Office, the NIHR UCLH BRC Joint Research Office and the UCL Institute for Healthcare Engineering. As such, we believe this consortium has the potential for significant impact opportunities for novel technical development and knowledge exchange that aims to change healthcare communities to adopt new ways of working. 

Examples of planned activities for industrial, academic and societal impact include:

- Publication of peer-reviewed journal articles describing the techniques and results from the project, presentations at major scientific conferences (such as FoM, MICCAI and ICRA) and dissemination of results more widely to the life science research and clinical healthcare research community at national meetings. 

- UK-Canada community building workshop: We will host a workshop in Canada to demonstrate the application of new automated microscopy and microsurgery techniques, involving a range of stakeholders from academics, industry and clinicians to discuss the implications for AI and automation technologies cell and neurobiology and regenerative medicine. Specifically this will lead to generation of case studies and policy work to help align and grow a common roadmap for the community.

- Supported by experienced communication officers, we will develop thought provoking images and movies. Acting as a tool to inspire the next generation of researchers and engage the wider public in scientific research, we anticipate that the new AI-driven imaging capability developed under this project will generate extremely rich media content and world-wide visibility of multidisciplinary Canada-UK research. We plan to exploit existing relationships with media outlets including the BBC, The Guardian and Wired Magazine to raise wider awareness of the research.

- In all teaching, we engage students with our latest cutting edge research which both enhances the learning experience, aids dissemination of our ideas and long-term research agenda ensuring impact. We intend to offer student research projects associated with microscopy, AI-driven automation and computer vision, bioimage analysis and biology to students enrolled in UCL CDTs focusing on AI-fundamentals and applied challenges 

- Following on from previous events, outputs of the project will contribute to a yearly demonstrator showcase as part of UCL Robotics Week to promote wider engagement end-user stakeholders within the life sciences, AI and Robotics. Other planned events include Science Rendezvous events

- The project will establish a clear exploitation plan at the start of the project. This includes guidance from expertise from UCLB, UoT and assoicated industry advisory boards to ensure we achieve maximum impact from the generated results."
18,07761DFA-885B-449A-AAFC-DD68DD45A477,AI for DIGILAB: A New Concept in Digital Infrastructure for Heritage Materials Research,"Over the last 20 years, digital imaging or digitisation of collections has become the norm within museums and archives. However, so far, it has mainly been focussed on recording what humans can see with their eyes, that is, colour RGB images and sometimes laser scanning of 3D objects. Growing interest by digital humanities scholars and medievalists in advanced imaging techniques and the new layers of information they can uncover affirms that the curatorial, art historical, and historical fields are receptive to a concept that has been explored by heritage scientists for several decades. We propose that the material composition of heritage objects analysed through various modalities of imaging spectroscopy such as reflectance spectral imaging and macro X-ray fluorescence (MA-XRF) scanning may be incorporated into digitisation campaigns to deliver a disruptive transformation in arts and humanities scholarship related to heritage. Since each material combination has its unique spectrum, imaging spectroscopy, depending on the modality, records to a greater or lesser extent, the material makeup (e.g. the pigments, dyes, binders, substrates) of an object. These added layers of information about heritage objects can lead to new insights and narratives about their creation, history of trade and cultural influences, and can impact significantly on conservation and preservation decisions. In addition, reflectance spectral imaging in the visible naturally gives the most accurate colour images thus removing the need for recording colour images. 
Nottingham Trent University's (NTU) ISAAC research group has made the first step in automatic collection of high spatial resolution reflectance spectral images of tens of square metres of wall paintings. Automatic data collection increases significantly the rate of data generation necessitating an automatic tool to process and reduce the data. While ML/AI has been used mostly in searching and organising digital content in the sector, the ISAAC team has pioneered their use in large scale heritage materials analysis. In addition to a new bespoke digital tool, we propose a new concept in digital research infrastructure through making the tool available to users remotely. The European Research Infrastructure for Heritage Science is divided into 4 platforms of operations: archives (ARCHLAB), mobile laboratory (MOLAB), fixed laboratory (FIXLAB) and digital laboratory (DIGILAB). While the first three platforms are well established, DIGILAB is in the concept phase, but offers opportunities for transformation in terms of access to digital tools and resources. Here we propose a model where DIGILAB functions as a data analysis lab where the user is helped remotely with their data analysis. The ML code will automatically process the image cubes into materials cluster maps and the experts will examine the results before releasing it to the users. A user interface and a visualisation add-on will also be developed to allow the user to view the outputs in a user-friendly manner. The remote access to digital lab aspect of the project aims to lower the barriers collections and scholars face in unlocking potentially relevant information encoded in the identity and distribution of materials used in the creation of heritage objects. Three varied case studies, each chosen for their different data-related challenges, will also serve to demonstrate and address issues in the workflow, starting from scientific data collection, then the new concept of DIGILAB for data reduction leading to material identification using complementary spectroscopic techniques, to finally address the research questions in history and conservation.",,"Beneficiaries
Primary:
Cultural institutes, both private and public, specifically curators, conservators and scientists, including consultant/freelance/SME conservators, archaeologists and conservation scientists, and indirectly the general public that GLAM engages with. Cultural organisations of various sizes, resource and knowhow availability, location irrespective of whether they are in the global developed or developing regions.
Secondary:
All sectors private or public that use modern analytical imaging technology to characterise/monitor materials. These could include remote sensing, biomedical diagnostics, industrial asset monitoring.

Impact goals
1. increase the incorporation of heritage materials information in the interpretation, presentation and care of collections and ultimately into large scale digitization campaigns
2. change the digitisation practice of cultural institutes by incorporating the recording of materials information using modern analytical imaging technology
3. narrow the gap in access to modern imaging technology and data science between institutes of different sizes (e.g. national versus regional) and locations (e.g. developed versus developing regions) through the use of the new concept of DIGILAB with the support of MOLAB for data collection 
4. increase the collaboration in studies of colonial collections with those in the former colonies
5. demonstrate the impact arts and humanities research can have in other disciplines (e.g. remote sensing and biomedical imaging) and in industry (e.g. quality control and asset management), based on the challenging demands posed by materials research in cultural heritage.
6. in the longer term the adoption of the new concept of DIGILAB and the combined MOLAB + DIGILAB offering by E-RIHS and other research infrastructures"
19,BA9D7B28-46E1-40AA-A152-3C76B2E0A4F7,Swansea University - Capital Award for Core Equipment,"Swansea University will use this equipment award from EPSRC to strategically invest in state-of-the-art equipment in key areas across the Colleges of Engineering and Science within the remit of EPSRC. The primary of objective of these interventions is to maximise the benefit to our sizable research community by enabling scientific excellence and in turn accelerate the careers of the next generation of researchers. This additional investment will act as a catalyst for research activity in key areas.

The equipment has been carefully selected to augment, enhance, and add value to the current research capabilities at the University. It will allow for the creation of novel testing techniques and will give its users a significant
advantage in terms of the data they collect and the methodologies they are able to employ. This, in turn, will lead to publication in high impact factor journals, create further opportunities for collaboration through extensive public engagement, and help leverage external funding to further develop these areas.

The specific subject areas that will be enhanced by the investment are: Artificial Intelligence, Machine Learning, Supercomputing, Photovoltaics, and Electrochemical Storage. 

These targeted subject areas will generate impactful research outcomes that will be both enabled and accelerated through this investment.",,"This grant will benefit researchers at SU working under the EPS remit, which includes all the academic beneficiaries and their research groups. By supporting world-leading research through the provision of world-class labs, the Near Infrared (NIR) equipment and Graphics Processing Unit (GPU) clusters will support researchers in the fields of:

-Materials for Energy Applications 
-Energy Storage
-Artificial Intelligence Technologies
-Machine Learning 
-Supercomputing
-Mathematics
-Photovoltaics, Solar Fuels and Photocatalysis
-Printing and coating
-Thermal Processing
-Sheet Steel Processing

By creating a positive and enabling research environment, this investment has the potential of generating and accelerating impact on economy, society, knowledge, and people. 

These state-of-the-art pieces of equipment will give their users significant advantages which will lead to higher impact factor journal articles and help leverage external funding. These equipment purchases are key-enabling factors for exciting, disruptive, and influential research."
20,C37E91C0-B2F1-4AB6-9B22-54A22C5383CB,EPSRC Capital Award for Core Equipment,"The funding available through this grant will be used exclusively for the procurement of equipment as required by the call. Seven items of equipment have been identified as key strategic priorities for the University, through a competitive review process, identifying where this capital investment will add most value in terms of research excellence. The investment opportunities outlined in this proposal are in addition to planned investments in capital equipment at the institutional level. However, to demonstrate its support for the strategic areas for investment outlined in this proposal, the University will provide a contribution towards the full cost of the equipment for those items which exceed the maximum limit offered by the EPSRC as part of this call. The primary beneficiaries from the additional resources requested in this proposal will be the researchers within the University of Cambridge, unlocking new research avenues and enhancing overall equipment portfolio available for sharing.",,"The new equipment acquired through the EPSRC Capital Award for Core Equipment will strategically strengthen the productivity of core equipment available to researchers working within the remit of the EPSRC's portfolio, as well as their collaborators, also in industry. This investment will complement the commitments that the University has already made towards supporting core equipment and will enable the University to build upon our provision of world class status for laboratory equipment. Existing equipment sharing project outreach activities will provide additional awareness of the equipment across the University and beyond. Although primarily supporting EPSRC-remit research, in line with other activities in Cambridge this investment will lead to wider impacts on economy and society in due course. Capitalising on the progress that has already been made towards enabling broad shared access to equipment will also further this proposal's role in enabling development of new technologies. New knowledge that will be generated, and translation of research outcomes from the proposed portfolio of this new core equipment set will undoubtedly lead to economic and social impacts - creating jobs, driving economy and UK's international competitiveness through spinout formation, and providing society with better products and solutions in the longer term. Due to a broad scope of this capital investment it is not straightforward to detail all potential beneficiaries and impacts, but though placing of the equipment in optimal environment for impact acceleration these will be enabled and fostered across the entire project portfolio. Likely frontline beneficiaries beyond Cambridge academic community range from the British Antarctic Survey (improving studies of environmental samples and underpinning understanding behind the climate emergency), industry in materials, energy, biotech, nano, optical and electronic sectors, to tech SMEs and spinouts working on materials science and device innovations. Likely industrial partners identified so far include: Aixtron, BP, HexagonFab, Hitachi Cambridge Laboratory, Paragraf, Prognomics. Sorex Sensors, Silson, Talga Technologies and more. The commercial direct and indirect impacts are expected to begin within the first year of the grant, continuing do develop, as well as new engagements to start also well beyond the duration of this award. Considering the integration of new equipment into departments, cross disciplinary research initiative and broader innovation ecosystem in Cambridge, the award is very likely to enable impact through new research projects and collaborations that it will make possible in near future. Training of a wide range of equipment users will produce a pipeline of skilled employees for both the HEIs as well as local technical consultancies, spin-outs and companies within the Cambridge cluster and beyond. Opening of the access to World Leading experimental capabilities to external users will also enhance the cutting-edge skillset capability building in the UK. Finally, the proposal will contribute to maintaining world leading position of Cambridge as the most vibrant university-innovation ecosystem in Europe, providing yet stronger reasons for new investment and companies to join the cluster, indirectly contributing to regional development around Cambridge and Peterborough."
21,114212A6-2E83-4101-A21D-474870F9B089,Artificial intelligence to create equitable multi-ethnic polygenic risk scores that improve clinical care,"Overall Objective: To develop state-of-the-art artificial intelligence (AI) methods which address the ethnic inequities inherent in genomic medicine and rapidly emerging polygenic risk scores (PRSs). We will subsequently apply these approaches to demonstrate improved screening, diagnosis and treatment of common disease.
 
Background: Recent breakthroughs in genomics and machine learning have generated personalized medicine tools that have the potential to improve patient care and health maintenance through PRSs. PRSs are risk predictors, which combine information across the entire genome to predict a person's risk of disease, or whether they have unfavorable disease risk factors, such as high cholesterol levels. While still evolving rapidly, these PRSs are more predictive of susceptibility to disease than many traditional disease risk factors. Since PRSs can predict risk of disease they could be helpful in screening programs, by identifying a group of individuals at such low risk of disease, that screening would be unlikely to be helpful-thereby allowing screening programs to focus on individuals at highest risk. They could also refine diagnosis, by identifying people in the population most likely to have a disease and then targeting required diagnostic testing in those at highest risk. Last, they can also improve disease treatment by identifying individuals most likely to benefit from treatment.
 
While PRSs could help to improve clinical care, they have been largely developed in individuals of European-only ancestry. This is because the large cohorts from which PRSs have been developed consist predominantly of European-ancestry participants. Since genetic make-up varies by ancestry, the performance of these PRSs in non-European ancestries is considerably worse; it is known that not only differences in in genetic risk factors lead to attenuation of performance of PRSs, but also that changes in correlation patterns in our genomes due to distinct population histories decrease predictive accuracy even if the actual genetic risk factors remain the same. 
 
This creates several important problems for the roll-out of such tests in the Canadian and UK healthcare environments. This is because approximately 22% of Canada's population and 12% of the UK are visible minorities. Further, these populations can have increased rates of health care utilization. Therefore, use of European-only PRSs could serve to worsen existing health disparities.
 
Canada's Chief Information Officer stated that, &quot;Using Artificial Intelligence in government means balancing innovation with the ethical and responsible use of emerging technologies.&quot; Using PRSs developed to benefit only the majority group in our societies would not respect these directives and therefore our program will serve to ensure the responsible use of AI. 
 
What are our Specific Goals?
1) To develop AI methods and open source software packages to improve the accuracy of PRSs for CHD and hyperlipidemia for individuals of non-European ancestry.
2) To compare the performance of these ancestry-adapted PRSs in individuals in a diverse set of cohorts, representing multiple ancestries in the UK and Canada. 
 
The Team: Recruiting leaders across the UK and Canada, we have a depth of expertise in AI methods development, human genetics, clinical medicine, cohort development for minority groups, and statistical genetics. This gender-balanced team involves an appropriate mix of senior and junior investigators.
 
Relevance: Success in this AI-enabled program will allow for the transfer of recent advances in genomic medicine to the citizens of Canada and the UK, regardless of ancestry. The AI methods developed will be widely applicable to other PRSs in development by other groups. As genomics takes root in regular clinician-patient interactions, our set of AI-based tools will ensure that the benefit derived from these advances will be shared by all citizens of our countries.",,"In this proposal, we aim to address a real-world clinical problem that if not resolved will impair the equitable uptake of AI-based genomics into clinical care. Given the urgency of this problem, we have incorporated into our program from its inception end-users who will directly benefit from this research. 

Who will benefit from this research? If successful, our research will predominantly benefit people who are of non-European descent. To realize this benefit, we have engaged four end-users (see Letters of Support). Here we describe each end-user and their role in the program.

The Integrated Health &amp; Social Services University Network for West-Central Montreal (known by its French acronym CIUSSS) provides clinical care to approximately 362,000 people who are served by a partnership of more than 30 healthcare facilities. Included is one of Montreal's leading hospitals and three specialized hospitals, five community care clinics, two rehabilitation centres, four residential centres, two long-term geriatric residences, and two day centres. Dr. Lawrence Rosenberg is the CEO of this CIUSSS and directly supports our program of research since the largest multicultural community in Qu&eacute;bec is served by this CIUSSS. Dr. Rosenberg is currently leading a team of researchers and clinicians to implement genomics-based medicine within the CIUSSS.

Health Data Research UK is the UK's national Institute for health data science, directed by Professor Andrew Morris. It is an independent, non profit organisation bringing together 22 of the UK's leading universities and research institutions to address a common mission of uniting the UK's health data to make discoveries that improve people's lives. HDR UK's vision is that every healthcare interaction and research endeavour will be enhanced by access to large scale data and advanced analytics. The ambition to generate advanced AI tools to address health inequalities and collaborative research presented in this proposal fully aligns with HDR UK's mission.

Genomics England is owned and funded by the Department of Health &amp; Social Care, England, set up to deliver the 100,000 Genomes Project. Its four main aims are to create an ethical and transparent genomics medicine programme based on patient consent; to bring benefit to patients and set up a genomic medicine service for the NHS; to enable new scientific discovery and medical insights; and to kick-start the development of a UK genomics industry. The 100,000 Genomes Project aims to bring the benefits of personalised medicine to the NHS. To make sure patients benefit from innovations in genomics and contribute towards delivering high quality care for all, now and for future generations. Involvement of Genomics England will provide an important route to further demonstrate the clinical utility of the equitable PRSs developed within this project.

Think Research represents our knowledge translation partner. The clinical impact of our research can only be realized if physicians understand when to order PRSs, how to interpret their results, document their findings and alter patient care. We are not naive to the tremendous difficulties encountered when attempting to change the behaviour of physicians. Therefore we have engaged with Think Research, a Canadian company that has specialized in electronic health record clinical support tools. Think Research's solutions are used in over 2,000 health care facilities world-wide. After completion of our program, our knowledge translation partner will maximize the clinical utility of our research by enabling uptake in health care systems in both Canada and the UK. Despite our engagement with Think Research, they will have no access to data, or the algorithms that were derived from data. 

Summary: Through careful engagement with four essential end-users we will maximize the clinical impact of our research program."
22,35DE4247-507D-4F4C-BE40-1B6DA589A44A,Creative AI: machine learning as a medium in artistic and curatorial practice,"Contemporary art institutions find themselves at the forefront of a wider cultural reflection on critical issues of social and ethical importance. In relation to Artificial Intelligence, however, cultural institutions are in a paradoxical position: on the one side, there is a pressing need to reflect critically and creatively on the ethical and societal impact of AI, and for this AI artworks play a pivotal role. On the other side, cultural institutions often do not have the necessary technical knowledge to support the production of such works, despite the fact that artists such as Trevor Paglen or Adam Harvey have made important contributions by critically using and questioning AI. This project aims to tackle this lack of curatorial knowledge by building tools and guidance for curators to advance institutional media literacy in AI. 

Faced with the situation that AI is currently having a transformational impact on our cultures and societies, and given the fact that exhibitions featuring AI artworks are central for delivering critical and ethical inquiries into this technology as well as for advancing a much needed public understanding, our project asks: how can we strengthen curatorial knowledge about AI artworks? 

To answer this question and research the missing knowledge of this new artistic medium, our project brings together researchers, artists, cultural practitioners and technologists by connecting the networks of two academic - KCL and NYU - and two contemporary art institutions - Serpentine Galleries and Rhizome/New Museum - situated in London and New York, two centres of both, contemporary art and AI. Each institution has gained its own genuine expertise in the emerging field of Creative AI: In July 2019, Serpentine Galleries have founded a Creative AI lab in collaboration with the Department of Digital Humanities, KCL, with the aim of exploring AI as a tool for artists and gaining a better understanding of the digital skills needed to support critical artistic engagement with this new technology. Around the same time, NYU's Digital Theory lab launched an inquiry into AI Aesthetics, and Rhizome/New Museum started to deliver workshops for artists exploring AI. 

Bringing these initiatives into a closer dialogue with one another by linking up their genuine knowledge through collaborative research and a series of workshops, the project will proceed in two steps. Firstly, the project will map existing creative AI tools and explain them targeting curators and artists; and explore how AI technology transforms artistic and with it curatorial practice, to be able to create a systematic introduction into the digital skills and knowledge curators need. Having understood aspects of production of AI artworks, the project will use this knowledge base for a second step, to explore how curators can translate the impact of this new technology into a critical but also creative response. For this, the project systematically studies the social, ethical, and last but not least the aesthetic aspects of contemporary AI. 

To explore and verify the above mentioned findings, the project will set up a creative AI experiment that runs parallel to its theoretical explorations. Together with a software engineer, a chatbot prototype will be built, designed intentionally to make aspects of the decision-making process of its AI technology visible through an innovative interface; the interface will reveal decision-making aspects when interacting. The chatbot will be trained on a dataset of interviews which the Serpentine Galleries's Artistic Director Hans Ulrich Obrist has been conducting and recording with artists, architects and thinkers on subjects of art and culture. 

Researchers and partners participating in this project share the insight that AI technology profits from and needs more cultural exploration. This project will provide curators with the right skills to support and inquire this technology.",,"Overview: This research will directly and immediately benefit anyone working with or about Creative Artificial Intelligence, first and foremost contemporary art galleries as well as more generally cultural institutions interested in involving artworks using Machine Learning; artists working with AI; start-up and computer companies invested in Creative AI; last but not least, our research will engage with and impact the public discourse about AI and ethics by making aspects of AI transparent to a general audience. 

Art galleries and cultural institutions: the needs of contemporary art galleries are central to this project and it is here where we will impact most clearly. Some of the project's outcomes - the AI tool map, the webinars, the project website, the reports - are targeted at contemporary art galleries working with AI - to inform their curatorial practice by delivering knowledge relevant for budgeting, managing and commissioning a new artwork involving AI, or by preparing the artwork's exhibition including the information that should be displayed or be covered in a catalogue, be explained in guided tours or introduced at associated events. This information is not just relevant for contemporary art galleries but also for the 1,800 museums registered with the Museum Association, of whom many are interested in using AI in a creative way to playfully process parts of their digitised collection. 

Artists: Some of the project's findings inform artists who are working with or would like to work with Machine Learning, for whom the introductory overview of existing tools to produce Creative AI is providing guidance, and who can act on the information of the webinars introducing the calibration of neural networks through interfaces often used by artists, and who want to learn about central issues regarding training data and aesthetic theories and societal topics linked to AI. 

Start-up and computer companies: Companies such as Adobe as well as start-ups explore the creative application of Machine Learning and our research into its usage in artworks provides pointers for this industry. Furthermore, the explainability of AI has become a recognised issue, to which our practical research experiment building a chatbot prototype whose interface reveals its decision-making process will contribute. 

General public: Given the fact that AI has become an omnipresent technology - from suggesting words when we text on our smartphones to wrongly or rightly identifying criminals for the Metropolitan Police using facial recognition -, the efforts of this project will directly (in its public facing events) and indirectly (through allowing cultural institutions to stage more of them) have an impact on the general public. This is the more the case as the exhibition at the Barbican, London 'AI: More than Human' was highly successful showing that there is a strong interest by the general public to engage with this topic.

Engaging with non-academic players is a profound part of this research project and having impact on the areas above will be actively initiated. We will approach players in the non-academic areas named above, whether this is via inviting them as guest speakers or to come to our events, or to give semi-structured expert interviews (artists, computer industry), or through spreading our findings via the social-media accounts of the four participating partners in this project."
23,95CC39F8-AFBB-4011-B752-E31D288110AA,Automating Representation Choice for AI Tools,"AI engines are ubiquitous in our lives: we talk to our mobile phones, ask directions from our sat-navs, and learn new facts and skills with our digital personal assistants. But most of the time, we need to learn how these systems work first: we have to adapt to them as they are not aware of our level of experience, expertise and preferences. AI engines are fast and can deal with a deluge of data much better than us, but they do so in machine-oriented ways, which are often inaccessible and unintelligible to humans. How can we build machines that will adapt to us?

The aim of this project is to identify and study how humans represent information that they want to work with and from which they will obtain new knowledge. Humans have the capability to choose the representation that works for them to enable them to solve a new problem, and moreover, if the representation needs to be changed, they can spot this and change it. Unlike humans, machines in general have fixed representations and do not have the understanding of the user. For example, sat-nav systems will only give directions with elementary spatial commands or route planning functions, whereas humans give directions in many forms, for instance in terms of landmarks or other geographic features that are based on shared knowledge.

We want to model in computational systems this inherently human ability to choose or change appropriate representations, and make machines do the same. We want to find out what are the cognitive processes that humans use to select representations, what criteria they use to choose them, how we can model this ability on machines, and thus how to engineer systems that will select effective representation to enhance people's problem solving and learning. 

This is a grand challenge, because it must marry human with machine capabilities. Our previous EPSRC funded feasibility project brought together an interdisciplinary team to combine expertise in computer science on automated reasoning with diagrammatic representations (Jamnik, Cambridge) with expertise in cognitive science on human problem solving and learning with representations (Cheng, Sussex). This interdisciplinary approach has been critical to the success of the feasibility project.

We previously showed that when humans choose a representation of a problem, they use cognitive and formal properties of the problem and its representation to make their choice. In this project, we build on and generalise this hypothesis and demonstrate its utility by building a mathematics tutor that intelligently picks good representations according to the skill level of different learners. We have the following goals:

1. Develop representation selection theory based on the formalisation of formal and cognitive properties.

2. Develop a cognitive theory to assess the efficacy of alternative representations and methods for selecting representations suited to the competencies of individual users.

3. Devise computational algorithms (software) that mechanise the right choice of representation based on the theoretical foundations.

4. Develop and test the algorithms on a range of domains to demonstrate the scalability and generality of the approach.

5. Build an AI tutoring system that implements automated and personalised representation choice based on the user's level of expertise and experience.

6. Empirically evaluate the capability of the tutoring system to select beneficial representations for supporting problem solving.

Our work is novel in that it will address the problem of appropriate representation choice. Moreover, we will build novel cognitive theories and computational models that will allow AI systems to operate in more human-like ways and adapt to the requirements of the problem and the needs of the user. Thus, the potential impact will span numerous domains where systems interact with humans to represent information and use it for extracting new knowledge.",,"&quot;Automating Representation Choice for AI Tools&quot; is an ambitious project where we want to find out how humans choose and also change representation during problem solving. We want to develop engines that will give machines the same capability. Such machines will accrue many of the benefits that humans obtain from changing representation, including: more effective communication with a human by selecting a representation that is well-suited to their level of familiarity with the topic; better understanding of a human by identifying and adopting the representation that the human favours; and greater flexibility in adapting to the needs of the user. As such, the project will have economic, societal and knowledge impact.

The impact of this work will be wide as AI systems are becoming ubiquitous. When interacting and exchanging information with humans, developers of such products need to represent information in human understandable ways. We will devise techniques that will help developers of AI systems to build products that choose representations appropriate for their users -- and thus have potential economic benefit to them. 

Making machines more human accessible will also benefit the society at large as it will potentially bring technical tools to those that are less technically versed.

We have already outlined how our work will contribute to the expansion of knowledge in Academic Beneficiaries. The techniques we will develop are novel and will provide a scientific advance in all areas (in academia and in industry) that must represent information for solving problems, reasoning, etc. 

Whilst we focus on education as our initial concrete application target, our contributions will be general and could apply across different domains. They will lead to better understanding of representations and their relation to human expertise and preferences. As intelligent tools make part of our everyday life, we will all benefit from having more human-like systems that adapt to us, rather than us adapting to them. 

Artificial Intelligence is a research area for growth by the EPSRC. The capability to select and adapt representations to suit human needs will provide a novel foundational capability for AI. The EPSRC funded Human-Like Computing (HLC) network recognises the importance of representation selection; the first of the cognitive science challenges in the network's strategy roadmap (https://epsrc.ukri.org/newsevents/pubs/human-like-computing-strategy-roadmap/) asks: &quot;How do we create systems that create and revise their mental representations to fit the problem being addressed?&quot; As a recognition of the importance and the potential impact of our work, the HLC network of interdisciplinary experts fully endorse this project (see Other Attachments).

To achieve maximum impact, we will follow a comprehensive dissemination strategy. We will publish our results in all relevant communities (e.g., artificial intelligence, cognitive science, automated reasoning, diagrams, knowledge representation, human-computer interaction, information visualisation, education) to achieve wide dissemination. We will demonstrate our work by organising a workshop and preparing tutoring material aimed at academic as well as more specific industrial communities. This will also provide a community building opportunity for people interested in representations and more generally, human-like computing. We will continue to participate at outreach activities to raise the awareness of the importance of human-like computing. We will create a web repository to enable free and public access to our papers, corpus of problems and their solutions, software and tutorials. Finally, we have enlisted an Advisory Board of experts spanning all areas relevant to this project, and coming from academia, education and industry - their advice will help us stay focused on relevant problems, influence diverse communities that they lead, and transfer our technology widely."
24,4CDB2D68-520E-42AE-A440-9DA58440E745,Automating Representation Choice for AI Tools,"AI engines are ubiquitous in our lives: we talk to our mobile phones, ask directions from our sat-navs, and learn new facts and skills with our digital personal assistants. But most of the time, we need to learn how these systems work first: we have to adapt to them as they are not aware of our level of experience, expertise and preferences. AI engines are fast and can deal with a deluge of data much better than us, but they do so in machine-oriented ways, which are often inaccessible and unintelligible to humans. How can we build machines that will adapt to us?

The aim of this project is to identify and study how humans represent information that they want to work with and from which they will obtain new knowledge. Humans have the capability to choose the representation that works for them to enable them to solve a new problem, and moreover, if the representation needs to be changed, they can spot this and change it. Unlike humans, machines in general have fixed representations and do not have the understanding of the user. For example, sat-nav systems will only give directions with elementary spatial commands or route planning functions, whereas humans give directions in many forms, for instance in terms of landmarks or other geographic features that are based on shared knowledge.

We want to model in computational systems this inherently human ability to choose or change appropriate representations, and make machines do the same. We want to find out what are the cognitive processes that humans use to select representations, what criteria they use to choose them, how we can model this ability on machines, and thus how to engineer systems that will select effective representation to enhance people's problem solving and learning. 

This is a grand challenge, because it must marry human with machine capabilities. Our previous EPSRC funded feasibility project brought together an interdisciplinary team to combine expertise in computer science on automated reasoning with diagrammatic representations (Jamnik, Cambridge) with expertise in cognitive science on human problem solving and learning with representations (Cheng, Sussex). This interdisciplinary approach has been critical to the success of the feasibility project.

We previously showed that when humans choose a representation of a problem, they use cognitive and formal properties of the problem and its representation to make their choice. In this project, we build on and generalise this hypothesis and demonstrate its utility by building a mathematics tutor that intelligently picks good representations according to the skill level of different learners. We have the following goals:

1. Develop representation selection theory based on the formalisation of formal and cognitive properties.

2. Develop a cognitive theory to assess the efficacy of alternative representations and methods for selecting representations suited to the competencies of individual users.

3. Devise computational algorithms (software) that mechanise the right choice of representation based on the theoretical foundations.

4. Develop and test the algorithms on a range of domains to demonstrate the scalability and generality of the approach.

5. Build an AI tutoring system that implements automated and personalised representation choice based on the user's level of expertise and experience.

6. Empirically evaluate the capability of the tutoring system to select beneficial representations for supporting problem solving.

Our work is novel in that it will address the problem of appropriate representation choice. Moreover, we will build novel cognitive theories and computational models that will allow AI systems to operate in more human-like ways and adapt to the requirements of the problem and the needs of the user. Thus, the potential impact will span numerous domains where systems interact with humans to represent information and use it for extracting new knowledge.",,
0,1E343A48-0DD7-4662-9360-4F6E90D5B267,Streamlining Social Decision Making for Improved Internet Standards,"Many decisions in today's world are made through a complex, dynamic process of interaction and communication between people and teams with different interests and priorities - so called &quot;distributed decision-making&quot; (DDM). For example, many businesses work across multiple geographically dispersed offices and timezones, with teams specialising in quite diverse areas. Each team may have its own goals and reward models, which do not necessarily coincide, and may be spread across multiple organisational units (e.g. different businesses or governments). Communication may happen via several different modalities with very different timescales and properties (e.g. email, instant messenger, and face-to-face meetings).

Unfortunately, although many organisations have started to document these processes and even make records available (particularly governmental organisations e.g. https://data.gov.uk/), we have no way to automatically analyse these records. If we did, we could produce tools to automatically summarise decisions, trace who made them, and why and how they were made (and why other decisions weren't made). From a societal standpoint this would help make these processes more accountable and transparent. We'd also be able to identify collaborative failures, biases and other problems, and thus help improve decision-making in future.

This project will develop these urgently required methods, using a combination of natural language processing and social network analysis. We will collate, annotate and publicly release the first multimodal dataset of real-world distributed decision-making. We will devise techniques to take natural language and semi-structured data to recognise the dialogue and interaction structures in decision making, and analyse those structures to produce summaries and evaluate the efficacy of the decision making process. We will then use the outputs to inform strategic interventions that can streamline and improve decision making. 

Our methods will be suitably generic to span several domains. However, the project will focus on one particular global organisation as its main use case: the Internet Engineering Task Force (IETF). This is an international forum responsible for producing Internet protocol standards - formal documents which specify the languages by which software and hardware &quot;speak&quot; across the Internet. To produce these documents, extensive international collaboration is performed - this spans several modalities including email discussions, collaborative document editing, face-to-face meetings and teleconferencing. Importantly, all of these modalities are documented via transparency reports ranging from public email archives to minutes from meetings. This project has partnered with the IETF to help model and streamline their decision making process. We will borrow from their experience, and employ our methods to extract decision making bottlenecks. We will devise tooling which will provide advice and proposed interventions to relevant parties within the IETF. Amongst many other things, we directly benefit the IETF, and the global Internet standards community, by helping them to uncover biases and help make important decision processes accountable.",,
1,A909B61E-90B1-4E1F-B2DE-E56A03EEF89B,Developing Artificial Intelligence and Deep Learning for the analysis of correlation spectroscopy data,"Nuclear magnetic resonance (NMR) spectroscopy is an unprecedented technique to obtain detailed information - at atomic resolution - about macromolecular machines in an environment similar to the cell. NMR spectroscopy has therefore become an imperative tool for the characterisation of large proteins, for the discovery of new molecular interactions, and also for the discovery of new drug-leads.

 Large proteins and macromolecular machines contain thousands of atoms. High-dimensional (3D, 4D, ..) NMR spectra are therefore required in order to separate the NMR signals for the individual atoms and to facilitate a characterisation large proteins. A common hurdle with high-dimensional NMR spectra is the time required to obtain these, because essentially a 1D NMR spectrum is required for each point within a 100x100 square (3D spectra) or within a 100x100x100 cube (4D spectra). This makes it very time-consuming and nearly impossible to obtain high-dimensional (&gt; 3D) spectra for large proteins.

 During the proposed project we will leverage the immense power and strength of Artificial Intelligence (AI) and Deep Learning to allow for fast acquisition of high-dimensional NMR spectra to characterise macromolecular machines. We will develop a new tool, where a new deep neural network will be designed and trained so that the required information about the macromolecule can be extracted orders of magnitude faster than using the traditional workflow because only a fraction of the points are recorded. For 4D spectra only about 1% of the points within the 100x100x100 cube need to be sampled. This is possible because the theoretical background of NMR spectroscopy is so well defied that sufficient training data easily can be generated to train the neural networks.

 The new tools, anchored in deep learning and AI, will not only allow for fast and accurate characterisation of molecular interactions but will also facilitate ultra-high-dimensional NMR that will allow for completely new NMR ventures and for even larger molecular machines to be characterised.","Non-uniformly sampled (NUS) NMR spectra has become important for obtaining ultra-high dimensional NMR spectra with high resolution. Being able to accurately reconstruct NUS NMR spectra allows for high-dimensional NMR spectra to characterise macromolecular machines, amongst others. Various algorithms have been developed to reconstruct the full dataset from sparsely sampled data, however, these are often slow and require many more sampled points than what theoretically is necessary.

The development of deep neural networks (DNN) have seen an impressive growth recently with many and highly different applications. The current biggest challenge for training complex DNNs is the availability of sufficient training data. Reconstruction and analysis of high-dimensional NUS NMR data are well suited for DNNs, because sufficient training data can easily be generated. It is therefore now time to take advantage of the immense potential of deep learning for the analysis of complex NMR spectra.

During the proposed research project DNNs will be designed and trained to analyse high-dimensional NMR spectra, such as, 3D triple resonance spectra and 4D methyl-methyl NOESY spectra. Whereas initial objectives aim at 'old-style' reconstruction, subsequent objectives aim at providing a multi-way decomposition of sparsely sampled high-dimensional NMR spectra. Such decompositions contain the same information as the fully reconstructed spectrum, but the information is concentrated and kept in shapes. Because of the high flexibility of deep learning and since a sufficient amount of training data can be produced, one can start to aim at entirely new perspectives for the analysis of NMR spectra where, for example, a trained DNN performs the entire analysis of a series of biomolecular NMR spectra in one single step. An objective is also to perform a combined analysis of triple-resonance NMR spectra to provide chemical shifts assignments of proteins - quickly, robustly and in a single step.","The outcome of our proposed research is in the form of new Deep Neural Networks (DNNs) with associated optimised parameters to fast and reliably extract information from ultra-sparsely sampled high-dimensional NMR spectra. 

Different mechanisms will be in place for disseminating our new tools to the identified beneficiaries. Firstly, we will publish our new deep learning analysis tools and their applications in archives such as arXiv.org and bioRxiv.org and in peer-reviewed international journals with as high impact as possible. Such publications will be available via open access to the international research community, including academia and industry, and to the general public. As we have done previously, we will aim at combining our developments with applications to challenging biological or biochemical problems, since publications of this combined nature generally will appeal to a broader audience and better showcase the strength of the developed tools. Both the PI and the PDRA will also present the results at national and international meetings; such meetings also allow for in-depth conversations that promote new collaborations.

Specific impact:
Researchers from both academia and the industrial sector in fields of structural biology, protein-ligand interactions, and NMR spectroscopy will benefit directly from our research. This group of researchers can immediately incorporate our new analysis tools into their own research programme. Specifically, our new tools will allow for substantially faster acquisition of NMR data because ultra-sparse sampling can be used, which in turn will make workflows faster and more efficient. Importantly, the automated chemical shift assignment tools, which we will develop, will allow for non-expert NMR spectroscopists to easily analyse protein-NMR spectra and obtain chemical shift assignments of proteins. These assignments can subsequently be used to determine structure, dynamics and quantify interactions, such as drug-protein interactions. It is anticipated that the proposed DNNs for automated analysis of sparsely sampled NMR spectra will be particularly beneficial in industrial settings.

 The research proposed will also have a significant impact on the fields of artificial intelligence and deep learning. One of the biggest challenges for developing new neural network architectures is training data and having sufficient data to properly train and cross-validate a new network architecture. Using deep learning to analyse and decompose high-dimensional NMR spectra provides a unique example where (i) sufficient training data can easily be generated, since the theory behind NMR spectroscopy is well known and (ii) large amount of experimental cross-validation data can be generated using standard NMR experiments. Thus, the case of analysis of NMR spectra can form a robust example where new and even more elaborate deep learning architectures can be designed, improved, and cross-validated.
 
The proposed research will indirectly benefit the general public. For example, our previous NMR-based tools and methods were used to improve the stability of coagulation factor VIII for the treatment of haemophilia A, which is of great societal impact. It is highly likely that the tools developed during the proposed research will both make analysis of NMR spectra faster and also applicable to a broader audience. NMR spectroscopy is used in many fields of science, including, material science, chemistry, and drug-discovery. Our new tools will have the ability to impact on all areas where NMR spectroscopy is used, which subsequently will benefit the general public."
2,ED8F823D-99C6-4CCE-8223-FBC1F0567EC6,ADRELO: Advancing Resilience in Low Income Housing Using Climate-Change Science and Big Data Analytics,"The project aims at enhancing the resilience of low-income communities living in disaster prone areas. The focus is on low-lying coastal zones that have a high risks of droughts and floods in selected parts of East Africa, Brazil and North America. It develops the geographic and socio-economic knowledge of persons living in slum and riverbed areas by gathering georeferenced data on infrastructures and information on the natural heritage of project sites. The project team will also investigate technology adoption barriers and diffusion drivers through designing and prototyping an affordable, disaster-resilient, low-income housing system that use sustainable locally-resourced materials. The development of urban spaces is a function of geographic location, economic history, urban development pattern, and therefore governance will have a bearing on resilience. Still, given that development (or lack thereof) of an urban center is an outcome of existing social, economic, and political inequities political inequities; policy packages for disaster preparedness that do not consider the unique circumstances of vulnerable populations can inadvertently cause harm to low- income households. Furthermore, policy packages will include environmental sustainability and public health considerations. The research will also contribute to accurate modelling of climate and extreme weather events at spatiotemporal level to increase the understanding of climate scientists while empowering policy makers in disaster related decision-making. Machine Learning and Big Data Analytics will be used for climate modelling and to identify optimal disaster resilient-housing urban design and planning policy packages considering projected climate change- related extreme weather scenarios between the current time and 2050. Whilst Big Climate Data is amenable to long-term climate prediction, data for localized and seasonal predictions is still uncertain and sparse. Machine Learning has potential to handle this uncertainty and data sparsity as other applications have demonstrated that it can work with either big data or sparse data.",,"The following are challenges to be solved to reduce disaster risks and enhance resilience. First, difficulty in obtaining accurate localized predictions make preparation and mitigation difficult and makes decision making uncertain and misinformed. Secondly, the inability to more elaborately link populations, climate change, disaster risk, policy actions and interventions with impacts on disaster risk reduction and resilience mean that the impacts of the actions of policy makers cannot be quantified. This project seeks to use big data and machine learning to produce models that perform localized predictions. Also, socio-economic modelling will be applied to link climate change, disaster risk, policy actions and impacts on disaster resilience. The resulting models will be integrated into visual analytics tools as part of a decision support system that can support policy making. The idea is to generate actionable insights that can be encourage decision makers to consider various policy choices while simultaneously evaluating the associated impacts to disaster risk reduction and increased resilience. Still, the project will develop technology for low income housing based on affordable and resilient materials and technology.

The application of social-econometric strategies to design, appraise and monitor the implementation of the structures proposed especially for the economic and social impacts is highly desirable. This will require an analysis of various variables such as the cost implications and access parameters of materials to the local communities, the demographic sub-contexts of post disaster resettlement as well as the possibility of ongoing sunk costs associated with post implementation and maintenance. The models tested will be applied locally and these can then be adapted for specific climate change-related global phenomena.

In dealing with specific risks such as drought and flooding, the data already generated can help understand the current and future socio-economic impacts of such risks at aggregate and dis-aggregate levels. This can then be evaluated against the backdrop of installing risk reducing structures such as shelters with access to food and medicines to ensure the transitionary period from disaster to reinstatement- especially where the vulnerable in the population like the elderly and children are concerned- renders itself with as little consequence as possible. In efforts directed at informing policy, expected future economic impacts such as increased output due to continuity of occupancy in the region and a constant or increasing population as opposed to periodic periods of prolonged evacuation will be examined to inform the implementation of longer term infrastructural adaptations that not only reduce the effects of any risks experienced, but also allow for shorter recovery times and benefits from economies of scale. 

The inter-disciplinary nature of the research will allow the project team to estimate not just the feasibility but also the sustainability of the affordable, resilient and affordable housing that emerge from the research. The socio-economic models will be integrated into decision support tools for policy makers. Furthermore, this will culminate into accessible policy reports and briefings towards ongoing feedback for the duration of the project as well as for suggested policy options after.

The project will have the following impacts: 1) A large set of data on weather, social economics, demographics, natural heritage, infrastructures will be available for the three sites in North east of Brazil, eastern USA and East Africa that at present do not exist. 2) The dwellers of low-lying areas that are vulnerable to humanitarian disaster will be assessed for each project site. 3) Climate models and visual models for the sites will be available. 4) Improved housing that are durable and affordable will be made public. 5) Visual model tools for policy making will also be made available."
3,A5BC9906-1D6F-4B36-BC94-79B93148C5E4,Abstract Forward Models for Modern Games,"The games industry is one of the fastest-growing industries in the world, with yearly revenues expected to increase from US$ 138bn in 2018 to US$ 180bn in 2021. The UK games industry is a worldwide leader that contributes significantly to wealth creation and export, with a clear growing tendency: 62% of the 2261 companies in the UK were founded in the last 8 years. Employing 12,000 people with sales valued in &pound;4.3bn for 2017, this industry is the second largest market in Europe and the fifth in the world. 

Games have also been excellent benchmarks for the advancement of AI. One of the most clear and recent examples of this is the progress on search methods in the game of Go. Go is a thousand years old board game of simple rules but complex strategy, where humans had dominated computer AIs since the beginning of the field. Monte Carlo Tree Search (MCTS), an AI technique that explores the different branches of actions that both players can take, became in 2016 the standard algorithm for creating Go AI players, giving birth to substantial research on variations and applications of this algorithm. Since then, MCTS has been used in thousands of other works in and outside games. This progress reached another milestone when Google Deepmind's Alpha Go mastered this game with a combination of MCTS and Deep Learning (DL).

MCTS uses a forward model (FM), which is a representation of the game state that allows to roll the state forward after applying any action in the game. This &quot;simulator&quot; is also used by other Statistical Forward Planning (SFP) methods that are also showing similar promise to MCTS in some domains, such as Rolling Horizon Evolutionary Algorithms (RHEA). It is however striking that despite the popularity and progress on SFP methods, they have barely reached the games industry. The most known uses of MCTS for Opponent AI in the games industry are in the Total War series by Creative Assembly, AI Factory on card games and Lionhead's tactical planning for Fable Legends. Given that the games industry is one of the fastest growing industries in the world and UK one may wonder why one of the top algorithms on AI in Games barely reaches far less than 0.01% of this industry.

The aim of this project is to incorporate an FM library into a modern games engine in order to facilitate research on the use of SFP techniques in large, complex, video-games. On the one hand, the project will address the technical and design problems of integrating a customisable FM that determines which elements of the real game state form part of the FM and how abstractions can be made. On the other hand, the project will aim to understand how SFP methods perform under these conditions in complex and large commercial-like games, investigating how these can be improved. The resultant framework will allow to test these methods in a wide range of games, with a special emphasis on proposing a Game AI competition for industry and researchers. Dissemination of the project's research outcomes will be guaranteed via open source libraries, frameworks, documentation and scientific papers.
 
This project builds naturally on the PI's recent work on GVGAI (for which he is main developer, organiser and coordinator of the competition, tracks and team - www.gvgai.net), and it proposes a step change on General Game AI research and its relevance to the games industry, adapting it to modern games. This project addresses directly the applicability of well-established methods such as MCTS/RHEA to large and complicated games and also the industry needs for fast, reliable and state of the art AI techniques. Our strong group of game industry partners (Microsoft Research, AI Factory, Bossa Studios, Creative Assembly and Gwaredd Mountain) will help steer the project into the interests of the game research and industry communities. Applications beyond games will also be explored with the help of our non-game industry partner (the Defence Science and Technology Laboratory).",,"The SCIENTIFIC COMMUNITY will benefit from this research. The AI in games community will benefit from having the possibility of testing their methods in commercial-like games. Having forward models in a large-scale games engine allows for research, across multiple game genres, the application of Statistical Forward Planning methods in complex environments. Researchers will benefit from being able to easily incorporate off-the-shelf advanced AI methods for non-player characters in complex game and non-game environments. 

A project page to disseminate the findings and download the software, documentation deliverables and papers produced will be set up. Data from game logs for authorial analysis and explainability will be provided to build a large-scale database of high-quality data for new behavioural insights. Papers will be submitted to high impact factor journals on games (IEEE Transactions on Games) and evolutionary computation (IEEE Transactions on Evolutionary Computation), and also presented at most relevant conferences such as IEEE Conference on Games (CoG) or the AAAI Artificial Intelligence for Interactive Digital Entertainment (AIIDE). Special sessions, workshops and tutorials will be proposed for these venues. The international competition proposed will provide a great exposure of the project and the research breakthroughs achieved. 

The VIDEO-GAMES INDUSTRY is a primary beneficiary of this proposal. The work performed will enable game companies to embed the latest AI methods in complex games, producing more interesting, challenging and fun games. The availability of stronger and better AI will reduce development time and costs and ease the automation of game evaluation and testing. By bringing the latest research to the industry, this project means a step change in the ways AI is applied in games, taking UK to a leading position in this industry. The hosting institution, QMUL, is part of the EPSRC IGGI CDT (EP/L015846/1) and its recent renewal. The goals and strategy of both IGGI and the Game AI group are closely aligned with this project: bringing the latest research in games to the industry. 

Impact will be achieved by developing and providing an open source software library for an industry-standard game engine. The AI algorithms developed during the project will be publicly available on the project website. Given the popularity of the games engine and programming language within the games developers community, the reach of the implemented software is large. A direct impact mechanism is the possibility of an internship of the PhD student in Creative Assembly, one of the project partners, where our research findings can be applied. We'll present and promote our work at conferences with strong industry presence, such as Develop, the London Game AI Meetup and CoG.

The EDUCATION PROGRAMMES, concretely Computer Games and Computer Science degrees with modules on games and AI, will benefit from this research. Final year and MSc projects, and PhD topics will be able to use our framework for further research on Game AI. The PI has experience in this model, as his previous benchmark (GVGAI) has been used for teaching and as a base for student projects. GVGAI also showed that the proposed competition is a great asset for education, with multiple students across the globe participating in the challenges as part of their modules or projects. The integration of the framework with a game engine will aid the creation of more interesting games at game jams. 

The project will produce interest among the GENERAL PUBLIC and media. The project will be promoted in showcase events, general interest articles (i.e MCV/Develop/ The Conversation) and regular press releases. The work will be promoted in public lectures and university open days. The team will also actively participate in events or magazines dedicated to the dissemination of research to the general public, such as Pint of Science nights or the CS4Fun Magazine."
4,1D6EC946-0398-4426-90D0-3C35D12854E1,The Alan Turing Institute 20/21 - 21/22,"The Alan Turing Institute, as the UK's national institute for data science and artificial intelligence, plays an important part in driving forward advances in these technologies in order to change the world for the better. 

The Institute is named in honour of Alan Turing, whose pioneering work in theoretical and applied mathematics, engineering and computing is considered to have laid the foundations for modern-day data science and artificial intelligence. The Institute's goals are to undertake world-class research, apply its research to real-world problems, driving economic impact and societal good, lead the training of a new generation of scientists, and shape the public conversation around data and algorithms.

After launching in 2015 with government funding from EPSRC and five founding universities, the Institute has grown an extensive network of university partners from across the UK and launched a number of major partnerships with industry, public and third sector. Today it is home to more than 500 researchers, a rapidly growing team of in house research software engineers and data scientists and a business team.",,
5,830DCA14-5EDB-423C-8D85-800BA69B22B9,MEFA: Mapping and Enabling Future Airspace,"Manned and unmanned airspace is undergoing a revolution. By 2030 air traffic is estimated to quadruple with a doubling of the total number of manned aircraft and unmanned air vehicles (UAVs). This explosive growth will change and congest already heavily used airspace. UAVs occupy airspace in a similar way to birds with both flying at overlapping altitudes and velocities. Therefore, as evidenced by the recent drone incursions at Gatwick airport, there is a pressing need to be able differentiate UAVs from natural organisms (e.g. birds) that use the same airspace. There are limited detailed data on how birds use airspace, especially in light of unprecedented rates of urbanisation, characterised by increasing high-rise building, increased artificial light (AL), and changing patterns of infrastructure. All are rapidly re-shaping habitats used by migratory and non-migratory species.

The interaction between built infrastructure and AL, and its influence on bird biology, is now the focus of research addressing migration ecology, especially of birds, and mortality caused by brightly lit urban structures (e.g. monuments, buildings, communication towers). Increased use of glass and other highly reflective surfaces on high-rise buildings has increased the frequency of bird strikes and thus bird mortality. In 2004, the British Trust for Ornithology (BTO) estimated 100 million birds struck windows each year in the UK.

This project primarily uses a 'staring' form of radar sensor developed specifically to track drones. Contrary to previous radar research, individual birds and drones are observable within small groups that allows finer measurement of trajectories than has been achieved previously. However, for sufficiently reliable surveillance of controlled unmanned-airspace, the fundamental challenge is to discriminate small drones from birds. Bird species have specific flight patterns that are distinguishable from those of UAVs. The research will develop algorithms to distinguish between drones and birds, individual birds in small groups (typically 2-5) and potentially individual birds in larger flocks. Deep learning algorithms will be developed and tested for their ability to distinguish between birds and drones, and between different bird groups. The project cuts across the EPSRC's themes of &quot;Living with Environmental Change (ecosystem challenge)&quot; and &quot;Global Uncertainties (threats to infrastructures)&quot;, to develop a cutting-edge system with the ability to simultaneously mitigate security risks to birds and humans alike.",,"Summary: By being able to differentiate birds from drones and different species of birds from one another this research will open new vistas on bird behaviour and the impact of urban developments on bird flight patterns, bird strikes, and mortality. It will create a monitoring system that is critical to the future commercial exploitation of the new unmanned airspace.

Societal Impact:
There is an acute need to understand how the complex land use configurations found in cities and the patterns in building form, design and function confine and structure urban airspaces, thereby constraining use by wildlife and unmanned aerial vehicles (UAVs). Crucially, planners and city engineers need better targeted information concerning the interaction between lighting and built infrastructure so the impact on wildlife can be minimized. Moreover, improved characterisation of the lighting infrastructure associated with different landcover and land use will help with future proofing through cycles of urban regeneration and change of use. 

Commercial Impact:
A recent House of Commons briefing paper (CBP 7734, 31 August 2017) stated &quot;Achieving the full and safe integration of drones (UAVs) into non-segregated airspace is the underlying policy objective of Government. To achieve this requires technology, which is not yet fully developed, for sensing and avoiding air traffic under all possible scenarios&quot;. Radar has been identified as a core component of a future Air Traffic Management (ATM) system that will have to cope with a quadrupling in the number of aircraft over the next 10-15 years. Currently, no radar sensors are able to detect UAVs and distinguish them from birds with a reliability that meets ATM safety standards. In addition, drone incursions at airports present a severe safety hazard as well as directly impacting their economic viability. Robust and reliable radar surveillance is a core component of any 24 hour, all-weather, counter-drone capability. Further, this dramatic change in airspace occupancy will also impact bird populations in unresolved ways. Therefore, this project will create an enhanced sensing capability to open up new commercial opportunities in line with EPSRC's prosperity outcomes while also providing answers to core emerging environmental questions.

Academic Impact and National Importance:
This research will benefit national and international researchers by establishing ways in which information is encoded into radar echoes enabling UAVs and birds to be differentiated. The core knowledge will be generic and applicable to other use cases such as for autonomous driving, vital-sign health monitoring of people as well as military and security applications etc. This project will also provide foundational information enabling design of future radar sensors for ATM/UTM in congested, low altitude skies. The UK will benefit through development of high-performing all-weather, day-night, sensors that can be sold across the world.

New Experts:
The research training component of this project will include internships in industry and leading laboratories so that the expertise is developed with an appreciation of exploitation requirements. The University of Birmingham and the University of Leicester will establish a team that will be expert in radar sensing, advanced signal processing for target recognition, bird measurements and behaviours. These skills will be exploited in the radar sensing industry, academia and government agencies such as the CAA, NATS, Government departments and the BTO."
6,99B40749-B89C-4201-AEC7-E57F9025827E,MEFA: Mapping and Enabling Future Airspace,"Manned and unmanned airspace is undergoing a revolution. By 2030 air traffic is estimated to quadruple with a doubling of the total number of manned aircraft and unmanned air vehicles (UAVs). This explosive growth will change and congest already heavily used airspace. UAVs occupy airspace in a similar way to birds with both flying at overlapping altitudes and velocities. Therefore, as evidenced by the recent drone incursions at Gatwick airport, there is a pressing need to be able differentiate UAVs from natural organisms (e.g. birds) that use the same airspace. There are limited detailed data on how birds use airspace, especially in light of unprecedented rates of urbanisation, characterised by increasing high-rise building, increased artificial light (AL), and changing patterns of infrastructure. All are rapidly re-shaping habitats used by migratory and non-migratory species.

The interaction between built infrastructure and AL, and its influence on bird biology, is now the focus of research addressing migration ecology, especially of birds, and mortality caused by brightly lit urban structures (e.g. monuments, buildings, communication towers). Increased use of glass and other highly reflective surfaces on high-rise buildings has increased the frequency of bird strikes and thus bird mortality. In 2004, the British Trust for Ornithology (BTO) estimated 100 million birds struck windows each year in the UK.

This project primarily uses a 'staring' form of radar sensor developed specifically to track drones. Contrary to previous radar research, individual birds and drones are observable within small groups that allows finer measurement of trajectories than has been achieved previously. However, for sufficiently reliable surveillance of controlled unmanned-airspace, the fundamental challenge is to discriminate small drones from birds. Bird species have specific flight patterns that are distinguishable from those of UAVs. The research will develop algorithms to distinguish between drones and birds, individual birds in small groups (typically 2-5) and potentially individual birds in larger flocks. Deep learning algorithms will be developed and tested for their ability to distinguish between birds and drones, and between different bird groups. The project cuts across the EPSRC's themes of &quot;Living with Environmental Change (ecosystem challenge)&quot; and &quot;Global Uncertainties (threats to infrastructures)&quot;, to develop a cutting-edge system with the ability to simultaneously mitigate security risks to birds and humans alike.",,
7,41F8C1EC-D239-493F-BC6F-2C0EE2C0B32D,21st Century Meat Inspector,"Poultry is the most popular meat in the UK, and its effective inspection within processing facilities is essential to ensure regulatory compliance. Poultry inspection is performed manually and is extremely challenging due to the short time available to inspect each bird and the sustained level of concentration required. This feasibility project will investigate how existing and new inspection technologies can be combined with advanced data analytics and incorporated into current meat inspection practices to deliver the 21st Century Meat Inspector. The feasibility project will focus specifically on post-mortem inspection and dead on arrival detection, within poultry. The project will adopt a benefits realisation approach to determine the requirements for any new technologies and ensure that business benefits are delivered to all stakeholders within the poultry chain. This interdisciplinary project includes expertise in a variety of complimentary inspection technologies (Optical (vis, NIR, IR, Hyperspectral), X-ray and Ultrasonic) and IT-enabled benefits realisation management with the Hartree Centre (STFC), Two Sisters Food Group and CSB as project partners.",,
8,48698003-56A2-4911-8E0F-6F69866E432F,Mobile Phone enabled Diagnostics for Infectious Disease Diagnosis: Low Cost Tools for Digital Health in East Africa,"Infectious diseases remain a primary cause of morbidity and mortality in many Low and Middle Income Countries, which suffer from limited health infrastructure, poverty and political insecurity, increasing the transmission of a broad range of human and animal infectious diseases.

Our vision for our network is that our new mobile phone enabled low cost DNA sensor technology, based upon using paper microfluidics, will bring both health and economic benefit to East Africa, including both Uganda and Tanzania, as well as other countries in Sub-Saharan Africa in future. The new diagnostic technology will I particular strengthen laboratory diagnostic capacity, which is a main focus for cooperation according to the WHO Cooperation Strategy for Uganda for example. 

We have already demonstrated the connectivity through apps and the cloud, accessing secure data storage and artificial intelligence/machine learning decision support tools to help healthcare providers make the correct diagnosis for multiple infectious diseases from the same sample. We now predict, that by using this technology, end users will be enabled to perform infectious disease diagnosis in humans and livestock using different samples, including water, milk and blood. In doing so we will be able to determine, quantitively, transmission pathways between animals and humans (whether this be through shared water sources or unpasteurised milk) - thereby helping address the following Sustainable Development Goals, SDGs, 2, 3, 6 and 15. This will provide healthcare technologists and epidemiologists better understanding of the impact of these diseases in veterinary care, as well as in human infection in the specific environment in Uganda and Tanzania - with resulting economic and health benefits, reducing poverty - address SDG 1.

We aim to use this activity as a case study to both exemplify the broader impact of mobile diagnostics on digital health platforms, to inform Governments, NGOs, Charities, Universities, Industry as well patients and the public more generally, as well as to build a wide network of stakeholders in mobile diagnostics platforms to identify and overcome barriers to deployment of digital health solutions in Uganda and Tanzania, and beyond. 

Our network will address one of the major health challenges highlighted by both Tanzania and Uganda, through the development of low-cost quantitative diagnostics and screening tools, enabling timely investigations of the factors contributing to the persistence of infectious diseases. In addition to delivering impact on increased health and economic benefits, the outcomes from the network will significantly decrease the impact of infectious diseases on women, who currently have a disproportionate burden. They not only provide the main support for young families, including responsibility around caring and cooking for children, and providing education, but also provide 80% of the labour for rural agricultural activities. Illness in women leads to a direct loss in the family's income, and prevention of disease can subsequently provide additional economic benefit (SDG 3, 5 &amp; 10). Similarly, if children are ill, they are often taken out from school, again resulting in a loss of income as well as missed education opportunities (SDG 4). In the case of human disease, as of 2017, malaria was the 3rd leading cause of death in both Uganda and Tanzania. By enabling delivery and connectivity of diagnosis in rural communities, using digital health, our network will allow local communities and governments to both increase the efficiency of treatment (through increased decision support) as well as build and implement surveillance strategies to prevent epidemics and re-emergence of diseases.",,"We expect the following impacts on the Sustainable Development Goals, SDG, to arise from the funding of this network (achieved through engagement with stakeholders that include charities, academics, Government and Non-Governmental Organisations, industry as well as patient groups and the public more widely), namely: (i) partnerships across Uganda and Tanzania, addressing SDG 17, between healthcare providers and veterinary scientists and ICT experts to not only enhance the health and well-being of patients and animals but to develop a better understanding of clinical diagnosis and disease transmission using digital health platforms; (ii) economic and societal benefits in East Africa more widely, through the development of translational pathways into industry, SDG 1 and 8. Longer term economic benefits may also arise as a consequence of increased commercial activity in ICT as well as in enhanced efficiency in agricultural practice and better diagnosis of human and animal disease, SDG 1, 2, 3, 5, 8, 10, 15; and (iii) capacity strengthening through the creation of cohorts of early career researchers /technologists with a set of multidisciplinary skills in bioengineering, diagnostics, manufacturing and ICT, SDG 4 and 9.

We have also highlighted 5 specific Impact Deliverables, that augment the general outcomes of the network, which also impact upon the SDGs, namely:

Deliverable 1. Following the delivery of co-creation and co-production sessions during the first workshop in Uganda, we will prepare a communication plan during the first 6 months of the project, including defining a web presence, publication, dissemination through social media and broader public engagement, SDG 17.

Deliverable 2. Use of our mobile phone enable platform as a case study to exemplify the broader impact of mobile diagnostics on digital health platforms, providing a report to inform Governments, NGOs, charities, Universities, Industry as well patients and the public more generally, SDG 1, 3, 8, 9, 17; 

Deliverable 3. The Ugandan Industrial Research Institute together with the industrial partners will take the lead on the market assessment of the digital health platforms which will feed into our final report, see Deliverable 2 (based upon equitable commercialisation), SDG 8, 9. 

Deliverable 4. Development of a generic quality management system/governance structure around the network to guarantee research integrity, standardisation of documentation, transparency and comparability of information between countries and between stakeholders from different disciplines or sectors, SDG 17.

Deliverable 5. Working with Bioengineers, Computer Scientists and Veterinary Scientists in Uganda and Tanzania, we will deliver educational and outreach projects within both Universities and local technical colleges, SDG 4, 5 and 8."
9,C722EFF3-7943-4493-85BE-9F916295027F,AI for Sound,"Imagine you are standing on a street corner in a city. Close your eyes: what do you hear? Perhaps some cars and busses driving on the road, footsteps of people on the pavement, beeps from a pedestrian crossing, rustling and clonks from shopping bags and boxes, and the hubbub of talking shoppers. You can do the same in a kitchen as someone is making breakfast, or as you are working in a busy office. Now, following the successful application of AI and machine learning technologies to the recognition of speech and images, we are beginning to build computer systems to tackle the challenging task of &quot;machine listening&quot;, to build computer systems to automatically analyse and recognize everyday real-world sound scenes and events.

This new technology has major potential applications in security, health &amp; wellbeing, environmental sensing, urban living, and the creative sector. Analysis of sounds in the home offers the potential to improve comfort, security, and healthcare services to inhabitants. In environmental sound sensing, analysis of urban sounds offers the potential to monitor and improve soundscapes experienced for people in towns and cities. In the creative sector, analysis of sounds also offers the potential to make better use of archives in museums and libraries, and production processes for broadcasters, programme makers, or games designers. The international market for sound recognition technology has been forecast to be worth around &pound;1bn by 2021, so there is significant potential for new tools in &quot;AI for sound&quot; to have a major benefit for the economy and society.

Nevertheless, realising the potential of computational analysis of sounds presents particular challenges for machine learning technologies. For example, current research use cases are often unrealistic; modern AI methods, such as deep learning, can produce promising results, but are still poorly understood; and current datasets may have unreliable or missing labels.

To tackle these and other key issues, this Fellowship will use a set of application sector use cases, spanning sound sensing in the home, in the workplace and in the outdoor environment, to drive advances in core machine learning research.

Specifically, the Fellowship will focus on four main application use cases: (i) monitoring of sounds of human activity in the home for assisted living; (ii) measuring of sounds in non-domestic buildings to improve the office and workplace environment; (iii) measuring sounds in smart cities to improve the urban environment; and (iv) developing tools to use sounds to help producers and consumers of broadcast creative content.

Through this Fellowship, we aim to deliver a step-change in research in this area, bringing &quot;AI for Sound&quot; technology out of the lab, helping to realize its potential to benefit society and the economy.",,"The proposed research has the potential to benefit the UK and international economy and society through machine recognition of sounds as a key enabling technology. The market for sound recognition technology has been forecast to be worth around &pound;1bn internationally by 2021 (Jeronimo. Driving New Revenue Streams from Intelligent Devices through Sound Recognition, IDC, Dec 2017), and the recent DCASE workshops in 2017 and 2018 have attracted around 40% industry representation. The UK acoustics industry has a turnover of &pound;4.6bn across 750 companies (UK Acoustics Network. UK Acoustics: Sound Economics. March 2019). Acoustics is relevant to many industry sectors, from aerospace and automotive to consumer goods and non-destructive testing, with significant potential for impact by new tools in &quot;AI for Sound&quot;. 

Example potential impacts from different sectors are given below.

Commercial private sector:
* Providers of remote health and social care, through new methods to use sound sensing to assist people to live independently for longer;
* Internet-of-things companies who supply smart buildings and smart cities with networked sensor systems, through access to novel acoustic algorithms for more sophisticated mapping.
* Acoustic consultants, through access to new ways of mapping and understanding soundscapes, which will help drive new design possibilities for the built environment.
* Commercial companies requiring sound sensing, through access to new audio research;
* Television and radio companies, through ability to use sound data exploration technologies in the creation, editing and re-use of audio and audiovisual programmes.
* Computer games companies, through new ways to reuse sound datasets creatively for new game sounds; 
* Audio archiving companies, through access to the latest algorithms and methods for annotating and exploring sound archives;
* Musicians, composers and sound artists, through ways to find and explore new sounds as part of their creative output;

Policy-makers and others in government and government agencies:
* Smart cities, through better ways to make sense of acoustic data and improve urban soundscapes; 
* Urban planning authorities, through new insights into the impact of sounds and how to visualize and understand these impacts;
* Environmental monitoring agencies, through new measurements of sound impact offering the potential to develop new noise policies and so improve wellbeing of citizens;

Public sector, third sector and others:
* Museums and other organizations with sound archives, through new software methods to allow people to explore and use their archives;
* Science promotion organizations, in particular through outputs from the projects on how people perceive and navigate sounds.
* Environmental organizations, through new ways to monitor biodiversity; 

Wider public:
* People living with dementia and others in need of assisted living to continue to live at home, through new and simpler monitoring methods enabled by sound sensing.
* People working in office workplaces, through new tools to measure impact of sound leading to new designs of workplace soundscapes.
* People living in urban environments, through improved city sound and noise policies and better designed soundscapes, making the urban environment more pleasant;
* Audiences of creative output involving audio and music, through availability of new creative outputs facilitated by creative access to new sounds.
* People interested in exploring audio recordings at home, school, college or university, either for educational or general interest purposes;
* Teachers in schools, colleges or universities who want to use sound examples for teaching audio or music;

Researchers employed on the project:
* Improved skills in research methodologies, which may be transferred into the commercial sector on completion of the project.

For specific plans for the realisation of impact, see &quot;Pathways to Impact&quot;."
10,31688E24-F4A0-4E35-A442-3147F8D95CFB,ACTION on cancer,"Death from cancer is typically both slow and painful, and few families have been spared its scourge. Cancer is also one of the world's greatest killers (13 million deaths and 22 million new cases per year by 2025), and it is estimated that every second person on the planet will develop cancer at some stage of their life. 

Over the last 30 years our knowledge about cancer has increased enormously, and now, for the first time, we understand the fundamental nature of the disease(s): malfunctioning in the way that cancer cells process information. All the cells in our bodies process information about their internal state, and communicate with their neighbours, and when this goes wrong cancer can occur.

As everyone's cells are different, and there are very many different ways that this information processing can go wrong and cause cancer, it is not possible to design a single treatment for cancer, or even for a sub-type of cancer such as breast cancer. Instead what is needed are personalised treatments tailored to each patient's cancer. However, such personalised treatments are very expensive to design, and the expertise to do so is limited. In addition, it is often necessary to execute custom designed experiments to better understand what is the best treatment. Therefore the only way to make personalised cancer treatment available to everyone is through laboratoryautomation, and the use of artificial intelligence (AI).

In this project we will develop ACTION, which will be a prototype AI system for the design of personalised cancer treatments. ACTION will focus on chemotherapies - design of drug cocktails. Given initial information about a cancer ACTION will extract all the relevant knowledge it can find about the cancer, both from databases and computational models of cellular information processing that scientists have developed. ACTION will rationally integrate this knowledge, and infer what extra knowledge is required to make the best decision on how to treat the cancer. ACTION will then automatically execute custom designed experiments using laboratory robotics to determine the missing information. Finally, using all the knowledge it has gathered, ACTION will decide on the best chemotherapy.

We will evaluate ACTION using different types of cancer cells grown in the laboratory. This avoids the ethical complexities of working with patients, and is much cheaper and faster. If the development of ACTION is successful it will then move to testing with patient derived cancers.",,"Impact Summary
The proposal is an ambitious one with a high potential for significant technological, Medical, economical, and societal impact.

Value
Cancer is one of the world's greatest killers (13 million deaths and 22 million new cases per year by 2025), and it is estimated that every second person on the planet will develop cancer at some stage of their life. The annual treatment of cancer costs the world ~$900 Billion per annum (more than any other disease), and there is deep concern that cancer treatment is becoming unsustainable, as new therapies can cost &gt; &pound;100,000 per patient per year. 

The Funding Landscape
The proposal aims take the concept of AI designed personalised drug treatment from TR2 (Principles demonstrated through experimentation) to TR3 (Early proof of concept demonstrated in the lab). The main objective measure of success of TR3 will be that ACTION will make automated recommendation on personalised cancer treatment that is as good or better that current 'best practice'

RDK has significant commercial experience. He was a founder of the spin-out company PharmaDM, which developed data mining tools for the pharmaceutical industry. RDK also consults for a wide range of multi-national companies. LS has less commercialisation experience, and she will undertake appropriate business and commercial training.

The applicants understand and have experience with the variety of follow on governmental and medical charity. For example, RDK was involved with Cambridge in a MRC Biomedical Catalyst project to develop an anti-malaria compound.

One year before the end of the project we will institute a meeting of the project Advisory Board to decide on an appropriate commercialisation pathway. To take ACTION to TR4 we will first seek proof-of-principle funding for this through the University of Manchester EPSRC Impact Acceleration Account. However, this will be insufficient to cover the high costs. We will therefore seek following on funding from Cancer Research UK, the MRC Developmental Pathway Funding Scheme (DPFS), etc. We will also approach our VC contacts.

The evidential support to produce a high quality follow on funding application will come from two main sources: publications, and Intellectual Property

Publications
The applicants have an excellent publication track-record, our papers have been published in Science, Nature, PNAS, etc. We will also attend and submit papers to relevant conferences.

Stakeholder Engagement 
Through participation in Big Mechanism we are aware of the complex landscape of stakeholders in cancer. There is a growing enthusiasm for the potential of AI to help in cancer treatment and diagnosis, however this can only succeed if all stakeholders are included and involved: 

RDK is engaged in cancer research in Manchester through membership of Manchester Cancer Research Centre (MCRC). To ensure a close relationship with MCRC we will invite Professor Andrew Hughes onto the ACTION advisory board. 

To advise ACTION in industrial involvement we will invite Professor Kevin White onto the ACTION advisory board. We have worked closely with him in Big Mechanism. He is a US National Cancer Institute board member, and President of Tempus, where he oversees its scientific operation.

Patient involvement is also essential. Through Big Mechanism we have worked closely with Cancer commons: a 'patient-centric not-for-profit network of patients, physicians, and scientists that help identify the best options for treating an individual's cancer'. Dr. Marty Tenenbaum, Chairman of Cancer Commons, is excited by our research, and we will ask him to be on the Advisory Board. He is both a renowned computer scientist, and a cancer survivor. 

Research capacity building 
The project will train three PDRAs in key areas of future science and technology: AI, machine learning, bioinformatics, medical informatics, and cancer biology."
11,85F2805A-D81D-4FA5-9A1D-6F745FE6AC04,FNR - Attenuating the Environmental Impact of our Buildings through Semantic-based Dynamic Life cycle Assessment (SemanticLCA),"Our vision is that humans can attenuate and control positively the impact of their buildings on the environment and mitigate the effects of climate change. This can be achieved by a new generation of life cycle assessment methods and tools that are model-based, continuously learn from real-time data, while informing effective operation and management strategies of buildings and districts.

In that respect, current LCA methods present important limitations and gaps, including:

(a) Lack of reasoning and decision support capabilities, such as exploring &quot;what if&quot; scenarios for the evaluation of alternative design options and devising adapted strategies, thus promoting active control of buildings and districts.
(b) Lack of alignment with domain models, e.g. BIM (Building Information Modelling), GIS (Geographical Information Systems), and LCA data structures. 
(c) Lack of support of temporal information. There is a need to factor in temporal information in the life cycle inventory (LCI) and Impact Assessment (LCIA) phases to address maintenance, operation, deconstruction, disposal and recycling stages. 

The proposed research addresses the challenge of leveraging digital built environment resources by using semantic web technologies to deliver life cycle assessment solutions to our built assets. Our hypothesis is that: life cycle assessment underpinned by semantics and informed by dynamic data paves the way to more accurate life cycle impact assessment while supporting life cycle decision making and active control of buildings and districts. 

In a nutshell, the aim of SemanticLCA is the development of a (near) real-time semantic capability that exploits a wide range of digital data sources and leverages artificial intelligence to assess the whole-life cycle environmental impacts of built assets. The following research questions are posited: 

RQ1: Can the use of semantics, including BIM (IFC) and GIS (CityGML), to integrate and contextualise existing life cycle inventory databases, provide a sound basis to streamline the life cycle assessment process of buildings and districts? 
RQ2: Can access to dynamic data, managed in a BIM and GIS friendly time series database, provide more accurate accounts of environmental impacts during the construction and operation stages? 
RQ3: Can the resulting SemanticLCA environment assist in decision making by non-experts by exploring a wide range of options and scenarios with the least environmental impact, while also advising on corrective plans?

Our work programme involves three Work-Packages (WP), each addressing one of our posited research questions, and a fourth cross-cutting WP addressing demonstration and validation activities. 

The evaluation will be carried out in two demonstration sites: Cardiff (UK) and Belval (Luxembourg). The Cardiff demonstration will be carried out in the Queen's building (School of Engineering) and scaled up to the 130 buildings owned and managed by Cardiff university, majority of which are located in the city centre. The LIST demonstration will be carried out in the Maison de l'Innovation in Belval and scaled up to the entire district of Belval (managed by Fonds Belval). Given the complexity of LCA at district level, validation will utilise a simulation based approach with a subset of use cases demonstrated and validated in real operation conditions. The validation work will leverage ongoing developments of city platforms for Cardiff and Belval, as illustrated on the CUSP website: www.cuspplatform.com.

SemanticLCA is supported by 10 partners and an experienced team of investigators from Cardiff University and LIST bringing together complementary expertise in: a) AI applications in the built environment, b) semantic contextualisation of multi-scale built environment data, c) intelligent cloud/edge computing, d) Life cycle assessment methods and tools, e) Building Information Modelling for asset modelling and energy efficiency.",,"SemanticLCA will deliver a new value proposition in the life cycle assessment of buildings and districts, that factors in multi-aspects real-time data, to continuously inform decision making, including the implementation of corrective measures that attenuate the environmental impacts of our built environment. This will pave the way to significant societal, environmental, and economic impacts. We will be among the first to manage in real-time the life cycle impact of buildings using real-time data grounded in semantic models, including BIM (Building Information Modelling) and GIS (Geographical Information Systems). We provide scientific excellence with environmental and societal impact because environmental impacts and their consequences on humans are factored into our approach. 

Knowledge impact: We will organise a series of Workshops for members of industry, academia and policy-makers, using case studies in Cardiff and Belval (Luxembourg) to demonstrate how SemanticLCA works. Our data, information and knowledge on the concept of dynamic life cycle assessment will be useful not only in evaluating the environmental impact of buildings already in use but also to inform practice by (a) revisiting and improving aspects of existing codes and guidelines, (b) informing the briefing process driven by a user-centred approach, (c) regulating the handover / commissioning stage, (d) devising operational strategies to continuously monitor and pro-actively adapt the indoor environment, energy consuming devices, and energy systems in place, to minimise impacts on the environment, and (e) establishing a learning culture that continuously informs practice. Additionally, we will disseminate our findings via our dedicated portal (www.sematicLCA.net) that will be maintained by Cardiff University beyond the project, thus forming a real legacy. 

Societal impact: SemanticLCA will provide a unique rich and multi-aspects data source for buildings and districts, hosted within our platform (www.semanticLCA.net) to support policy making around the environmental impacts of our built environment in the UK, Luxembourg and globally. Resulting data will also underpin the future use of active control systems in buildings and districts, and our engagement with policy makers will allow such systems to be realised in retrofitted and new buildings. As such, we will exploit our partners' (including BRE) network of contacts to gain access to, and engage with, policy makers. We will take our findings on a UK and Europe wide roadshow to highlight the potential of SemanticLCA. 

Economic impact: Our project partners will be the primary adopters and beneficiaries of SemanticLCA, and we will facilitate regular meetings and webinars to encourage uptake. The second tranche of adopters will be a broader cross-section of the construction industry, encouraged by our academic network of contacts (via Digital Built Britain and related initiatives, including in Luxembourg and wider Europe facilitated by support from the ECTP - European Construction Technology Platform) as well as our ambitious dissemination programme through our planned workshops, public exhibition and conference. We will also target facility managers and SMEs involved in sensors and sensing, all of whom stand to benefit from this transformation.

People impact: Our team, which will include all investigators, Post Doctoral Research Assistants (PDRAs), and a PhD student (fully funded by Cardiff university), will grow organically into a powerful grouping through outstanding people development. Academics involved in SemanticLCA will become part of an entirely new, pioneering, inter-disciplinary research field at the intersection of Semantics (including BIM) and Artificial Intelligence. We will feed our research into new taught programmes, and run annual summer schools for students showing interest in our approach. We will run the SemanticLCA Research Society to engage research colleagues around the globe."
12,F382EAE1-264F-4C8D-AB41-3EB87A1C38D1,Communication-Aware Dynamic Edge Computing (CONNECT),"Internet of things (IoT) is slowly permeating every aspect of our lives; however, we are far from having a truly intelligent IoT. Smart sensors generate massive amounts of data continuously; for instance, an autonomous vehicle is expected to generate about one gigabyte of data per second, but more often than not data is not systematically processed, stored, or analyzed for better inference. Many specialized machine learning (ML) algorithms have been developed to learn from sensor measurements, but these assume a centralized setting, where data is available at a central processor with powerful computation capabilities. This centralized approach assumes that the massive amount of sensor data is transmitted to a cloud center, which may not be feasible due to limitations of the devices and channels, not meet the stringent delay constraints of most applications, e.g., controlling an autonomous vehicle, or the privacy requirements of users. In the CONNECT project, our goal is to develop real edge intelligence by enabling edge nodes to make local decisions rapidly and reliably in a collaborative manner. This will be achieved by developing novel caching, distributed computing and networking methodologies to enable federated/ distributed learning taking into account the network dynamics and physical channel variations. The developed joint computing, caching and communication framework will then be applied to a hierarchical heterogeneous architecture for vehicular ad-hoc networks (VANETs). This will not only enable efficient and reliable learning across mobile nodes, but also improve the security and privacy of autonomous cars by limiting decision making to local neighborhood. Integration of caching, computing and networking will be demonstrated both through large-scale simulations, and on a small-scale implementation platform, consisting of two cars and a roadside unit at Koc University. This project is expected to enable many data intensive edge applications, from multimedia content streaming to participatory data collection in mobile networks, including autonomous cars, drones, mobile robots and mobile cellular users.",,N/I
13,6E657FD6-3E9E-40E5-A802-70BB66ED90C6,Probing for New Physics at the LHC: Unraveling the Higgs Mechanism through Polarisation and Hadronic Decays,"The Large Hadron Collider (LHC) at CERN collides protons at the highest energy produced in the laboratory. The ATLAS experiment collects 40 million collision &quot;pictures&quot; per second, selects a small fraction (the most interesting ones) using fast electronic systems, and stores 1 GB of data per second for further analysis. Particle physicists like myself analyse these huge amounts of data, investigating the particles that make up our Universe and studying their interactions.

In 2012, the Higgs boson was discovered, confirming a prediction of the Standard Model of particle physics (SM), a theory that accurately predicts a wide range of observed phenomena. Yet, cosmological observations suggest that the SM only explains 4% of the Universe, its &quot;visible&quot; part, with 96% being unknown, and for this reason called dark matter and dark energy. The goal of particle physics is to measure the SM as accurately as possible and search for physics phenomena Beyond the SM (BSM) that dark matter is possibly made of.

The vast majority of proton collisions involve its constituents: quarks and the gluons (carriers of the strong force) which hold them together. My work involves the study of the much rarer collision of (weak) vector bosons (W and Z, carriers of the electroweak force) emitted by the protons, using the LHC as a weak boson collider! These processes, known as vector boson scattering (VBS), are highly sensitive to new physics, and could shed light on undiscovered particles that don't interact with quarks and gluons. Gaining a better understanding of the electroweak force through VBS is one very promising path to solving the mystery of dark matter, that some theories predict to be a weakly-interacting particle, and separately, to understanding how fundamental particles acquire mass (and whether the Higgs boson is alone in this process). Studying the electroweak force could be key to explaining the tiny mass of the neutrinos.

But probing VBS is very challenging as these are a tiny fraction of collisions, and any subtle sign of new particles is difficult to uncover. My research focuses on techniques to extract the rare VBS events from proton-proton collision data. Using these to select only 60 events among many billions, I had a lead role in the group that observed the production of two W bosons of the same charge, a very rare process that happens once per 20 000 billion collisions (typically once per day at the LHC).

The precision of the measurements can be further improved by collecting more data. But an even bigger impact can come from new analysis techniques, such as the identification of hadronic W boson decays (when the W decays to quarks). This is very difficult to do, as many other, more frequent, processes also produce quarks. To achieve this, my research will involve complex machine learning algorithms, similar to those that allow for automatic face recognition or driver-less cars.

Also of great interest is the Higgs boson, its interaction with weak bosons, and its self-interaction (HH), even rarer than VBS and which requires an upgraded LHC. These are very important probes of the SM that could yield hints of new physics, as small deviations from the SM can have a large impact on event rates.

To improve the precision of measurements, I perform R&amp;D on the detectors used to collect the data, in particular silicon pixel detectors, which are similar to the sensors in digital cameras. At the heart of particle detectors, they are the first that the collision products encounter. I devise and study new detector concepts to cope with the challenges of future particle colliders. High precision silicon detectors are essential to identify the collision event characteristics, the first step towards unraveling the mysteries of our Universe.

Finally, I am sensitive to data preservation so that future theories can be tested against ATLAS data, so that the work we do today can help future generations shed light on Nature.",,
14,7F0D5724-D3D5-4B88-9995-D84009D61E54,Simulating value creation opportunities at inspection processes using digital twins,"The ante-mortem and post-mortem inspection protocols for quality and safety assurance of meat production in UK abattoirs is set out in the operational guidelines of the Food Standard Agency's (FSA) manual for official controls. These official guidelines outline the processes, resources, and reporting protocols used by Official Veterinarians (OVs) and Meat Health Inspectors (MHI) to carry out and file both Ante-Mortem (AM) and Post-Mortem (PM) inspection reports. However, it has emerged from the &quot;Review of 21st Century Abattoirs&quot; by a team of researchers that due to variations in production throughputs, species processed, and kill-speeds across abattoirs in the UK, it is somewhat challenging for the FSA and Food Business Organisations (FBOs) to empirically determine the key resource constraints and Key Performance Indicators (KPIs) of efficient and effective AM and PM inspections routines. 

In order to enable inspectors from the FSA and FBOs to discharge their duties more efficiently and accurately, it is imperative to develop deeper insight into the process functionalities and relevant metrics that have direct implications for inspection efficiency, quality and safety scrutiny within abattoirs, which itself is another challenge. However, a thorough analysis is possible to obtain detailed process information through the following:
(1) A detailed map of the process layout/configuration requirements of AM/PM critical control points.
(2) A detailed map of the manpower requirements and standardised operating protocols for AM/PM inspections and reporting by OVs.
(3) A detailed outline of in-process KPIs and constraints of AM/PM inspection processes and reporting 
(4) Relative and absolute benchmarking of available technology interventions that can be incorporated to improve the accuracy and efficiency of specific AM/PM inspections beyond what is obtainable from visual checks, palpations and offal incisions.
(5) Evaluation of process capabilities to incorporate recommended technology interventions within a specific process.

While there is scope to augment existing inspection processes, routines and data streams with technologies such as imaging technologies, line-mounted and hand-held sensor technologies, Artificial Intelligence-based fault detection and diagnostic technologies and track-and-trace applications, it is important to carry out a detailed map of the current or AS-IS operating inspection model along the four streams outlined above in order to:
(a) standardise AM and PM inspections
(b) match the right inspection process, resource and technology requirements to abattoirs by throughput, species and kill-speed, and
(c) inform investment in the right layout, process or technology along with right combination of human and technology interventions to usher in an era of 21st century abattoir inspections by the FSA and FBOs.

Practically, it is challenging to determine the effect of technology or process interventions on existing operations. Simulations have been used as a supporting digital technology to trial possible innovations in a risk-free virtual environment before introducing either a new technology or a process. Determination of the future-state requires the digital twin developed in this project to be validated and its viability to be determined. This will be achieved through a series of use-cases where the current state will be modelled and the future case simulated, providing an excellent starting point for the development of a digital twin.",,
15,C12893BF-5D7F-4C70-A779-1D16842F0277,"Microfluidic Molecular Communications: Design, Theory, and Manufacture","Molecular communication (MC) provides a way for nano/microdevices to communicate information over distance via chemical signals in nanometer to micrometer scale environments. The successful realization of MC will allow its future main applications, including drug delivery and environmental monitoring. The main hindrance for the MC application stands in the lack of nano/micro-devices capable of processing the time-varying chemical concentration signals in the biochemical environment. One promising solution is to design and implement programmable digital and analog building blocks, as they are fundamental building blocks for the signal processing at MC transceivers. With two existing approaches in realizing these building blocks, namely, biological circuits and chemical circuits, synthesizing biological circuits faces challenges such as slow speed, unreliability, and non-scalability, which motivates us to design novel chemical circuits-based functions for rapid prototyping and testing communication systems. 

Conventional chemical circuits designs are mainly based on chemical reaction networks (CRNs) to achieve various concentration transformation during the steady state from the input to the output with all chemical reactions occurring in same &quot;point&quot; location. This kind of design does not fit for the time-varying signals in communication system due to that the temporal information can be invisible to even state-of-the-art molecular sensors with high chemical specificity that respond only to the total amount of the signaling molecules. Thus, this project aims to design the chemical reaction-based microfluidic MC prototypes with time-varying chemical signal processing functionalities, including modulation and
demodulation, encoding and decoding, emission and detection. This also facilitates the microfluidic drug delivery prototype design and cancer cell on chip testing under time-varying drug concentration signal. 

This project has the ambitious vision to develop novel time-varying chemical concentration signal processing methodology for microfluidic MC and microfluidic drug delivery. In the long run, 
1) our microfluidic MC results will enable the implementation of MC functionality into nanoscale machines, by downsizing the proposed components through the utilization of nanomaterials with fluidic properties, and by translating the functional chemistry into biological circuit designs; 
2) our microfluidic drug delivery results will revolutionize the conventional drug delivery testing approach by enabling ICT technologies for novel in-vitro microfluidics for drug delivery, allowing rapid measurement of therapeutic effect, toxicology, to reduce development costs and minimize the use of animal models.",,"The MIMIC project is the UK's first innovative effort to realize the signal processing and communication capabilities of the microfluidic device via chemical reactions, to address its fundamental theoretical and experimental aspects of microfluidic molecular communication, and to exploit efficient in-vitro drug delivery on cancer cell on chip. 

The general communication and detection of nano/micro-devices are important to achieve precision embedded sensing and actuation in a wide range of future applications, including environmental monitoring, and drug delivery. The economic potential of this idea is enormous, which we can estimate by looking at the related market for nano-medicine (such as precision drug delivery): studies estimate the present market size at $96.9 billion (2016), with a 14.1% per year growth (BCC Research Report). This project contributes to &quot;Digital Signal Processing&quot; and &quot;Microsystems&quot; EPSRC research areas, and also directly aligns to the EPSRC Healthcare Technologies Grand Challenges by developing the enabling ICT technologies for the novel in vitro microfluidics for drug delivery.

The immediate beneficiaries will be the telecommunication, microfluidics, and drug delivery industries, in particular, our industry partners, Elveflow, and Mediwise. We will exploit the economic impact of our microfluidic prototypes 1) for delivering drugs to cancer cell on chip in the conditions closest to the physiological conditions via working with Elveflow; 2) for intelligent insulin delivery, with potential integration with Mediwise's GlucoWise platform with blood glucose monitoring capability. The long-term benefits of molecular communication research, as a potential enabling technology for the nanoscale communication in 6G, will also be exploited with IEEE P1906.1.1 standard committee on nanoscale communication system and Wireless World Research Forum (WWRF).

From the academic impact perspective, our communication components design, analysis, and optimization based on chemical circuits and microfluidics contribute to the field of molecular communication; our chemical circuits analysis and design contributes to the field of chemistry and synthetic biology; our flow-based microfluidic analysis and design contributes to the field of microfluidics; and our microfluidic drug delivery prototype contributes to the field of drug delivery. In the long run, 1) our microfluidic MC results will enable the implementation of MC functionality into nanoscale machines, by downsizing the proposed components through the utilization of nanomaterials with fluidic properties, and by translating the functional chemistry into biological circuit designs; 2) our microfluidic drug delivery results will revolutionize the conventional drug delivery testing approach by enabling ICT technologies for novel in-vitro microfluidics for drug delivery, allowing rapid measurement of therapeutic effect, toxicology, to reduce development costs and minimize the use of animal models.

By bringing together academic experts from chemistry, microfluidics, drug delivery, and molecular communication, as well as industry practitioners in microfluidics and bioengineering, we are able to cross-fertilize academic research between disparate but important disciplines and accelerate the molecular communication industry."
16,09471111-58E6-4E13-B428-79BF63289339,Mesoscale structural biology using deep learning,"There are many structures in the cell which are thought to be the same (or almost the same) every time they form. Examples include the nuclear pore complex and the centriole. Structures from a lengthscale from around 30nm to a micron can be imaged by a form of fluorescence microscopy called localisation microscopy, where the position of each individual fluorophore is found to high precision. The localisation microscopy methods which are simplest to analyse and least likely to produce artifacts create images where the 3D structure is projected down onto a 2D image. This means that it is difficult to deduce what the 3D structure is. There are a number of other microscopy techniques, particularly cryo-electron microscopy, which have faced similar challenges. In general, this is approached by putting images into a number of classes which are then averaged to improve signal to noise and a model is then optimised to fit all of the information. 
However, there is a property of localisation microscopy which means that we can take a different approach, which has the potential to fit the data much better. In localisation microscopy the position of each individual fluorophore is found, and the image of the sample is then reconstructed by displaying a Gaussian at the location of each fluorophore. This means that the system used to display the data can be easily created as a differentiable renderer (i.e. a system of display where the first derivative at each point can be calculated).
We will use this property to create a deep learning based optimisation system which will generate an optimised 3D model of points to describe a dataset with many 2D images of the structure. The model will start off as a random distribution of points. At each stage of the optimisation the model will be compared to all the 2D images, and for each of them the angle which produces the best fit to the data will be found. The model will then be changed and the process repeated, gradually optimising the model fit the data. The final result will be a 3D model which incorporates all the information from the different 2D images. This will be an unusual application of deep learning, since instead of training a network which will be useful for people to use directly, the training of the network will lead to the creation of the final model. 
Since we are fitting to each individual image, it will not be necessary to perform averaging of the images to improve the signal to noise. For relatively large structures such as the ones we are considering, this is an advantage because the structures are likely to flex or deform to some extent. Averaging would therefore wash out structure. In contrast, we can build deformation into our model and therefore will get an accurate structure back even if there are slight variations between different instances of the structure.
We will test the performance of our method on simulations and experimental data. Simulations will allow us to assess the impact that experimental effects will have on our results. In particular, there is an uncertainty associated with the localisation of each fluorophore, and a certain proportion of the proteins are either not labelled or not detected. 
The method will then be tested on experimental datasets of different centriole proteins, each with several thousand images of individual centrioles. Since this is not enough to train a deep learning network, we will carry out data augmentation, in which the image is shifted slightly and rotated in the x,y plane to create new images. This artificially creates more data and assists the network in learning small shifts and rotations. The results of fitting to experimental data will be compared to images of the same structures imaged using another super-resolution microscopy technique where the sample is embedded in a gel which is then expanded. This will allow us to be confident that our method is able to reproduce real structure from experimental data.","We will create a deep learning based method to allow a 3D model consisting of a number of points to be fitted to a large number of 2D images of the structure under consideration. For each image in the dataset, the rotation of the 3D model that allows the best fit of the data will be found. The model will then be optimised to minimise the total error. This will allow the optimisation of the sample model without assuming particular symmetry constraints.
The architecture of the deep learning network that extracts the pose information will comprise an encoding section that predicts a rotation, a differentiable renderer, and a loss function that takes an input and output image as its parameters. To allow the system to converge on the correct structure, input and output images will be heavily blurred initially (i.e. a large Gaussian will be used to render them from the point data), and the blur will be decreased as the model is optimised. Since real biological structures a few hundred nanometres in size often exhibit some variation in structure, we will allow a limited affine transformation for each image to model small amounts of flex and distortion.
We will test on simulations of different structures to better understand what type of data this system will perform well on. In particular, the impact of localisation precision and the labelling rate will be tested by varying both systematically. This will inform our treatment of experimental data, since the data can be filtered for higher localisation precision, at the cost of a lower labelling rate. The performance on experimental data will be tested on localisation microscopy data of the centriole from the Manley lab at EPFL, with the performance of the method being cross checked against images of the same proteins imaged with expansion microscopy combined with structured illumination microscopy.","The initial impact is expected to be seen in an improved ability to reconstruct 3D structures from sets of 2D localisation microscopy images. We will initially target proteins of the centriole for reconstruction, with other possible targets being the nuclear pore complex and clathrin coated pits. These are all systems of high biomedical importance and in the longer term the greatest impact of this project is likely to be enabling and accelerating new biomedical research.

The basic approach which we are developing could also be of much wider interest. We have already made initial contact with Professor Helen Saibil, a prominent member of the EM community, who thought that the method could be of considerable interest to those developing EM software. More generally, the method could also have applications for other types of fluorescence microscopy, since many techniques have a much worse resolution in z than in xy, meaning that in effect each image is a projection. For continuous structures (i.e. anything except small points), our approach could be the base of a method to reproduce the 3D structure of the sample at better than the resolution limit.

We have links to a number of microscopy companies, including Nikon. When the algorithm is developed we will approach microscope companies both with a view towards entering into a dialogue into how their systems might be of use for acquiring data that could then be used with our method, and also potentially with regard to interest in a commercial version of the algorithm. Microscope companies stand to benefit from our work because it would extend the experiments that could be carried out on their systems and give users greater confidence in their results. In turn, their users will benefit because we will be able to advise on changes to the hardware and software of the system which would optimise performance.

The post-doctoral researcher employed on the project will receive training in python programming with PyTorch, neural network architecture and testing procedures, image analysis and super-resolution microscopy. With regard to the deep learning component of the project, the approach taken here is highly unusual in the context of microscopy, where most uses of deep learning take fairly standard approaches to classification, or creation of images using generative adversarial networks. In contrast, here we are using computer vision/deep learning approaches at the cutting edge of the engineering field, using the operation of the neural network itself as a tool with which to perform the model optimisation. Both advanced microscopy and deep learning are rapidly growing areas, with many jobs being created and a shortage of people with in-depth training. We anticipate that the training and experience that the postdoc would acquire over the course of the project would be highly beneficial in enabling their future career choices."
17,A603C73A-BB37-4835-83A4-B8EA8615490D,"AURA (Archives in the UK/ Republic of Ireland &amp; AI): Bringing together Digital Humanists, Computer Scientists &amp; stakeholders to unlock cultural assets","The AURA network brings together Digital Humanists, Computer Scientists, archivists and other stakeholders to unlock cultural assets held in &quot;dark&quot; digital archives currently closed to users. Not so long ago, historians, literary scholars and other scholars would read letters and other papers preserved in Special Collections Libraries. Of course, this analogue world has not disappeared, but the digital revolution has profoundly changed the way we encounter archives. Born-digital archives are now better preserved and managed thanks to the development of open-access and commercial software. Yet, preserving born-digital records is not enough. We also need access to these archival materials, in order to produce new knowledge and foster public engagement. 

Archives are meant to be used, not locked away. A central problem is that most born-digital archives are closed to users due to privacy, copyright or technical issues. Even when access is possible (as in the case of web archives), users often need to physically travel to repositories rather than consult materials remotely. In order to unlock cultural assets, we need to bring the best minds together and harness the latest technology. 

At present, applying Artificial Intelligence to archives remains at the exploratory stage. Yet, automation is no longer a choice, it is a necessity. AI can be used to separate personal and business emails and improve accessibility to non-confidential records; identify sections of documents that refer to personal data allowing partial views or limited access to the archival content; extract named entities (people names, dates, events) from archives and link them to external sources.

While access to digital archives is essential, we also need to anticipate the moment when born-digital records will be more accessible. To make sense of this mass of data, new methodologies are urgently needed, combining traditional humanistic methods with data-rich approaches. Collaborations between Humanities scholars, Computer Scientists, archivists and other stakeholders are therefore essential to make archives more accessible, but also to design new methodologies to analyse huge amounts of data.

The network will focus on three major themes, that will be explored in each of the three workshops: &quot;Open Data versus Privacy&quot; (Workshop 1 in Dublin); &quot;AI and Archives: Current Challenges and Prospects of Born-digital archives&quot; (Workshop 2 in London); &quot;AI and Archives: What comes next?&quot; (Workshop 3 in Edinburgh). 

The workshops will be carefully structured to include a mix of short presentations, speed meetings with interdisciplinary teams to discuss a specific question relating to the overall workshop theme, and practical activities designed to lead to mutually beneficial, sustainable collaborations. We will also create wiki pages for each workshop to foster asynchronous discussion, and build on the participation of people that cannot be onsite. In addition, a project website will keep track of all the network activities in the form of reports, blog posts and recordings of presentations. Associated social media, as well as a dedicated listserv, will help us connect with interested parties - in academia, archival institutions and beyond.",,"First, the AURA network will impact on government as creators of records, including born-digital records. In the UK, these records need to be transferred for permanent preservation after twenty years (rather than the previous thirty years). This changing rule means that archival institutions now face an incoming flood of digital records created by government at the turn of the twenty-first century, a key period in the transition from print to digital. Emails largely replaced letters, electronic documents were no longer systematically printed out. With this mass of data, it is no longer possible to manually review individual records to identify confidential or sensitive materials. Automation is becoming essential for appraisal and selection. Policy makers will join us for network activities, and a policy paper (to be completed after Workshop 1) will allow us to consolidate our engagement between government, archives, and academics. We will also benefit from the support of the International Council on Archives (Project Partner), an international non-profit organisation with close links to policy makers and governing bodies.

Second, AURA will benefit archivists, including early-career archivists whose work is increasingly dominated by digital records. Indeed, many archivists are now de facto digital archivists. Sharing expertise with Computer Scientists and Humanities Scholars will benefit these archivists and their institutions in multiple ways. In their letters of support for the network, our project partners articulate two main kinds of benefits. At the stage of appraisal and selection, computational methods are increasingly viewed as essential. The next step is to provide access to born-digital archives, and here again, automation is central to identify non-problematic records that can be made available. As one of our partners writes, &quot;this project is a timely and vital one to ensure that we can continue working towards making our collections available for the public good and to serve our researchers.&quot;

Alongside The National Archives, our non-academic project partners include The National Library of Ireland; The National Library of Scotland; the British Library; the Irish Traditional Music Archive and the International Council on Archives. There are many smaller institutions that would benefit from our network, and the network activities will include a number of places for additional participants to join us. 

By bringing Computer Scientists, Humanities scholars, archivists and policy makers together, the AURA network will ensure that the resulting scholarship is informed by the needs of all the actors of the archive &quot;circuit.&quot; The two special issues of journals will be academically rigorous but engaged with the practical experience of working in non-academic institutions. Articles will be written in a clear style for this multi-disciplinary audience.

Third, the AURA network will benefit a wider audience. Archives are of course not reserved to academic researchers. For example, many family historians will be interested in the changing nature of sources (from print to digital) and in new methods to analyse digital archives. Our project partners already do extensive public engagement work, and will promote the network to this wide audience. The National Library of Ireland will host a tour of their literary exhibitions in conjunction with the first workshop in Dublin. This will be a good way to engage with a broad audience on the topic of the changing nature of literary collections in the twenty-first century.

We will publicise our discussions via a project website and wiki pages to keep track of all the activities developed in the project in the form of reports, blog posts and recordings of presentations. We will also produce two articles on the issue of cultural assets locked to the public. We will link these outputs to the UNESCO #WorldHeritage campaign to foster a two-way dialogue with the general public."
18,D31AC537-0909-4630-AA1B-B7349B9BCFCE,FOCUS: Intelligent Fibre Optic Monitoring to Inform the Construction of Underground Services,"UK construction is a multi-billion pound industry. While it is the most vital cog in the UK economy for creating physical assets, it is widely regarded as slow to innovate. High risks and the significant cost of mistakes promotes a level of conservatism which is much greater compared to other industries. Change therefore tends to be iterative and cautious. Supported by the UK Government through the implementation of various construction initiatives, such as 'Construction 2025' and 'Transforming Construction', the industry is beginning to embrace technology in a transformative way. The technological revolution is already under way for 'above-ground' construction activities, with modular construction and building information modelling being primary examples. One of the biggest obstacles to underground construction making similar gains is uncertainty surrounding how structures interact with soils during construction operations i.e. 'soil-structure interaction' (SSI). Soil-structure interaction plays a critical role in underground construction operations yet the tools that are used to predict them remain remarkably over-conservative. This is because predictive models for SSI are non-existent, over-simplified or are calibrated against measured data obtained from model-scale replicas of the process in the laboratory, essentially representing an 'ideal' soil-structure interface. 

The work described in this proposal will develop the underpinning engineering science for SSI design applied to underground construction. Laboratory testing and numerical modelling will be used to elucidate the mechanics of soil-structure interface behaviour such as the role of strain level, stress level and time on the development of soil-structure contact stresses and pore water pressures. Intelligent monitoring systems will be developed to measure and monitor soil-structure contact stresses on live construction projects to provide (i) field data for rigorous validation of developed design methods and (ii) real-time, automated feedback to site engineers to inform construction processes and provide 'early warning' of adverse responses. Recent advances in fibre optic sensing will be exploited to develop novel multi-directional contact stress sensors. The new sensors will alleviate limitations associated with traditional transducers such as excessive sensor flexibility (which actually influences the soil stress field the sensors are intended to measure) and immunity to electromagnetic noise and water damage. A multi-directional interface shear apparatus will be developed to validate the contact stress sensors and provide additional insight into the behaviour of an 'ideal' soil-structure interface in the laboratory. The monitoring system will employ machine learning algorithms in the form of Bayesian non-parametrics such that prior data from previous construction projects may be synthesised with newly-acquired data to provide a robust data-driven decision-making process. The monitoring system will be deployed on live construction projects in the UK alongside industry partners. A suite of new design methods tailored specifically for underground construction operations will be developed, informed by the field monitoring, laboratory testing and numerical modelling. Embracing the innovation and technology developed in this project will allow the construction industry to obtain and utilise intelligent and actionable data that can save time and money, and improve construction safety. This will contribute to the UK becoming a global hub for the rapidly growing market for construction-related services throughout the world.",,"The potential to translate research outcomes into economical, societal, and environmental benefits are very substantial. The construction industry would benefit significantly from the proposed SSI design methods and monitoring systems. These methods and systems will reduce the uncertainty associated with soil-structure interaction during underground construction, leading to improved project outcomes. For example a build-up of soil-structure interface friction will be identified early on in projects, thereby allowing preventative action to be taken, avoiding work stoppages and project delays. The development of an underground construction research group at Oxford will also produce the next generation of construction professionals to revitalise the industry.
The public sector will benefit from this research because a reduction in underground construction risks are associated with lower construction costs as a result of reduced contingency in project budgets and more reliable project timelines. Construction activity is one of the cornerstones of the UK's economy and contributes significantly to economic growth. It reflects dynamic growth in other industries and is itself a core provider of jobs. Beyond the construction activity itself, construction also drives demand for various building materials. Often perceived as being behind the curve, it is critical that the construction industry embraces innovative technology to guarantee UK prosperity.
The PI will also personally benefit as the research leader of FOCUS by further developing his skills and expanding his research team. The research team will benefit from the valuable training in state-of-the-art facilities offered by the industry partners. They will also benefit from a number of career development opportunities including outreach events and a very wide range of courses run by the Mathematics and Physical Life Sciences Division at Oxford. In the long-term, the PI intends to integrate 'Construction Engineering' into the undergraduate curriculum which is, perhaps surprisingly, not currently taught at Oxford. The Oxford engineering undergraduates will therefore also benefit from both a new course and the inclusion of cutting-edge research being conducted by the PI's group.
The construction industry is the single largest global consumer of resources and raw materials, so even a 1% reduction in construction costs would save society huge sums globally. Value therefore lies in improving construction practice, leading to efficiency gains and reduced consumption of materials. Society as a whole will experience a major benefit from significant reductions in CO2 emissions on planned construction projects. Driving down both the economic and environmental costs of construction will boost the resilience of the UK infrastructure sector.
The construction industry is generally perceived by the public as an old-fashioned, wasteful and male-dominated industry which has, in part, contributed to a major skills shortage in the UK construction sector. Attracting young engineers and tradespeople into construction has become increasingly challenging. Inevitable retirements of experienced UK construction personnel means the UK skills shortage is likely to worsen. The public engagement / outreach activities outlined in 'Pathways to Impact' will increase public awareness of modern underground construction practice. They will also highlight the huge variety of construction related careers. For the industry to begin appealing to those who currently view it as very static, it is crucial that young people are made of aware of these opportunities and developments."
19,709F56F1-8E83-4FE3-B86A-184BA79747BE,Turing AI Fellowships Phase 1,"Anna Scaife, University of Manchester, &pound;1,458,618
AI4Astro: AI for Discovery in Data Intensive Astrophysics

The data rates from modern scientific facilities are increasing and it is no longer possible for scientists to extract scientifically valuable information by hand. This is particularly important for the Square Kilometre Array (SKA) telescopes. Consequently, in this era of big data astrophysics using machine learning to extract scientific information is essential to realise a timely scientific return from facilities such as the SKA. This research will consider how existing techniques can be adapted to achieve the key scientific goals of the SKA telescope. It will target the development of new machine learning approaches which address key aspects of SKA scientific processing.


Maria Liakata, University of Warwick, &pound;1,227,974
Creating time sensitive sensors from language &amp; heterogeneous user generated content

Widespread use of digital technology has made it possible to obtain language data (social media, SMS) as well as heterogeneous data (mobile phone use, sensors). Such data can provide useful behavioural cues on an individual level and for the wider population, enabling the creation of longitudinal digital phenotypes. Current methods in natural language processing (NLP) are not well suited to time sensitive, sparse and missing data or personalised models of language use. The proposed research will address specific challenges within NLP, will have an application to AI and mental health, and outputs will include novel tools for personalised monitoring behaviour through language use and user generated content over time.


Neil Lawrence, University of Cambridge, &pound;2,380,212
Innovation to Deployment: Machine Learning Systems Design

The AI systems this project is developing and deploying are based on interconnected machine learning components. This research focuses on AI-assisted design and monitoring of these systems to ensure they perform robustly, safely and accurately in their deployed environment. This project addresses the entire pipeline of AI system development, from data acquisition to decision making, and proposes an ecosystem that includes system monitoring for performance, interpretability and fairness. This project places these ideas in a wider context that also considers the availability, quality and ethics of data.


Timothy Dodwell, University of Exeter, &pound;1,336,283
Intelligent Virtual Test Pyramids for High Value Manufacturing

There is a paradox in aerospace manufacturing. The aim is to design an aircraft that has a very small probability of failing. Yet to remain commercially viable, a manufacturer can afford only a few tests of the fully assembled plane. How can engineers confidently predict the failure of a low-probability event? This research will develop novel, unified AI methods that intelligently fuse models and data enabling industry to slash conservatism in engineering design, leading to faster, lighter, more sustainable aircraft.


Yarin Gal, University of Oxford, &pound;1,379,408
Democratizing Safe and Robust AI through Public Challenges in Bayesian Deep Learning

Probabilistic approaches to deep learning AI, such as Bayesian Deep Learning, are in use in industry and academia. In medical applications they are used to solve problems of AI safety and robustness. But major obstacles stand in the way of widespread adoption. This project proposes building new AI challenges to assess safety and robustness, derived from applications of AI in industry. The challenges identified will set the course for a community-driven effort leading to a self-sustained ecosystem and will bridge practitioners and AI researchers. This will offer new research opportunities for the AI community, helping to develop new safe and robust AI tools. Democratising safe and robust AI aligns with the UK's strategic plan set by Hall and Presenti and will help put the UK at the forefront of AI globally.",,
20,DFCEE1E3-C391-449E-94BC-216A3F80ACA1,UKRI Trustworthy Autonomous Systems Hub,"Public opinion on complex scientific topics can have dramatic effects on industrial sectors (e.g. GM crops, fracking, global warming). In order to realise the industrial and societal benefits of Autonomous Systems, they must be trustworthy by design and default, judged both through objective processes of systematic assurance and certification, and via the more subjective lens of users, industry, and the public. To address this and deliver it across the Trustworthy Autonomous Systems (TAS) programme, the UK Research Hub for TAS (TAS-UK) assembles a team that is world renowned for research in understanding the socially embedded nature of technologies. TASK-UK will establish a collaborative platform for the UK to deliver world-leading best practices for the design, regulation and operation of 'socially beneficial' autonomous systems which are both trustworthy in principle, and trusted in practice by individuals, society and government.

TAS-UK will work to bring together those within a broader landscape of TAS research, including the TAS nodes, to deliver the fundamental scientific principles that underpin TAS; it will provide a focal point for market and society-led research into TAS; and provide a visible and open door to engage a broad range of end-users, international collaborators and investors. TAS-UK will do this by delivering three key programmes to deliver the overall TAS programme, including the Research Programme, the Advocacy &amp; Engagement Programme, and the Skills Programme. 

The core of the Research Programme is to amplify and shape TAS research and innovation in the UK, building on existing programmes and linking with the seven TAS nodes to deliver a coherent programme to ensure coverage of the fundamental research issues. 

The Advocacy &amp; Engagement Programme will create a set of mechanisms for engagement and co-creation with the public, public sector actors, government, the third sector, and industry to help define best practices, assurance processes, and formulate policy. It will engage in cross-sector industry and partner connection and brokering across nodes. 

The Skills Programme will create a structured pipeline for future leaders in TAS research and innovation with new training programmes and openly available resources for broader upskilling and reskilling in TAS industry.",,
21,1A1279D1-B39C-4CAF-B432-64BFC5FC8225,Designing Conversational Assistants to Reduce Gender Bias,"Biased technology disadvantages certain groups of society, e.g. based on their race or gender. Recently, biased machine learning has received increased attention. Here, we address a different type of bias which is not learnt from data, but encoded during the design process. We illustrate this problem on the example of Conversational Assistants, such as Amazon's Alexa, Apple's Siri, Microsoft's Cortana, or Google's Assistant, which are predominately modelled as young, submissive women. According to UNESCO, this bears the risk of reinforcing gender stereotypes.

In this proposal, we will explore this claim via psychological studies on how conversational gendering (expressed through voice, content and style) influences human behaviour in both online and offline interactions. Based on the insights gained, we will establish a principled framework for designing and developing alternative conversational personas which are less likely to perpetuate bias. A persona can be viewed as a composite of elements of identity (background facts or user profile), language behaviour, and interaction style. This framework will include state-of-the-art data-efficient NLP deep learning tools for generating dialogue responses which are consistent with a given persona. The persona parameters can be specified by non-expert users in order to to facilitate more inclusive design, as well as to enable a wider critical discussion.",,
22,EC1A3AD5-414B-4A37-9E57-9FC128D84701,Designing Conversational Assistants to Reduce Gender Bias,"Biased technology disadvantages certain groups of society, e.g. based on their race or gender. Recently, biased machine learning has received increased attention. Here, we address a different type of bias which is not learnt from data, but encoded during the design process. We illustrate this problem on the example of Conversational Assistants, such as Amazon's Alexa, Apple's Siri, Microsoft's Cortana, or Google's Assistant, which are predominately modelled as young, submissive women. According to UNESCO, this bears the risk of reinforcing gender stereotypes.

In this proposal, we will explore this claim via psychological studies on how conversational gendering (expressed through voice, content and style) influences human behaviour in both online and offline interactions. Based on the insights gained, we will establish a principled framework for designing and developing alternative conversational personas which are less likely to perpetuate bias. A persona can be viewed as a composite of elements of identity (background facts or user profile), language behaviour, and interaction style. This framework will include state-of-the-art data-efficient NLP deep learning tools for generating dialogue responses which are consistent with a given persona. The persona parameters can be specified by non-expert users in order to to facilitate more inclusive design, as well as to enable a wider critical discussion.",,"UNESCO points out that the &quot;clock is ticking&quot; for establishing appropriate design norms for conversational assistants: One the one hand, they are new enough that the public's perception is still highly malleable. On the other hand, the adoption of this technology is growing at an unprecedented rate: According to NPR and Edison Research (2018) users are picking up smart speakers at a much faster rate than they did smartphones or tablets. And Gartner predicts that 75% of U.S. households will have smart speakers by 2020. As such, this research has potential to reach and impact millions of customers. In order to realise this impact we will:

* Work with decision and policy makers such as the Scottish Parliament and the UNESCO to ensure oversight.
* Disseminate our results to industry via conference talks, industry-focused events and invited visits.
* Educate a future workforce and investigate how to attract a more diverse workforce into the sector in collaboration with existing training programmes, such as &quot;Data Education in Schools&quot; and &quot;Equate Scotland&quot;.
* Engage the public via outreach activities and by facilitating participatory design workshops.
* Closely work with the BBC on a showcase demonstrator."
23,7CB4A13F-B764-4106-8A03-19CF79D394A1,Twenty20Insight,"As digital technology permeates every area of modern life, we risk becoming over-dependent on complex systems that operate in an opaque way, creating a risk that they exhibit emergent properties that adversely affect their users or their wider environment. This is particularly true as developers increasingly rely on AI or ML techniques as a means to define system behaviour when the problem space is too complex or poorly understood for human developers to explicitly specify that behaviour. We are tackling incompletely understood problems by developing systems whose behaviour and wider impact are by necessity also incompletely understood. This trend, which is largely enabled by an abundance of data harvested from (e.g.) mobile devices, sensors and social media, is radically changing how systems are developed and how they are used. We need a new approach to software engineering that 

(i) places greater emphasis on making explicit the risks of unintended behaviour for innovative new software products either through limitations on our understanding of the envisioned product's behaviour or through misuse, and 

(ii) actively supports explainability of the exposed behaviour by the running system.

Twenty20Insight is an interdisciplinary project bringing together academic experts in Software Engineering (SE), RE, Design Thinking and ML to help system stakeholders and developers understand and reason about the impact of intelligent systems on the world in which they operate.",,"We expect the project to have a major impact on five classes of beneficiaries beyond the academic research community:

Industrial partners. Twenty20Insight will have an early impact on our industrial partners, with whom we will work to advance the state-of-the-art in stakeholder engagement for the co-design of intelligent systems, and techniques for reasoning about uncertainty and making ML systems' decision-making more transparent.


Early career researchers. The project will provide the attached PDRAs with advanced training in research methods, publication and career development, which will eventually develop the PDRAs into independent researchers by the end of the project. 

Research community. Twenty20Insight will develop fundamental new knowledge about engineering intelligent systems both within and at the boundaries between the design, RE/SE, AI and ML research communities. 

Industries revolutionised by AI. AI/ML can be applied to nearly every industrial sector. Designing the increasingly complex AI systems requires stakeholders to understand both the Horizon of Possibilities and Envelope of Acceptability when applying ML to their problems. This project has a great potential to tackle such a problem and thus will significantly impact all the industries revolutionised by AI. 

Software development companies. The new tools and techniques that will be developed in this project will have the potential to enhance software development companies to meet the increasing demand for intelligent systems operating in an uncertain world. 

Wider society. The principal beneficiary of initiatives that aim to generate transformative ideas in order to advance technology, such as the Twenty20Insight project, is ultimately the wider society. This is particularly true of Twenty20Insight since one our explicit aims is to discover techniques to better understand systems' societal impact and enable stakeholders to handle such knowledge in a systematic and transparent way."
24,4AD30D19-6A33-414E-A6E0-1EFEB4360EFE,"Learning, Approximating and Minimising Streaming Automata for Large-scale Optimisation","The proposed research lies at the interface of the areas of verification and machine learning, interactions of which are attracting a lot of attention currently and of potential huge benefits for both sides.

Verification is this domain of computer science aiming at checking and certifying computer systems. Computer systems are increasingly used at all levels of society and peoples' lives and it is paramount to verify that they behave the way they are designed to and that we expect (examples of crucial importance, among many others, are embedded software for planes auto-pilot or self-driving cars). Unfortunately, the verification of complex systems encounters limits: there is no universal fully automated way to verify every system and one needs to find a good trade-off between the constraints of time, memory space and accuracy, which are often difficult to overcome.

Machine learning has been studied since the 50's and regained much attention recently with breakthroughs in speech recognition, image processing or game playing. The development of neural networks (studied since the 60's) awarded Hinton, LeCun, and Bengio the Turing award 2019 and using deep learning, the British firm DeepMind developed its successful AlphaGo and AlphaGo Zero which were impressive steps forward and reaffirmed the amazing potential of machine learning. 

This project proposes to apply learning techniques in verification to improve the efficiency of some algorithms which certify computer systems and to compute fast accurate models for real-life systems.

Automata are one of the mathematical tools used in verification to model computer or real-life systems. Giving certifications on these systems often boils down to running some algorithms on the corresponding automata. The efficiency of such algorithms usually depends on the size of the considered automaton. Minimising automata is thus a paramount problem in verification, as a way to verify large computer or real-life systems faster.
This proposal aims at studying the minimisation of some streaming models of quantitative automata using machine learning techniques. The kind of automata we are going to focus on, are streaming models, in the sense that the input is not stored but received as a stream of data and dealt with on the fly, thus being particularly suitable for the treatment of big data. They are also suited to deal with optimisation problems such as minimising the resource consumption of a system or computing the worst-case running time of a program, for example.

Minimising these kind of automata is highly challenging and linked with the long-standing open problem of the determinisation of max-plus automata. This proposal gives several directions of research, such as using learning methods to tackle it.",,"The proposed research is at the boundary of verification and machine learning. The study of the interactions between these two fields is attracting a lot of attention currently as they are potentially hugely beneficial for each other:
- the development of machine learning and the amazing results obtained recently in image processing, speech recognition and game-playing, give reasons to think that learning techniques can be used to speed-up verification processes in order to certify large scale complex computer or real-life systems fast. This project will have immediate impact in this direction. 
- it is commonly accepted now that there is an urgent need to develop models able to verify artificial intelligence systems and make them trustworthy. By carrying out her project, the PI will gain expertise in machine learning with the aim of developing a future project in verification of artificial intelligence systems.

A first immediate impact of the project will be academic, as developed in the Academic Impact section in the Case for Support. A longer-term impact will be in applying this research in the two directions mentioned above. The proposed research will open new directions of research and long-term projects. The collaborations proposed by the PI with experts in Machine Learning who have links with industrial partners, as well as the environment she is working in at City, in particular her already established collaborations with some members of the Centre for Software Reliability, provide an avenue to explore to increase the impact of the proposed research. 

The Pathways to Impact document details how the impact of the project will be enhanced via dissemination of the results, collaborations with other institutions and organisation of a workshop."
0,3143C3AF-E3B3-4338-9F75-6005E3C63C0C,Twenty20Insight,"As digital technology permeates every area of modern life, we risk becoming over-dependent on complex systems that operate in an opaque way, creating a risk that they exhibit emergent properties that adversely affect their users or their wider environment. This is particularly true as developers increasingly rely on AI or ML techniques as a means to define system behaviour when the problem space is too complex or poorly understood for human developers to explicitly specify that behaviour. We are tackling incompletely understood problems by developing systems whose behaviour and wider impact are by necessity also incompletely understood. This trend, which is largely enabled by an abundance of data harvested from (e.g.) mobile devices, sensors and social media, is radically changing how systems are developed and how they are used. We need a new approach to software engineering that 

(i) places greater emphasis on making explicit the risks of unintended behaviour for innovative new software products either through limitations on our understanding of the envisioned product's behaviour or through misuse, and 

(ii) actively supports explainability of the exposed behaviour by the running system.

Twenty20Insight is an interdisciplinary project bringing together academic experts in Software Engineering (SE), RE, Design Thinking and ML to help system stakeholders and developers understand and reason about the impact of intelligent systems on the world in which they operate.",,
1,E2E5C789-BFB9-4349-BFFD-B3341295F7AC,DECIDE - Delivering Enhanced Biodiversity Information with Adaptive Citizen Science and Intelligent Digital Engagements,"Biodiversity is under increasing pressure, with consequent impacts on the benefits people gain from nature. This means that it is vital to include biodiversity in our decision-making and for this we need high quality, fine-resolution, spatial biodiversity information. With this information we can better value nature, and this can be done formally through a process called 'natural capital' assessment, such as by government agencies or local economic partnerships. We also need this information to develop better plans for protecting nature, undertaking ecological restoration to develop resilient ecological networks, and make good decisions about infrastructure development (to achieve net biodiversity gain, as is the ambition in Defra's 25 Year Environment Plan). Much of our existing biodiversity information comes from volunteer-collected species records (a process often called 'citizen science'). However, in many cases, people record where and when they want - leading to large spatial unevenness in recording, both at a national scale and at a local scale. The people and organisations who need to use biodiversity information don't simply require more records: they require better information. This requires us to construct good biodiversity models generated from the available data, communicate these models well, and preferentially target effort to add records from times and places that optimally improve the model outputs. This project seeks to achieve all of this by addressing three important questions. Firstly, can we enhance existing biodiversity information through near real-time, fine resolution, species distribution models? Secondly, can we make biodiversity information more accessible and useful to end users through data flows and automated data communication? Thirdly, can we encourage adaptive sampling behaviour in recorders, by using intelligent digital engagements, so that they re-deploy a portion of their effort to optimally improve biodiversity models? Our team is expertly placed to address these questions because we are a multidisciplinary team (environmental, computer, social and data scientists), and we will use a service design approach that actively engages data users (from national to local levels) and biodiversity recorders alongside the research team. In this project we will produce fine-resolution distribution models for about 1000 insect species across the UK (in this study focusing on butterflies, moths and grasshoppers) using earth observation sensor data, and a data lab (an online analysis platform) to automatically update outputs as new data are available. It is important to communicate these results and their uncertainty so, in collaboration, with data end users we will develop interactive and automatically-generated visualisations and text to do this effectively. We will also develop ways of assessing when and where new data will be most valuable in improving the model outputs. This, when combined with constraints (such as land access or people's recording preferences) will be communicated to recorders as bespoke recommendations via a web app. This will be developed for recording butterflies and grasshoppers (a sunny day activity), and recording moths (supported by our provision of portable, low cost light traps). We will engage recorders through established recording projects across the UK, including with partners in London (many people, but relatively few biodiversity data) and North and East Yorkshire (fewer people, and a wide variety of land uses). Throughout this project our work flows will be implemented in an data lab, so they will be flexible for use with any species and indeed could be adapted for any environmental data. The outcome of this project will be a process for enhancing biodiversity information that can be incorporated into existing recording projects and data streams, so that the outputs will be accessible and useful, for the benefit of nature and people.",,"Our project seeks to deliver improved biodiversity information for end users. At this stage of delivery the outputs will be fine-resolution species distribution models for c. 1000 species of insect. The information will be comprehensive because it will be based on model outputs, and will be fine-resolution (expected to be 100m resolution). Model outputs include uncertainty, so we will invest in automated data communication to ensure they are communicated effectively. 

Data users in government agencies, local authorities, conservation NGOs and business-relevant organisations (such as Local Enterprise Partnerships, utility companies and major land owners) are a primary group of stakeholders that will benefit from our project. We will engage with representatives from these organisations through our co-design process to ensure that our outputs meet their needs. Currently, data users have information on species records at the 1-10km resolution, which is useful for large-scale strategic needs. Data users can also commission site-based surveys to meet specific operational needs. However, there are a range of needs for data that need to be met through comprehensive fine-scale biodiversity information, as will be delivered through this project. Natural capital assessments are being developed at local to regional scales (as well as nationally), and planning policies (in England, but with equivalents in other countries) need to deliver net biodiversity gain and coherent ecological networks. Our delivery of accessible, high quality information supports the vital inclusion of biodiversity in these plans. 

Some of the local users (e.g. from local economic partnerships or local councils) will not be as familiar with biodiversity model outputs as users in national government agencies, so communication of results and their uncertainty needs to be effective (through visualization and text) and, with models being updated in near real time, the communication needs to be automated. We will work with data users through our co-design process to ensure that these needs are met. We will also make our outputs available via a data lab, to enable easy access to outputs, without users requiring a large data infrastructure. In our project, we have chosen to work in two target regions with project partners to focus our engagement with local data users. Our project partners already have good networks in these regions and we will link to local economic partnerships, councils, agencies and businesses who have need for biodiversity data. 

A second group of stakeholders who will benefit are citizen science recorders. Many tens of thousands of people voluntarily provide biological records each year in the UK providing millions of pounds worth of contributions. Our development of Adaptive Citizen Science through Intelligent Digital Engagements will enable their volunteer resource to used more effectively and intentionally, and by providing better feedback, aligned with their motivations, will support an even more engaged citizen science base, better able to support the increasing demands for high quality biodiversity information. 

Our approach of using adaptive sampling to provide bespoke recommendations to citizen science recorders in order to target their effort to the times and places that will be most informative is novel and has only been considered in a couple of other projects in the world. This will be the first large-scale test of this approach and so our increased understanding of the motivations of citizen science recorders, combined with evaluation of their response to recommendations from adaptive sampling will benefit others designing and running citizen science projects in environmental science and beyond. In our team, we represent high profile citizen science (iSpot, iRecord and the Biological Records Centre), so we are well-placed to implement the learning and tools from this project, ensuring a good legacy of this investment."
2,26CF08D3-C46F-49F0-A283-E6FF277E0A97,CiViL: Common-sense- and Visually-enhanced natural Language generation,"One of the most compelling problems in Artificial Intelligence is to create computational agents capable of interacting in real-world environments using natural language. Computational agents such as robots can offer multiple benefits to society, for instance, they can be used to look after the ageing population, act as companions, can be used for skills training or even provide assistance in public spaces. These are extremely challenging tasks due to their complex interdisciplinary nature, which spans across several fields including Natural Language Generation, engineering, computer vision, and robotics. 

Communication through language is the most vital and natural way of interaction. Humans are able to effectively communicate with each other using natural language, utilising common-sense knowledge and by making inferences about other people's backgrounds based on previous interactions with them. At the same time, they can successfully describe their surroundings, even when encountering unknown entities and object. For decades, researchers have tried to recreate the way humans communicate through natural language and although there are major breakthroughs during recent years (such as Apple's Siri or Amazon's Alexa), Natural Language Generation systems still lack the ability to reason, exploit common-sense knowledge, and utilise multi-modal information from a variety of sources such as knowledge bases, images, and videos. 

This project aims to develop a framework for common-sense- and visually- enhanced Natural Language Generation that can enable natural real-time communication between humans and artificial agents such as robots to enable effective collaboration between humans and robots. Human-Robot Interaction poses additional challenges to Natural Language Generation due to uncertainty derived from the dynamic environments and the non-deterministic fashion of interaction. For instance, the viewpoint of a situated robot will change when the robot moves and hence its representation of the world, which will result in failure of current state-of-art methods, which are not able to adapt to changing environments. The project aims to investigate methods for linking various modalities, taking into account their dynamic nature. To achieve natural, efficient and intuitive communication capabilities, agents will also need to acquire human-like abilities in synthesising knowledge and expression. The conditions under which external knowledge bases (such as Wikipedia) can be used to enhance natural language generation still have to be explored as well as whether existing knowledge bases are useful for language generation. 

The novel ways to integrate multi-modal data for language generation will lead to more robust and efficient interactions and will have an impact on natural language generation, social robotics, computer vision, and related fields. This might, in turn, spawn entirely novel applications, such as explaining exact procedures for e-health treatments and enhance tutoring systems for educational purposes.",,"Autonomous and intelligent systems are becoming prevalent. The International Federation of Robotics reports that in 2017, Europe increased their sales for personal/domestic robots by 25% to about 8.5 million units (value ~US$2.1bn) [1]. They are projecting a growth of 30-35% per year until 2020 for household robotics, which will be responsible for a variety of tasks ranging from repetitive tasks, such as household maintenance, to looking after the ageing population, assisting people with disabilities as well as education and entertainment. The Office of National Statistics reports that the UK's population is getting older with almost one-fifth of the population aged 65 and over in 2016. Additionally, according to the UK Government data, 22% of UK citizens reported a disability in 2016/17, ranging from mobility disabilities to mental health and vision impairments [2]. These challenges open up a plethora of opportunities for care and assistive robots, which are able to effectively communicate with humans of all ages in an intuitive and effective manner. The most intuitive mode of communication between robots and humans is through natural language. The interaction normally takes place in a situated environment, e.g. at home or at work, where the need for recognising and understanding the surroundings is important as well as being able to associate common-sense knowledge to make further inferences.

In addition to the international academic community, other stakeholders will benefit from this research: 

- The results of this research will have a long-term influence on new applications with the aim to improve health, well-being, and quality of life as well as enable equal opportunities for education and health for all citizens. For instance, novel health applications will be able to improve people's mental health by offering support and hence reduce the financial burden of health services. Innovative education applications will offer everyone the opportunity to learn, retrain, or upscale skills, for instance, robots used for training by showing and explaining how to perform a task, and provide feedback and guidance by being able to recognise how humans interact with objects. 

- The standardisation of natural language generation technologies will provide evidence which will inform public policies at national and international level. 

- Social robots will help businesses reduce costs for training, especially in cases where training can be associated with high costs and when precision is detrimental. In addition, by enhancing knowledge and understanding of the underlying technologies, innovative industrial applications will be realised which in turn will create opportunities for high-skilled roles and offer opportunities for foreign investments. 


References:

[1] The International Federation of Robotics. WR 2018 Service Robots Executive Summary_revised (accessed June 2019). https://ifr.org/downloads/press2018/Executive_Summary_WR_Service_Robots_2018.pdf
[2] Department for Work &amp; Pensions. The Family Resources Survey (accessed June 2019). https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/692771/family-resources-survey-2016-17.pdf"
3,0D10700E-CABA-44C6-AB70-3A521781E65F,DECIDE - Delivering Enhanced Biodiversity Information with Adaptive Citizen Science and Intelligent Digital Engagements,"Biodiversity is under increasing pressure, with consequent impacts on the benefits people gain from nature. This means that it is vital to include biodiversity in our decision-making and for this we need high quality, fine-resolution, spatial biodiversity information. With this information we can better value nature, and this can be done formally through a process called 'natural capital' assessment, such as by government agencies or local economic partnerships. We also need this information to develop better plans for protecting nature, undertaking ecological restoration to develop resilient ecological networks, and make good decisions about infrastructure development (to achieve net biodiversity gain, as is the ambition in Defra's 25 Year Environment Plan). Much of our existing biodiversity information comes from volunteer-collected species records (a process often called 'citizen science'). However, in many cases, people record where and when they want - leading to large spatial unevenness in recording, both at a national scale and at a local scale. The people and organisations who need to use biodiversity information don't simply require more records: they require better information. This requires us to construct good biodiversity models generated from the available data, communicate these models well, and preferentially target effort to add records from times and places that optimally improve the model outputs. This project seeks to achieve all of this by addressing three important questions. Firstly, can we enhance existing biodiversity information through near real-time, fine resolution, species distribution models? Secondly, can we make biodiversity information more accessible and useful to end users through data flows and automated data communication? Thirdly, can we encourage adaptive sampling behaviour in recorders, by using intelligent digital engagements, so that they re-deploy a portion of their effort to optimally improve biodiversity models? Our team is expertly placed to address these questions because we are a multidisciplinary team (environmental, computer, social and data scientists), and we will use a service design approach that actively engages data users (from national to local levels) and biodiversity recorders alongside the research team. In this project we will produce fine-resolution distribution models for about 1000 insect species across the UK (in this study focusing on butterflies, moths and grasshoppers) using earth observation sensor data, and a data lab (an online analysis platform) to automatically update outputs as new data are available. It is important to communicate these results and their uncertainty so, in collaboration, with data end users we will develop interactive and automatically-generated visualisations and text to do this effectively. We will also develop ways of assessing when and where new data will be most valuable in improving the model outputs. This, when combined with constraints (such as land access or people's recording preferences) will be communicated to recorders as bespoke recommendations via a web app. This will be developed for recording butterflies and grasshoppers (a sunny day activity), and recording moths (supported by our provision of portable, low cost light traps). We will engage recorders through established recording projects across the UK, including with partners in London (many people, but relatively few biodiversity data) and North and East Yorkshire (fewer people, and a wide variety of land uses). Throughout this project our work flows will be implemented in an data lab, so they will be flexible for use with any species and indeed could be adapted for any environmental data. The outcome of this project will be a process for enhancing biodiversity information that can be incorporated into existing recording projects and data streams, so that the outputs will be accessible and useful, for the benefit of nature and people.",,"Our project seeks to deliver improved biodiversity information for end users. At this stage of delivery the outputs will be fine-resolution species distribution models for c. 1000 species of insect. The information will be comprehensive because it will be based on model outputs, and will be fine-resolution (expected to be 100m resolution). Model outputs include uncertainty, so we will invest in automated data communication to ensure they are communicated effectively. 

Data users in government agencies, local authorities, conservation NGOs and business-relevant organisations (such as Local Enterprise Partnerships, utility companies and major land owners) are a primary group of stakeholders that will benefit from our project. We will engage with representatives from these organisations through our co-design process to ensure that our outputs meet their needs. Currently, data users have information on species records at the 1-10km resolution, which is useful for large-scale strategic needs. Data users can also commission site-based surveys to meet specific operational needs. However, there are a range of needs for data that need to be met through comprehensive fine-scale biodiversity information, as will be delivered through this project. Natural capital assessments are being developed at local to regional scales (as well as nationally), and planning policies (in England, but with equivalents in other countries) need to deliver net biodiversity gain and coherent ecological networks. Our delivery of accessible, high quality information supports the vital inclusion of biodiversity in these plans. 

Some of the local users (e.g. from local economic partnerships or local councils) will not be as familiar with biodiversity model outputs as users in national government agencies, so communication of results and their uncertainty needs to be effective (through visualization and text) and, with models being updated in near real time, the communication needs to be automated. We will work with data users through our co-design process to ensure that these needs are met. We will also make our outputs available via a data lab, to enable easy access to outputs, without users requiring a large data infrastructure. In our project, we have chosen to work in two target regions with project partners to focus our engagement with local data users. Our project partners already have good networks in these regions and we will link to local economic partnerships, councils, agencies and businesses who have need for biodiversity data. 

A second group of stakeholders who will benefit are citizen science recorders. Many tens of thousands of people voluntarily provide biological records each year in the UK providing millions of pounds worth of contributions. Our development of Adaptive Citizen Science through Intelligent Digital Engagements will enable their volunteer resource to used more effectively and intentionally, and by providing better feedback, aligned with their motivations, will support an even more engaged citizen science base, better able to support the increasing demands for high quality biodiversity information. 

Our approach of using adaptive sampling to provide bespoke recommendations to citizen science recorders in order to target their effort to the times and places that will be most informative is novel and has only been considered in a couple of other projects in the world. This will be the first large-scale test of this approach and so our increased understanding of the motivations of citizen science recorders, combined with evaluation of their response to recommendations from adaptive sampling will benefit others designing and running citizen science projects in environmental science and beyond. In our team, we represent high profile citizen science (iSpot, iRecord and the Biological Records Centre), so we are well-placed to implement the learning and tools from this project, ensuring a good legacy of this investment."
4,E63CDFA3-D8BC-44BE-B0D2-EF9BC510C2C9,From data and theory to computational models of more effective virtual human gestures,"Metaphors play a central role in how people think about and convey abstract ideas. In fact, 'conveying an idea' is a form of metaphor, as if an idea is a physical object that can be physically transferred. We speak of getting ideas across to someone, of giving someone an idea, of possessing ideas (having) or grasping an idea. Research has extensively documented a wide array of metaphor use in our language and more fundamentally in our thought processes

Such metaphors are also reflected in our gesturing. Specifically, metaphoric gestures provide a physical manifestation for these metaphors. Speakers mimic holding an imaginary object to suggest an idea. The importance of the idea may be indicated by the size of this imaginary object. Speakers also use the physical space to structure their discourse. For example, contrasting differences between ideas, this versus that, may be conveyed by gestures that refer to the ideas as physical objects located in distinct locations, thus differences in location imply differences in the ideas. Particular locations in space can also derive meaning from the underlying metaphor, For example, a common metaphor for time is that events in time are points in space along a line so that, in some cultures, events located on the left are understood as being in the past while events on the right are in the future. 

Gestures are known to influence understanding and information recall: listeners are more likely to learn and remember an instruction when it is accompanied by gesture (Cook &amp; Goldin-Meadow, 2006). They also impact interpersonal attitudes of listeners towards speakers, by influencing inferences about personal and social features (Goldin-Meadow &amp; Alibali, 2013). In particular, speakers who use metaphoric gestures are judged more persuasive and competent than those who do not (Maricchiolo et al. 2009). By associating visual features to abstract concepts and laying them out in space, metaphors and metaphoric gestures enable the use of spatial reasoning mechanisms on abstract concepts and facilitate comprehension (Beaudoin- Ryan &amp; Goldin-Meadow, 2014; Calbris, 2011; Kendon, 2004; McNeill, 2005). 

The focus of this research is the computational modeling of the relation between metaphor and gesture. Beyond fostering a deeper understanding of the relation between abstract concepts, metaphor and physical gesture, the main goal of this work is to improve Human-Computer Interaction (HCI). HCI research has explored how we can make the machine more human-like. Specifically, virtual humans, autonomous agents with anthropomorphic features and behaviors engage users in face-to-face interactions, using the same verbal and nonverbal behaviors as humans. Virtual human technology has proven beneficial in a wide range of applications, including virtual patients that teach doctors how to break bad news to patients, tutors that motivate students to learn, assistants to the elders and persuasive health interventions seeking behavioral change.

This proposal is a multidisciplinary effort to computationally model metaphoric gestures and systematically evaluate their effect in human-virtual human interaction. We also see this work as having a significant impact on the design of social robots that can interact with people.",,"The impact of this work in part stems from the importance of gestures in human social interaction and that human social interaction is increasingly between human and facsimiles of people. Graphics based virtual humans and social robots with anthropomorphic features and behaviors engage users using the same verbal and nonverbal behaviors that people use when interacting with each other. These characters are used to help patients adhere to healthy behavior, provide companions for the elderly, assess a user's mental states such as depression, teach job interview skills to adults with autism spectrum disorder, teach doctors better bed side manner and train cross-cultural negotiation skills in the military. Virtual characters are also being deployed in web and messaging technologies to engage users and enliven the interaction. These technologies exploit the fact that nonverbal behaviors of participants powerfully influences face-to-face interaction. 

The key goal of the research is to fundamentally improve the nonverbal capabilities of these artificial social agents by providing the capacity for meaningful gesturing grounded in the pervasive role of metaphor in human thought, language and social interaction. This will fundamental enhance the effectiveness of these social agents in the wide range of applications that use these persuasive technologies. The proposed research will enhance knowledge of the relation between metaphors and gestures, develop models of this relation, improve technologies for realizing gestures in aritificial social agents and empirical evaluate these technologies in user studies.

Our impact strategy is twofold. First is to ensure the resulting models and technologies are promulgated, specifically to inform and facilitate the work of researchers and developers of artificial social agents. Second is to provide a deeper understanding of how metaphors relate to gestures and how the resulting gestures influence people. This understanding is both a basic research question as well as being necessary for effective application of the technology. It is the expected outcome of the proposed empirical work on a) how metaphors, including multiple metaphors, map into gestures and sequences of gestures and b) whether these gestures, when realized in an artificial agent, are understood by, and influence, people. 

Impact tools
Publications: The results of this work will be published in technology conferences and journals such as Autonomous Agents and Multi-Agent Systems, Intelligent Virtual Agents and Human Robot Interaction. Because the data collected/analyzed, the resulting models and empirical studies will also be relevant to psychologists and linguists, we will publish these results in appropriate venues to reach psychologists and linguists, such as the journal Gesture.
Website: Here, the data collected, the models and software for driving gesturing with be available for download. It will be available separately as well as part of the virtual human toolkit, an open source toolkit for realizing virtual humans. On the website, visitors will be able to vividly experience a virtual human, who can process dialog input from users and illustrate the gesturing.
Training: This work will also be integrated into the University of Glasgow's UKRI Centre for Doctoral Training in Socially Intelligent Artificial Agents. Specifically the work will be used in classwork and research projects, educating the next generation of researchers in technology, helping to disseminate the technology in their papers and future employment.

Public Dissemination strategy
We will also increase the public's awareness through public engagement events and news media. We will reference our website in all articles and in press releases about our research."
5,641C8634-0DAB-4FA4-ACBB-354CF8D2FDE8,"REPHRAIN: Research centre on Privacy, Harm Reduction and Adversarial Influence online","The REsearch centre on Privacy, Harm Reduction and Adversarial INfluence online (REPHRAIN) will bring together the UK's substantial academic, industry, policy and third sector capabilities to address the current tensions and imbalances between the substantial benefits to be gained by full participation in the digital economy and the potential for harm through loss of privacy, insecurity, disinformation and a myriad of other online harms. 

Combining world-leading experts from the Universities of Bristol, Edinburgh, Bath, King's and UCL, the REPHRAIN Centre will use an interdisciplinary approach - alongside principles of responsible innovation and creative engagement - to develop new insights that allow the socio-economic benefits of a digital economy to be maximised whilst minimising the online harms that emerge from this. 

REPHRAIN's leadership team will drive these insights in technical, social, behavioural, policy and regulatory research on privacy, privacy enhancing technologies and online harms, through an initial scoping phase and 25 inaugural projects. 

The work of REPHRAIN will be focused around three core missions and four engagement and impact objectives. Mission 1 emphasises the requirement to deliver privacy at scale whilst mitigating its misuse to inflict harms. This will focus on reconciling the tension between data privacy and lawful expectations of transparency by not only drawing heavily on advances in privacy-enhancing technologies (PETs), but also leveraging the full range of socio-technical approaches to rethink how we can best address potential trade-offs. Mission 2 emphasises the need to minimise harms whilst maximising the benefits from a sharing-driven digital economy, redressing citizens' rights in transactions in the data-driven economic model by transforming the narrative from privacy as confidentiality only to also include agency, control, transparency and ethical and social values. Finally, Mission 3 focuses on addressing the balance between individual agency and social good, developing a rigorous understanding of what privacy represents for different sectors and groups in society (including those hard to reach), the different online harms to which they may be exposed, and the cultural and societal nuances impacting effectiveness of harm-reduction approaches in practice.

These missions are supported by four engagement and impact objectives that represent core pillars of REPHRAIN's approach: (1) design and engagement; (2) adoption and adoptability; (3) responsible, inclusive and ethical innovation; and (4) policy and regulation. Combined, these objectives will deliver co-production, co-creation and impact at scale across academia, industry, policy and the third sector.

These activities will be complemented by a capability fund, which will ensure that REPHRAIN activities remain flexible and responsive to current issues, addressing emerging capability gaps, maximising impact and cultivating a public space for collaboration. REPHRAIN will be managed by a Strategic Board and supported by an External Advisory Group, the REPHRAIN Ethics Board, and will work with multiple external stakeholders across industry, public, and the third sector. 

Outcomes from the centre will be synthesised into the REPHRAIN Toolbox - a one-stop resource for researchers, practitioners, policy-makers, regulators and citizens - which will contribute to developing a culture of continuous learning, collaboration and open engagement and reflection within the area of online harm reduction. 

Overall, REPHRAIN focuses on interdisciplinary leadership provided by a highly experienced team and supported by state-of-the-art facilities, to develop and apply scientific expertise to ensure that the benefits of a digital society can be enjoyed safely and securely by all.",,
6,499139C2-AB31-411C-A347-44C8B25713A8,DECIDE - Delivering Enhanced Biodiversity Information with Adaptive Citizen Science and Intelligent Digital Engagements,"Biodiversity is under increasing pressure, with consequent impacts on the benefits people gain from nature. This means that it is vital to include biodiversity in our decision-making and for this we need high quality, fine-resolution, spatial biodiversity information. With this information we can better value nature, and this can be done formally through a process called 'natural capital' assessment, such as by government agencies or local economic partnerships. We also need this information to develop better plans for protecting nature, undertaking ecological restoration to develop resilient ecological networks, and make good decisions about infrastructure development (to achieve net biodiversity gain, as is the ambition in Defra's 25 Year Environment Plan). Much of our existing biodiversity information comes from volunteer-collected species records (a process often called 'citizen science'). However, in many cases, people record where and when they want - leading to large spatial unevenness in recording, both at a national scale and at a local scale. The people and organisations who need to use biodiversity information don't simply require more records: they require better information. This requires us to construct good biodiversity models generated from the available data, communicate these models well, and preferentially target effort to add records from times and places that optimally improve the model outputs. This project seeks to achieve all of this by addressing three important questions. Firstly, can we enhance existing biodiversity information through near real-time, fine resolution, species distribution models? Secondly, can we make biodiversity information more accessible and useful to end users through data flows and automated data communication? Thirdly, can we encourage adaptive sampling behaviour in recorders, by using intelligent digital engagements, so that they re-deploy a portion of their effort to optimally improve biodiversity models? Our team is expertly placed to address these questions because we are a multidisciplinary team (environmental, computer, social and data scientists), and we will use a service design approach that actively engages data users (from national to local levels) and biodiversity recorders alongside the research team. In this project we will produce fine-resolution distribution models for about 1000 insect species across the UK (in this study focusing on butterflies, moths and grasshoppers) using earth observation sensor data, and a data lab (an online analysis platform) to automatically update outputs as new data are available. It is important to communicate these results and their uncertainty so, in collaboration, with data end users we will develop interactive and automatically-generated visualisations and text to do this effectively. We will also develop ways of assessing when and where new data will be most valuable in improving the model outputs. This, when combined with constraints (such as land access or people's recording preferences) will be communicated to recorders as bespoke recommendations via a web app. This will be developed for recording butterflies and grasshoppers (a sunny day activity), and recording moths (supported by our provision of portable, low cost light traps). We will engage recorders through established recording projects across the UK, including with partners in London (many people, but relatively few biodiversity data) and North and East Yorkshire (fewer people, and a wide variety of land uses). Throughout this project our work flows will be implemented in an data lab, so they will be flexible for use with any species and indeed could be adapted for any environmental data. The outcome of this project will be a process for enhancing biodiversity information that can be incorporated into existing recording projects and data streams, so that the outputs will be accessible and useful, for the benefit of nature and people.",,"Our project seeks to deliver improved biodiversity information for end users. At this stage of delivery the outputs will be fine-resolution species distribution models for c. 1000 species of insect. The information will be comprehensive because it will be based on model outputs, and will be fine-resolution (expected to be 100m resolution). Model outputs include uncertainty, so we will invest in automated data communication to ensure they are communicated effectively. 

Data users in government agencies, local authorities, conservation NGOs and business-relevant organisations (such as Local Enterprise Partnerships, utility companies and major land owners) are a primary group of stakeholders that will benefit from our project. We will engage with representatives from these organisations through our co-design process to ensure that our outputs meet their needs. Currently, data users have information on species records at the 1-10km resolution, which is useful for large-scale strategic needs. Data users can also commission site-based surveys to meet specific operational needs. However, there are a range of needs for data that need to be met through comprehensive fine-scale biodiversity information, as will be delivered through this project. Natural capital assessments are being developed at local to regional scales (as well as nationally), and planning policies (in England, but with equivalents in other countries) need to deliver net biodiversity gain and coherent ecological networks. Our delivery of accessible, high quality information supports the vital inclusion of biodiversity in these plans. 

Some of the local users (e.g. from local economic partnerships or local councils) will not be as familiar with biodiversity model outputs as users in national government agencies, so communication of results and their uncertainty needs to be effective (through visualization and text) and, with models being updated in near real time, the communication needs to be automated. We will work with data users through our co-design process to ensure that these needs are met. We will also make our outputs available via a data lab, to enable easy access to outputs, without users requiring a large data infrastructure. In our project, we have chosen to work in two target regions with project partners to focus our engagement with local data users. Our project partners already have good networks in these regions and we will link to local economic partnerships, councils, agencies and businesses who have need for biodiversity data. 

A second group of stakeholders who will benefit are citizen science recorders. Many tens of thousands of people voluntarily provide biological records each year in the UK providing millions of pounds worth of contributions. Our development of Adaptive Citizen Science through Intelligent Digital Engagements will enable their volunteer resource to used more effectively and intentionally, and by providing better feedback, aligned with their motivations, will support an even more engaged citizen science base, better able to support the increasing demands for high quality biodiversity information. 

Our approach of using adaptive sampling to provide bespoke recommendations to citizen science recorders in order to target their effort to the times and places that will be most informative is novel and has only been considered in a couple of other projects in the world. This will be the first large-scale test of this approach and so our increased understanding of the motivations of citizen science recorders, combined with evaluation of their response to recommendations from adaptive sampling will benefit others designing and running citizen science projects in environmental science and beyond. In our team, we represent high profile citizen science (iSpot, iRecord and the Biological Records Centre), so we are well-placed to implement the learning and tools from this project, ensuring a good legacy of this investment."
7,65289789-D790-41E2-8095-8A8B63AEAC7B,SENSUM: Smart SENSing of landscapes Undergoing hazardous hydrogeological Movement,"Floods and landslides affect the UK every year, both inland and along the coast, causing disruption, occasional fatalities and severe economic loss. An increase in storminess under climate change and population pressure are resulting in an increase in landslide and flood hazards in the UK and globally and threatening the defences put in place to manage these hazards. Monitoring of unstable hillslopes and flood-prone rivers as well as defences designed to manage these is increasingly vital. Landslides and floods are both triggered by heavy rainfall, often occur at the same time, and may interact to generate a chain reaction of knock-on hazardous effects. SENSUM proposes a new integrated way to tackle these 'hydrogeological' hazards, taking advantage of advances in Wireless Sensor Network (WSN) and Internet of Things (IoT) technologies, microelectronics and machine learning. Those exciting new tools will be used to monitor the stability of defences, provide warnings of hazard events, and improve mathematical models and visualisation of hazardous phenomena. 

Landslides and floods have traditionally been monitored using a combination of satellite-based remote-sensing techniques and wired ground-based instruments to measure factors that control the related hazard, such as river flow level, displacement and soil moisture. Wireless sensor networks (WSNs) show great potential for monitoring and early warning of these hazards. Their main advantage is their use of easily deployable, low-power sensors enabling continuous, long-term, low-cost monitoring of the environment. For landslides and floods, which occur infrequently and unpredictably, this is an important technological advance. SENSUM proposes to develop innovative smart tracking devices, embedded in boulders and woody debris on hillslopes and in rivers to give real-time warning of movement related to landslide and flood processes. Collaborating closely with external partners, the team of experts in the SENSUM project will develop and test the tracking devices both in dedicated laboratory experiments and in the field, with the deployment of trial networks of smart boulders and woody debris in different localities in the UK and abroad. The large set of data obtained from sites and experiments will be used to improve mathematical models, to develop innovative early warning systems and in 3D digital visualisations. This integrated approach will enable us to establish a comprehensive understanding of landslide and flood processes which will significantly reduce risk to society.

The SENSUM team is a diverse, interdisciplinary and multinational team made up of a range of environmental scientists and engineers, computer scientists and science communication specialists from three leading UK universities: University of Exeter, University of East Anglia and University of Plymouth and will involve several project partners including the Environment Agency, Forest England, Natural England and AECOM. It will work closely with these project partners to design an effective digital environment for monitoring and managing landslide and flood hazards in the UK, and to target applied risk management challenges. For example, in the UK, the Environment Agency is tasked with giving a 2-hour warning to the population affected by floods. However, these warnings are lacking in the upland areas of the UK's landscape due to a lack of instruments to monitor river flow. The smart tracking devices embedded within boulder and woody debris in landslides and river channels proposed by SENSUM will help address that limitation, and therefore will significantly improve early warning of movement and consequently the assessment of potential high-risk natural events. The team will also engage stakeholders and the general public through the creation of compelling visualizations of landslide and flood hazards and through project workshops and outreach activities.",,"SENSUM will develop and demonstrate several new technologies for landslide and hazard assessment and management in the UK and globally. It will also collect novel and valuable datasets on dynamics of landslides and floods that will enhance understanding and models of landslides and floods. Through close collaboration with stakeholders at workshops and field visits, SENSUM will ensure the positive economic and societal impacts and legacy of new technologies and process understanding during and beyond the duration of the project.
 
The smart tracking devices that SENSUM will develop will be a low cost device (at &pound;200 for first prototypes) that will be useful for both academics and industry for tracking movement and monitoring hazards with WSNs and IoT technology. SENSUM will build on a proof of concept study by members of the team to demonstrate the full potential of this sensor technology for detecting and tracking landslide and flood hazards. The technology will provide a low cost solution to enhance the monitoring of these hazards by businesses such as AECOM with wider economic and societal benefits. The tracking devices will also enable the SENSUM team to collect unprecedented datasets and understanding of landslide and flood processes. These results will also be shared with stakeholders to further enhance management practice.
 
The smart tracking devices will also be useful for monitoring stability and movement of wood in rivers, informing Natural Flood Risk Management (NFM) practice in the UK. In particular tagging of large wood and boulders used to create 'leaky dams' will fill a big gap in understanding of the engineering performance of these structures. Discussions on this work have already begun between the SENSUM team and the Environment Agency (EA). Outcomes from this research could inform the development of industry guidance inputting directly to the development of the CIRIA-led NFM Design Guide that the EA is involved in and will help target maintenance of these structures by the EA and land managers. More accurate characterisation of the rheology and of the dynamics of the flows produced by SENSUM will be used as input parameters in numerical models to improve their predictive capability of landslides and debris transport in floods. This will help with decision making, for example regarding installation of mitigation measures and leaky dams to minimize risk to critical infrastructure and affected area.
 
SENSUM will be a pioneer in demonstrating the potential of machine learning in combination with WSN technology for the early warning of hydrogeological hazards. It will develop a practically applicable system to provide early warning of floods and landslides and help to mitigate their impacts on communities and infrastructure. The early warning system underpinned by IoT networking technologies will be co-designed and shared with stakeholders and PPs at SENSUM project workshops. We will also design a web interface for visualizing data from wireless sensor networks for the benefit of academics studying landslide-flood dynamics and to help stakeholders to visualize and monitor movement e.g. around critical infrastructure. The SENSUM team will work to integrate sensor technology, the web interface and early warning system to provide a complete monitoring system to revolutionize landslide and flood management.
 
Multiple stakeholders will benefit from the 3D visualizations of hazardous landslide and flood events that will be produced by SENSUM based on novel datasets, improved model simulations and images collected through citizen science. These will be projected in the unique Immersive Vision Theatre (IVT) and widely shared with an inflatable IVT via outreach events in the UK. This dynamic digital hazard visualization will be a powerful tool for facilitating exchanges between scientists and at-risk populations and ultimately helping to increase preparedness and resilience to landslide and flood risks."
8,A62D3D4F-D3F6-4043-8A19-9DD66BA32442,"Phantom trust: Faith, language, and digital inequalities in Southwest Kenya","Artificial Intelligence (AI) is in the ascendant the world over. This is especially the case when it comes to machine learning and big data, which are said to offer a technical fix to human questions of trust. Such rhetoric obscures its embeddedness in a specific socio-cultural context, while downplaying the extent to which trust is an ethical and political issue rather than a strictly technical one. But most social sciences, unlike mathematics and computing, have had little to say about the trust that such technologies are said and designed to foster. My proposed fellowship marks a step towards addressing this imbalance, through research activities as well as by building an interdisciplinary network of social scientists seeking to develop publicly engaged scholarship on AI. 

More broadly, the fellowship will help consolidate my doctoral research on trust, by bringing it to bear on recent developments in AI, as well as by integrating some additional research into my existing material, with a view of developing a monograph within the next two years. In my thesis, I argued that - in Kenya as elsewhere - popular narratives about trust link up with the historical reproduction and transformation of social and economic inequalities. Having situated local narratives of trust and faith in a history of missionary and colonial projects, I now wish to consolidate my historical analysis with additional archival data and to weave in fresh ethnographic data on digital infrastructures and their consequences for spiritual and political-economic concerns in the everyday lives of ordinary Kenyans. For example, I am especially interested in how global narratives on artificial intelligence and social trust are impacting the social lives of the microfinance groups I worked with in my doctoral research. If, as argued in the thesis, microcredit borrowers draw on a theologically-diverse language of faith to negotiate the terms of trust and cooperation, it is less clear whether this language of faith can also account for how people respond to and make sense of the data infrastructures that increasingly profile them through machine-learning algorithms and credit-scores. Thus, the fellowship asks: 

1. How do algorithmic metrics of creditworthiness and trustworthiness affect formal and informal credit arrangements in rural Southwest Kenya?
2. How do people encounter AI algorithms in a context where trust is said to be elusive, declining, fraught by inequality, and often expressed in the language of religious faith?
3. How do religious commitments influence notions of trustworthiness and relations of trust in social, political, and economic life? 

The fellowship offers answers to these questions in three ways. First, I will revise a review article, co-authored with Professor Laura Bear, following its presentation at an interdisciplinary workshop on AI before the beginning of the fellowship. This will initiate disciplinary and interdisciplinary conversations on AI, while developing the review for publication will help me situate my theoretical approach to trust as a discursive or linguistic phenomenon in the wider literature on trust in the social sciences. The review should also offer me a chance to develop an ethnographic methodology which meets the exigencies of studying digital spaces and interfaces. At the same time, I will be completing an ethnographic journal article on the role of religion in Kenyan political campaigns. Second, for a quarter of the fellowship's duration, I will be conducting additional archival and ethnographic fieldwork to supplement my doctoral materials. Third, I will be integrating the fresh data into my previously collected research data by way of developing a book proposal, revising two thesis chapters as a monograph chapters, and disseminating my research findings to specialist and non-specialist audiences alike.",,
9,08240E2A-1872-4CE9-94AF-532F5E1847F7,DECIDE: Delivering Enhanced Biodiversity Information with Adaptive Citizen Science and Intelligent Digital Engagements,"Biodiversity is under increasing pressure, with consequent impacts on the benefits people gain from nature. This means that it is vital to include biodiversity in our decision-making and for this we need high quality, fine-resolution, spatial biodiversity information. With this information we can better value nature, and this can be done formally through a process called 'natural capital' assessment, such as by government agencies or local economic partnerships. We also need this information to develop better plans for protecting nature, undertaking ecological restoration to develop resilient ecological networks, and make good decisions about infrastructure development (to achieve net biodiversity gain, as is the ambition in Defra's 25 Year Environment Plan). Much of our existing biodiversity information comes from volunteer-collected species records (a process often called 'citizen science'). However, in many cases, people record where and when they want - leading to large spatial unevenness in recording, both at a national scale and at a local scale. The people and organisations who need to use biodiversity information don't simply require more records: they require better information. This requires us to construct good biodiversity models generated from the available data, communicate these models well, and preferentially target effort to add records from times and places that optimally improve the model outputs. This project seeks to achieve all of this by addressing three important questions. Firstly, can we enhance existing biodiversity information through near real-time, fine resolution, species distribution models? Secondly, can we make biodiversity information more accessible and useful to end users through data flows and automated data communication? Thirdly, can we encourage adaptive sampling behaviour in recorders, by using intelligent digital engagements, so that they re-deploy a portion of their effort to optimally improve biodiversity models? Our team is expertly placed to address these questions because we are a multidisciplinary team (environmental, computer, social and data scientists), and we will use a service design approach that actively engages data users (from national to local levels) and biodiversity recorders alongside the research team. In this project we will produce fine-resolution distribution models for about 1000 insect species across the UK (in this study focusing on butterflies, moths and grasshoppers) using earth observation sensor data, and a data lab (an online analysis platform) to automatically update outputs as new data are available. It is important to communicate these results and their uncertainty so, in collaboration, with data end users we will develop interactive and automatically-generated visualisations and text to do this effectively. We will also develop ways of assessing when and where new data will be most valuable in improving the model outputs. This, when combined with constraints (such as land access or people's recording preferences) will be communicated to recorders as bespoke recommendations via a web app. This will be developed for recording butterflies and grasshoppers (a sunny day activity), and recording moths (supported by our provision of portable, low cost light traps). We will engage recorders through established recording projects across the UK, including with partners in London (many people, but relatively few biodiversity data) and North and East Yorkshire (fewer people, and a wide variety of land uses). Throughout this project our work flows will be implemented in an data lab, so they will be flexible for use with any species and indeed could be adapted for any environmental data. The outcome of this project will be a process for enhancing biodiversity information that can be incorporated into existing recording projects and data streams, so that the outputs will be accessible and useful, for the benefit of nature and people.",,"Our project seeks to deliver improved biodiversity information for end users. At this stage of delivery the outputs will be fine-resolution species distribution models for c. 1000 species of insect. The information will be comprehensive because it will be based on model outputs, and will be fine-resolution (expected to be 100m resolution). Model outputs include uncertainty, so we will invest in automated data communication to ensure they are communicated effectively. 

Data users in government agencies, local authorities, conservation NGOs and business-relevant organisations (such as Local Enterprise Partnerships, utility companies and major land owners) are a primary group of stakeholders that will benefit from our project. We will engage with representatives from these organisations through our co-design process to ensure that our outputs meet their needs. Currently, data users have information on species records at the 1-10km resolution, which is useful for large-scale strategic needs. Data users can also commission site-based surveys to meet specific operational needs. However, there are a range of needs for data that need to be met through comprehensive fine-scale biodiversity information, as will be delivered through this project. Natural capital assessments are being developed at local to regional scales (as well as nationally), and planning policies (in England, but with equivalents in other countries) need to deliver net biodiversity gain and coherent ecological networks. Our delivery of accessible, high quality information supports the vital inclusion of biodiversity in these plans. 

Some of the local users (e.g. from local economic partnerships or local councils) will not be as familiar with biodiversity model outputs as users in national government agencies, so communication of results and their uncertainty needs to be effective (through visualization and text) and, with models being updated in near real time, the communication needs to be automated. We will work with data users through our co-design process to ensure that these needs are met. We will also make our outputs available via a data lab, to enable easy access to outputs, without users requiring a large data infrastructure. In our project, we have chosen to work in two target regions with project partners to focus our engagement with local data users. Our project partners already have good networks in these regions and we will link to local economic partnerships, councils, agencies and businesses who have need for biodiversity data. 

A second group of stakeholders who will benefit are citizen science recorders. Many tens of thousands of people voluntarily provide biological records each year in the UK providing millions of pounds worth of contributions. Our development of Adaptive Citizen Science through Intelligent Digital Engagements will enable their volunteer resource to used more effectively and intentionally, and by providing better feedback, aligned with their motivations, will support an even more engaged citizen science base, better able to support the increasing demands for high quality biodiversity information. 

Our approach of using adaptive sampling to provide bespoke recommendations to citizen science recorders in order to target their effort to the times and places that will be most informative is novel and has only been considered in a couple of other projects in the world. This will be the first large-scale test of this approach and so our increased understanding of the motivations of citizen science recorders, combined with evaluation of their response to recommendations from adaptive sampling will benefit others designing and running citizen science projects in environmental science and beyond. In our team, we represent high profile citizen science (iSpot, iRecord and the Biological Records Centre), so we are well-placed to implement the learning and tools from this project, ensuring a good legacy of of this investment."
10,4BD4C60D-80FE-4696-86B2-08FF7BCC663C,Learn From The Best: training AI using biological expert attention,"Artificial intelligence (AI) is having a massive impact on many disciplines, including biological science. Its power is impressive and its adoption will change the nature of research, but at the moment the way it is developed has severe practical limitations. Despite the recent developments in machine learning and AI, humans still possess an unrivalled ability to just look at a picture, and understand exactly what is going on. A human expert is able to look at a picture of a plant with disease symptoms, for example, and immediately quantify the severity of the infection. AI promises to revolutionise bioimage analysis, but as of today an expert human will outperform an AI given only a small set of images to learn from.

One important difference between humans and modern AI is the way we are taught to perform a task. A human will learn which parts of an image are important, then scan the images to find these areas before coming to reach a scoring decision. AI is typically trained using labeled data, where only the output label matters. An AI does not know which parts of the image are important, or where it should look. This often leads to poor performance when the task is challenging, or when only small datasets are available. To achieve the impressive results as has been documented in the news, current AI must use very laborious and inefficient training processes, which are often impractical in a real world scientific setting.

This project will develop a new, smarter way to train artificial intelligence methods, using similar mechanisms to how human experts make decisions. To do this, our AI will study how human experts approach the same problems by using gaze tracking to see where an expert looked, and when. The result will be AI methods that learn to look in the right places, and so are able to take more difficult scoring decisions with less training data than they would previously need. Put simply, we believe that an AI that is able to look in the correct places before making a decision will be more effective than one that attempts to simply make a decision without knowing where to look.

In this project we will first develop the hardware and software approaches necessary to capture expert human gaze during image scoring. This raw gaze information will be processed using novel algorithms, and fed into a new deep learning AI system along with the labelled scores, guiding it towards more informed decision making. The AI will examine where in the images human experts looked when providing an image label, and will learn to look in those same places when it replicates the same task. This is a new approach to training AI. Finally, we will build a new type of deep neural network AI system that can be guided by this additional information, knowing where to look, and what to do.

We will demonstrate this work on important datasets of plant disease, but we also believe this approach will massively reduce the time required to annotate datasets across all fields of life and biomedical science, and at the same time produce even more impressive and accurate AI results. This could represent a step-change in the adoption and ease of use of AI tools in the world of bioscience, allowing for more efficient training on smaller image datasets.","Human experts can become very good at making scoring decisions on images. High throughput phenotyping experiments demand trained experts make decisions quantifying properties in images, for example how diseased a specimen is, or deciding if a sample is showing a particular response. Deep machine learning allows for computers to make similar such decisions; but requires a lengthy and precise image annotation process in order to train.

One key difference between a human expert and a trained deep network is the human's ability to identify the important locations within an image first, and only then combine these into a score or prediction. AI is typically trained in a supervised manner, a series of example images and output labels are provided in the hope the AI generates a reliable mechanism of mapping from image to score. This is often not the case; when no explanation is provided during the training process for what parts of an image are important (as during semantic segmentation), the accuracy of the trained AI is limited.

What if we could guide a deep network to first identify the important parts of an image, as a human expert does? This project will do this by combining traditional annotated labelling of images with expert eye tracking information captured cost-free at the same time. Human gaze can now be captured accurately and in real time using low cost eye tracking hardware. This gaze information will be used to train the network to identify where the expert was looking when scoring. These regions will be used to make more informed, more accurate decisions. We will demonstrate the improvement attentional training can provide on three varied datasets featuring plant disease, but the approach will remain general, applicable to any scoring task within the biosciences, and further afield in areas such as medical diagnosis.","The project aims to deliver three main outputs:
1. Develop a new method to collect and combine eye tracking data with deep learning to produce more efficient deep networks
2. Demonstrate the approach on three plant phenotyping datasets examining disease severity
3. Publish and disseminate this method via the use of our novel software and cheap, commercially available eye tracking hardware, including to a wider audience outside plant science

Who will benefit from these outputs, and how?
Research scientists
As detailed in the Academic Beneficiaries, both bioscientists, computer scientists and scientists from further afield will benefit from the resource in a wide variety of ways. We anticipate equal benefit from industry and academic research alike. Specifically, the new approach will enable faster, cheaper and potentially more accurate AI development for a wide variety of phenotyping and diagnostic experiments.

Industry
Any industrial application that involves deep learning across image datasets with text labels will potentially benefit from this approach, both in and outside of bioscience. For example, commercial, high throughput phenotyping systems will directly benefit from the reduced training requirements of the approach. Phenotyping as part of breeding programs now has the potential to become fully high throughput and easier to retrain across varying research questions and domains.

Biomedical Imaging
As a team we are excited about expanding the approach to cover biomedical image datasets, and specifically the task of clinical diagnosis via machine learning. Again providing an easier mechanism to train deep learning will be of great assistance in a research area where AI is already making a sizable impact. Combined with the explainable aspect of the networks, it will allow for software which can show where an AI looked in an image when making a decision.

Public
The public will benefit through numerous outreach activities throughout the project. Eye tracking will be demonstrated via an interactive demo where we can show first hand the power of seeing where someone is looking when examining an image. This will allow us to discuss both the technology involved (eye tracking) the AI concepts in development (deep learning) and the underlying science (plant phenotyping as it relates to global food security)

Training of PDRAs
The project promises to deliver excellent impact through training of the project team and the public. The biology PDRA will gain experience working in a multi-disciplinary environment, and working knowledge of modern approaches to machine learning and AI. These are extremely valuable skills. The CS PDRA will gain experience developing software tools for a wide audience, with specific and different needs to the computer science research community. They will also likely gain new skills working with eye tracking hardware.

The project will provide code and software allowing people to gain experience in the collection of attentional data, and training of models. A workshop alongside the project will explore its use beyond the plant science exemplar datasets used here. Additional online training will be provided in the form of a tutorial video, in a similar format to those prepared previously by the investigators in the project team."
11,D100D162-B06A-4ED9-B88D-0065724FA04A,Membrane-Cyber-Physical System (m-CPS) for Smart Water Treatment,"Filter membranes play a critical role in providing clean drinking water, access to which is one of the most pivotal human rights. Typically, the operation of the filters has relied on manual, local monitoring of operational markers such as flow rates and contaminants' concentrations. This need for hands on expert maintenance is preventing membrane technology from reaching its full potential. To correct this, the monitoring of water filter needs to be achieved by sensors, transmitting data in real-time for centralised artificial intelligence (AI) based analysis. Such an AI driven water filter system must be scalable to meet with the global demands for clean water. There is therefore a massive global opportunity for membrane systems to benefit from being implemented as cyber-physical systems (CPS). 

This discipline hopping grant (DHG) will provide the PI and discipline hopper Das with an immersive information and communication technology (ICT) experience. It will enable him to bring the ICT capabilities and use of smart wireless-sensor technologies for autonomous, real-time monitoring, together with AI driven data analytics within the broader area of CPS into his home discipline relating to membrane water treatment. This will be achieved by supporting/mentoring the PI at 50% FTE for 2 years to experience ways for developing a membrane-CPS (m-CPS) based on intelligent CPS architecture, embedded with a smart wireless sensor network (WSN) for continuous real-time monitoring of the performance of a membrane-treatment unit enhanced by cloud-based AI data analytics and decision making.",,"It is envisaged that the impacts of this DHG will be significant and long lasting, which would be achieved via six pathways to impact (PWI). Below is a discussion on who might benefit from the DHG, and how.

PWI1 - Career development:
During the DHG, Das will work with ICT experts (CIs and academic project partners) and possible end users of m-CPS (Anax technologies and Severn Trent Water) in order to align common thinking, disseminate results, and learn contemporary ICT skills and practices. Once completed, the ICT skills obtained in the DHG will also be transferrable to Das' other interests on real-time and remote soil quality monitoring. Thus, in the short to medium term, the DHG will increase Das' portfolio of research. This would improve the UK future research base, particularly, ICT based water engineering approaches.

PWI2 - ICT skills:
One important impact of the DHG will be that Das will acquire new ICT skills and knowledge (e.g., CPS, WSN design, cloud-based data analytics), which will help answer future research questions in his home discipline. These will provide a pathway for effective remote monitoring of water quality in the membrane-based system and deployment of such systems in remote areas. Therefore, the DHG provides to the UK academics an excellent pathway for long-term impacts via application-specific research.

PWI3 - Knowledge transfer:
By providing an immersive ICT environment to Das, the DHG will enable him to gain knowledge, experience and practical skills on ICT (e.g., CPS design, configuration of the physical WSN layer, developing relevant AI algorithms for the cyber layer). At the same time, the DHG will lead to transfer of vital knowledge acquired and generated during the DHG from the non-ICT to ICT as Das will share his knowledge and expertise from water engineering with the CPS, WSN and data analytic experts (CIs and academic project partners). During the course of the DHG, Das will also engage with industrial partners (Anax Technologies and Severn Trent Water), find further opportunities for knowledge transfer and apply the new ICT knowledge in the future (e.g., via KTP project) in water engineering fields via their specific needs. 

PWI4 - Wider societal and academic impacts:
In the long run, the DHG will have major societal and academic impacts via enterprise projects, and funded by the UKRI, GCRF and/or the Newton Fund with the project partners from both the UK and abroad. This is because the DHG will address one of the most fundamental needs of the people, i.e., providing clean drinking water and monitoring of the water quality in the treatment system. 

PWI5 - Students' and academic colleagues' knowledge:
Beyond the DHG period, the project will allow Das to be involved in joint supervisions to enhance the learning of post-doctoral, doctoral, masters and undergraduate researchers' projects relating to ICT-based water treatment. These activities are expected to lead to journal publications and/or students' reports in fulfilling the requirements of their studies which would be a real impact of the DHG as Das would enhance knowledge in a different discipline (ICT). Furthermore, Das will be able to contribute to the development of new ICT based chemical engineering module for teaching in his parent department. 

PWI6 - Wider knowledge generation and dissemination:
As m-CPS is still a new concept, it is expected that the developed methodology and obtained results will be published in leading journal which are outside Das' normal publication routes. Das will attend a conference in a field which he would not normally connect. He will also organise a project closure workshop. These will provide important pathways for engagements with the ICT community and design of additional future impact-oriented activities. His engagements with the project partners will also provide the opportunities to explore further routes for knowledge generation and dissemination and, impact generation."
12,24C6F14C-D8AD-4C4E-B92C-246A894598A7,"Exaggeration, cohesion, and fragmentation in on-line forums","On-line forums can support the formation of social communities with shared interests and needs. They can also have a negative side if groups of users support each other in divisive attitudes or false beliefs. The social fragmentation resulting from these so-called echo-chamber effects has been identified as an engine behind the rise of violence and extremism, political gridlock, and decreases in social mobility. This project is motivated by the observation that echo-chamber effects involve a gradual shift from more moderate language to more extreme language. Further, damage repair is difficult when extreme social fragmentation has already occurred. The ability to use patterns in on-line language for early detection of on-line social fragmentation would thus be a major breakthrough in supporting earlier, and more effective, intervention against harmful trends in on-line forums.

We have identified two major challenges in creating this capability. First, current NLP methods are poor at understanding expressions whose meaning is a degree on a scale, such as a scale defined on the dimensions of cost, quality, honesty, or performance. For example, &quot;rather racist&quot;, &quot;really racist&quot;, and &quot;incredibly racist&quot; express different degrees of disapproval, but such differences are not adequately captured by current algorithms. This limitation is central to our problem, because echo-chamber effects often involve incremental exaggerations of factual claims, emotions, or attitudes. The second challenge results from the fact that methods for using linguistic content in the analysis of social behaviour are limited. While much research has uncovered systematic associations between word choices and social groups, very little has addressed relationships between linguistic inferences and social trends. However, tracking the gradual shifts towards semantic extremes in echo-chamber effects requires making certain linguistic inferences. This is because inferring which underlying dimension of meaning is relevant in any specific case critically depends on information about who is talking and what they are talking about. For example, &quot;Liverpool is far better&quot; might to relate a scale of cultural excellence in a discussion amongst music fans, but to a scale of costs amongst people who are discussing housing. A fundamental advance in the methodology for combining linguistic and social information is thus needed to characterise echo-chamber effects on-line and make predictions about risks of future fragmentation. 

The project is a new collaboration between an experimental and computational linguist (the PI) and an expert in machine learning and social network analysis (the Co-I). Its components integrate the expertise of both collaborators. Advanced text-mining and data analytics will be used to generate the materials for a large-scale and experimentally normed data set of scalar expressions, using archives of the popular on-line forum Reddit. No normed data set of this type exists, and it will provide the training and test materials needed to develop and evaluate new algorithms. Using a modular work plan, the project team will first develop and validate separate algorithms to assess and predict the meanings of scalar expressions, and the level of fragmentation in the social network of Reddit users. These components will then be integrated using advanced graph-based machine learning methods. The primary outcome of the project will be a software package that will facilitate the work of on-line moderators by flagging subReddits or threads that display early stages of echo-chamber effects. The normed data set will also be extremely valuable for improving NLP applications that require nontrivial semantic inference, such as sentiment analysis, chatbots, and question-answering systems. More generally, the project is a demonstration project for advanced methodology in processing linguistic meaning in relation to social relationships and human behaviour.",,"The impact of the project will be realised in the following ways:

Academic impact: 
If successful, the project will play a key role in the development of novel techniques for semantic inference, an important and open research area in statistical NLP where current methods are largely insufficient. The normed data set and software implementation developed in the project will serve as standard benchmark for further studies in NLP, and contribute to the promotion of reproducible research and open science. The project will further benefit research on text and document analysis, language modelling, and computational linguistics.

Beyond NLP and text-related analysis, the project contributes to a number of emerging research areas. One the one hand, the handling of heterogeneous and network-structured data from on-line discussion forums enables new areas of application for state-of-the-art data analysis methods, e.g., graph signal processing and geometric deep learning. On the other hand, the utilisation of linguistic content in the analysis of social behaviour is an excellent example of and opens new possibilities in the new fields of computational social science and social data science.

Industry impact:
Challenges in statistical NLP are pervasive in today's commercial systems. Reddit, Facebook, and Google all face similar challenges in monitoring and analysing on-line materials, and will benefit from the advanced capabilities in this domain that our project will provide. The research on semantic inference will benefit commercial dialogue systems and chatbots developed by corporations such as Apple and Amazon, while the consequent improvements in sentiment analysis of text may prove valuable for marketing or recommender systems developed at Amazon and Netflix.

Societal impact:
Segregation and inequality are widespread phenomena that challenges societies across the globe. The project would lead to better understanding of social fragmentation and the echo-chamber effects, in particular in the on-line setting in the digital age, which may help mitigate negative economic outcomes such as lower social mobility and higher inequality. Timely prediction of trends of social fragmentation also provides opportunities for early intervention to break potentially harmful echo chambers, and promote social cohesion both on-line and off-line.

Competitiveness of UK research:
The project focuses on the interplay between statistical NLP and machine learning/artificial intelligence, two priority areas recognised by the EPSRC and UKRI. Furthermore, the combination of ideas in NLP, machine learning, network science, and social sciences is perfectly aligned with the UK`s emphasis on cross-disciplinary research. The project further strengthens these areas for which the UK is already well-known internationally. Through the EPSRC CDTs and Oxford post-graduate programmes that the PI and Co-I are involved in, it will help to ensure that the UK continues playing a leading role in these key areas through the training of next-generation researchers and policy-makers."
13,B89B19F5-E18E-4BA7-999B-44821A991730,Next Stage Digital Economy Centre in the Decentralised Digital Economy (DECaDE),"Data-driven innovation is transforming every sector of our digital economy (DE) into a de-centralised marketplace; accommodation (AirBnb), transportation (Uber), logistics (Deliveroo), user-generated vs. broadcast content in the creative industries (YouTube). We are witnessing an inexorable shift from classical models centred upon monolithic institutions, to a dynamic and decentralised economy in which anyone is a potential producer and consumer. A gig economy, underpinned by digital products and services co-created through shorter-lived, diverse peer-to-peer engagements.

Yet, the platforms that enable this DE are increasingly built on centralised architectures. These are not controlled by society, but by large organisations making commercial decisions far from the social contexts they affect. There is an urgent need to disrupt this relationship, to deliver proper governance that empowers society to take control of the DE and enables people to assert greater agency over the vast centralised silos of data that drive these platforms. 

We stand on the cusp of a second wave of DE disruption, driven by bleeding edge data-driven technologies (AI) and secure, distributed data sharing infrastructures such as Distributed Ledger Technologies (DLT), in which data is no longer siloed but becomes a fluid, de-centralised commodity shifting power away from tech giants to individuals and de-centralised organisations. This future Decentralised Digital Economy (DDE) enables people and organisations to work together, to trade, and ultimately to trust via frictionless digital interactions free from reliance upon centralised third parties, but often with reliance upon autonomous services. 

This shift in agency and power is a game changing opportunity for society to take back control over its digital economy - but we have a limited window of opportunity to get it right. We have already witnessed de-centralisation in the financial sector, where the lack of regulation and clear governance of crypto-currencies has proven a double-edged sword, allowing free exchange of value across the globe, but that is coupled with fraudulent company flotations and currency rates rigged by large mining pools. This is a consequence of technology-driven innovation unchecked by socio-economic insight; a lack of knowledge making policy makers impotent in the face of the tech giants. We are now at the tipping point of similar wide-sweeping disruption across all sectors in the DDE, a transformation that will radically redefine our models of value and how it is created, the ways in which we work, and how we use and extract value from our data.

DECaDE represents a critical and timely opportunity to shape this emerging de-centralised digital economy (DDE), to develop insights that define a new 21st century model of work and value creation in the DDE, and ensure a prosperous, safe and inclusive society for all. 

DECaDE is a 60 month centre, comprising 21 people and building upon over 8.6 million pounds of feasibility scale UKRI/EPSRC investments in DLT and Human Data Interaction (HDI) held by the proposing team. DECaDE is a three-way partnership between the Universities of Surrey and Edinburgh, and the Digital Catapult DLT Field Labs. The latter is a full member of the consortium, through which we have co-created this research programme and with whom we will engage in further co-creation of the future DDE through diverse end-users in the public and private sector to support the competitive position of the UK",,"DECaDE will enhance the competitive position of the UK, working with 34 industry and government end-user partners (match investment 4.1M of total match 6.8M) to deliver a prosperous, safe and inclusive digital economy (DE). DECaDE will deliver long-term horizontal impact across all sectors within the DE, as it transitions to a fully decentralised form in which data becomes a fluid, de-centralised commodity, shifting governance back to society and generating radical new opportunities for working and co-creating value. The Digital Catapult (DC) Field Labs are front and centre of DECaDE as the primary mechanism for co-creating this impact, engaging current (and future) end-users to co-create and translate research within a 'living lab'. 

TH1 delivers vertical impact across the key creative industries sectors covering content production: TV broadcast via the BBC, Sony; music via Blokur; video streaming via Youtube. Outcomes include new business models and ways to co-create value from creative content via improved IP attribution, facilitating re-use of content through smart contract enabled micropayments and novel models of content ownership (Audience Strategies, Bristows). DECaDE will develop new ways to protect content though digital provenance, working with key actors in different media forms (Adobe: image/video, Dimension and Insurent: 3D assets/4D Video, Coinmode: virtual assets in video games, BBC/National Archives: cultural heritage). A use case in news integrity will be run with BBC News, leveraging Synthesia video rewrite technology to produce test footage. Exploitation will be driven through licensing technology, delivering growth in audience and revenue.

TH2. Working with digital identity platform providers and (TRUU, Consult Hyperion) and NCSC/GCHQ we will map the capabilities and governance structures of self-sovereign ID schemes that utilise DLT and develop policy recommendations around the interplay between UK legislation and self-sovereign identity. TH2 will output novel federated machine learning technology that can leverage de-centralised data to collaboratively build AI models and help users visualize (Wallscope) and extract value from data understanding the legal frameworks to enable that in decentralised form (Bristows). With DC Field Labs we will co-create a data marketplace that enables data to move with fluidity and (controlled) autonomy between creator and provider, co-creating personalised services and compensating the owner (buildingon COMEHERE). This will unlock new economic models for data that will deliver broad impact across all sectors of the DE. Our frameworks will adapt ODI's ongoing work on Data Trusts and embed RRI throughout via Data and AI Ethics workshops at DC Machine Intelligence Garage.

TH3 delivers impact for those working in future gig economy enabled by the DDE; focusing on casual or voluntary sectors (Volunteer Scotland and Oxfam, building on OXCHAIN), and professional services. We will develop technical prototype of a self-sovereign work record, developed alongside UKGov departments (DWP Scottish Government, HMRC) and the SCRIPT centre. Within the Field Labs, work with DWP will study how definitions of the value of work evolve (e.g. toward reputation, or qualification) and relates to the provenance and integrity of a self-sovereign work record. Integration of payment technology from Fintech providers (DCT, Clarion Bond, Thomson) will develop new forms of value exchange that are linked to the individuals work record, and from this we will develop policy recommendations with HMRC for digital taxation in this ecosystem, as well as legal insights into employment law (with Fieldfisher, UCU) in the DDE. 

DECaDE will deliver training and capability in DE research; including 9 PhDs and 5 PDRAs with translational opportunities toward full academic posts and for industry fellows to embed within the centre. DE researchers will undertake international exchanges with Blockchain@UBC"
14,0D94EDC8-90E6-4D57-BD2B-609EC02F5948,"Cohomology, Machine Learning and String Model Building","The proposed research capitalizes on a newly discovered class of mathematical formulae underpinning the realisation of string theory solutions that unify all known forms of matter and forces. String theory has had a profound impact on the development of both mathematics and physics and, more recently, on the construction of machine learning algorithms.

String theory is a high-energy, extra-dimensional and supersymmetric theory, in which many ideas about physics beyond the Standard Model can be incorporated in a natural way. Although difficult, making contact with experimental physics is an imperative for string theory, requiring a sustained effort in developing the existing models up to the point where they can communicate with experimental results such as the LHC data. The difficulty is not conceptual, but rather mathematical and computational in nature. String theory is geometrical par excellence and, as such, one needs to identify the specific geometry that reduces it to the Standard Model of particle physics at low energies.

The project contributes in an essential way to the resolution of this problem. It uses experimental mathematics derived from string theory to uncover and understand new algebraic and geometric structures. The new structures feed back into string theory, providing unexpected shortcuts to incredibly hard computations. It is rare to find a new type of mathematical structure that has so much potential for problem solving. This interplay between mathematics and physics is characteristic to string theory and has crucially contributed to making it the principal driving force in fundamental particle physics.

Machine learning techniques have seen a wide range of applications in numerous areas of science and in industry. String theory and, more broadly, physics require a qualitatively different kind of machine learning, focused not only on results, but also on uncovering the mechanisms underlying them. The proposal goes beyond the standard 'black box' approach that gives correct results but no explanations by using machine lerning for the formulation of mathematically precise conjectures that can subsequently be approached using methods of algebraic geometry, everything converging towards the ultimate goal of understanding the physical implications of string theory.",,"The proposed research is fundamental in nature, being part of the quest for an ultimate theory of the universe and its mathematical foundation. This pursuit is highly regarded by the general public, having entered the collective consciousness through the impressive work of people such as Stephen Hawking. Its main virtue resides in its cultural transformative power and has significantly influenced policy and decision makers in science worldwide. 

The economic impact of the programme is less immediate. On the other hand, as historical evidence proves, investment in fundamental research and in particular in mathematical physics has brought major long-term benefits for society - the effectiveness of GPS navigation, to give just one example, being unthinkable in the absence of Einstein's theory of general relativity. The timescales for this kind of foundational new knowledge in mathematics and physics can be long, in general. 

Two more immediate aspects of the proposed research impact are easier to identify. The first is related to the use of machine learning for future economic development. The approach taken in the project for machine learning the mechanisms that give rise to certain patterns can be adapted for practical identification problems in economic trends. The second aspect is the direct training of young people in analytic and computational skills transferable to areas outside of academia, such as business, industry and policy making, requiring well-grounded analytical frameworks. 

The research programme is linked with an important range of outreach activities including talks to local schools and the public as well as popular science publications, which will bring the fruits of the work to the wider society."
15,D11FC503-B047-4BDF-A837-065454AD0071,Heriot-Watt University EPSRC Core Equipment Award 2020,"This proposal for core equipment is part of the University's strategy to enhance its long-term competitiveness in addressing two intertwined Engineering research challenges, namely Developing materials, processes and manufacturing for healthier living and Creating a smarter, safer world by innovation in data, signals, comms, robotics and AI. Critical underpinning technical capability to address these challenges has been identified in the following areas; advanced manufacturing, sensor development, data communication &amp; processing as well as robotics and artificial intelligence. We are requesting core equipment to support our highly-interdisciplinary world class research in these areas, which all closely align with the EPSRC portfolio.",,
16,3E05F5BD-DFBC-4C5D-BA2D-F05C12019169,Runoff - Aerial Artificial Intelligence Land Use mapping Demonstrator,"Real time risk mapping of agricultural catchments is time consuming and for the purposes of flood risk mapping (particularly after an event) is made difficult due to flooding or soil conditions. The challenge is to map the risk factors associated with flooding or agricultural diffuse pollution in a non-invasive - less labour intensive way using an airborne system with built-in intelligence . The added benefit is that the land surface is not damaged at a critical time, and that mitigation strategies can be rapidly deployed.",,
17,ECC41656-E7A0-41C5-B7C5-235847DF6D1D,UKRI Trustworthy Autonomous Systems Node in Governance and Regulation,"How can we trust autonomous computer-based systems? Autonomous means &quot;independent and having the power to make your own decisions&quot;. This proposal tackles the issue of trusting autonomous systems (AS) by building: experience of regulatory structure and practice, notions of cause, responsibility and liability, and tools to create evidence of trustworthiness into modern development practice. Modern development practice includes continuous integration and continuous delivery. These practices allow continuous gathering of operational experience, its amplification through the use of simulators, and the folding of that experience into development decisions. This, combined with notions of anticipatory regulation and incremental trust building form the basis for new practice in the development of autonomous systems where regulation, systems, and evidence of dependable behaviour co-evolve incrementally to support our trust in systems.

This proposal is in consortium with a multi-disciplinary team from Edinburgh, Heriot-Watt, Glasgow, KCL, Nottingham and Sussex, bringing together computer science and AI specialists, legal scholars, AI ethicists, as well as experts in science and technology studies and design ethnography. Together, we present a novel software engineering and governance methodology that includes: 
 1) New frameworks that help bridge gaps between legal and ethical principles (including emerging questions around privacy, fairness, accountability and transparency) and an autonomous systems design process that entails rapid iterations driven by emerging technologies (including, e.g. machine learning in-the-loop decision making systems) 
 2) New tools for an ecosystem of regulators, developers and trusted third parties to address not only functionality or correctness (the focus of many other Nodes) but also questions of how systems fail, and how one can manage evidence associated with this to facilitate better governance.
 3) Evidence base from full-cycle case studies of taking AS through regulatory processes, as experienced by our partners, to facilitate policy discussion regarding reflexive regulation practices.",,
18,23B8B4D4-A51D-4F7A-A667-A519E8329860,UKRI Trustworthy Autonomous Systems Node in Resilience,"Imagine a future where autonomous systems are widely available to improve our lives. In this future, autonomous robots unobtrusively maintain the infrastructure of our cities, and support people in living fulfilled independent lives. In this future, autonomous software reliably diagnoses disease at early stages, and dependably manages our road traffic to maximise flow and minimise environmental impact.

Before this vision becomes reality, several major limitations of current autonomous systems need to be addressed. Key among these limitations is their reduced resilience: today's autonomous systems cannot avoid, withstand, recover from, adapt, and evolve to handle the uncertainty, change, faults, failure, adversity, and other disruptions present in such applications.

Recent and forthcoming technological advances will provide autonomous systems with many of the sensors, actuators and other functional building blocks required to achieve the desired resilience levels, but this is not enough. To be resilient and trustworthy in these important applications, future autonomous systems will also need to use these building blocks effectively, so that they achieve complex technical requirements without violating our social, legal, ethical, empathy and cultural (SLEEC) rules and norms. Additionally, they will need to provide us with compelling evidence that the decisions and actions supporting their resilience satisfy both technical and SLEEC-compliance goals.

To address these challenging needs, our project will develop a comprehensive toolbox of mathematically based notations and models, SLEEC-compliant resilience-enhancing methods, and systematic approaches for developing, deploying, optimising, and assuring highly resilient autonomous systems and systems of systems. To this end, we will capture the multidisciplinary nature of the social and technical aspects of the environment in which autonomous systems operate - and of the systems themselves - via mathematical models. For that, we have a team of Computer Scientists, Engineers, Psychologists, Philosophers, Lawyers, and Mathematicians, with an extensive track record of delivering research in all areas of the project. Working with such a mathematical model, autonomous systems will determine which resilience- enhancing actions are feasible, meet technical requirements, and are compliant with the relevant SLEEC rules and norms. Like humans, our autonomous systems will be able to reduce uncertainty, and to predict, detect and respond to change, faults, failures and adversity, proactively and efficiently. Like humans, if needed, our autonomous systems will share knowledge and services with humans and other autonomous agents. Like humans, if needed, our autonomous systems will cooperate with one another and with humans, and will proactively seek assistance from experts.

Our work will deliver a step change in developing resilient autonomous systems and systems of systems. Developers will have notations and guidance to specify the socio-technical norms and rules applicable to the operational context of their autonomous systems, and techniques to design resilient autonomous systems that are trustworthy and compliant with these norms and rules. Additionally, developers will have guidance to build autonomous systems that can tolerate disruption, making the system usable in a larger set of circumstances. Finally, they will have techniques to develop resilient autonomous systems that can share information and services with peer systems and humans, and methods for providing evidence of the resilience of their systems. In such a context, autonomous systems and systems of systems will be highly resilient and trustworthy.",,
19,6D0A4054-D3D1-48AC-A5C6-F0EE87E624A3,UKRI Trustworthy Autonomous Systems Node in Functionality,"'Autonomous systems' are machines with some form of decision-making ability, which allows them to act independently from a human controller. This kind of technology is already all around us, from traction control systems in cars, to the helpful assistant in mobile phones and computers (Siri, Alexa, Cortana). Some of these systems have more autonomy than others, meaning that some are very predictable and will only react in the way they are initially set up, whereas others have more freedom and can learn and react in ways that go beyond their initial setup. This can make them more useful, but also less predictable.

Some autonomous systems have the potential to change what they do, and we call this 'evolving functionality'. This means that a system designed to do a certain task in a certain way, may 'evolve' over time to either do the same task a different way, or to do a different task. All without a human controller telling it what to do. These kinds of systems are being developed because they are potentially very useful, with a wide range of possible applications ranging from minimal down-time manufacturing through to emergency response and robotic surgery. The ability to evolve in functionality offers the potential for autonomous systems to move from conducting well defined tasks in predictable situations, to undertaking complex tasks in changing real-world environments.

However, systems that can evolve in function lead to legitimate concerns about safety, responsibility and trust. We learn to trust technology because it is reliable, and when a technology is not reliable, we discard it because it cannot be trusted to function properly. But it may be difficult to learn to trust technology whose function is changing. We might also ask important questions about how functional evolutions are monitored, tested and regulated for safety in appropriate ways. For example, just because a robot with the ability to adapt to handle different shaped objects passes safety testing in a warehouse does not mean that it will necessarily be safe if it is used to do a similar task in a surgical setting. It is also unclear who, if anyone, bears the responsibility for the outcome of functional evolution - whether positive or negative. 

This research seeks to explore and address these issues, by asking how we can, or should, place trust in autonomous systems with evolving functionality. Our approach is to use three evolving technologies - swarm systems, soft robotics and unmanned air vehicles - which operate in fundamentally different ways, to allow our findings to be used across a wide range of different application areas. We will study these systems in real time to explore both how these systems are developed and how features can be built into the design process to increase trustworthiness, termed Design-for-Trustworthiness. This will support the development of autonomous systems with the ability to adapt, evolve and improve, but with the reassurance that these systems have been developed with methods that ensure they are safe, reliable, and trustworthy.",,
20,9E1711DF-BFE6-42DC-B78F-2E720FB678E6,Centre for Digital Citizens - Next Stage Digital Economy Centre,"The Centre for Digital Citizens (CDC) will address emerging challenges of digital citizenship, taking an inclusive, participatory approach to the design and evaluation of new technologies and services that support 'smart', 'data-rich' living in urban, rural and coastal communities. Core to the Centre's work will be the incubation of sustainable 'Digital Social Innovations' (DSI) that will ensure digital technologies support diverse end-user communities and will have long-lasting social value and impact beyond the life of the Centre. Our technological innovations will be co-created between academic, industrial, public and third sector partners, with citizens supporting co-creation and delivery of research. Through these activities, CDC will incubate user-led social innovation and sustainable impact for the Digital Economy (DE), at scale, in ways that have previously been difficult to achieve.

The CDC will build on a substantial joint legacy and critical mass of DE funded research between Newcastle and Northumbria universities, developing the trajectory of work demonstrated in our highly successful Social Inclusion for the Digital Economy (SIDE) hub, our Digital Civics Centre for Doctoral Training and our Digital Economy Research Centre (DERC). The CDC is a response to recent research that has challenged simplified notions of the smart urban environment and its inhabitants, and highlighted the risks of emerging algorithmic and automated futures. The Centre will leverage our pioneering participatory design and co-creative research, our expertise in digital participatory platforms and data-driven technologies, to deliver new kinds of innovation for the DE, that empowers citizens.

The CDC will focus on four critical Citizen Challenge areas arising from our prior work: 'The Well Citizen' addresses how use of shared personal data, and publicly available large-scale data, can inform citizens' self-awareness of personal health and wellbeing, of health inequalities, and of broader environmental and community wellbeing; 'The Safe Citizen' critically examines online and offline safety, including issues around algorithmic social justice and the role of new data technologies in supporting fair, secure and equitable societies;
'The Connected Citizen' explores next-generation citizen-led digital public services, which can support and sustain civic engagement and action in communities, and engagement in wider socio-political issues through new sustainable (openly managed) digital platforms; and 'The Ageless Citizen' investigates opportunities for technology-enhanced lifelong learning and opportunities for intergenerational engagement and technologies to support growth across an entire lifecourse. CDC pilot projects will be spread across the urban, rural and costal geography of the North East of England, embedded in communities with diverse socio-economic profiles and needs.

Driving our programme to address these challenges is our 'Engaged Citizen Commissioning Framework'. This framework will support citizens' active engagement in the co-creation of research and critical inquiry. The framework will use design-led 'initiation mechanisms' (e.g. participatory design workshops, hackathons, community events, citizen labs, open innovation and co-production platform experiments) to support the co-creation of research activities. Our 'Innovation Fellows' (postdoctoral researchers) will engage in a 24-month social innovation programme within the CDC. They will pilot DSI projects as part of highly interdisciplinary, multi-stakeholder teams, including academics and end-users (e.g. Community Groups, NGO's, Charities, Government, and Industry partners). The outcome of these pilots will be the development of further collaborative bids (Research Council / Innovate UK / Charity / Industry funded), venture capital pitches, spin-outs and/or social enterprises. In this way the Centre will act as a catalyst for future innovation-focused DE activity.",,"The Centre for Digital Citizens (CDC) will collaborate with its large network of partners to provide routes to impact. The co-created nature of the research programme will ensure that it addresses the real-world needs of our partner organisations, from inception through to exploitation. Below we summarise key impacts that the Centre will have, split out by beneficiary:

1) Commercial / industry partners: Will benefit from direct tech-transfer from research projects; the development of new citizen data commons resources, which will have commercially exploitable value and support new kinds of digital service; ethical guidelines and best practice knowledge transfer for the design of socially inclusive and fair digital services; and new spin-out developments and social enterprise creation, designed to work collaboratively with industry partners, creating new business opportunities.

2) Venture capital funders: Will benefit from access to a new source of piloted, user-led and co-created digital technology platforms and services, available for financing and development.

3) Government: Will benefit from future public service delivery policy development. We will continue to inform Government agencies around best practice, addressing issues of digital citizenship, including local government service provision, civic engagement / consultation and urban planning. There will be tech transfer through Local Government adoption of digital platforms. We will work with cross-council authorities (North of Tyne Combined and NE LEP) to reconcile policy and service commissioning recommendations across authority boundaries.

4) Charities and third sector groups: Are underserved communities receiving relatively little support from the digital sector, often due to their budgetary constraints, yet are increasingly important for citizens to access services and experience citizenship. DERC and Digital Civics work has shown this sector offers a rich and vibrant economy that would benefit from greater adoption of digital platforms to help commission, deliver and coordinate services. New kinds of digital economies could thrive with new kinds of digital service developed through the CDC. These services will offer direct economic benefit such as cost savings, efficiencies and increased organisational reach, to charities and third sector groups.

5) Learning communities: Lifelong learning platforms developed by the CDC will provide opportunities for educational enrichment and digital skills development at a number of levels. Such opportunities will be provided within schools and beyond including reskilling post-formal education. This is likely to have an impact on the numbers of skilled people in work, and could provide numerous opportunities for older adult (including peri-retired and retired) populations.

6) Citizens and local communities: large numbers of citizens (estimated 1000+) will benefit from involvement in either social innovation pilots or the Citizen's Assembly. Pilots will be embedded in NE communities with unmet needs and whose sense of citizenship is impacted by health inequality, online harms, a lack of community infrastructure and connectivity, and limited lifecourse development opportunities. The CDC will also provide new data resources, and models of common data ownership and re-use. These will pilot direct economic benefits for citizens and provide new routes to activism and campaigning on local issues. Those using services piloted through the CDC will also receive the wellbeing benefits of more and better service access, and greater participatory voice in service development and provision.

7) DE Researchers: will benefit from Digital Social Innovation training. The pipeline of DE trained researchers will be increased, in academic, government, commercial and third sectors. With a broad range of skills, these researchers will directly contribute to the digital economy and will provide increasing levels of social innovation."
21,E5F14D8A-F3AB-4AE0-87E0-F1A6BED1C338,UKRI Trustworthy Autonomous Systems Node in Security,"Autonomous Systems (AS) are cyberphysical complex systems that combine artificial intelligence with multi-layer operations. Security for dynamic and networked ASs has to develop new methods to address an uncertain and shifting operational environment and usage space. As such, we have developed an ambitious program to develop fundamental secure AS research covering both the technical and social aspects of security. Our research program is coupled with internationally leading test facilities for AS and security, providing a research platform for not only this TAS node, but the whole TAS ecosystem. To enhance impact, we have built a partnership with leading AS operators in the UK and across the world, ranging from industrial designers to frontline end-users. Our long-term goal is to translate the internationally leading research into real-world AS impact via a number of impact pathways. The research will accelerate UK's position as a leader in secure AS research and promote a safer society.",,
22,DC679870-02B9-4A11-9C54-57B2E2D87FAE,CAMERA 2.0,"Intelligent Visual and Interactive Technology allows us to perceive, understand and re-create the world around us. With it we can digitise the world with 3D cameras, use Artificial Intelligence (AI) to predict and enhance the health of people within our world or to educate and train them. It allows us to experience this world, or imagined ones, through immersive technologies, movies and video games, and interact with these worlds through technologies that analyse our movement and behaviour. 

There is a clear benefit to applying this technology across domains, for specific health or education purposes, but doing so requires coordinated action and genuine democratisation of the underpinning technologies, such that non-expert users are empowered. 

To address this challenge, CAMERA 2.0 will perform world-leading research in Intelligent Visual and Interactive Technology - underpinned by academic and partner expertise across Computer Vision, Computer Graphics, Human Computer Interaction (HCI) and AI - and engage a range of partners to generate impact and translate this technology across a range of themes. 
This multi-disciplinary approach is supported by academic and external partner expertise spanning healthcare, biomechanics, sports performance and psychology. These collaborations will allow us to carry out new research, create new impacts and develop further partnerships that would otherwise be impossible to achieve.

This proposal builds on our highly successful Next Stage Digital Economy Centre for the Analysis of Motion Entertainment Research and Applications (CAMERA). Over the last 4 years, we have built a team of 14 academics and over 40 PhDs and researchers who have created real impact, alongside our partners, across themes of i) Entertainment; ii) Health, Rehabilitation and Assistive Technologies and; iii) Human Performance Enhancement.

CAMERA 2.0 will also focus on three themes, supported by over 20 impact partners: 
i) Creative Science and Technology, 
ii) Digital Health and Assistive Technology and
iii) Human Performance Enhancement. 

Furthermore, CAMERA 2.0 will work closely with our EPSRC CDT in Digital Entertainment and our new UKRI CDT in Accountable, Responsible and Transparent AI (ART-AI). 

Our research programme will deliver continuing impact through four primary mechanisms: (i) Theme Driven Impact Projects, 
(ii) Cross-Cutting Theme R&amp;D Challenges, 
(iii) Reactive Impact Projects and 
(iv) Open Community Engagement.

Theme Driven Impact Projects will be 12 to 24-month projects co-designed through sand-pits and co-delivered with partners. Although primarily aligned with a single theme they will overlap with at least one other.

Our Cross-Cutting Theme R&amp;D Challenges engage with R&amp;D challenges shared by partners/academics across themes. Translating innovations across themes not only democratises and accelerates technology adoption but can significantly enhance impact. This will be addressed through key research projects, that support and feed into all other activities.

Our reactive model allows us to carry out commercial projects as research impact vehicles at short notice - essential being able to work with the short-deadline driven creative sector. CAMERA 2.0 evolves our unique reactive impact model by placing our CAMERA student technical team at its core under the supervision of our experienced studio managers. 

Impact through Open Engagement. Our ambition is to raise the level of UK and international DE research through collaboration and technology democratisation. CAMERA 2.0 will operate an open-door model for reasonable access to facilities, data, software and training. In coordination with commitments from the University of Bath and external EU funding we are expanding our physical facilities and technical team to provide assisted motion capture and immersive technology training for free to over 100 creative industries, HEIs and healthcare companies.",,"This proposal builds on our successful Next Stage Digital Economy Centre for the Analysis of Motion Entertainment Research and Applications (CAMERA), founded in 2015 (www.camera.ac.uk). Over the last 3 years, our team of 14 academics and over 40 PhDs and researchers have created real impact with partners.

This includes commercial projects such as Raindance Festival nominated 'Is Anna OK?' with the BBC, and BAFTA nominated '11:11 Memories Retold' with Aardman and Bandai Namco (XBox, Steam and PS4). 

By translating visual technologies across themes, impacts included developing personalised prosthetic limb liners, powered ankle prostheses and myoelectric arm prostheses with the NHS, Blatchford Healthcare and Open Bionics. We have also developed markerless biomechanical measurement technology with British Skeleton and the Ministry of Defence (MoD). 

CAMERA has published over 100 academic papers across these themes, and our multi-disciplinary approach has enabled us to unlock new translational research opportunities with support from EPSRC, H2020, AHRC, MRC, Versus Arthritis, MoD and InnovateUK. This includes being core partners in the ~&pound;5m Bristol + Bath AHRC Creative Cluster, attracting &pound;1.5m to lead a new South West Digital Innovation Business Acceleration Hub and unlocking &pound;1.8m in European Commission funding to increase our studio infrastructure from 2020. 

CAMERA 2.0 will evolve to drive the needs of the modern Digital Economy (DE). Our impact work will deliver continuing impact through four primary mechanisms: (i) Theme Driven Impact Projects, (ii) Cross-Cutting Theme R&amp;D Challenges, (iii) Reactive Impact Projects and (iv) Open Community Engagement.

Each of these involve working with our partners and generating rapid, real-world impact.

Theme Impact Projects will be co-designed through sand-pits and co-delivered with partners.

Our Cross-Theme R&amp;D Challenges work to engage with the shared R&amp;D challenges of partners and academics across themes. Translating innovations across themes will accelerate technology adoption and significantly enhance impact.

Our reactive model also allows us to carry out direct commercial projects as research impact vehicles at short notice - essential to working with the short deadline driven creative sector. CAMERA 2.0 evolves our unique reactive impact model by placing our CAMERA student technical team at its core under the supervision of our experienced studio managers. 

Our central ambition is to raise the level of UK and international DE research through collaboration and technology democratisation. CAMERA 2.0 will operate an open-door model for reasonable access to facilities, data, software and training. 

We are expanding our physical facilities and technical team to provide assisted motion capture and immersive technology training for free to over 100 creative industries, HEIs and healthcare companies. 
CAMERA 2.0 will open this access even further by working with our partners and our wider network, hosting workshops to co-create projects and exchange knowledge."
23,3E938593-5C92-4A68-842C-9A6D353206FE,EPSRC Core Equipment for Aston University,"The college of engineering carries out world-leading research across 3 interdisciplinary institutes/groups:
EBRI (Energy and Bioproducts Research Institute )
AIPT (Aston Institute of Photonic Technologies)
ASTUTE (Aston Centre for Urban Technology and the Environment)

The success and impact of these institutes is critically dependent on their access to state-of-the art equipment to deliver internationally competitive research. Investment in a Microactivity Effi at EBRI will allow us to analyze and develop innovative chemcial processes that deliver much-needed industrial products with lower energy, water, carbon and resource footprints. Investment in a dual comb laser at AIPT will allow more accurate measuremetns of a variety of liquid and chemcial compounds, including atmospheric pollutants harmful to health. The procurement of a machine learning server will allow our researchers to process vastly increased quantities of data more quickly and effcieintly to deliver accurate and meaningful results in a timescale useful to industry, policy makers and other researchers.",,
24,29A8EEF8-EBA1-4BF8-8586-B4686B18245D,"CHIMERA: Collaborative Healthcare Innovation through Mathematics, EngineeRing and AI","Hospitals collect a wealth of physiological data that provide information on patient health. Full use of this data is significantly limited by its complexity and by a limited mechanistic understanding of the relationship between internal physiology and external measurement. Addressing this challenge requires multidisciplinary collaboration between mathematicians developing new biomechanical models, clinicians who measure and interpret the data to treat patients, and statistical and computational scientists to bridge the two-way translation between model output and real-life data. CHIMERA is designed to foster such collaboration to generate new understanding of physiology, new methods for relating physiology to real time data, and, finally, to translate these into practice, improving outcomes for patients by supporting clinical decision making.

CHIMERA will start by focusing on the most critically ill patients within hospital intensive care units: such patients have by far the most monitoring data and are most likely to benefit from improved understanding of what that data can tell us about their underlying physical state. Each year about 20,000 children and 300,000 adults in the UK need intensive care. These critically ill patients are continuously monitored at the bedside, including measurements of heart rate, breathing rate, blood pressure and other vital sign data. However, the wealth of these physiological data are not currently used to inform clinical decision making and clinicians can only really use real-time snapshots of the physiology to guide their decisions.

CHIMERA will address this unmet opportunity to use individual patient physiological data to support clinical decision making, with the potential to impact on patient management across the UK and beyond. This will be achieved through a multidisciplinary Hub which brings together experts in mathematics, statistics, data science and machine learning, with unique, high volume and rich data sets from both adult and paediatric Intensive Care Units provided through embedded Project Partnerships with Great Ormond Street Hospital (GOSH) and University College London Hospital (UCLH). CHIMERA will deliver new mathematical frameworks to learn the biophysical relationships that govern the interdependencies between physiological variables, based on data sets for thousands of patients through these project partners. Clinical impact will be achieved through an extensive series of clinically-led, multidisciplinary workshops themed around specific opportunities to improve care, for example identifying deteriorating patients in advance of an adverse event such as heart attack or stroke, or advance warning systems to diagnose sepsis. These workshops will include partnering with the Alan Turing Institute (the national centre for AI and Data Science), will be open to national participation, and will provide a mechanism to fund new projects by making available seed corn funding, PhD studentships and researcher resource for new interdisciplinary teams and partnerships. CHIMERA will build new links with clinical centres, companies and academic units across the UK and internationally, expand to work with a variety of patient monitoring data, and provide dedicated support to nurture new projects, funding bids and collaborations. In this way, we will build CHIMERA to a self-sustaining, multidisciplinary and vibrant Centre for the application of mathematical and data sciences tools in patient care.",,"CLINICAL AND ECONOMIC IMPACT

Intensive care units (ICUs) treat the country's most critically ill patients. Intensive care is characterised by high resource use (both in equipment and staff), uncertainty in outcome and length of stay; and high levels of stress for patients and their families. Patients are intensively monitored, with almost all patients receiving continuous bedside monitoring for their heart rate, blood pressure, temperature, and how Oxygen and Carbon Dioxide are being used by the body. Such vital signs provide doctors and nurses crucial information about how well the patient's body is recovering from critical illness, particularly given patients are usually sedated and cannot provide direct feedback. Currently, clinicians can only really use real-time snapshots of the physiology to guide their decisions, with existing automatic monitor alarms being generally crude and unsophisticated. 

CHIMERA's main clinical impact will be new mathematical and data science techniques that can support the decisions of the clinical team. Our models will provide clinicians with a better idea of how well the patient's body is recovering by using clues hidden in the wealth of physiological data collected for that patient since their admission to ICU. The possibilities for improving care in this way are practically endless but some of our foci include: 
- ensuring that patients are weaned off organ support only when their bodies are ready to support themselves, but not longer than necessary; 
- spotting deterioration before it becomes obvious through some adverse event such as a heart attack or stroke. 
- diagnosing sepsis (a treatable but very dangerous condition) more quickly than can be done currently
- enabling better treatment for rare or complex conditions where a patient's physiology is very different from the norm making the monitor data harder to interpret (e.g. a severe heart defect). 

In the longer term our work should lead to new clinical tools that could increase survival after ICU and reduce the time needed for patients to recover. With ICU stays typically costing &pound;2000 - &pound;5000 a day, any systematic reductions in length of stay could generate significant savings for the NHS. Meanwhile, we will be leveraging the enormous amount of data currently collected for each patient in ICU but that is currently not used in decision making. 

During the lifetime of this hub, the impact will be more indirect as we increase our understanding of what the physiological data is telling us about how well someone's body is working, and discover promising avenues for new larger projects to develop clinical support tools. However, we will explore relationships with private sector partners (e.g. Microsoft, Google DeepMind) alongside our clinical partners in years 3 and 4 on how to translate our research into practice.

WIDER SOCIAL IMPACT

There has been a great deal of media coverage about the potential of data science and AI to improve health services, both in the UK and abroad. This has led to unrealistic expectations for what data science can actually achieve, and a public debate over how sensitive patient data should be used by researchers and by whom - particularly whether private companies such as Google, DeepMind should have access to NHS data. 

CHIMERA will address both issues. Firstly, through our programme of public and patient engagement we will explain what we are researching, its potential to improve care as well as its limitations and what it cannot do. Secondly, we will engage the public and patients on how such new large datasets should be managed and governed - for instance, how do we balance the opportunities from private sector partners with risks to patient data and patient trust? How can we safely scale up the Hub's activities while ensuring transparent governance and accountability? Who owns the data and its anonymised derivatives, how should they be used and how will we communicate their use?"
0,757F597C-17D9-4EC4-8904-B7B783277839,UKRI Trustworthy Autonomous Systems Node in Trust,"Engineered systems are increasingly being used autonomously, making decisions and taking actions without human intervention. These Autonomous Systems are already being deployed in industrial sectors but in controlled scenarios (e.g. static automated production lines, fixed sensors). They start to get into difficulties when the task increases in complexity or the environment is uncontrolled (e.g. drones for offshore windfarm inspection), or where there is a high interaction with people and entities in the world (e.g. self-driving cars) or where they have to work as a team (e.g. cobots working in a factory). 

The EN-TRUST Vision is that these systems learn situations where trust is typically lost unnecessarily, adapting this prediction for specific people and contexts. Stakeholder trust will be managed through transparent interaction, increasing the confidence of the stakeholders to use the Autonomous Systems, meaning that they can be adopted in scenarios never before thought possible, such as doing the jobs that endanger humans (e.g. first responders or pandemic related tasks). 

The EN-TRUST 'Trust' Node will perform foundational research on how humans and Autonomous Systems (AS) can work together by building a shared reality, based on mutual understanding through trustworthy interaction. The EN-TRUST Node will create a UK research centre of excellence for trust that will inform the design of Autonomous Systems going forward, ensuring that they are widely used and accepted in a variety of applications. This cross-cutting multidisciplinary approach is grounded in Psychology and Cognitive Science and consists of three &quot;pillars of trust&quot;: 1) computational models of human trust in AS; 2) adaptation of these models in the face of errors and uncontrolled environments; and 3) user validation and evaluation across a broad range of sectors in realistic scenarios. This EN-TRUST framework will explore how to best establish, maintain and repair trust by incorporating the subjective view of human trust towards Autonomous Systems, thus maximising their positive societal and economic benefits.",,
1,F4A7BAA1-8D2B-479E-9BE0-E4D12CA0003B,CONSULT: Collaborative Mobile Decision Support for Managing Multiple Morbidities,"The provision of healthcare to people with long-term conditions is a growing challenge, which is particularly acute for the growing proportion of the UK population that suffers from multiple morbidities.

Research has established that involving patients in the management of their own disease has long-term health benefits. Advances in wireless sensor technology means that it is practical for patients to monitor a wide range of health and wellness data at home, including blood pressure, heart function and glucose levels, without direct supervision by medical personnel. The advent of smart phone technologies, appearing widely throughout the nation's population, enables the exciting possibility of putting state-of-the-art intelligent decision-support systems into the hands of the general public.

However, such sensor data is currently disconnected both from the patient context, provided by the Electronic Health Record, and from the treatment plan, based on current best-evidence guidelines and customised by the patient's GP. In cases of multi-morbidities, there is no clear strategy for combining multiple guidelines into a coherent whole. Furthermore, personalised treatment plans are rigid and do not dynamically adapt to changes in a patient's circumstances. Finally, the record of patient condition and decisions made is not routinely captured in a standardised way, preventing learning from feedback about treatment effectiveness. 

To address these problems, CONSULT will combine wireless &quot;wellness&quot; sensors with intelligent software running on mobile devices, to support patient decision making, and thus actively engage patients in managing their healthcare. Our software will use computational argumentation to help patients follow treatment guidelines and will learn details specific to individuals, personalising treatment advice within medically sound limits. Critically, the software will detect conflicts in treatment guidelines that frequently arise in the management of multiple morbidities. The software will provide advice regarding which treatment options to follow, when the conflicts can be resolved by the patient and when a resolution requires an intervention from a clinician. The software will thus help patients handle routine maintenance of their conditions, while ensuring that medical professionals are consulted when appropriate. This will enable patients to take charge of their own conditions, while being fully supported in both traditional and new ways. By routinely capturing the data provenance of the recommendations made, actions taken and the resulting patient progress, the software will provide valuable insights into the effectiveness of treatments and underlying guidelines in multi-morbidity scenarios.

The technology will be evaluated across multiple dimensions in a proof-of-concept study, engaging stroke patients, their carers and medical professionals, while capitalising on King's College London's world-leading position in stroke research and its established patient groups, particularly those connected to the South London Stroke Register programme.

Helping patients to govern their own care will reduce the demands made on medical professionals, while reaping the health benefits of self-management. Integrating live information from monitoring devices will make it possible to distinguish between situations that need attention from medical professionals, and those that do not, reducing the number of extra appointments that patients and doctors need to schedule. Using live information will also make it possible to detect changes in the course of a disease, allowing pre-emptive actions to be taken, and thus reducing the amount of time that someone suffering from a long-term condition may have to spend in hospital. Overall, our approach will not only provide more efficient care, but also allow care to be better tailored to the needs of each individual.",,"Chronic health conditions are widespread in the UK. NHS England estimates that around 15 million people in England (30% of the population) suffer from chronic health conditions. In Scotland and Wales, the proportion of the population affected is even higher. Such conditions require constant management, and they account for 50% of all GP appointments and 64% of all outpatient appointments. With careful monitoring, it is possible for those who are chronically ill to lead high-functioning lives and to have their care managed at home. However, many chronic conditions can easily lead to hospital stays, with the result that 70% of all inpatient bed days are required to treat the chronically ill. The prevalence of chronic conditions is closely correlated with age. For example, 14% of those younger than 40 report a long-term condition, compared with 58% of those over 60. With an ageing population nationwide, dealing with chronic conditions will consume even more resources in the future.

The number of people with more than one chronic disease is also growing. It is predicted that there will be 2.9 million people with two or more long-term conditions by 2018, an increase of one million since 2008. Such multiple morbidities are difficult to navigate because traditionally each disease has been managed separately, so drug regimes and treatment plans are developed in isolation and may conflict with each other. This growth in multiple morbidities presents a further challenge to our healthcare system: by 2018, dealing with them will cost GBP 5 billion more than in 2011.

A key feature of our proposed CONSULT approach is that it goes far beyond what is possible with medical advice web sites. By adopting a collaborative approach based on integrating wellness sensor data with a patient's electronic health record (EHR), it is possible to provide personalised care in home settings, reducing the amount of hospital and GP time required, and improving treatment outcomes. By specifically targeting the issues that arise in handling multiple morbidities, our approach aims to help the most vulnerable members of the chronically ill population.

Specifically, our research will impact several categories of stakeholders:

* Patients will be assisted in the management of their conditions. Our work will literally put up-to-date information and support at their fingertips. Our goal is to help patients sort through what is relevant to them, understand their options, avoid information overload and make the best decisions, even when treatment guidelines conflict. By engaging patients in this way, we aim to help them obtain the health benefits that have been shown for the chronically ill who self-manage their treatment. Overall, our approach offers both economic benefits, reducing the cost of long-term care, and social benefits in terms of increased quality of life.

* Carers will be empowered in the management of patients with chronic conditions. Our work will provide live support and connection to their patients, and the added security of knowing that medical professionals will be informed of any relevant changes in patients' conditions.

* Clinicians' efforts will be better apportioned. Since wellness sensor data will be integrated into the EHR, clinicians will be able to monitor patients' conditions without bringing them into clinic, and receive alarms when there are situations that require immediate attention.

In addition, medical researchers will be able to obtain integrated data from sensors and EHRs to conduct observational studies on efficiency of treatments and accuracy of measurements in the home setting. The provenance of data collected, backing decision support, will enable commissioning bodies to gain unique insight into the efficiency and cost-effectiveness of treatments. Technology will also impact commercial decision-support providers looking to deliver collaborative home care solutions."
2,7EC76CFA-5516-40FB-A7DD-EFF65A483B3C,Integrated Technology Platform to Support Optimal Management of Ageing with Diabetes,"Latest data suggests that in the UK ~10% of the annual NHS &pound;100 billion budget is spent on treating diabetes, equating to &pound;192 million a week. Of this, nearly 80% is spent on treating irreversible, but preventable diabetes-related complications. Currently there are 3.8 million people diagnosed with diabetes, an estimated 900,000 are yet to be diagnosed, and in 2030 it is expected that more than 5 million people in the UK will have diabetes (as 80-85% of cases of type 2 diabetes is caused by obesity).
Uncontrolled diabetes in older people leads to a range of problems. Hypoglycaemia (low blood glucose) causes a slowing of cognition and may result in acute confusion, accidents and falls and an increased risk of developing dementia. Conversely, hyperglycaemia (high blood glucose) increases the risk of infections, dehydration, and in the longer-term can lead to a significantly higher loss of muscle quality and strength (sarcopenia) as well as irreversible damage to eyes, kidneys, and nerves supplying the feet. This places significant demands on NHS services including GP callouts, ambulance services, A&amp;E attendances and lengthy hospital admissions.
Increasing physical activity levels in people with diabetes would lead to better outcomes in terms of less diabetes-related complications, less depression, slower rates of cognitive decline, lower rates of cardiovascular disease and ultimately less healthcare resource use. However, some people with diabetes find it hard to exercise as the risk of hypoglycaemia is increased. 

The complex relationship between diabetes, physical and cognitive decline, and ageing is not well understood (understudied) often leading to sub-optimal management of people with diabetes as they get older. This in turn results in higher risk of diabetes related complications and increased incidence of morbidity and disability in this population in later years. 

The aim of this proposal is to provide a single technology platform that will implement a data-driven approach to the analysis of this complex relationship via automated machine learning (ML)-driven analytics based on the real-time remote monitoring of the key diabetes markers (Blood Glucose, Insulin, Carbohydrates) and incorporating physical activity measures, as well as cognitive assessment scores. This integrated environment will provide decision support for optimal diabetes management and service planning and provision for healthcare, social and community care. This will enable a shift from the current unsustainable, static and reactive management model, to a future-proof dynamic, intelligent proactive model that will impact in the following ways, for:
 - Patients: a personalisation of support to enable pro-active engagement and empowerment; improved quality of life; healthier ageing.
- Clinicians / Carers: remote monitoring; decision support; prioritisation of those most in need; improved cognitive screening.
- CCG and commissioners / wider NHS / Social service: optimised use of limited healthcare and social care resources; optimal pathways of care.",,
3,23EB349E-ACD2-46B4-9460-127FAA17BD61,EPSRC Core Equipment 2020 - Lancaster University,"The two pieces of apparatus sought under this funding call are i): Cell Sorter apparatus and ii) A Graphical Processing Unit (GPU).

i) The Cell Sorter provides the capability to sort and separate cells and particles individually from complex heterogeneous mixtures such as blood or a homogeneous population of non-biological particles in materials science with specific properties from complex particulate mixtures. The apparatus has become an essential element or enabling technology for several broad research areas. The novelty of the apparatus is that hitherto the means to achieve these kinds of separation were complex and technically difficult and actually often were the main research goals in their own right. This apparatus however, facilitates relatively simply and quickly and without requirements for advanced user skills to operate the equipment, state of the art sorting capability. The simplicity of use of the apparatus will allow non-specialist users to access the technology and so open up the possibilities of a number of new research projects. Thus physicists and engineers who would not normally have the background in biological separation techniques will be able to prepare cell types for study using their specialist techniques and interests. As an example specific artificial subtratum-cell interactions that underlie the development of sterile healthcare materials will be greatly enabled by such capabilities. This technology, therefore, will greatly expand the possibilities of the potential user base and so be of value to a wide range of EPSRC researchers improving their research efficiency and productivity and making more effective use of staff time. 

ii) Graphical Processing Units (GPU) will enhance the university's capacity for computationally demanding research in many research areas. They are required to address the computationally demanding tasks associated with Artificial Intelligence (AI), Machine Learning and Data Science. These areas are central pillars of the current UKRI Strategic Priorities that are critical to the development of the UK's world-leading data-driven economy, digital society within a trusted cyber-secure environment. There are developments in each of these in all faculties at Lancaster (see eg. https://www.lancaster.ac.uk/dsi/ ) with clear impacts in the Digital Economy priority (among others) at EPSRC.",,
4,81A8EB76-0CAF-4E65-AC55-D07AB1B3E1A6,Visual AI: An Open World Interpretable Visual Transformer,"With the advent of deep learning and the availability of big data, it is now possible to train machine learning algorithms for a multitude of visual tasks, such as tagging personal image collections in the cloud, recognizing faces, and 3D shape scanning with phones. However, each of these tasks currently requires training a neural network on a very large image dataset specifically collected and labelled for that task. The resulting networks are good experts for the target task, but they only understand the 'closed world' experienced during training and can 'say' nothing useful about other content, nor can they be applied to other tasks without retraining, nor do they have an ability to explain their decisions or to recognise their limitations. Furthermore, current visual algorithms are usually 'single modal', they 'close their ears' to the other modalities (audio, text) that may be readily available.

The core objective of the Programme is to develop the next generation of audio-visual algorithms that does not have these limitations. We will carry out fundamental research to develop a Visual Transformer capable of visual analysis with the flexibility and interpretability of a human visual system, and aided by the other 'senses' - audio and text. It will be able to continually learn from raw data streams without requiring the traditional 'strong supervision' of a new dataset for each new task, and deliver and distill semantic and geometric information over a multitude of data types (for example, videos with audio, very large scale image and video datasets, and medical images with text records).

The Visual Transformer will be a key component of next generation AI, able to address multiple downstream audio-visual tasks, significantly superseding the current limitations of computer vision systems, and enabling new and far reaching applications.

A second objective addresses transfer and translation. We seek impact in a variety of other academic disciplines and industry which today greatly under-utilise the power of the latest computer vision ideas. We will target these disciplines to enable them to leapfrog the divide between what they use (or do not use) today which is dominated by manual review and highly interactive analysis frame-by-frame, to a new era where automated visual analytics of very large datasets becomes the norm. In short, our goal is to ensure that the newly developed methods are used by industry and academic researchers in other areas, and turned into products for societal and economic benefit. To this end open source software, datasets, and demonstrators will be disseminated on the project website.

The ubiquity of digital images and videos means that every UK citizen may potentially benefit from the Programme research in different ways. One example is smart audio-visual glasses, that can pay attention to a person talking by using their lip movements to mask out other ambient sounds. A second is an app that can answer visual questions (or retrieve matches) for text-queries over large scale audio-visual collections, such as a person's entire personal videos. A third is AI-guided medical screening, that can aid a minimally trained healthcare professional to perform medical scans.",,"The proposed programme encompasses new methodology and applied research in computer vision and other modalities (audio, text) that will enable analysis and search of image and video content while learning new things, with human-like flexibility and interpretability. These capabilities will encourage end user take up of computer vision technologies and commercial interest in embedding these technologies in products.

The Programme will have Economic and Societal impact by
1. Enabling UK industry to leverage AI in their activities with a key strategic advantage.
2. Developing new and improved computer vision technologies that will require substantially less training data to solve problems and is thus suitable for commercialisation by a wide range of companies.
3. Enhancing the visual and audio capabilities and knowledge base of UK industries, including small ones.
4. Enhancing quality of life by improving, for instance, healthcare capabilities, surveillance, environmental monitoring, and the means of accessing and enjoying personal digital media.
5. Reducing the cost and risk of collecting manual annotations for deploying AI technology, especially for sensitive data such as medical records.
6. Collaborating directly with companies and organizations that we have already identified, and will work with over the course of the Programme.
7. Training the next generation of computer vision researchers who will be equipped to support the imaging needs of science, technology and wider society for the future.

Impact on Knowledge includes
1. Realisation of new approaches to essential computer vision technology, and the dissemination of research findings through publications, conference presentations, summer school teaching, and the distribution of open source software and image databases.
2. Sharing knowledge with industrial collaborators via Transfer and Application Projects (TAPs) and other activities leading to adoption of advanced computer vision methods across many disciplines of science, engineering and medicine that currently do not use them.
3. Communication of advances to a public audience through website articles, Show and Tell events, social and broadcast media, and other co-ordinated public understanding activities"
5,8F67B73C-8601-4520-8D0A-4610544CEC82,Designing Conversational Assistants to Reduce Gender Bias,"Biased technology disadvantages certain groups of society, e.g. based on their race or gender. Recently, biased machine learning has received increased attention. Here, we address a different type of bias which is not learnt from data, but encoded during the design process. We illustrate this problem on the example of Conversational Assistants, such as Amazon's Alexa, Apple's Siri, Microsoft's Cortana, or Google's Assistant, which are predominately modelled as young, submissive women. According to UNESCO, this bears the risk of reinforcing gender stereotypes.

In this proposal, we will explore this claim via psychological studies on how conversational gendering (expressed through voice, content and style) influences human behaviour in both online and offline interactions. Based on the insights gained, we will establish a principled framework for designing and developing alternative conversational personas which are less likely to perpetuate bias. A persona can be viewed as a composite of elements of identity (background facts or user profile), language behaviour, and interaction style. This framework will include state-of-the-art data-efficient NLP deep learning tools for generating dialogue responses which are consistent with a given persona. The persona parameters can be specified by non-expert users in order to to facilitate more inclusive design, as well as to enable a wider critical discussion.",,
6,6295814E-A86C-47EA-843E-AE3AB44F3B57,Modeling Idiomaticity in Human and Artificial Language Processing,"This project will develop computational models with the ability to recognize and accurately process idiomatic (non-literal) language that are linguistically motivated and cognitively-inspired by human processing data. Equipping models with the ability to process idiomatic expressions is particularly important for obtaining more accurate representations as these can lead to gains in downstream tasks, such as machine translation and text simplification. The originality of this work is in integrating linguistic and cognitive clues about human idiomatic language processing in the construction of models for word and phrase representations, and in integrating them in downstream tasks. 

The main objectives and research challenges of this project:
1:To explore cognitive and linguistic clues linked to idiomaticity that can be used to guide models for word and phrase representations 
2: To investigate idiomatically-aware models. 
3: To explore alternative forms of integrating these models in applications and to develop a framework for idiomaticity evaluation in word and phrase representation models.
4. To release software implementations of the proposed models to facilitate reproducibility and wider adoption by the research community.

This proposal targets a crucial limitation in standard NLP models, as idiomaticity is everywhere in human communication, with potential benefits to various applications that include natural language interfaces, such as conversational agents, computer assisted language learning platforms, question answering and information retrieval systems. As a consequence we anticipate the proposal will have a wide academic impact in the community. Moreover, enabling more precise language understanding and generation also has the potential of enhancing accessibility and digital inclusion, through promoting more natural and accurate communication between humans and machines. We intend to demonstrate the additional potential benefits of these models through interactions with our external collaborations, including by means of an advisory board. The board will include other academics and industrial partners working on related topics, such as, Dr. Fabio Kepler (Unbabel Portugal, for machine translation), Prof. Mathieu Constant (Universit&eacute; de Lorraine, France, for parsing and idiomaticity), Prof. Lucia Specia (Imperial College, UK, for text simplification) and Dr. Afsaneh Fazly (Samsung, Canada, for idiomaticity).

This proposal targets a crucial limitation in standard NLP models, as idiomaticity is everywhere in human communication, with potential benefits to various applications that include natural language interfaces, such as conversational agents, computer assisted language learning platforms, question answering and information retrieval systems. As a consequence we anticipate the proposal will have a wide academic impact in the community. Moreover, enabling more precise language understanding and generation also has the potential of enhancing accessibility and digital inclusion, through promoting more natural and accurate communication between humans and machines. We intend to demonstrate the additional potential benefits of these models through interactions with our external collaborations, including by means of an advisory board.",,"Correctly representing and treating idiomatic expressions can have a positive impact on the accuracy of NLP applications such as machine translation (MT), text simplification (TS), text summarization, dialog systems, among others. Consequently, we anticipate an impact on the following areas:

- Economy
NLP service providers are highly interested in providing accurate outputs in order to avoid misleading their users, which can result in liability for these providers. In this project we will focus on two downstream applications, text simplification and machine translation, which are available as stand-alone commercial platforms. However, there is a large variety of products that also involve understanding and generating natural language, such as conversational agents and digital assistants, which can be found around the world in software and hardware including commercial websites, computers, mobile phones and other portable devices, car navigation systems, etc. As the users' trust on NLP applications increases, their economic value also increases, therefore, our approaches can positively impact the NLP industry. 

- Society
As text simplification and machine translation reach millions of users worldwide, advances in idiomatic language processing has the potential of bringing a positive impact by improving their experience. We address both tasks, however, the results of this project have a high potential to improve the reliability of a larger variety of NLP downstream tasks, providing users with more accurate and human-friendly interfaces. This can also enable low-advantaged or vulnerable users, such as non-native speakers of English or low-literate individuals, to fully access and comprehend information that contains idiomatic expressions.

- Knowledge
This proposal targets a fundamental limitation in traditional NLP approaches that needs to be adequately addressed in precise language technology. As a consequence, we anticipate publication of the resources and methods developed together with results obtained in high profile NLP conferences. Moreover, to enhance reproducibility of results and to promote reusability of resources, we will make them available in the project dedicated website and Github repository. This project that will focus on analysis, modelling and application has the potential to encourage discussions about responsible innovation and ethics in NLP.

- People
This proposal has the potential for helping to establish the careers of both the Sheffield PI and Co-I, bringing enormous positive impacts on their careers. It will enable the PI to use her previous experience coordinating projects in Brazil, to establish a research profile in the UK and solidify her position in the field, while using her research expertise to address an important shortcoming in current language technology. This EPSRC grant will be instrumental in allowing both the Sheffield PI and Co-I to build a collaboration network, advance their research and demonstrate a successful project delivery in the UK for securing further funding. It will also enable the RA to work on a well-established fast-growing NLP group, one of the largest in Europe, with connections to the NLP and ML main centres around the world. Him(er) will also benefit from knowledge exchange with the PI and CO-Is, working on a relevant NLP topic. BSc and MSc students may also benefit from this project, as we will propose small projects related to this topic."
7,341BE095-7486-4165-916D-59BF89490823,Unmute: Opening Spoken Language Interaction to the Currently Unheard,"Everyday, millions of users talk to machines, be they voice assistants on their mobile phones or standalone devices - such as Alexa or Google Home - in the homes and workplaces. By using Natural Language Processing for input and output through the medium of speech in an intelligent user interface, people are able to access a plethora of content and services. These systems rely on a several factors: i) firstly the use of automatic speech recognition (ASR) that can convert the speech signal into usable language tokens; ii) the availability of knowledge and resources in the system to address the user need; and, iii) an intelligent user interface interaction design that fits the contexts and capabilities of the end user. While state-of-the-art systems are beginning to address the needs of &quot;conventional&quot; users (i.e. those who speak a widely spoken and written language; and who have relatively high degrees of literacy, exposure to digital interactions and other resources), there are many hundreds millions of people who are being excluded globally. Paradoxically, these users who have resource constraints (such as low digital and textual literacy) could be the ones to most benefit from advances in spoken NLP systems opening up economic, social and educational possibilities currently unmet. 

This project addresses the limitations of today's approaches to open up intelligent interfaces to the currently digitally 'unheard'. The challenges we will address are threefold. Firstly, and crucially, there is a need to explore highly innovative ASR techniques that can cope with languages that have limited or even no textual resources. Conventionally, ASR systems rely on vast amounts of transcribed speech to develop and train models. Our focus is on languages where there is little of this data and indeed on languages where there is no established written form. For the systems to be useful to the sorts of community we target, they have to of course make available relevant content that is in the language these users use; so the second challenge is establish infrastructures and user involvement to provide ways to generate such content. In doing so we hope to produce a blueprint and toolkit that can be used by many other low or zero-resource language communities. Finally, the user communities we will work with to develop these new approaches have a different perspective to &quot;conventional&quot; users and the third challenge is to surface the needs and values when interacting with an intelligent interface for content and services so that the underlying algorithms and the interaction devices and styles are appropriate and effective. Prior work by our team has shown that assumptions of what works in speech assistants that are deployed in conventional settings break down when these systems are exposed to groups in informal settlements in India and townships in South Africa. By taking this approach, we expect to innovate on and disrupt the interface styles and interaction devices currently used in intelligent speech and language interfaces, addressing the need of not just currently excluded users but offering up new possibilities for the rest of the world too. 

The work brings together two world leading groups in a new collaboration - Edinburgh's CSTR with its long track record on speech technology innovation; and, the FIT Lab at Swansea's computational foundry that has pioneered interaction innovation for and with emergent users for over a decade, developing and advocating responsible innovation with communities in rural, peri-urban and urban developing world contexts. These groups are joined by both existing and new collaborations with NGO, spin-out and international academic stakeholders who will help shape the work and ensure it has direct and sustainable impact.",,"We are focussed on providing a voice to hundreds of millions of people who are currently 'unheard'. This exclusion comes in two forms: i) pragmatically, they cannot use their voice to speak to intelligent assistants because the ASR/NLP and information interaction technology does not accommodate the low or zero resource language they speak nor addresses their context of use; and, ii) more philosophically, they are not involved in the 'future making' of new technologies. That is, most technology is &quot;designed in California&quot; (as you can see if you turn over many mobile devices, to see this brand and boast). As a core element of the responsible innovation in this project, we are aiming to produce technology that these excluded users can benefit from and involve them directly in the design of not just their future but the wider world's as we uncover non-Californian perspectives and possibilities. 

Voice assistants and related technology may offer a highly appropriate and effective way for the currently unheard to create and participate in socially and economically beneficial information interaction. From education, to health, to entrepreneurial services and advertising, the use of the platforms created in this project could lead to sustained and profound impacts. We would expect - and during the lifetime of the project will endeavour to develop - strong interest in the innovations from NGOs and Governmental organisation who wish to engage digitally excluded groups. Commercial opportunities also are likely as companies - including ones in the UK - aim to provide services that go beyond the conventional consumer base that is looks to be saturated within the next 5 years (a similar trend has been seen, for example, in smartphone deployment with companies highly focussed now on how to reach &quot;the next billion&quot; given the slow down in market growth amongst &quot;developed&quot; world economies). 

By acting as a beacon for highly impactful, ambitious and human-centred innovation, the work will act as an attractor of talent to UK. The country already has a strong reputation in AI/language processing and bringing this together with clearly and purposefully driven human methods and interface technologies will help contribute to setting the UK apart. There is a growing disquiet globally about what the coming world of intelligent interfaces, AI, big data etc will do to people's jobs, society cohesion and even identity. This project will showcase ways that by working directly with end-users we can drive adventurous science that amplifies human capabilities and abilities rather than reducing them or making them redundant. Our work will then also contribute to the important and timely debate on how to responsibly, ethically and positively use advances in machine learning and AI."
8,84407A76-7F33-4782-8346-CD3A1BC7C18E,SAIS: Secure AI assistantS,"There is an unprecedented integration of AI assistants into everyday life, from the personal AI assistants running in our smart phones and homes, to enterprise AI assistants for increased productivity at the workplace, to health AI assistants. Only in the UK, 7M users interact with AI assistants every day, and 13M on a weekly basis. A crucial issue is how secure AI assistants are, as they make extensive use of AI and learn continually. Also, AI assistants are complex systems with different AI models interacting with each other and with the various stakeholders and the wider ecosystem in which AI assistants are embedded. This ranges from adversarial settings, where malicious actors exploit vulnerabilities that arise from the use of AI models to make AI assistants behave in an insecure way, to accidental ones, where negligent actors introduce security issues or use AIS insecurely. Beyond the technical complexities, users of AI assistants are known to have mental models that are highly incomplete and they do not know how to protect themselves. 

SAIS (Secure AI assistantS) is a cross-disciplinary collaboration between the Departments of Informatics, Digital Humanities and The Policy Institute at King's College London, and the Department of Computing at Imperial College London, working with non-academic partners: Microsoft, Humley, Hospify, Mycroft, policy and regulation experts, and the general public, including non-technical users. SAIS will provide an understanding of attacks on AIS considering the whole AIS ecosystem, the AI models used in them, and all the stakeholders involved, particularly focusing on the feasibility and severity of potential attacks on AIS from a strategic threat and risk approach. Based on this understanding, SAIS will propose methods to specify, verify and monitor the security behaviour of AIS based on model- based AI techniques known to provide richer foundations than data-driven ones for explanations on the behaviour of AI-based systems. This will result in a multifaceted approach, including: a) novel specification and verification techniques for AIS, such as methods to verify the machine learning models used by AIS; b) novel methods to dynamically reason about the expected behaviour of AIS to be able to audit and detect any degradation or deviation from that expected behaviour based on normative systems and data provenance; iii) co-created security explanations following a techno-cultural method to increase users' literacy of AIS security in a way that users can comprehend.",,"SAIS will run a programme of activities with a view to maximising: 

*Networking and Community Building on AI Security* by engaging with the other projects funded in this call and the wider academic, industry, government and policy stakeholders and the general public. We aim to engender a sustained and collaborative approach to AI Security to create a community of practice both during and after the funding period for the project. This will include knowledge sharing, best practice and translation activities to enable the community to strengthen and for stakeholder to engage and learn from one another, and work to sustain the community beyond the initial funding. Activities will include community symposium, meetings, and meetups; an online community of practice; the creation of an AI Security Problem Book; and White reports of Use Cases. 

*Industry Impact* by working in tandem with four companies: Microsoft, Humley, Hospify, Mycroft, and their real-world Use Cases of personal, enterprise, and health AI assistants. This will be catalysed through participatory and co-creation activities embedded in the research programme, as well as regular individual and plenary partner meetings, and secondments from the research team to the companies. We will also explore opportunities for innovation and commercial exploitation.

*Policy Impact* will be facilitated by the establishment of a Policy Advisory Group with relevant policy experts, think tanks, and civil servants from the Policy Institute's current networks to provide expert advice on potential opportunities and challenges that may arise for policy impact; running a Policy Lab to bring together research, policy and practitioner expertise to assess the evidence base identified in the project and make progress on policy challenges arising from it; and creating a Policy Briefing with summarised Lab's results to be made publicly available and used by the Advisory Group and the Policy Institute to disseminate findings further through their policy networks and as a community resource.

*Public Engagement* to build public awareness, reflexivity, and understanding of AI assistant security by running activities facilitated by KCL's Public Engagement team including: Concept development of a format/concept that can be run to engage publics with; Immersive experience to engage the public with research at the Science Gallery London; Public Workshops right after the immersive experience to bring together the research team with target audiences and enable dialogue between them; and Digital Engagement to maximise reach and target a broad end-user base. A dedicated expert will help the team to plan and evaluate the impact on building public awareness, reflexivity, and understanding about privacy and smart personal assistants.

*Developing SAIS Team's Skills* will be delivered leveraging KCL and Imperial's training centres for staff; secondments in industry; interacting with other projects funded in this call, and the general public, industry, and policy bodies; and ORBIT's one-day foundation course on Responsible Research and Innovation."
9,61966E3B-A81E-4DB2-B900-DB8F10ED2E2D,SAIS: Secure AI assistantS,"There is an unprecedented integration of AI assistants into everyday life, from the personal AI assistants running in our smart phones and homes, to enterprise AI assistants for increased productivity at the workplace, to health AI assistants. Only in the UK, 7M users interact with AI assistants every day, and 13M on a weekly basis. A crucial issue is how secure AI assistants are, as they make extensive use of AI and learn continually. Also, AI assistants are complex systems with different AI models interacting with each other and with the various stakeholders and the wider ecosystem in which AI assistants are embedded. This ranges from adversarial settings, where malicious actors exploit vulnerabilities that arise from the use of AI models to make AI assistants behave in an insecure way, to accidental ones, where negligent actors introduce security issues or use AIS insecurely. Beyond the technical complexities, users of AI assistants are known to have mental models that are highly incomplete and they do not know how to protect themselves. 

SAIS (Secure AI assistantS) is a cross-disciplinary collaboration between the Departments of Informatics, Digital Humanities and The Policy Institute at King's College London, and the Department of Computing at Imperial College London, working with non-academic partners: Microsoft, Humley, Hospify, Mycroft, policy and regulation experts, and the general public, including non-technical users. SAIS will provide an understanding of attacks on AIS considering the whole AIS ecosystem, the AI models used in them, and all the stakeholders involved, particularly focusing on the feasibility and severity of potential attacks on AIS from a strategic threat and risk approach. Based on this understanding, SAIS will propose methods to specify, verify and monitor the security behaviour of AIS based on model- based AI techniques known to provide richer foundations than data-driven ones for explanations on the behaviour of AI-based systems. This will result in a multifaceted approach, including: a) novel specification and verification techniques for AIS, such as methods to verify the machine learning models used by AIS; b) novel methods to dynamically reason about the expected behaviour of AIS to be able to audit and detect any degradation or deviation from that expected behaviour based on normative systems and data provenance; iii) co-created security explanations following a techno-cultural method to increase users' literacy of AIS security in a way that users can comprehend.",,
10,D618B924-D15E-46A0-9E30-E211A0AC1A44,Machine Learning in International Trade Research - Evaluating the Impact of Trade Agreements,"International trade is of vital importance for modern economies, and governments around the world try to shape their countries' exports and imports through numerous interventions. Given the problems facing trade negotiations through the World Trade Organization (WTO), countries have increasingly turned to preferential trade agreements (PTAs) involving only one or a small number of partners. At the same time, attention has shifted from reductions of import tariffs to the role of non-tariff barriers such as differences in regulations and technical standards. Accordingly, modern PTAs contain a host of provisions besides tariff reductions, in areas as diverse as services trade, competition policy or public procurement.

A key question in international trade research is how to estimate the effects of PTAs and their individual provisions on trade flows. We argue that methods from the machine learning literature can help address this challenge, and that such methods are often superior to existing approaches. We use the term 'machine learning' to refer to algorithms used for statistical prediction that are trained on subsets of the available data to make forecasts of quantifiable outcomes (here: trade flows). While such algorithms have started to be applied in economic research, they have not been used for the analysis of PTAs nor in international economics more generally.

First, machine learning can help evaluate the suitability of existing methods for estimating PTA effects. Such methods evaluate PTAs by comparing the trade flows observed after the implementation of an agreement to a so-called counterfactual outcome that shows what would have happened to trade flows in the absence of a PTA. This counterfactual is invariably based on a specific statistical model. Currently, by far the most common model is the so-called gravity equation. The estimated effect does of course depend on how well the gravity equation predicts counterfactual trade flows. We will use machine learning to develop a more flexible forecast to which we can compare the gravity equation's predictive power.

Machine learning can also help improve existing methods for PTA evaluation. Implicitly, approaches based on the gravity equation construct a counterfactual by using an average of the changes in trade flows between countries not involved in a PTA. Similar approaches have been applied in a range of contexts besides international trade. Recent methodological advances have shown how these approaches can be improved by applying machine learning to select more complex combinations of control units (here: countries not participating in a PTA) than simple averages. Despite their potential, these techniques have not been applied in international trade research, and we propose to adapt them to this context.

Finally, machine learning can be used to determine the relative importance of individual PTA provisions. The key challenge existing research has faced is that many PTAs contain similar provisions, making it difficult to estimate their effect on trade flows separately. Thus, researchers usually aggregate provisions in some way, for example by combining them into broad groups. This limits the relevance to policymakers who need to know if they should include a given individual provision in a PTA. This problem is reminiscent of the issue of 'feature selection' in machine learning where algorithms must decide which of many potentially relevant variables to include for forecasting purposes. We plan to use a subgroup of these methods that allow to identify the subset of variables (here: provisions) with the largest effect and to accurately estimate their impact.

Overall, the proposed research will deepen our understanding of how PTAs impact trade flows. This, and the empirical techniques we plan to develop, will help researchers and policymakers involved in the design and evaluation of PTAs and ultimately contribute to a better, more evidence-based trade policy.",,"Apart from academic users, our research will benefit policymakers from a range of institutions. These include UK government departments such as the Department for International Trade (DIT) and the Department for Exiting the European Union (DExEU); international organisations such as the WTO, OECD, IMF and the World Bank; central banks such as the Bank of England; and industry organisations such as the Confederation of British Industry or the British Chambers of Commerce.

All these user groups monitor and assess trade policy and often carry out their own evaluations of trade agreements. Our proposed work will either improve existing methods for doing so or - in the context of individual trade agreement provisions - make sophisticated evaluations possible in the first place. The results of our research related to existing trade agreements, as well as the statistical techniques and related software packages we plan to develop, will help policymakers' decision finding process and ultimately contribute to a better, more evidence-based trade policy. Given that the United Kingdom may soon have to take charge of its own trade policy after Brexit and is currently (re)negotiating a substantial number of trade agreements, we believe that this is more important than ever.

As explained in more detail in the Pathways to Impact document, we will reach our target user groups through a variety of approaches, including policy briefings, publications targeted at non-academic audiences, conferences and invited presentations, and a project conference. We will also provide custom-made packages for standard statistical software to help users implement the techniques we plan to develop. Finally, the third part of the project (measuring the impact of trade agreement provisions) will be supported by the World Bank as explained in the Case for Support. This will enhance the impact of our research, both directly because the World Bank is a potential user itself as well as indirectly through the Bank's network of stakeholders interested in the effects of trade agreement provisions."
11,322FCFB2-DE14-4E50-85DF-0F5A86D7D1AF,Turing AI Fellowship: Rigorous time-complexity analysis of co-evolutionary algorithms,"Optimisation -- the problem of identifying a satisficing solution among a vast set of candidates -- is not only a fundamental problem in Artificial Intelligence and Computer Science, but essential to the competitiveness of UK businesses. Real-world optimisation problems are often tackled using evolutionary algorithms, which are optimisation techniques inspired by Darwin's principles of natural selection.

Optimisation with classical evolutionary algorithms has a fundamental problem. These algorithms depend on a user-provided fitness function to rank candidate solutions. However, for real world problems, the quality of candidate solutions often depend on complex adversarial effects such as competitors which are difficult for the user to foresee, and thus rarely reflected in the fitness function. Solutions obtained by an evolutionary algorithm using an idealised fitness function, will therefore not necessarily perform well when deployed in a complex and adversarial real-world setting.

So-called co-evolutionary algorithms can potentially solve this problem. They simulate a competition between two populations, the &quot;prey&quot; which attempt to discover good solutions, and the &quot;predators&quot; which attempt to find flaws in these. This idea greatly circumvents the need for the user to provide a fitness function which foresees all ways solutions can fail.

However, due to limited understanding of their working principles, co-evolutionary algorithms are plagued by a number of pathological behaviours, including loss of gradient, relative over-generalisation, and mediocre objective stasis. The causes and potential remedies for these pathological behaviours are poorly understood, currently limiting the usefulness of these algorithms.

The project has been designed to bring a break-through in the theoretical understanding of co-evolutionary algorithms. We will develop the first mathematically rigorous theory which can predict when a co-evolutionary algorithm reaches a solution efficiently, and when pathological behaviour occurs. This theory has the potential to make co-evolutionary algorithms a reliable optimisation method for complex real-world problems.",,
12,9E48FC8C-980F-4AE6-8FCD-B3AF02408F4F,Turing AI Fellowship: PHOTONics for ultrafast Artificial Intelligence,"In today's society, the massive deployment of smart devices, the popularity of online services and social networks, and the increasing global data traffic, makes the ability to process large data volumes absolutely crucial. Demand for Artificial Intelligence (AI) has therefore exploded, fuelled by an increasing number of industries (e.g. energy, finance, healthcare, defence) heavily relying on the efficient processing of large data sets. Nonetheless, the ever-growing data processing demand creates a pressing need to find new paradigms in AI going beyond current systems, capable of operating at very high speeds whilst retaining low energy consumption.
 
The human brain is exceptional at performing very quickly, and efficiently, highly complex computing tasks such as recognising patterns, faces in images or a specific song from just a few sounds. As a result, computing approaches inspired by the powerful capabilities of networks of neurons in the brain are the subject of increasing research interest world-wide, and are in fact already used by current AI platforms to perform these (and other) complex functions.

Whilst these brain-inspired artificial neural networks (ANNs) are supported to date by traditional micro-electronic technologies, photonic techniques for brain emulation have also recently started to emerge due to their unique and superior properties. These include very high speeds and reduced interference, among others. Remarkably, ubiquitous photonic devices such as vertical-cavity surface emitting lasers (VCSELs), the very same devices used in supermarket barcode scanners, computer mice and in mobile phones for auto-focus functionalities, can exhibit responses analogous to those of neurons but up to 1 billion times faster. VCSELs are also compact, inexpensive and allow practical routes for integration in chip modules with very low footprints (just a few mm2) making them ideal for the development of ultrafast photonic ANNs using ultrafast light signals instead of electric currents to operate. This permits exploring radically new research directions aiming at exploiting the full potential of light-enabled technologies for new paradigms in ultrafast AI. This Fellowship project will focus on this key challenge to develop transformative photonic ANNs using VCSELs as building blocks capable of performing complex computational tasks at ultrafast speeds, using data rates below 1 billionth of a second to operate. These will include the ultrafast prediction of complex data signals, of interest for example in meteorology forecasting, to very high speed data classification of interest in green-energy systems (e.g. analysis of wind patterns in off-shore wind-energy farms).
 
The research milestones of this programme are: (1) the design and fabrication of photonic ANNs using coupled VCSELs as building blocks, emulating the operation of the human brain at ultrafast speeds; (2) the development of chip-scale modules of VCSEL based photonic ANNs; (3) the demonstration of complex data processing tasks with photonic ANNs at ultrafast speeds (at data rates below 1 billionth of a second); (4) the delivery of photonic systems for AI, tackling key functionalities across strategic UK economic sectors (e.g. energy, defence).

In summary, by bringing together the hitherto disparate fields of brain-inspired computing and photonics, this programme proposes unique pioneering research in photonic ANNs for future ultrafast AI technologies.",,
13,5BE7635B-6315-4CD0-8C1B-EC037E038F69,Turing AI Fellowship: Machine Learning Foundations of Digital Twins,"The proposed programme of research will establish the machine learning foundations and artificial intelligence methodologies for Digital Twins. Digital Twins are digital representations of real-world physical phenomena and assets, that are coupled with the corresponding physical twin through instrumentation and live data and information flows. This research programme will establish next-generation Digital Twins that will enable decision makers to perform accurate but simulated &quot;what-if&quot; scenarios in order to better understand the real world phenomena and improve overall decision making and outcomes.",,
14,FFDE0028-0D73-4E0D-B1DA-79F159F1E5B9,Machine learning methods for studying the trajectories of young offenders in administrative data,"Administrative data has the potential to open new and invaluable research opportunities to better understand societal phenomena and support evidence-based policy-making. One research area administrative data can significantly enhance is the analysis of life-course trajectories across key domains of interest to social scientists, including education, economic activity, health, and crime. Administrative data is a rich source of longitudinal information on key socio-economic outcomes and public services use that can be particularly useful for policy-relevant analysis. Yet, in the UK, it has been relatively untapped so far. As administrative data and administrative linked data becomes increasingly available to researchers, it is essential that research teams, both in academia or in government, are equipped with the appropriate methodological tool kit to take full advantage of the research possibilities these data offer. 
 
In this bid, the Institute for Fiscal Studies (IFS) proposes to collaborate with the Ministry of Justice (MoJ) to address this challenge: we propose a package of activities to advance the use of machine learning techniques to exploit the richness of MoJ's administrative and linked administrative datasets as sources of information on individuals' offending and educational trajectories and their journeys through the justice system. These activities include: 

a) Innovative research investigating the nature and drivers of offenders' trajectories and how these are shaped by the justice system, by applying, for the first time, sequence analysis methods in UK administrative and administrative linked data. 

b) A secondment of an IFS researcher to the MoJ to ensure the co-production and policy relevance of the research, enhance MoJ's analytical capability, and strengthen exchange of ideas and knowledge between the two institutions. 

c) A training workshop led by an international expert in sequence analysis methods to build research capability, within research institutions and government, in the use of machine learning techniques for administrative data analysis. 

d) A research workshop at the end of the grant for researchers from the IFS and other institutions, as well as analysts and policy-makers from MoJ and other relevant departments, to present and discuss current research using machine learning methods in administrative data. 

The project team, led by Professor Imran Rasul, Professor of Economics at University College London and IFS' Research Director, has a strong track record of academic and policy work based on UK and international administrative data. By establishing the viability of these methods in UK administrative linked data and demonstrating their usefulness to better understand individuals' journeys through public service systems, the research will make significant contributions to life-course studies conducted across the social sciences. The research will produce at least two academic outputs, accompanied by non-technical, policy-oriented briefs. The team's collaboration with MoJ, as well as the IFS' well-established links with policy-makers and the media, will ensure that these outputs are well disseminated beyond academia and used to inform the policy-making process.",,
15,B61C4E98-321C-4B9E-B8CF-F493A2055090,Understanding barriers to accurate early laboratory diagnosis and patient centric control of Gestational Diabetes Mellitus,"Gestational diabetes mellitus (GDM) is a condition characterised by high blood glucose levels, with first onset during pregnancy. GDM increases the risk of complications for both mother and child. Evidently, early detection and treatment improve outcomes, but many women are at serious risk of going undiagnosed due to a lack of universally accepted diagnostic criteria, and disagreement over the glucose range deemed healthy. The most commonly used range is reported by the World Health Organisation to be 'somehow arbitrary'. Furthermore, poorly controlled GDM leads to adverse maternal and infant outcomes and increased likelihood of developing type 2 diabetes later in life. On the other hand, tight glucose control increases the risk of severe hypoglycaemia (low glucose levels), which may also compromise the wellbeing of mother and child. 

The overall clinical goal is to improve the criteria, enable diagnosis as early as possible in pregnancy, discover better GDM markers and improve management of the condition to ensure that blood glucose levels remain under control throughout pregnancy. In addition to the impact on wellbeing and quality of life for both mother and child, any improvement in the management of this condition will reduce the burden on the national economy. 

By facilitating hospital-based training for an engineering scientist, this discipline hop has the following aims:

- gaining for the principal investigator of a full grasp of the clinical challenges preventing the transformation of diagnosis and treatment of GDM;

- the establishment of a permanent network linking engineering science specialists with endocrinologists, obstetricians, pathologists, other allied healthcare colleagues and patients to tackle together the unsolved challenges of effective GDM care;

- the co-creation by the principal investigator and the other stakeholders above of a research strategy to improve GDM care.

The project will look at utilisation of routinely collected NHS data for research, give consideration to glucose variability to go beyond diagnosis based on glucose levels at single time points, and to personalisation for better management of glucose variability throughout pregnancy. 

The training will include:

(a) engagement with patients and clinical professionals to understand how GDM is currently managed in the clinic, the practical realities constraining current practice, and how patients and different clinical professionals envisage improving care beyond today's approaches founded on population averages towards personalised alternatives

(b) training in available databases and laboratory sample testing to learn the structure of routinely collected NHS data, identify their limitations, and the implications for data analyses and modelling

(c) training in Good Clinical Practice covering ethical and regulatory requirements for research, the code of conduct for clinic-based research, a researcher's responsibilities towards study participants, limitations of measurements, reliability of data and other areas that are typically left unexplored by engineering and physical sciences researchers, and

(d) experience in recruitment for longitudinal measurements, together with assessment of our ability to recruit and the feasibility of personalisation based on longitudinal data obtained from continuous glucose monitoring, via preliminary data analysis and modelling.

As a long-term goal, the emergent collaboration aims to support the EPSRC Healthy Nation programme via better understanding of individual glucose variability, optimised care through effective diagnosis, patient-specific prediction and evidence-based treatment planning, minimisation of costs of care and reduction of risks to GDM patients and their children. The knowledge acquired from the project will form a platform to translate our findings into a large scale trial, which in turn can form the basis of changing current clinical practice in this field.",,"This proposed discipline hopping training and the subsequent research will bring benefits to the following areas and individuals. 
 
People with gestational diabetes mellitus (GDM), healthcare professionals and wider society: GDM increases the risk of complications for both mother and child during pregnancy, at childbirth and beyond with increasing prevalence. It is estimated to affect 7% of all pregnancies. Evidence suggests that early detection and treatment improve pregnancy outcomes, whereas mis-diagnosis and a poorly controlled condition can lead to adverse maternal and infant outcomes, including pre-eclampsia, fetal macrosomia and respiratory distress syndrome, to name a few. They also increase the likelihood of developing type 2 diabetes later in life. Indeed, the incidence of type 2 diabetes among women with GDM is 20 to 50% within 5 years of giving birth. Currently, there are two main goals in the management of GDM: early diagnosis and prevention of complications through effective treatment. The existing diagnostic threshold is reported by the World Health Organisation to be 'somehow arbitrary', requiring further research to find ways of improving diagnosis and reduce the costs of treatment. People with GDM and the wider society will benefit from this line of research via improved accuracy and timing of diagnosis, effective personalised management and knowledge about self-management of GDM, ultimately easing the condition, preventing its complications, reducing negative impact on new born child, and reducing the numbers of type 2 diabetes cases due to GDM complications. This approach may also facilitate diagnostic interpretations in other areas of metabolic medicine, for example type 2 diabetes. Overall, this will improve quality of life, mitigate distress, reduce morbidity of patients with GDM, improve wellbeing of their friends and family and the wider community. 

Economy: GDM is a growing public health concern and is associated with overall increased healthcare costs. The prevalence of GDM is increasing alongside the prevalence of diabetes mellitus, with the total costs for NHS diabetes care being over &pound;1.5m an hour or 10% of the NHS budget for England and Wales. This equates to over &pound;25,000 spent on diabetes every minute. In total, an estimated &pound;14 billion is spent a year on treating diabetes and its complications. Figures for GDM are influenced by criteria used for diagnosis due to the fact that lower cut-off thresholds identify more women as having GDM, and by general obesity numbers and screening strategies. As is evident from research publications, modest improvements in glycaemic control and diagnostic strategy may generate a significant reduction in the cost of complications, at the same time as increasing the ability to participate productively within society, with significant savings to the UK economy. Improved GDM care also has great economic benefits through a reduced need for hospital admission due to hyper- and hypoglycaemia and costly procedures resulting from admission for GDM-related complications. There is also the potential for improved work productivity and contribution towards national GDP through avoidance of hypoglycaemia, which is essential for the quality of life and fitness to work of many individuals. 

Knowledge: This cross-disciplinary initiative will form a critical pathway to economic and social impact through advances in the exploitation of biomedical engineering approaches in healthcare. Two fundamentally novel aspects relevant to healthcare are: (1) quantitative knowledge, developed for the first time, to understand the variety of glucose variations in women developing GDM; (2) a novel personalised approach to early diagnosis and management of GDM, aiming to establish a long-term programme for clinical use that is tailored to individual patients' phenotypes and needs."
16,F39DFDC5-DB8F-424C-95FD-166B1B01C021,Turing AI Fellowship: Reinforcement Learning for Healthcare,"In this Turing Artificial Intelligence Acceleration Fellowship, I will focus on artificial intelligence for medical treatments and therapies. I take the view that AI is a question on how to realise artificial systems that solve practical problems currently requiring human intelligence to solve, such as those solved by clinicians, nurses and therapists. Critical care is high risk and highly invasive environment caring for the sickest patients at greatest risk of death. Patients within this environment are highly monitored, enabling sudden changes in physiology to be attended to immediately. In addition, this monitoring requires a heavier staffing ratio (often 1:1 nursing; 1:8 medical) and variances in human factors and non-technical pressures (e.g. staffing, skill-mix, finances) leads to critical care delivery being disparate. 

AI in healthcare is a hard problem as, due to the diversity and variability of human nature, systems have to cope with unexpected circumstances when solving perceptual, reasoning or planning problems. Crucially, AI has two facets: Understanding from data, and Agency. While rapid strides have been made on learning from data, e.g. how to make medical diagnosis more precise and faster than human experts, there is little work on how to carry on after the diagnosis, e.g. which therapy and treatment to conduct. The latter requires agency and has seen fewer applications as it is a harder problem to solve. 

My clinical partners and I want to develop the required AI algorithms that can learn and distil the best plan of action to treat a specific patient, from the expert knowledge of clinicians. We will focus on an area of AI called RL that has been successful in enabling robots and self-driving cars to learn a form of autonomous agency. We want to transform these methods into the healthcare domain. This will require the development of new RL algorithms, able to efficiently understand the state of a patient from noisy and ambiguous hospital data. The system will not only learn to recommend interventions such as prescribing drugs and changing dosages as needed per patient but to make these recommendations in a manner that is meaningful to the clinical decision-makers and helps them make the best final decision on a course of action. 

The methods developed as part of this project can be used in different applications beyond healthcare. Many sectors within industry, such as aerospace, or energy, deal with similar bottlenecks. These are highly regulated environments, with great need for decisions making support, but a scarcity of highly skilled human experts. With sufficient data, our methods can be applied to these sectors as well, to distil the required human expertise and best practices from top experts, and use them to drive decision making all over the sector, for increased efficiency and safety.",,
17,206B9547-E606-4CA6-B218-9C66BE5E64FE,EPSRC-SFI: SpheryStream,"This proposal addresses the important and not well-studied area in the fast-developing engineering space of the virtual reality and immersive technology. Our final vision is to endow any final user with an unprecedented sense of full immersion in virtual environments for any applicative scenario and with any connectivity level. This requires VR systems that operate at scale, in a personalized manner, remaining bandwidth-tolerant whilst meeting quality and latency criteria. This can be accomplished only by a fundamental revolution of the coding/streaming/rendering chain that has to put the interactive user at the heart of the system rather than at the end of the chain.
Specifically, bringing our final vision to reality requires constant innovation in VR streaming technology to overcome the following main challenges: i) the spherical format of the video content in a scenario in which media codecs are optimized for planar content; ii) the interactive behaviour of users that introduces uncertainty on the popularity of the video content. To overcome the former challenges, the spherical video is currently projected on a bidimensional planar domain, but this introduces noticeable deformation since the sphere is not a developable surface. To address the uncertainty of users' behaviours, predicting models have been investigated recently, but none of them actually work on the spherical (not deformed) domain. 

To overcome the above challenges in immersive technology, there is the need to develop an efficient tool for navigation patterns analysis in the spherical domain and leverage on that to predict users' behaviour and build the entire coding-delivery-rendering pipeline as a user- and geometry-centric system. This proposal focuses on develop novel VR delivery strategies, which will rely on the analysis and prediction of the navigation patterns of the omnidirectional content navigation, in a user, content- and application-dependent fashion. Predicting users' behaviour will allow to delivery only the content that will be actually consumed by the users, minimising the transmission resources consumption while maximising the user's Quality of Experience. The key novelty will be to address the above challenges working directly on the spherical domain, rather than on the projected (thus deformed) bidimensional domain. The developed VR system will be tested via a real omnidirectional video streaming testbed that will be implemented within the scope of the project. The outcomes of the project will be three-folds: i) user- content- and application-dependent VR streaming platform, optimized from the source codec to the delivery platform; ii) users analysis tools able to assess users' behaviour (when interactive with the content) in an objective fashion to identify common navigation patterns, and to predict future behaviours; iii) VR streaming testbed to test the proposed technology.",,"The project is focused on a significant technical challenge for twenty-first century society: how to ensure reliable and high-quality immersive communications for real-world use cases such as healthcare, creative technology, e-education, and high-risk missions. Specific impacts include:

1. People: the project will impact the science and engineering communities by training new researchers that are able to operate at the interface between multimedia systems, video processing, machine learning, and networking hence enhancing the pipeline of highly skilled people required to fuel the autonomy revolution.

2. Knowledge: the project will impact through knowledge creation at the cutting edge of the immersive and multimedia systems roadmap. The investigators have an excellent track record of dissemination and this will continue. In addition to high quality publications in leading international peer review journals and conferences, substantial effort will be dedicated to public communication around VR systems. In addition to high quality publications in leading international peer review journals and conferences, substantial effort will be dedicated to public communication around learning users' behaviour in interactive systems and about immersive communications. Both topics are of broad interest to the public and impact will be sought by: open access dissemination, interdisciplinary collaborations. 

3. Economy: Immersive technologies will deliver economic growth and transform the way we communicate, work and play with high impact across several sectors of strong national important, such as manufacturing technologies, creative technology, healthcare and education with a global market size reaching $150bn by 2020 in only the creative technology sector. The analysis recognises immersive's strategic importance and the value of immersive technology underpinning significant and pervasive economic benefits. It is well known that responsible innovation will be key to reaping such economic benefits. This project will deliver fundamental results that can be built into the design and implementation phase for immersive systems. This is essential underpinning work to develop the fundamental technology to enable any future immersive communication system to ensure the full economic benefits of immersive applications can be realised. 

3. Society: Robust results in developing a reliable communication platform for VR systems are essential to underpin public engagement in VR systems. The specific architectures that will be developed for VR systems will have societal impact in domains such as e-education, and cultural heritage. The work will also inform and contribute to standards bodies and Government policy on communication standards and policies for novel media systems."
18,965DA82F-A7AA-44BE-BE80-A6EC515817C5,Turing AI Fellowship: Machine Learning for Molecular Design,"Many existing challenges, from personalized health care to energy production and storage, require the design and manufacture of new molecules. However, identifying new molecules with desired properties is difficult and time-consuming. We aim at accelerating this process by exploiting advances in data availability, computing power, and AI. 

We will create generative models of molecules that operate by placing atoms in 3D space. These are more realistic and can produce better predictions than alternative approaches based on molecular graphs. Our models will guarantee that the generated molecules are synthetically accessible upfront. This will be achieved by mirroring realistic real-world processes for molecule generation where reactants are first selected, and then combined into more complex molecules via chemical reactions. Additionally, our methods will be reliable, by accounting for uncertainty in parameter estimation, and data-efficient, by jointly learning from different data sources.

Our contributions will have a broad impact on materials science, leading to more effective flow batteries, solar cell components, and organic light-emitting diodes. We will also contribute to accelerate the drug discovery process, leading to more economic and effective drugs that can significantly improve the health and lifestyle of millions.",,
19,0E1787A6-B3F0-4FAE-A61E-80F1AC46350B,Visitor Interaction and Machine Curation in the Virtual Liverpool Biennial,"This project looks at how audiences interact with machine-curated virtual exhibitions, specifically in the context of the 2020-2021 Virtual Liverpool Biennial. Using machine learning technologies as curators (rather than as, say, search engines) could potentially change the landscape of online exhibitions, which are currently largely websites with some pictures of artworks (and thus look more like exhibition catalogues than the exhibitions themselves). 

The project will look in particular at how different types of audience (e.g. local Liverpool residents who might not visit other biennials, vs people in the international contemporary arts scene who do the whole &quot;biennial circuit&quot;) interact and engage with the co-curated virtual biennial: looking especially at how their curatorial choices or preferences might differ. Finally, the project will look at the link between virtual exhibitions and the physical event; and point towards possible new hybrid (online and physical) models for biennials and other art exhibitions.",,
20,F88669EB-7634-4BF5-B0B4-7C9B94A96543,Turing AI Fellowship: clinAIcan - developing clinical applications of artificial intelligence for cancer,"Cancer is an evolving disease. No two cancers are ever exactly the same, no two cancer cells are even likely to be the same at the molecular level. 'Omics technologies allow us to measure the molecular activity of cancer to determine how it changes as the cancer develops. However, this is difficult to do with real patients as we only ever have access to a cancer only when it has developed and, once diagnosed, the cancer will be treated, perturbing it from its natural untreated trajectory. It is never possible to directly measure what the cancer was like before diagnosis, in its earliest stages of development, nor what would happen to the cancer if it was untreated or treated in a different way. 

In this project, we aim to develop artificial intelligence (AI) technologies that will allow us to describe how cancers evolve at the molecular level. We exploit the fact that cancer, whilst never exactly identical, they often share similar development trajectories which we can learn by collating information from across deep high-resolution molecular profiles of many cancers. As patients will never be diagnosed at exactly the same point of disease progression, each patient therefore occupies a unique point on the common disease trajectory. A collection of patients therefore should represent a continuum along these trajectories. AI can therefore help us to understand how cancers change over time by leveraging information from across many patients without us having to actually follow and observe cancers as they develop in individual patients.

In this research, we will develop models of cancer progression using a rich-body of modern AI techniques that we will make novel adaptations to enable their application to 'omics data. We will then use these technologies and work with a range of academic, industry and charity partners to identify prototypic applications of this research that might including helping to improve treatment decision making for cancer, provide patients with more detailed information about their disease and treatment options in an accessible way and to improve the efficiency and efficacy of cancer clinical trials.",,
21,D91B8C4B-38B1-4D9E-91BC-C7EA95F58FE4,"Turing AI Fellowship: Adaptive, Robust, and Resilient AI Systems for the FuturE","The Office for Artificial Intelligence (AI) estimates that AI could add &pound;232 billion to the UK economy by 2030, increasing productivity in some industries by 30%. However, to be truly transformational, the integration of AI throughout the global economy requires understanding and trust in the AI systems deployed. The super-human ability for decision-making in new AI systems requires huge volumes of data with thousands of variables, dependencies and uncertainties. Unregulated application of uncertified data-driven AI, limited by data bias and a lack of transparency, brings huge risks and necessitates a community-wide change. AI systems of the future must also be able to learn on-the-job to avoid becoming a high-interest credit card of huge technical debt. There is thus a timely and unmet need for a new theory and framework to enable the creation and analysis of data-driven AI systems that are adaptive, resilient, robust, explainable, and certifiable, with provable and practically relevant performance guarantees.

This ambitious fellowship, ARaISE, will deliver a radically new framework for the creation of beneficial data-driven AI systems advancing far beyond classical theories by including certifiable robustness and learning in the problem setting. These new theories will enable a formal understanding of the fundamental limits of large-scale data-driven AI, independent of the application area and learning algorithms. This will enable AI practitioners, through understanding such limitations, to influence policy and prevent incidents before they occur.

By connecting different and disparate areas of AI and Machine Learning, working with a world-class team of experts, and by engaging with stakeholders across strategic UK industries and sectors (Healthcare, Manufacturing, Space and Earth Observation, Smart Materials, and Security), ARaISE will create high-value, trustworthy, transformative and responsible AI, capable of reliably 'learning on-the-job' from humans to guarantee capability and trust. Novel human-centric AI, designed to function for the benefit of society, will complement and connect to existing work in the AI research arena, enabling co-development with project partners and focus on strategic industry challenges to ensure real-world relevance is built into research programme and its outputs, facilitating capacity and capability growth. ARaISE will generate gold standard tools for tasks that are currently heavily reliant upon human input and will support long-term global transformation.

Impact and knowledge exchange activities, embedded throughout this programme of work, will support uptake of developed novel AI systems and, through leadership and ambassadorial activities, will support a step-change in how AI systems are built and maintained to ensure resilient, robust, adaptive and trustworthy operation.

The inclusive research programme has been designed to support the career development of the project team and wider stakeholder group maximising the potential for flexible career paths whilst maintaining flexibility to creatively support the team to develop exciting new technology with real world relevance and guide future AI research.

The issues of AI and ethics underpin the programme with responsible research and innovation embedded throughout its activities. Raising public and AI practitioners' awareness, and ultimately influencing policy by active engagement with the UK and AI ethics expertise and policymakers, will ensure that the outcomes are socially beneficial, ethical, trusted and deployable in real world situations. Planned engagement with the ATI, CDTs, partners, and their networks, the development of new partnerships, methodologies and applications, will encourage links between these organisations, build UK expertise, skills and capacity in AI and contribute to realising government investment in UK Societal Challenges and ensure that the UK remains at the forefront of the AI revolution.",,
22,E7E352F6-BD91-4950-8366-0F5494415C31,Turing AI Fellowship: Advancing Multi-Agent Deep Reinforcement Learning for Sequential Decision Making in Real-World Applications,"Despite being far from having reached 'artificial general intelligence' - the broad and deep capability for a machine to comprehend our surroundings - progress has been made in the last few years towards a more specialised AI: the ability to effectively address well-defined, specific goals in a given environment, which is the kind of task-oriented intelligence that is part of many human jobs. Much of this progress has been enabled by deep reinforcement learning (DRL), one of the most promising and fast-growing areas within machine learning. 
In DRL, an autonomous decision maker - the &quot;agent&quot; - learns how to make optimal decisions that will eventually lead to reaching a final goal. DRL holds the promise of enabling autonomous systems to learn large repertoires of collaborative and adaptive behavioural skills without human intervention, with application in a range of settings from simple games to industrial process automation to modelling human learning and cognition. 

Many real-world applications are characterised by the interplay of multiple decision-makers that operate in the same shared-resources environment and need to accomplish goals cooperatively. For instance, some of the most advanced industrial multi-agent systems in the world today are assembly lines and warehouse management systems. Whether the agents are robots, autonomous vehicles or clinical decision-makers, there is a strong desire for and increasing commercial interest in these systems: they are attractive because they can operate on their own in the world, alongside humans, under realistic constraints (e.g. guided by only partial information and with limited communication bandwidth). This research programme will extend the DRL methodology to systems comprising of many interacting agents that must cooperatively achieve a common goal: multi-agent DRL, or MADRL.",,
23,E14CA242-0FCA-40AB-AE39-DA9893A6C59D,Turing AI Fellowship: AI for Intelligent Neurotechnology and Human-Machine Symbiosis,"Wearable neurotechnology utilization is expected to increase dramatically in the coming years, with applications in enabling movement-independent control and communication, rehabilitation, treating disease and improving health, recreation and sport among others. There are multiple driving forces:- continued advances in underlying science and technology; increasing demand for solutions to repair the nervous system; increase in the ageing population worldwide producing a need for solutions to age-related, neurodegenerative disorders, and &quot;assistive&quot; brain-computer interface (BCI) technologies; and commercial demand for nonmedical BCIs. There is a significant opportunity for the UK to lead in the development of AI-enabled neurotechnology R&amp;D.

There are a number of key challenges to be addressed, mainly associated with the complexity of signals measured from the brain. AI has the potential to revolutionise the neurotechnology industry and neurotechnology presents an excellent challenge for AI. This fellowship will build on the award-winning AI and neurotechnology research of the fellow and offer real potential for impact through established clinical partnerships and in the neurotechnology industry. 

The objective of this project is to build on award-winning AI and neurotechnology R&amp;D to address key shortcomings of neurotechnology that limit its widespread use and adoption using a range of key neural network technologies in a state-of-the-art framework for processing neural signals developed by the proposed fellow. 

The AI technologies developed for neurotechnology will be applied across sectors to demonstrate translational AI through engagement with at least 10 companies across at least 5 sectors during the fellowship, to demonstrate societal and economic benefit and interdisciplinary and translational AI skills development. The project has multiple industry, clinical and academic partners and is expected to produce world-leading AI technologies and propel the fellow to world-leading status in developing AI for neurotechnology which will impact widely. 

A major focus of the project is ensuring the expectations of the fellow role are met. This includes:-
-Ensuring the processes and resources are in place to build a world-leading profile by the end of the fellowship;
-Focusing on planning research of the team as new results emerge and hypothesis are tested, to refine and develop a high-quality programme of ambitious, novel and creative research, in AI-enabled Neurotechnology. Specific focus will be ensuring meticulous planning, execution and follow-up to produce world-leading results;
-Continuing to perform my leadership role as director of the ISRC and leader of the data analytics theme, expanding the team and actively seek to develop into a position of higher leadership of the research agenda at Ulster, and in the national and international research community;
-Focusing on strengthening relationships and collaborations with colleagues in industry and academia, and maximising the potential for flexible career paths for researchers within the team 
-Acting as an ambassador and advocate for AI, science and ED&amp;I including by continuing to actively provide opinions and engaging with questions around AI and ethics, and responsible research and innovation (RRI). A focus will be embedding this throughout the activities of the fellowship but across the region and internationally;
-Seeking to engage with and influence the strategic direction of the UK AI research and innovation landscape through engagement with their peers, policymakers, and other stakeholders including the public through.
-Ensuring that the fundamental research is developed to have a high likelihood of impact on UK society/economy through trials across a range of patient groups to develop the evidence base and transfer of intellectual property to products, in particular through NeuroCONCISE Ltd, a main project partner.",,
24,8AAF428C-ECB3-4F32-837E-31BE4376D8CF,Innovating Sema: Community-building of Live Coding Language Design and Performance with Machine Learning,"The Innovating Sema project will secure the long-term existence and use of our innovative live coding platform for machine learning. After an intense development process during the MIMIC grant (AH/R002657/1) we have developed a potential paradigm-shifting system that empowers users of programming languages for artistic expression to design their own languages for interfacing with machine learning. This has not been done before and through our user studies and workshops we are confident that our system works. Our project has the potential to demystify machine learning through hands-on use, thus generating understanding and trust in modern AI. It enables non-expert users to engage with creative AI. Now is the time to secure the longevity and wide use of the Sema system promoting understanding of computing through the two pillars of language design and machine learning.

The MIMIC project is a direct response to significant changes taking place in the domain of computing and the arts. Recent developments in Artificial Intelligence and Machine Learning lead to a revolution in how music and art is being created. However, these are early days concerning the integration of this new technology into software aimed at creatives. Due to the complexities of machine learning, and the lack of usable tools, such approaches have only been usable by experts. In order to study how we might bring these new fruitful technologies of new art making to the people, we created new, user-friendly technologies. They enable laypeople, both composers as well as amateur musicians, to understand and apply these new computational techniques in their own creative work.

The unexpected potential for impact we discovered during the development of Sema is twofold: 

1) We were going to create a high level live coding language for machine learning. However, responding to survey feedback, we decided to generalise and scale our approach by creating a system for *other people to design their own* live coding languages, using our language grammar compilation technology. This makes artists equally the creators of their own tools as well as the creators of artistic work with those tools. By designing and installing our system online and building a community that contributes to its development, we are effectively collaborating with users in filling a musical toolbox with tools and instruments. 

2) During the development of Sema, we become attuned to how distant machine learning is to the general public. People do not easily understand new AI and this technology is often seen as a negative force in society (from automation anxiety, filter bubbles, bias, and algorithms pushing people to the political extremes). Our project enables people to explore machine learning, and thereby gain an understanding of how machine learning and artificial neural networks are designed and trained, through hands-on experience. Music is an ideal platform to give people an understanding of machine learning as it is a collaborative, fun and risk-free environment for experimentation (unlike self driving cars or stock market algorithms).

By creating an online resource with our software (including language generator, live coding environment, machine learning architectures and models, and user contributed mini-languages) together with solid documentation and tutorials, we will secure the longevity and impact of our Sema project. Through an extensive user-engagement plan, we will create the conditions for an international community of users to grow independently of our own practice and research. This community will be initially formed by our own workshops, mailing lists, media appearances, social media communication, and dissemination through our own networks of colleagues who teach computer music internationally.",,
0,E3B8B43E-65B2-4D22-BDBF-7438435EC0EF,AEOLIAN (Artificial intelligence for cultural organisations),"How can we unlock &quot;dark&quot; digital archives closed to the public? What is the role of Artificial Intelligence (AI) in making digitised and born-digital cultural records more accessible to users, on both sides of the Atlantic? AEOLIAN (Artificial intelligence for cultural organisations) focuses on born-digital and digitised collections that are currently closed to researchers and other users due to privacy concerns, copyright and other issues. 

Archives are meant to be used, not locked away. In order to unlock cultural assets, we need to work across disciplines and harness the latest technology. AEOLIAN brings together Digital Humanists, Computer Scientists, archivists and other stakeholders to transform the access and use of born-digital and digitised collections which are currently hidden away. 

Analysing vast amounts of data cannot be done manually: automation is no longer a choice, it is a necessity. Artificial Intelligence can be used to improve access to non-confidential materials through sensitivity review, for example by distinguishing between personal and business emails. AEOLIAN aims to unlock born-digital and digitised collections and open them up to a large number of users. 

Access to digital archives is essential, but we also need to anticipate the moment when born-digital records will be more accessible. To make sense of this mass of data, new methodologies are urgently needed, combining traditional methods in the humanities with data-rich approaches. Collaborations between humanities scholars, computer scientists, archivists and other stakeholders are therefore essential to make archives more accessible, but also to design new methodologies to analyse huge amounts of data.

AI and machine learning create opportunities, but also challenges, for libraries, archives and museums. The project will address larger questions in the humanities - including ethical and social considerations at the centre of current debates on AI and digital technologies.

The AEOLIAN project will lead to the following research outputs:
_6 online workshops , which will result in the creation of an international network of theorists and practitioners working with born-digital and digitised archives. 
_5 case studies of US and UK cultural organisations . These case studies will feed into an open-access 100-page report for an interdisciplinary audience outlining avenues for future research.
_2 collections of essays published as special issue of journal or edited collection. 

The final report will offer a roadmap on born-digital and digitised cultural assets, based on 5 case studies of specific collections in the UK and US and detailed interviews. Crucially, it will also develop specific ideas for interdisciplinary research areas to solve the issue of access to digital cultural assets, which could form the basis of future research initiatives.

Archives are of course not reserved to academic researchers. The online workshops and the website will foster public engagement on the topic of the changing nature of archival collections (from print to digital) in the twenty-first century.

The website will keep track of all the project activities in the form of presentation materials from all workshop participants, video recordings of workshop presentations, and case studies that will then feed into the final report. Associated social media will help us connect with interested parties - in academia, archival institutions and beyond.",,
1,880D04D4-E364-4F32-A22A-D7B307EB99B6,Robust Multimodal Fusion For Low-Level Tasks,"There is a silent but steady revolution happening in all sectors of the economy, from agriculture through manufacturing to services. In virtually all activities in these sectors, processes are being constantly monitored and improved via data collection and analysis. While there has been tremendous progress in data collection through a panoply of new sensor technologies, data analysis has revealed to be a much more challenging task. Indeed, in many situations, the data generated by sensors often comes in quantities so large that most of it ends up being discarded. Also, many times, sensors collect different types of data about the same phenomenon, the so-called multimodal data. However, it is hard to determine how the different types of data relate to each other or, in particular, what one sensing modality tells about another sensing modality. 

In this project, we address the challenge of making sensing of multimodal data, that is, data that refers to the same phenomenon, but reveals different aspects from it and is usually presented in different formats. For example, several modalities can be used to diagnose cancer, including blood tests, imaging technologies like magnetic resonance (MR) and computed tomography (CT), genetic data, and family history information. Each of these modalities is typically insufficient to perform an accurate diagnosis but, when considered together, they usually lead to an undeniable conclusion.

Our departing point is the realization that different sensing modalities have different costs, where &quot;cost&quot; can be financial, refer to safety or societal issues, or both. For instance, in the above example of cancer diagnosis, CT imaging involves exposing patients to X-ray radiation which, ironically, can provoke cancer. MR imaging, on the other hand, exposes patients to strong magnetics fields, a procedure that is generally safe. A pertinent question is then whether we can perform both MR and CT imaging, but use a lower dose of radiation in CT (obtaining a poor-resolution CT) and, afterward, improve the resolution of CT by leveraging information from MR. This, of course, requires learning what type of information can be transferred between different modalities. Another example scenario is autonomous driving, in which sensors like radar, LiDAR, or infrared cameras, although much more expensive than conventional cameras, collect information that is critical to driving in safe conditions. In this case, is it possible to use cheaper, lower-resolution sensors and enhance them with information from conventional cameras? These examples also demonstrate that many of the scenarios in which we collect multimodal data also have robustness requirements, namely, precision of diagnosis in cancer detection and safety in autonomous driving. 

Our goal is then to develop data processing algorithms that effectively capture common information across multimodal data, leverage these structures to improve reconstruction, prediction, or classification of the costlier (or all) modalities, and are verifiable and robust. We do this by combining learning-based approaches with model-based approaches. Over the last years, learning-based approaches, namely deep learning methods, have reached unprecedented performance, and work by extracting information from large datasets. Unfortunately, they are vulnerable to so-called generalization errors, which occur when the data to which they are applied differs significantly from the data used in the learning process. On the other hand, model-based methods tend to be more robust, but have poorer performance in general. The approaches we propose to explore use learning-based techniques to determine correspondences across modalities, extracting relevant common information, and integrate that common information into model-based schemes. Their ultimate goal is to compensate cost and quality imbalances across the modalities while, at the same time, providing robustness and verifiability.",,"Several groups will benefit from this project's research, from academic communities and companies working in the healthcare and robotics sectors, to national defense and, ultimately, the general public. 

The participation in conferences and workshops like ICASSP, ICIP, ICML, BMVC, and AIP will ensure dissemination of the research outputs among the different academic communities in the areas of signal and image processing, machine learning and computer vision, and multimodal data processing. A more focused avenue for dissemination will be the organization of a small workshop at HWU. The theme of the workshop will be on multimodal signal processing, and we will invite two or three top researchers (national and international) in the area. To maximize its impact, we aim to make professional recordings of the talks freely available.

Although this project will develop fundamental signal processing tools, it has potential application in specific domains, for example, healthcare and autonomous robotic navigation. Two of our partners, Canon Medical Research Europe and SeeByte, are world-leading companies in translating research ideas into commercial products. Through joint supervision of students and regular meetings, these companies will help us identify the most promising research directions and applications of our algorithms and theory. The impact of these interactions, especially if it results in commercial products, can be significant. As mentioned in the case for support and pathways to impact, the PI is involved the UDRC Phase III project, which is funded both by Dstl (MoD) and EPSRC. The meetings associated to UDRC often take place at Dstl, and representatives of several defense companies (Thales, MBDA, Leonardo, BAE Systems, etc) are regular participants. This presents an ideal platform to advertise the research outputs of this proposal within the defense community and, thus, to maximize its national impact.

Finally, the wider public will also benefit from the research, not only indirectly via safer technology for automated healthcare, autonomous driving, and improved security systems, but also directly via outreach activities. With the help of undergraduate students at HWU, we will create demos and didactic videos related to the research, which will be presented at events like the Edinburgh Science Festival, secondary schools, and HWU open days. With these activities, we expect not only to improve the awareness of the general public on signal processing and its applications, but also to attract more students into this area."
2,4E8168FD-F5B3-4DB7-AD51-E4257B7CD076,"Theatre, A.I. and 'ludic technologies'","We propose a research network to investigate three related yet distinct fields of: contemporary theatre, artificial intelligence and ludic technologies. The main focus of the network will be to investigate these fields from the perspective of humanist discourses and artistic practice. We feel that this perspective has often been omitted in the current discussions and academic discourses on A.I. and thus it would be a timely consideration. We also feel that in our current post-digital age, there is scope, novelty and timeliness in an interdisciplinary research approach to these three fields, in particular when considering how contemporary artistic practices can question our relationship with newly emerging technologies. 

The focus and the originality of this network lies in the exploration of the way that these three disciplines intertwine and an exploration of A.I. through humanist and artistic practice perspectives. With the ever-increasing investments and developments in this area there is a strong case for an investigation of its cultural impact. Through this focus there is a potential to enable the development of a coherent theoretical framework through which the bringing together of theatre, A.I. and ludic technologies can be conceptualised. We also feel that many geographical areas such as mid-Wales have been underrepresented in the discussions surrounding A.I. in favour of a more globalised domain. Hence the network will enable the activation of more local discourses on these topics.
 
The network will bring together academics from the field of A.I. research, computer studies, game studies, arts and humanities, theatre and performance artists/practitioners with the aim of exploring the above research agenda. This will be achieved through three events (three symposia/workshops) running over the course of two years. Each symposium will consist of small conference-like proceedings where participants will share research papers and discuss their findings. Each proceeding will be followed by a workshop event led by an invited artist/practitioner/designer.",,"In this section we will briefly outline how the impact of this project will be achieved for three different stakeholder groups. For each group we will consider impact in terms of both short-term and long-term objectives. 

The first group of stakeholders will consist of practitioners in the field of contemporary theatre and performance, A.I. design and videogames design. The short-term impact will be achieved as follows. The research network will enable this group to discuss new ideas and directions for their practice in the context of theoretical framings and knowledge transfer espoused by the project. This will be achieved through three symposia. Each symposium will last for a day and will consist of two stages: a keynote and a presentation of papers by members of the group followed by discussions. The second stage will consist of performance events, conceptual discussions and workshops delivered by the invited practitioners. There will be long-term impact in building new relationships between practitioners and academic researchers, leading to potential future collaborations. This will be achieved through collaborations on the edited volume and future publications, a joint exhibition project of conceptual work generated by the network, future symposia and grant applications.

We identify the second group of stakeholders as potential external collaborators such as the Digital Catapult Innovation centre, National Theatre Wales and S4C. We will approach members of these organisations and invite them to participate in the network's activities in order to establish relationships and links that will be mutually beneficial. The short-term impact on this group will be achieved as follows. Digital Catapult would benefit from new critical perspectives and discussions of future directions in which A.I. will be implemented in artistic practice. NTW would benefit from insights into how A.I. and 'ludic technologies' can be implemented in contemporary performance and theatre practice. These insights would benefit and inform the programming agenda and strategy of NTW. In a similar vein this research network would also benefit S4C in terms of programming and future project involvement. The long-term impact will be achieved by forging links between academia, media industry and businesses which will lead to potential future collaborations on interdisciplinary projects.

The third group of stakeholders are the non-academic audiences. The short-term impact will be achieved as follows. Non-academic participants will have the opportunity to engage and participate in the organised workshops. There will be opportunities to spectate and also to explore new technologies and experience. External collaborators such as NTW and S4C have an extensive experience in organising and curating public-facing events and their involvement will contribute towards extending the social reach of the network. We can collaborate with their marketing teams in order to identify and target audiences who would be interested in the topics of the network. The long-term impact will be achieved as follows. The network will host an interactive website that will disseminate information about the project and hold video documentation of the events. Also, as a final event, we will organise a gallery exhibition of ideas and concepts generated by the network and produce a catalogue which will document this work (printed and online form)."
3,2271BBF9-41F5-48E4-8B01-5340846F69F3,GraphNEx: Graph Neural Networks for Explainable Artificial Intelligence,"GraphNEx will contribute a graph-based framework for developing inherently explainable AI. Unlike current AI systems that utilise complex networks to learn high-dimensional, abstract representations of data, GraphNEx embeds symbolic meaning within AI frameworks. We will combine semantic reasoning over knowledge bases with simple modular learning on new data observations, to adaptively evolve the graphical knowledge base. The core concept is to decompose the monolithic block of highly connected layers of deep learning architectures into many smaller, simpler units. Units perform a precise task with an interpretable output, associating a value or vector to nodes. Nodes will be connected depending on their (learned) similarity, correlation in the data or closeness in some space. We will employ the concepts and tools from graph signal processing and graph machine learning (e.g. Graph Neural Networks) to extrapolate semantic concepts and meaningful relationships from sub-graphs (concepts) within the knowledge base that can be used for semantic reasoning. By enforcing sparsity and domain-specific priors between concepts we will promote human interpretability. The integration of game-based user feedback will ensure that explanations (and therefore core mechanisms of the AI system) are understandable and relevant to humans. We will validate the GraphNEx framework, including both model performance and the performance of the explanations, with two application scenarios in which transparency and trust are critical: System Genetics to assist the discovery of clinically and biologically relevant concepts on large, multimodal genetic and genomic datasets; and Privacy Protection to safeguard personal information from unwanted non-essential inferences from multimedia data (images, video and audio) as well as to support informed consent. These applications, for which understanding the decision process is paramount for acquisition of new knowledge and for trust by the users, will demonstrate the utility of the GraphNEx framework to adapt to contexts where data are heterogeneous, incomplete, noisy and time-varying, providing new inherently explainable AI models, and a means to explain existing AI systems via post-hoc analyses.",,
4,5570C7CF-F469-487F-BD44-7DC4E14DD654,SAI: Social Explainable Artificial Intelligence,"The recent wave of machine-learning (ML) based Artificial-Intelligence (AI) technologies is having a huge societal and economic impact, with AI being (often silently) embedded in most of our everyday experiences (such as virtual assistants, tracking devices, social media, recommender systems). The research community (and society in general) has already realised that the current centralised approach to AI, whereby our personal data are centrally collected and processed through opaque ML systems (&quot;black-boxes&quot;), is not an acceptable and sustainable model in the long run. We posit that the &quot;next wave&quot; of ML-driven AI should be (i) human-centric, (ii) explainable, and (iii) more distributed and decentralised (i.e., not centrally controlled). These principles address the societal and ethical expectations for trustworthy, privacy-respectful AI, such as those recommended at the European Level (e.g., human agency, transparency, explainability included in the AI HLEG report on Ethics Guidelines for Trustworthy AI). They also fit a clear trend to develop decentralised ML for strictly technical reasons, e.g., performance, scalability, real-time constraints.
SAI will develop the scientific foundations for novel ML-based AI systems ensuring (i) individuation: in SAI each individual is associated with their own &quot;Personal AI Valet&quot; (PAIV), which acts as the individual's proxy in a complex ecosystem of interacting PAIVs; (ii) personalisation: PAIVs process individuals' data via explainable AI models tailored to the specific characteristics of their human twins; (iii) purposeful interaction: PAIVs interact with each other, to build global AI models and/or come up with collective decisions starting from the local (i.e., individual) models; (iv) human-centricity: novel AI algorithms and the interaction between PAIVs are driven by (quantifiable) models of the individual and social behaviour of their human users; (v) explainability: explainable ML techniques are extended through quantifiable human behavioural models and network science analysis to make both local and global AI models explainable-by-design.

The ultimate goal of SAI is to provide the foundational elements enabling a decentralised collective of explainable PAIVs to evolve local and global AI models, whose processes and decisions are transparent, explainable and tailored to the needs and constraints of individual users. We provide a concrete example of a SAI-enabled scenario.

To this end, the project will deliver (i) the PAIV, a personal digital platform, where every person can privately and safely integrate, store, and extract meaning from their own digital tracks, as well as interact with PAIVs of other users; (ii) human-centric local AI models; (iii) global, decentralised AI models, emerging from human-centric interactions between PAIVs; (iv) personalised explainability at the level of local and global AI models; and (v) concrete use cases to validate the SAI design principles, based on real datasets complemented, when needed, by synthetic datasets obtained from well-established models of human behaviour, in the areas of private traffic management, opinion diffusion/fake news detection in social media, and pandemic tracking and control.",,
5,DCE75DEF-4BB9-4547-946C-D77E56AD3142,Multi-disciplinary Use Cases for Convergent new Approaches to AI explainability,"Developing and testing methodologies that allow to interpret the predictions of the AI algorithms in terms of transparency, interpretability, and explainability has become today one of the most important open questions in AI. In this proposal we bring together researchers from different fields with complementary skills, essential to be able to understand the behaviour of the AI algorithms, that will be studied with an interesting set of multidisciplinary use-cases in which explainable AI can play a crucial role and that will be used to quantify strengths and highlight, and possible solve, weaknesses of the available explainable AI methods in different applicative contexts. One aspect hindering so far substantial progress towards explainability is the fact that several proposed solutions in explainable AI proved to be effective after being tailored to specific applications, and frequently not easily transferred to other domains. In this project, we will test the same array of techniques for explainability to use-cases intentionally chosen to be quite heterogeneous with respect to the types of data, learning tasks, scientific questions. The proposed use-cases range from High Energy Physics AI applications, to applied AI in medical imaging, to AI applied for the diagnosis of pulmonary, tracheal and nasal airways diseases, to machine-learning techniques of explainability used to improve analysis and modelling in neuroscience. For each use-case, the research project will consist of three phases. In the first part, we will apply state-of-the-art explainability techniques, properly chosen based on the requirements, to the case under consideration. In the second part, shortcomings of the techniques will be identified. Most notably, issues of scalability to high-dimensional and raw data, where noise can be prevalent compared to the signal of interest, will be taken into consideration, as long as the level of certifiability afforded by each algorithm. In the final phase, algorithms and knowledge built in each use-case will be combined in order to document the results and to develop general procedures and engineering pipelines useful for the exploitation of xAI methods in general and different domains.",,
6,2153EA24-4D18-4147-A9AA-97D8BDF6E9BF,CausalXRL: Causal eXplanations in Reinforcement Learning,"Deep reinforcement learning (RL) systems are approaching or surpassing human-level performance in specific domains, from games to decision support to continuous control, albeit in non-critical environments, and usually learning via random explorations. Despite these prodigious achievements, many applications cannot be considered today because we need to understand and explain how these AI systems make their decisions before letting them interact with, and possibly impact on, human beings and society.
 
There are two main obstacles for AI agents to explain their decisions: they have to be able to provide it at a level human beings can understand, and they have to deal with causal relations rather than statistical correlations. Hence, we believe the key to explainable AI, in particular for decision support, is to build or learn causal models of the system being intervened upon. Thus, instead of standard machine learning and reinforcement learning networks, we will leverage the new science of causal inference to equip deep RL systems with the ability to learn, plan with, and justifiably explore cause-effect relationships in their environment. RL systems based on this novel CausalXRL architecture will provide cause-effect and counterfactual justifications for their suggested actions, allowing them to fulfill the right to an explanation in human-centric environments.
 
We will implement the CausalXRL architecture as a bio-plausible (neuromorphic) algorithm to enable its deployment in resource-limited, e.g., mobile, environments. We will demonstrate the broad applicability and impact of CausalXRL on several use cases, ranging from neuro-rehabilitation to intensive care, farming and education.",,
7,AB177CB1-D088-479B-BDDC-961B714E2411,Digital approaches to the capture and analysis of watermarks using the manuscripts of Isaac Newton as a test case,"This project will investigate two research areas with general application in digital humanities scholarship, using the dispersed manuscript corpus of Isaac Newton as a test case. The immediate purpose of the test case will be to use artificial intelligence to assist with the identification and classification of watermarks in Newton material and, in the process, to build a general tool to assist with the organisation and dating of manuscripts. The project also has much wider significance. The project's first stage will be the methodological investigation of techniques for the production of images of watermarks which are suitable for automated analysis, using both new photography and the exploration of the potential latent in existing images. During the second stage, we will develop computer vision methods to systematically cluster and match the assembled corpus of watermark images across manuscripts and collections. Methods developed through this project will be transferrable to watermark collections beyond that of Newton's corpus, creating a methodology for scholars seeking to analyse, date, and organise historical collections via watermark matching, and for conservators seeking to establish standardised surveying and documentation methods while imaging and digitising watermarked documents. A final stage of the project will allow us to disseminate our findings through research workshops, web tools, and improvements to online databases, as well as traditional publications in journals.
Since the groundbreaking early twentieth-century research of Charles Mo&iuml;se Briquet, watermarks have formed a central part in the dating of otherwise undated manuscripts. Briquet's monumental 1907 catalogue, Les filigranes, made it possible, in principle, to date (and to some extent localise) pre-1600 watermarks found by researchers in manuscripts by reference to exemplars in Briquet's catalogue. While this catalogue and others have been digitised thanks to the Bernstein consortium (https://memoryofpaper.eu/), advances in research and technology have revealed the limitations of the traditional approach, which requires time-consuming procedures and some degree of expertise for the identification of each single watermark. It is very difficult to find exact matches between watermarks in situ and those reproduced in any catalogue, first due to the limited comprehensiveness of the catalogues, and, second, because each individual watermark is produced in two &quot;twin&quot; versions, never perfectly identical, and suffers deformation over time as a result of repeated use in the paper manufacturing process. By developing and enhancing new approaches and techniques to improve the acquisition and analysis of watermarks, we hope to solve basic problems and thereby provide benefit to all who must rely upon paper documents for chronological evidence. 
While computer vision has made significant progress in recent years thanks to machine learning and artificial intelligence, this project will build on cutting-edge work already undertaken by the Ecole Nationale des Chartes and its partners (notably the computer scientists at &Eacute;cole des Ponts ParisTech) to investigate the problem of matching images, specifically of watermarks, across formats (photographs and tracings). In creating a corpus of images used to train and develop the open source software created by the Ecole des Chartes we will build on recent work by The National Archives (TNA) to use comparatively affordable equipment and techniques to produce images of watermarks that are highly suitable for machine analysis. The project will develop and apply both of these approaches in order to attempt to enhance the computer-vision software so that it may be able to unlock the latent information held in thousands of existing images shot in reflected light which institutions have already digitised and made accessible through IIIF.",,
8,674BEA11-F2BF-4048-B697-EF4204AD6FCD,From Lima to Canton and Beyond: An AI-aided heritage materials research platform for studying globalisation through art,"Inspired by the Enlightenment, from the late 18th century, the European colonial powers such as Britain and Spain, and local officials in their colonial dependencies, were collecting information from around the world. Maps and charts, as well as scientific drawings of flora and fauna, were commissioned, and local artists were often employed to draw and paint these. Several Spanish scientific expeditions, such as the Royal Botanical Expedition to the Viceroyalty of New Granada (Ecuador, Colombia, and Venezuela today), led by Jos&eacute; Celestino Mutis from 1783 to 1816, among others, hired local artists, such as Francisco Javier Cort&eacute;s to provide the illustrations.

Botanical paintings following European conventions of scientific drawings were also commissioned from Chinese export artists in Canton (Guangzhou, China) by the Royal Horticultural Society and the Royal Botanical Gardens in London and painted on Whatman papers sent from London. Similarly, during this period, the East Indian Company was actively employing Chinese and Indian artists to paint flora and fauna in South East Asia and India respectively. These cultural encounters have often resulted in hybrid artistic practices. 

By about 1818, in the context of the coming of Peruvian independence, watercolours of Peruvian subjects are documented in Lima. The earliest are associated with Cort&eacute;s, soon to be joined by his presumed pupil, Francisco &quot;Pancho&quot; Fierro in the 1820s. By the 1830s, watercolours by Fierro, his followers, and imitators became widespread, and were eventually produced in the thousands, with the phenomenon tapering off around 1850-60. Similar ethnographic drawings are found in Colombia and Ecuador. In the meantime, trade in Chinese export painting, depicting daily life of locals, from Canton flourished in the late 18th to the 19th century. Costume paintings of Peruvian types are also found in albums with provenances and artistic styles suggesting that they were made in studios in Canton. These works produced in north-western South America and in Canton from 1780 to 1850 are connected to a complex web of social, political, artistic, geographic, economic, and technological phenomena, all of which affected the motives for their creation, the materials from which they were made, the means of their dispersal and preservation, and the lives of the people who made, sold, bought, and collected them. 

This project will focus on a large group of ethnographic as well as selected scientific watercolours (e.g. maps and botanical drawings), made for export to Europe or North America by local artists in north-western Latin America and Asia. These are now found in widely dispersed public collections in the US and UK. One of the project goals is to use the study as a lens to reveal details of global trade and information exchange networks among the Americas, Asia and Europe ca. 1780-1850. Pigments, dyes and paper are commodities that were traded extensively throughout history; their identity and the way they are used are often traceable to their geographic and cultural origins. This period also saw the synthesis of new pigments, especially in Europe, making it easier to date an object using these pigments. 

Advanced imaging and material analysis techniques, used by heritage scientists and conservators to detect, identify and understand the composition of artworks/heritage items, create large data sets that require expert processing and interpretation (hence, creating a barrier to entry and use by non-scientists). This project aims to streamline the data collection and interpretation processes and open the results to researchers and audiences in the humanities by (1) advancing an AI-assisted method of data analysis, (2) providing an online, linked open data platform for the results and their interpretation, and (3) demonstrating the impact of the collected and interpreted data for humanities research in this large scale humanities-led project.",,
9,6A918CCA-899A-491B-A5FA-EE6040CB8E6E,Unlocking the Colonial Archive: Harnessing Artificial Intelligence for Indigenous and Spanish American Historical Collections,"The Spanish empire controlled the vast majority of the western hemisphere's lands and peoples for more than three centuries. Its vast administration in the Americas depended on the work of royal notaries, Indigenous artists, and printers. They produced prodigious amounts of documents, written or printed on paper, which fill archives and libraries today. Despite the extensive documentation, present-day understanding of the Spanish colonial enterprise is fragmentary. Once the initial barrier of archival access has been overcome, scholars and other publics then must decipher archaic penmanship, obscure writing conventions, and unfamiliar Indigenous imagery. This project seeks to lower these barriers by introducing artificial intelligence (AI) technologies into representative Indigenous and Spanish colonial archives in Mexico and the U.S., and training them to convert the &quot;unreadable&quot; archive into worldwide accessible data. The project has the potential to revolutionize how cultural institutions provide access to their colonial collections and how humanities researchers can undertake cutting-edge digital scholarship. 

In a highly interdisciplinary collaboration between archaeologists, historians, web scientists, designers, and computer scientists, the &quot;Unlocking the Colonial Archive&quot; project will create a step-change in the way a broad spectrum of researchers and the public engage with and use countless early modern Indigenous and Spanish collections dispersed throughout the world. Using machine learning and the exceptional collections of the LLILAS Benson library (US) and the General Archive of the Nation (Mexico), the project will tackle three challenges in interconnected research areas to: (a) accomplish the automated transcription of 16th- and 17th-century historical colonial documents that combine Spanish with Indigenous languages such as Nahuatl, Mixtec, Huastec, and Otomi, among others; (b) develop methods to carry out text mining in large historical collections; and (c) develop techniques to facilitate the automated identification of iconographic and other pictorial features in Indigenous maps and printed books. 

The development of such approaches will not only facilitate the searching, retrieval, and reading of these materials, but will also transform the accessibility and analysis of large textual and image collections. With a strong commitment to a decolonial approach, both in terms of archival practices and in the critical use of technologies, the project will create freely available, enhanced open digital collections. As such, &quot;Unlocking the Colonial Archive&quot; will work in close partnership with Mexican, UK, US, Portuguese, and Spanish researchers and institutions, training scholars and interested members of the public on transferable skills and digital methods, and it will produce innovative, reproducible workflows that Latin American scholars and cultural institutions around the world can adopt and implement.",,
10,C31D56B3-9A23-4AD9-BD85-752248E3D09F,Northwest European Seasonal Weather Prediction from Complex Systems Modelling,"The atmospheric circulation and jet stream (giant current of air) over the North Atlantic strongly influence seasonal weather conditions over Northwest Europe. Recent extreme seasons have been characterised by distinctive jet stream patterns, and jet strength and location is closely linked with extreme weather conditions experienced across the UK and Northwest Europe. Seasonal weather characteristics have major effects on people's livelihoods and the economy, for example about &pound;1.5 billion in the UK in winter 2013/14, so producing reliable seasonal forecasts some months ahead would have significant benefits for society. Seasonal weather conditions also have major impacts on agriculture, food security, energy supply, public health/wellbeing, and severe weather planning. 

Until recently, North Atlantic atmospheric variability was thought to be largely due to unpredictable fluctuations. However, dynamical (that is, physics-based) seasonal forecasting systems run on giant supercomputers have led to some recent advances in forecasting skill, mainly for winter forecasts. Many factors appear to influence North Atlantic atmospheric circulation and jet-stream changes; possible influences can be broadly grouped into effects from variations in sea-ice extent and snow cover, North Atlantic sea-surface temperature variations, tropical influences such as the El-Ni&ntilde;o Southern Oscillation, changes in the higher atmosphere (stratosphere) circulation, changes in energy from the Sun, and volcanic eruptions. These drivers of jet stream variability can oppose or reinforce one another, and there are indications of interactions between them. Drivers of jet-stream variability show seasonal variation, and distinctive drivers of jet-stream variability operate in different seasons. While some observed drivers can be reproduced in computer models of the climate system, improved understanding of more recently identified drivers of the North Atlantic jet stream is crucial for making progress in Northwest Europe seasonal climate predictions.

The focus of government-funded research is on dynamical forecast systems; however, such forecasts are not always accurate. Furthermore, despite recent efforts to assess and improve their performance, dynamical model forecasts show little skill in summer. In the mid latitudes, including the UK and Northwest Europe, statistical forecasting has been neglected; however, recent developments in advanced statistical techniques, under the umbrella of 'machine learning', have taken place outside the climate-science community and are relatively quick and cheap to implement. There is thus considerable scope for applying complex statistical methods to the seasonal forecasting problem. Using a novel application of an established complex systems modelling approach called NARMAX (a type of machine learning, the results of which are highly interpretable), this project seeks to significantly improve current seasonal forecasts, extend skillful seasonal forecasting to seasons beyond winter, identify factors that contribute skill to the forecast, develop seasonal forecasts for Northwest Europe on a regional basis, and assess the benefits of skillful probabilistic seasonal forecasts to interested end users such as the agri-food industry. Our project plan effectively builds on promising pilot study results that we have recently published in the Quarterly Journal of the Royal Meteorological Society. Our novel application of NARMAX is likely to significantly improve forecast skill and help to inform development of the next generation of dynamical seasonal forecasting systems. 

We also seek to engage end users of seasonal forecasts, focusing mainly on the effects of improved seasonal forecasts on the agri-food industry: reflecting our links in this field but also because it has been relatively little studied compared with other key areas.",,"Ways in which potential beneficiaries may make use of the research

1. Meteorological services
We expect that meteorological/climatological services including the UK Met Office, European Centre for Medium-Range Weather Forecasts (ECMWF), US National Atmospheric &amp; Oceanic Administration (NOAA), Icelandic Met Office (IMO), Danish Meteorological Institute (DMI) and Finnish Meteorological Institute (FMI), with whom we have a track record of successful collaboration (and continue to collaborate with most of the above on existing projects), will benefit from the improved understanding of Northwest European seasonal weather prediction to be gained through the research. Prof. Hanna and Dr. Hall collaborated on a University of Sheffield Project Sunshine research project with our Project Partner Prof. Adam Scaife (Head of Monthly to Decadal Prediction at the Met Office), who - with the potential of improving Met Office seasonal forecasts - writes in strong support of the proposal. We also have a direct link to the ECMWF through Co-I Dr. Weisheimer. Improving Northwest Europe seasonal weather forecasts will also help inform on the risk of occurrence of related extreme weather events (WP1.2). World Meteorological Organization reports show that mitigating mid-latitude extreme weather events - that are largely caused by jet-stream fluctuations which will be better understood through this project - is worth many billions of dollars.

2. End users of seasonal prediction
An important impact is the economic benefits of improved UK seasonal weather forecasts. Here we will focus on the agri-food industry, which we expect will benefit significantly from our improved seasonal forecasts. Cost-loss models assess the costs to decision-makers of taking action to protect against an adverse weather event, evaluated against losses that would be incurred if no action is taken (the cost-loss ratio). The probabilistic nature of the forecasts mean that users have different probability thresholds at which action becomes worthwhile in response to a forecast: we will work with agri-food partners (G Growers, Berry Gardens), Sainsbury's , the Environment Agency (EA), Centre for Ecology &amp; Hydrology and the Met Office to (i) provide an analysis of these thresholds and (ii) assess economic value against the cost-loss ratio.

3. General public
Our study topic has a huge latent interest amongst the wider population. Seasonal forecasts are always of widespread interest, while the jet stream is a recently popular household buzzword, following much-publicised extreme weather events of the last decade. There is great scope to present our results to the general public to take opportunity of this interest as public impacts of highly variable seasons and extreme weather events continue to appear regularly in the media."
11,26FD641F-7F84-4B39-8A71-A4F1FA23DF7B,Natural Language Generation for Low-resource Domains,"It is expected that by 2021, Artificial Intelligence (AI) based dialogue systems such as Amazon's Alexa and Apple's Siri will exceed the earth's population [1]. Such interactive technology products have already become prevalent in many aspects of everyday life, offering support for decision making, education, and health as well as entertainment, by effectively communicating in natural language to answer questions, describe or summarise data, and assist in multiple areas. To develop such systems, however, AI requires access to vast amounts of examples of dialogues, which can (1) be hard to attain in many domains due to unavailability; and (2) pose privacy concerns, impacting user uptake [2]. Current response generation techniques are heavily based on pre-specified templates that limit language coverage. Generating naturally fluent responses is heavily dependant on example dialogues, that are scarcely available in many domains. To address these interlinked challenges, the project will firstly develop natural language generation techniques that are able to learn from limited resources by reusing the knowledge learnt in other data-rich domains, similar to the way the human brain learns new skills efficiently by building on prior knowledge. Secondly, we will develop novel privacy-preserving AI methods to address the second important challenge, and eliminate the risk for de-anonymisation of data. 

Although recent advances in understanding natural language have made it possible to accurately predict the meaning of users' utterances and hence accurately inform the personal assistants' actions, responding in natural language remains a bottleneck for the current generation of dialogue systems and personal assistants. As more interactive systems generating natural language become available, the need for natural variability and novelty in the generated text becomes significant in order to increase end-user satisfaction and engagement. Therefore the project will also develop AI approaches that generate text that shows novelty and variability for enriching the word choice while keeping the semantics of the generated text unchanged. Finally, many real-world applications such as personal assistants (and also chatbots and social robots) that support health or education, will benefit from generated responses that show empathy and adapt to users' psychological state. This requires a deep understanding of emotions from text, therefore, this project will, for the first time, develop and integrate innovative, natural language 'concept' based approaches, to understand user emotions from underlying text, and inform novel text generation approaches. Practical case studies provided by our industrial partners will be used to validate our developed AI approaches, throughout this ambitious project.

References:
[1] https://ovum.informa.com/resources/product-content/virtual-digital-assistants-to-overtake-world-population-by-2021 
[2] https://www.independent.co.uk/life-style/gadgets-and-tech/news/amazon-alexa-echo-listening-spy-security-a8865056.html",,"This multi-disciplinary project will have impacts beyond academia. 

*Privacy-preserving personal assistants/NLG systems*

As the use of personal assistants grows globally, the need for privacy-preserving approaches to NLG increases. Most industries handle sensitive information such as personal and private data and although data scientists strive to anonymise data records, sophisticated de-anonymisation approaches can pose a threat not only to privacy but also current legislation. Therefore, the impact of innovative approaches that respect privacy and ethical constraints will be enormous.

*Increase productivity by automating the descriptions of products*

Additionally, industries that rely on online presence will benefit from approaches that automatically generate text summaries from structured data, such as descriptions of products and services, since these approaches can increase productivity by automating repetitive and laborious tasks. In addition, diversity-enriched NLG approaches can make the content of automatically generated summaries more interesting and less repetitive, and hence increase the overall user experience.

*Automatic narrative and report generation*

Narrative generation from data, such as automatic news story generation, will benefit from more natural NLG approaches that offer variability and empathy as stories can be enriched with emotion, and interesting non-repetitive text. Business intelligence and analytics reporting will also benefit from the approaches developed here, as privacy is integral when communicating data and insights. In addition, NLG has been shown to enhance decision making support [5].

*Support Health and Well-being*

AI-powered personal assistants, such as the Alli-chat developed by our project partner, have started to become prevalent in health support [1]. It can also be preferable for supporting specific groups such as younger people for stigma-attached conditions such as mental health. Younger people are particularly less likely to seek help when facing mental health challenges [4], therefore, it is of vital importance to create a safe space for younger people that empowers them to seek private advice and information regarding mental well-being which can be achieved through trusted, privacy-sensitive, empathy-enriched personal assistants. Promotion of mental well-being, management, and prevention of mental health illnesses has been indeed identified as a core priority of the World Health Organisation's mental health action plan 2013-2020 [2] as well as in NHS's &quot;Five Years Forward View&quot; for mental health [3].

References: 
[1] https://www.healthcareitnews.com/news/special-report-ai-voice-assistants-making-impact-healthcare
[2] https://www.who.int/mental_health/publications/action_plan/en/
[3] https://www.england.nhs.uk/wp-content/uploads/2014/10/5yfv-web.pdf
[4] Marcus, M. A., Westra, H. A., Eastwood, J. D., Barnes, K. L., &amp; Mobilizing Minds Research Group (2012). What are young adults saying about mental health? An analysis of Internet blogs. Journal of medical Internet research, 14(1), e17. doi:10.2196/jmir.1868
[5] Gkatzia et al. (2017). Data-to-Text Generation Improves Decision-Making Under Uncertainty. IEEE Computational Intelligence Magazine, Special Issue on Natural Language Generation with Computational Intelligence."
12,8E385200-519F-46E3-80C6-C2318A4E3A06,REcoVER: Learning algorithms for REsilient and VErsatile Robots,"Robots have the potential to deliver tremendous benefits to our society by assisting us in all aspects of our everyday life. For example, they could increase the quality of life of elderly people by allowing them to stay longer at home on their own, through preparing meals, cleaning the house, and assisting them to get dressed. However, robots such as legged robots are also very complex machines, which are highly prone to damage when they are not operating in the well-controlled environments of factories. Moreover, because of this complexity and the large variety of environments they might encounter, it impossible for engineers to anticipate all the damage situations that the robot may encounter and to program its reactions accordingly. 

A promising approach to overcome this difficulty is to enable robots to learn on their own how to face and how to respond to the different situations they encounter. This approach shares similarities with the way humans and animals react in analogous circumstances. For instance, a child with a sprained ankle learns on his own how to walk with only one foot in order to minimise the pain. The objective of this research project is to develop the algorithmic foundations that allow robots to do the same. In previous works, we have developed creative learning algorithms that enable (physical) legged robots to overcome the loss of a leg by learning how to walk forward in less than two minutes. However, in these works, the algorithms were configured to solve a single task (i.e., walking forward), which does not leverage the versatility of legged robots and their capability, for instance, to walk in every direction, to jump, and to crawl.

The ambition of this project is to extend the adaptation capabilities of our algorithms to the entire range of the robots' abilities. This will be achieved by employing recent advances in hierarchical reinforcement learning to transfer knowledge during the adaptation process across the different skills of the robots. The combination of these hierarchical skill repertoires with our online-adaptation algorithms will enable robots to quickly transfer the result of their adaptation on one skill to the other skills. For instance, after finding a new way to walk forward, a robot might have discovered that it cannot rely on its front-left leg. With the proposed project, this information will be automatically used by the robot to speed-up the adaptation process when it will try, for instance, to learn to turn by avoiding to use the front-left leg too. In addition to damage recovery, the same algorithm will enable robots to adapt from changes in their environment, for instance by changing their behaviours depending on whether they walk on flat concrete floor or on sloping grassy ground. 

Increasing the adaptation capabilities of versatile robots aims in the long term to enable the use of robots to substitutes humans in the most dangerous task they have to perform. For instance, thanks to robots with improved adaptation abilities, it would be possible to send robots searching for survivors after an earthquake or to operate in a nuclear plant after a disaster. Improving the ability of robots to overcome unknown situations is one of the key requirements to enable them to be a significant part of our daily life. 

This research will be undertaken at Imperial College London, in the department of computing. The project will benefit from state of the art robotic facilities, including a quadruped robot, a hexapod robot and a motion capture system, to develop and experiment a new generation of learning algorithms for resilient robots.",,
13,4A272666-EB2B-41A0-AFFE-CE2F154024B3,ClearSky: cloud-free monitoring of UK agriculture,"Earth Observation satellites can monitor every part of the globe, offering a detailed view of the Earth's surface on a daily basis. This is a valuable source of intelligence for the agricultural industry, since one can - in theory - map all farmland to determine, for example, the rate of growth of crops, signatures of crop stress and disease, the presence of flooding or signs of drought. These factors are vital to the efficient production of food and maintaining food security. On one hand, a precise view of crop health allows farmers to optimise the use of fertiliser and water, and the rapid identification of threats allows food producers to respond in a timely manner to mitigate their impact on the food supply.

There is a problem: cloud cover. Clouds obscure the view of Earth Observation satellites in the visible and infrared spectral range, and this is where the vast majority of remote sensing analysis is performed. This limits the value of imagery, because if it is cloudy when the satellite passes over, the intelligence is lost. 

Our solution is to exploit radar data. Satellites equipped with radar can reflect radio waves off the ground and detect their reflectance. Since radio waves are not affected by cloud, this offers an uninterrupted and reliable view of the ground. However, the challenge has been in the interpretation of radar data. We have solved this challenge by developing an algorithm that can accurately predict what would have been seen in the traditional V/IR images (e.g. the RGB image our eyes would see) for cases where the surface is obscured by cloud. We call our algorithm ClearSky.

In this project we will use ClearSky to monitor every single field in the UK, every week. Our predictions will allow us to assess the presence and density of vegetation and its rate of growth, as well as determining local threats such as flooding and the onset of drought. Our aim is to deliver this intelligence direct to farmers and the food supply industry in order to aid the decision making process with regards the most efficient use of resources in producing food, and enabling us to be agile to potential threats to food security associated with a changing climate.",,
14,5980DE09-A90C-44D1-AB80-52E76154B02B,The Dickens Code,"Shorthand was an important part of Dickens's toolkit as a writer, but although he used it extensively for parliamentary reporting, letter writing, and note taking, little is known about how he did so. The unique system that he developed, based upon Gurney's *Brachygraphy*, is complex and puzzling; Dickens himself called it a 'savage stenographic mystery'.
There are at least 10 known manuscripts of Dickens's shorthand, dating from the 1830s to the late 1860s. These manuscripts are located in 6 archives across the world, as well as 2 private collections. Several manuscripts remain undeciphered, including a letter from the 1850s and a set of shorthand booklets collected by Dickens's shorthand pupil, Arthur Stone. These booklets, totalling c.70 pages, include 6 undeciphered shorthand dictation exercises of 1-2 pages each. Dickens's shorthand has proved extremely difficult to decode and, in most cases, experts have been unable to locate the source texts used for the exercises. They could be published or unpublished passages written by Dickens, or by another author. The mystery of these undeciphered texts is as compelling for the public as it is for academics and the sesquicentenary of Dickens's death in 2020 provides an ideal opportunity to harness wider interest in solving the 'Dickens Code'.
The material is novel in its own right and the task of deciphering it provides a test case with implications far beyond Dickens Studies. An approach that combines machine learning's power to identify patterns across datasets with contextual interpretation by volunteers is likeliest to succeed. However, the limited corpus and idiosyncratic nature of Dickens's shorthand creates barriers to machine learning methods, while the complexity of the material places additional demands upon the human interpreter.
Tackling these challenges offers a template for approaching similarly complex decoding problems (e.g. the ancient shorthand of the Vindolanda tablets), where human expertise and technology have to work hand-in-hand. However, to understand these challenges and identify potential solutions, the 'Dickens Code' problem needs to be viewed in the round. Accordingly, this project will convene a network that draws expertise from different disciplinary areas (Dickens Studies, Digital Humanities, Forensic Linguistics, and Informatics) and stakeholder groups (museums and archives).
The network will meet at 2 symposia-'Digital Humanities and the Dickens Code' (Leicester, May 2020) and 'Textual Mysteries and Crowdsourced Solutions' (BSR, October 2020)-with discussion sustained between meetings via a Jiscmail list and blog. Network-generated insights will shape outputs, including an online exhibition of Dickens's shorthand. Additional targeted engagement of key user groups (primary school workshops; PG mini-Hackathon; Dickens Universe workshop) will ensure that outputs such as the 'Cracking the Code' resource, decoding games, and Zooniverse pilot are fit for purpose. The network's findings will also be collated and shared via a journal article, conference paper, blogs, podcasts, report for the *Dickensian*, 'top tips' toolkit, and social media.
In Dickens Studies, enhanced understanding of the author's shorthand will lead to internationally significant insights about Dickens's creative process. For Informatics, the 'Dickens Code' may generate modified or novel approaches to handwritten coded material with broader applicability. In Digital Humanities, the 'Dickens Code' provides a template for engaging users with 'difficult' content. Beyond the Academy, increased public awareness of Dickens's shorthand will bring to light a little-known aspect of the life of one of the world's most famous authors. Throughout his career, Dickens sought to cultivate a close relationship with his readers; 150 years on, the 'Dickens Code' seeks to revitalise this connection, by enabling academics and non-academics to work together to uncover Dickens's last unknown texts.",,"Museums
Our confirmed museum partners and participants will allow items from their collections to be digitised, supporting preservation of the original manuscripts by providing a high-quality facsimile. By making items available to wider audiences, the online exhibition will boost the profile of all their collections. This is because these museums contain other items of Dickens material to which the public can be linked 'stenographically' via the exhibition: to Carlton's stenographic papers in the Dickens Museum, to the Forster Collection at the V&amp;A, to Dickens's correspondence at the John Rylands Library and Free Library, and to his letters to Macready at the Morgan Library. The 'Dickens Code' is also complementary to the V&amp;A's 'Deciphering Dickens' project, which explores Dickens's handwritten manuscripts, and this synergy will enhance engagement with both projects.
The 'Cracking the Code' resource developed for primary schools will be shared with curators and education officers at each institution, so that it can be adapted for use as part of museum-based decoding workshops, complementing existing programmes of activity planned for Dickens's sesquicentenary year.
Key learning about the presentation of dense, coded material online may also be of interest to museums beyond the network. Ongoing learning linked to this topic will be captured in a blog and form part of the journal article.

Primary and high school students
The PI will work with Year 5 and Year 6 primary school teachers and pupils (aged 9-11) to develop and pilot a decoding workshop for this age group. This will be linked to learning objectives in the Key Stage 2 English curriculum, as well as the requirement to engage pupils with authors from Britain's literary heritage. National Curriculum guidance for primary schools positions 'decoding' (using phonics to work out the pronunciation of unfamiliar words) as a key skill at KS2. Age-appropriate deciphering of *Brachygraphy*'s consonant symbols, which are easily relatable in class to modern text messages, will increase children's awareness of prefixes, suffixes, and the role of morphological structure in word formation and how they contribute to word/sentence meaning. Pupils will benefit from applying comprehension skills in a novel context, while also being inspired by the 'mystery' aspect of Dickens's shorthand-and contributing to its resolution.
The workshops will be delivered to 3 local schools in July 2020, identified through liaison with UoL's central outreach team. Evaluation of these workshops will feed into a downloadable resource, developed iteratively in collaboration with teachers at participating schools. This resource will be disseminated via the 'Dickens Code' website and promoted through existing links with the English Association to maximise uptake.
The Co-I will work with high school teachers at the Dickens Universe workshop, discussing how to use Dickens's shorthand with different age groups in the classroom.

The wider Anglophone public
A lecture and workshop hosted by the Co-I at Dickens Universe (July 2020) will introduce members of the public to Dickens's shorthand, as well as engaging them with the principles of *Brachygraphy* through a decoding workshop. The Co-I will also deliver a public talk related to Dickens and stenography at the BSR on the evening before symposium 2 (October 2020). In both cases, attendees will have the opportunity to learn about the influence of Dickens's stenographic training upon his writing, gaining a more rounded perspective. These events will also serve to officially launch the 'Dickens Code' website in the US and Europe, enabling attendees to engage with Dickens's shorthand on their own terms, via the online exhibition. This exhibition will be available to users across the globe, providing a context for a distinct and underexplored aspect of the work of a major and culturally iconic figure and stimulating interest in deciphering the shorthand."
15,7A7416B8-A0E2-4097-914B-E52130AF821D,Keep Learning,"Combinatorial problems are ubiquitous across many sectors in today's world: delivering optimised solutions can lead to considerable economic benefits in many fields such as logistics, packing, design and scheduling (of either people or processes). In a typical scenario, instances (for example, a set of goods to deliver) arrive frequently in a continual stream and a solution needs to be quickly produced. Although there are many well-known approaches to developing optimisation algorithms, most suffer from a problem that is now becoming apparent across the breadth of Artificial Intelligence: systems are limited to performing well on data that is similar to that encountered in their design process, and are unable to adapt when encountering situations outside of their original programming.


For real-world optimisation this is particularly problematic. If optimisers are trained in a one-off process then deployed, the system remains static, despite the fact that optimisation occurs in a dynamic world of changing instance characteristics, changing user-requirements and changes in operating environments that influence solution quality (e.g. breakdowns in a factory or traffic in a city). Such changes may be either gradual, or sudden. In the best case this leads to systems that deliver sub-optimal performance, while at worst, systems that are completely unfit for purpose. Moreover, a system that does not adapt wastes an obvious opportunity to improve its own performance over time as it solves more and more instances.

The targeted breakthrough of this proposal is to develop a dynamic optimisation system that continually adapts its operating mechanism and its algorithms over time to remain fit-for-purpose - a radical switch from the current one-off design and deployment approach to design of optimisers. The system will:

- Go beyond simply being reactive to being proactive in that it will predict the nature of upcoming instances and speculate about potential future scenarios. In response to these predictions, it will autonomously pre-generate and/or reconfigure suitable algorithms, followed by creation of appropriate mappings from instance to solver, in order to pre-prepare for these future scenarios. It will also respond to user requests to generate instances with specific characteristics and solvers to match them, based on the user's in-depth knowledge of their own business and sector.

- Autonomously improve its own behaviour over time, continually updating its algorithms and methods as it learns from its experience of solving more and more instances.

- Support optimisation with respect to multiple user objectives and requirements via its use of a diverse portfolios of algorithms, that range from those which generate acceptable solutions in a very short time to those that have long running time but deliver the highest possible quality.

To succeed we will make novel advances in building proactive, continually self-adapting systems and in optimisation/algorithm-selection, enhanced by integration with the latest tools from machine-learning. Benefits will be realised by any business that attempts to optimise their processes in dynamic environments, in which customer demands vary, business requirements change, and the operating environment is subject to unexpected changes. Relevant application domains include (but are not limited to) workforce scheduling, logistics and infrastructure design",,
16,56E6BA9F-9312-400B-B5EC-101BBD97AB7C,deeP redUced oRder predIctive Fluid dYnamics model (PURIFY),"Simulating large area urban turbulence flows in real time accurately remains a pressing challenge, yet to be addressed. Reduce Order Modelling (ROM) provides a means of real time simulation; traditional model reduction methods such as balanced truncation, the reduced-basis method, and (balanced) proper orthogonal decomposition (POD), have been developed. However challenges remain: the traditional methods restrict the state to evolve in a linear subspace; in addition, big area simulation cannot be conducted in real time. In addition, it is hard to define how accuracy the derived ROM is since there is no appropriate error estimator derived for the ROM. Also, existing ROM is either physically or data-driven and there is no physically informed data-driven ROM. Advancing in machine learning and in observational and simulation capabilities offer an opportunity to integrate simulation and data science approaches more intensively.
The proposed research offers scientific advances within the field of fluid flow modelling in order to underpin the establishment of novel sophisticated tools that will allow real-time simulations and prediction of air flows (and subsequently personal exposure to air pollutants); principal features include real-time air pollution predictions for next few hours. This can help minimise exposure to the population, especially vulnerable young/elderly patients. Individuals will be able to decide whether to go outside or not, exercise or make their daily plans. 
The proposed new model combines physical models and data sciences technologies, in particular, the deep learning for predicting the non-linearity of turbulence flows, thus making the air flows predictions more reliable and accurate. An autoencoder deep neural network will potentially capture the non-linearity of urban turbulence flows and more accurate than the traditional model reduction methods (e.g. POD). Commercialisation of the research outputs will be undertaken in partnership with VortexIoT, Looker Tech and Spire Global. These are international leading companies in the fields of sensor technology, software company and provision of technical professional services.
The approach is based on an advanced fast-running computational model to manage and predict the airflow, air quality in a city, guide effective responses in emergencies and help people reduce the air pollution exposure time. The development of novel deep learning ROM, reducing computational times by several orders of magnitude, will make currently unsurmountable problems tractable:- e.g. detailed air flows through a big area or an entire city. The specific technology that distinguishes this project are the potential use of deep learning reduced order modelling, new computational domain decomposition, new error analysis for DLROM and data assimilation method. 
The research objectives are: to develop deep learning based ROM framework with domain decomposition methods; to develop ROM framework with Autoencoder network that will project the system into a non-linear subspace, thus increasing accuracy; to develop ROMs with variable material properties such as variable initial or boundary conditions (different wind direction for example); ROM based data assimilation and optimisation methods; to perform an error analysis and optimal improvement for ROMs using machine learning methods.
The research has substantive health, environmental and economic impacts. Beneficiaries may include the general public through fast response to avoid exposure to air pollution technical professional services consultancy companies, local and national government environment bodies (e.g. NRW in Wales), computational engineering companies who will benefit from more efficient fluid flow model outcomes. In addition, public sector (spend budget savings in health) and air quality sensor device manufacturers incorporating the proposed modelling approach to enhance their offering (increased profits from sales).",,"The rapidly increasing global population and occupation of urban areas has exacerbated major societal challenges: poor air quality; insufficient water availability/quality; waste disposal issues; energy consumption. These issues highlight the demand for innovative strategies and methods to design and manage cities. Key to this goal is effective decision-making tools, underpinning strong city planning. Existing systems (Urban Flows Observatory; Urban Observatory) gather data from mobile and fixed sensors and deploy sophisticated visualisation tools. Simulation tools to model urban flows (Street-in-Grid model) illustrate the concentrations of air pollutants in complex urban canopy configurations and the background concentrations in the overlying atmosphere. Climate models studying interactions of atmosphere, oceans, land surface and ice have multiple linkages to air quality as many air pollutant sources also emit CO2 and greenhouse gases. However, existing traditional models notably lack accuracy in predicting non-linear systems in large computational areas in real time (e.g. urban turbulence flow). This tempers the ability to plan and protect (especially for the elderly, infirm or young) against exposure to low air quality. 
The proposed research will innovatively surmount these challenges to facilitate the development of a complete efficient modelling system, combining physical models and data sciences technologies, in particular, the deep learning for predicting the non-linearity of turbulence flows, thus making the air flows predictions fast (i.e. real time), more reliable and accurate. This will involve setting up an advanced fast-running computational model to manage and predict the airflow, air quality in a city, and guide effective response in emergencies and help people reduce the air pollution exposure time. 
The eventually commercialised product will orchestrate salient socio-economic impact. The impact beneficiaries and categories are: 

Health Impact - A new fast response model will orchestrate marked benefits for people living in urban centres. improved public health outcomes for vulnerable people, via more informed air quality forecasts, as data analysis tools will be incorporated to modern (e.g. phone apps), and traditional media channels. This will also enable guide response in emergencies, supporting the delivery of healthcare professional services by reducing bottlenecks from hospital and health centre admissions. 

Knowledge - Beneficiaries include technical professional services/consultancy companies, local and national government environment bodies (e.g. NRW in Wales), computational engineering companies. These will benefit from knowledge enhancement/uptake of new practises and technology within industry. 

Economic Impact - The development and eventual commercialisation of the model will realise significant financial benefits for the industrial partners (and supply chains). No competitive solution exists with the capacity to manage and accurately predict airflow and report air quality real-time variation in a city environment. Consultancy companies could offer superior services to their clients translating in higher revenues. Air quality monitoring hardware developers will be able to offer an enhanced service coupling their real time data collection with our powerful prediction tool. 
Finally, local governments and the UK economy can be strengthened from innovation and cost-effective technology to underpin strong city planning

Environment Impact - Energy efficient building solutions and low carbon transport systems will reduce air pollution from travel and traffic. 

Public Engagement -The use of Computational Engineering to address the challenges society is facing with urbanisation promises to stimulate public interest and lead to heightened understanding. Beneficiaries include school children; respiratory patient support groups; environment regulator and policy makers."
17,B33C40E9-4CEF-49C9-B940-116247CCD830,Multimodal Video Search by Examples (MVSE),"How to effectively and efficiently search for content from large video archives such as BBC TV programmes is a significant challenge. Search is typically done via keyword queries using pre-defined metadata such as titles, tags and viewer's notes. However, it is difficult to use keywords to search for specific moments in a video where a particular speaker talks about a specific topic at a particular location. Most videos have little or no metadata about content in the video, and automatic metadata extraction is not yet sufficiently reliable. Furthermore, metadata may change over time and cannot cover all content. Therefore, search by keyword is not a desirable approach for a comprehensive and long-lasting video search solution. 

Video search by examples is a desirable alternative as it allows search for content by one or more examples of the interested content without having to specify interest in keyword. However, video search by examples is notoriously challenging, and its performance is still poor. To improve search performance, multiple modalities should be considered - image, sound, voice and text, as each modality provides a separate search cue so multiple cues should identify more relevant content. This is multimodal video search by examples (MVSE). This is an emerging area of research, and the current state of the art is far from desirable so there is a long way to go. There is no commercial service for MVSE.

This proposal has been co-created with BBC R&amp;D through the BBC Data Science Partnership via a number of online meetings and one face to face meeting involving all partners. The proposal has been informed by recent unpublished ethnographic research on how current BBC staff (producers, journalists, archivists) search for media content. It was found that they were very interested in knowledge retrieval from archives or other sources but they required richer metadata and cataloguing of non-verbal data. 

In this proposal we will study efficient, effective, scalable and robust MVSE where video archives are large, historical and dynamic; and the modalities are person (face or voice), context, and topic. The aim is to develop a framework for MVSE and validate it through the development of a prototype search tool. Such a search tool will be useful for organisations such as the BBC and British Library, who maintain large collections of video archives and want to provide a search tool for their own staff as well as for the public. It will also be useful for companies such as Youtube who host videos from the public and want to enable video search by examples. We will address key challenges in the development of an efficient, effective, scalable and robust MVSE solution, including video segmentation, content representation, hashing, ranking and fusion. 

This proposal is planned for three years, involving three institutions (Cambridge, Surrey, Ulster) and one partner (the BBC) who will contribute significant resources (estimated at &pound;128.4k) to the project (see Letter of Support from the BBC).",,"The project's objective is to provide scalable next-generation 'search by example' functionality across national video archives. The project will develop beyond the state of the art in video segmentation, content representation/matching/ranking functionality and these outputs are intended to provide positive, disruptive impact in multimedia search capability across the media industry nationally and internationally.

The beneficiaries of this project's outputs will include academics, journalists, broadcasters, TV viewers, multimedia companies and organisations hosting and managing large video or multimedia repositories. 

Journalists and broadcasters will directly benefit by time efficiency savings and the rapid discovery of relevant content when using this new technology. This will in turn provide better, more relevant and more enriched TV programming in less time thus having economic savings. This will have a benefit to TV viewers who will enjoy more relevant TV programmes by the effective repurposing of content within big media archives. As a key partner, the immediate beneficiary will be the BBC who will likely adopt and integrate the new technology within their workflows to improve the discovery of media content when producing TV programmes. However, the technologies developed are transferable to other broadcasters and indeed major online companies such as Youtube who rely on semantically enriched search technologies. 

Academics will benefit by the dissemination and inspiration of the project's new research findings and search technologies for rapidly discovering relevant video/multimedia content based on new intelligent algorithms. 

The pathways to impact document provides an outline of a series of activities including co-creation workshops and licensing to increase the likelihood of research impact and adoption of the novel, disruptive technologies produced in this project."
18,B22A992F-A44A-4980-8946-C92C32F2DCB0,Encyclopedic Lexical Representations for Natural Language Processing,"The field of Natural Language Processing (NLP) has made unprecedented progress over the last decade, fuelled by the introduction of increasingly powerful neural network models. These models have an impressive ability to discover patterns in training examples, and to transfer these patterns to previously unseen test cases. Despite their strong performance in many NLP tasks, however, the extent to which they &quot;understand&quot; language is still remarkably limited. The key underlying problem is that language understanding requires a vast amount of world knowledge, which current NLP systems are largely lacking. In this project, we focus on conceptual knowledge, and more in particular on: 

(i) capturing what properties are associated with a given concept (e.g. lions are dangerous, boats can float); 
(ii) characterising how different concepts are related (e.g. brooms are used for cleaning, bees produce honey).

Our proposed approach relies on the fact that Wikipedia contains a wealth of such knowledge. A key problem, however, is that important properties and relationships are often not explicitly mentioned in text, especially if they follow straightforwardly from other information, for a human reader (e.g. if X is an animal that can fly then X probably has wings). Apart from learning to extract knowledge expressed in text, we thus also have to learn how to reason about conceptual knowledge.

A central question is how conceptual knowledge should be represented. Current NLP systems heavily rely on vector representations. Each concept is then represented by a single vector. It is now well-understood how such representations can be learned, and they are straightforward to incorporate into neural network architectures. However, they also have important theoretical limitations in terms of what knowledge they can capture, and they only allow for shallow and heuristic forms of reasoning. In contrast, in symbolic AI, conceptual knowledge is typically represented using facts and rules. This enables powerful forms of reasoning, but symbolic representations are harder to learn and to use in neural networks. Moreover, symbolic representations are also limited because they cannot capture aspects of knowledge that are matters of degree (e.g. similarity and typicality), which is especially restrictive when modelling commonsense knowledge.

The solution we propose relies on a novel hybrid representation framework, which combines the main advantages of vector representations with those of symbolic methods. In particular, we will explicitly represent properties and relationships, as in symbolic frameworks, but these properties and relations will be encoded as vectors. Each concept will thus be associated with several property vectors, while pairs of related concepts will be associated with one or more relation vectors. Our vectors will thus intuitively play the same role that facts play in symbolic frameworks, with associated neural network models then playing the role of rules.

The main output from this project will consist in a comprehensive resource, in which conceptual knowledge is encoded in this hybrid way. We expect that our resource will play an important role in NLP, given the importance of conceptual knowledge for language understanding and its highly complementary nature to existing resources. To demonstrate its usefulness, we will focus on two challenging applications: reading comprehension and topic/trend modelling. We will also develop three case studies. In one case study, we will learn representations of companies, by using our resource to summarise the activities of companies in a semantically meaningful way. In another case study, we will use our resource to identify news stories that are relevant to a given theme. Finally, we will use our methods to learn semantically coherent descriptions of emerging trends in patents.",,
19,615EE290-4180-458E-A43F-7240AED537BE,Multimodal Video Search by Examples (MVSE),"How to effectively and efficiently search for content from large video archives such as BBC TV programmes is a significant challenge. Search is typically done via keyword queries using pre-defined metadata such as titles, tags and viewer's notes. However, it is difficult to use keywords to search for specific moments in a video where a particular speaker talks about a specific topic at a particular location. Most videos have little or no metadata about content in the video, and automatic metadata extraction is not yet sufficiently reliable. Furthermore, metadata may change over time and cannot cover all content. Therefore, search by keyword is not a desirable approach for a comprehensive and long-lasting video search solution. 

Video search by examples is a desirable alternative as it allows search for content by one or more examples of the interested content without having to specify interest in keyword. However, video search by examples is notoriously challenging, and its performance is still poor. To improve search performance, multiple modalities should be considered - image, sound, voice and text, as each modality provides a separate search cue so multiple cues should identify more relevant content. This is multimodal video search by examples (MVSE). This is an emerging area of research, and the current state of the art is far from desirable so there is a long way to go. There is no commercial service for MVSE.

This proposal has been co-created with BBC R&amp;D through the BBC Data Science Partnership via a number of online meetings and one face to face meeting involving all partners. The proposal has been informed by recent unpublished ethnographic research on how current BBC staff (producers, journalists, archivists) search for media content. It was found that they were very interested in knowledge retrieval from archives or other sources but they required richer metadata and cataloguing of non-verbal data. 

In this proposal we will study efficient, effective, scalable and robust MVSE where video archives are large, historical and dynamic; and the modalities are person (face or voice), context, and topic. The aim is to develop a framework for MVSE and validate it through the development of a prototype search tool. Such a search tool will be useful for organisations such as the BBC and British Library, who maintain large collections of video archives and want to provide a search tool for their own staff as well as for the public. It will also be useful for companies such as Youtube who host videos from the public and want to enable video search by examples. We will address key challenges in the development of an efficient, effective, scalable and robust MVSE solution, including video segmentation, content representation, hashing, ranking and fusion. 

This proposal is planned for three years, involving three institutions (Cambridge, Surrey, Ulster) and one partner (the BBC) who will contribute significant resources (estimated at &pound;128.4k) to the project (see Letter of Support from the BBC).",,
20,D7B887DD-5E5D-48E0-99FA-76CF4FB8385E,Computational constructivism: The algorithmic basis of discovery,"One of the defining aspects of being human is an ability to flexibly generate new ideas and hypotheses. For example, we readily come up with possible faults if our car breaks down, or plausible maladies when we feel unwell. We can brainstorm anything from party ideas, to corporate strategies, to magical creatures, and frequently hypothesise hidden motivations and beliefs in our peers to explain why they act the way they do. Our ideas often combine familiar objects, concepts, and relations, making them symbolic, easy to communicate, and a ready guide for follow up queries or evidence seeking. For example, suppose you came home from work to find your house in disarray. You might quickly suspect you have been burgled and investigate by checking whether valuables are missing. If you then discover feathers on the floor this might inspire other possibilities. Perhaps a bird got in through an open window and ran amok. These kinds of inventive inferences come quickly and easily for us, but are surprisingly difficult for artificial intelligence systems. Part of this difficulty is that, for the kinds of natural domains mentioned above, there is typically an infinite number of possibilities one could generate, but few good ones. Our best ideas have the character of &quot;ah ha&quot; moments, immediately providing a better explanation than preceding candidates and potentially becoming a lasting addition to one's beliefs or knowledge base.

 The key aim of this project is to develop algorithms that emulate the way humans generate, adapt and actively investigate such hypotheses in everyday life. The basic idea is that we combine our more primitive concepts to form more complex ideas, essentially &quot;trying out&quot; different combinations of primitives and connectives when searching for a better explanation, or adapting one that does not fit the latest evidence. Such a search process is governed by overarching principles of simplicity and fit to the evidence, but constrained by our finite thinking time and capacity. For example, in the above example you might rapidly generate, refine or overturn several hypotheses as you investigate the mess, discovering a feather duster, cleaning products, and finally your partner in the midst of a spring clean.

&quot;Program induction&quot; is a powerful new mathematical framework for constructing symbolic models or programs that can explain or reproduce observations. Induced programs can grow in structure and complexity as evidence is encountered, reusing past solutions as and composing them to solve new problems. We propose to use this as a framework to capture and ultimately synthesise humanlike hypothesis generation. To closely examine human hypothesis generation, we will combine theoretical work in the program induction framework with experiments with human adults. In our inductive learning tasks, participants and our algorithms will both observe and create their own physical scenes made up of simple geometric blocks and test them to discover and generate hypotheses regarding under what conditions they will produce a novel causal effect (i.e. in our pilot, produce a &quot;newly discovered form of radiation&quot;). This setup allows us to explore arbitrarily complex hidden causal effects that can involve combinations of features and relations, meaning the participants (and our algorithms) must use hypothesis generation, reasoning and active testing to identify the ground truth in each case.

Through our modelling and our experiments we expect to deepen understanding of the mechanisms that underpin the uniquely human ability to make explanatory inferences. We expect our findings to influence robotics, and AI communities providing insight into how to build artificial systems that can better emulate, understand and be understood by humans. The goal of this project thus to develop a precise algorithmic account of idea generation in human learning that we call &quot;computational constructivism&quot;.",,"1. Who might benefit from this research? And How?

By modelling the mechanisms by which people discover patterns and constraints in data, our project will lead to systems that can emulate, understand, and be understood by humans and will inform the design of teaching resources to convey scientific and mathematical principles and facilitate learning. The stakeholders who stand to benefit from such systems thus include:

 a) Developers of systems that perform well in low-resource or &quot;small data&quot; settings

 b) Manufacturers of robots and other systems where counter-intuitive or surprising behaviour can be costly and &quot;programming by example&quot; may open the door to novel applications. 

c) Educators who will benefit from a better understanding of how examples can be tailored to help a child
discover a scientific or mathematical principle.


To reach the tech community including developers and roboticists we have planned a number of dissemination activities including:

a) Connecting with businesses though AIMday collider events and the Data Science Research Days organised by the University Edinburgh; running a training session with a London-based AI consultancy
b) Training our PhD and MSc students many of whom have already gone on to work with major tech industry players including Google, Facebook, Amazon and Netflix
c) Publishing generalist articles in popular tech publications like Wired and the MIT Tech Review

To reach educators we plan to:

a) Publish generalist articles on the Gender Teaching Council for Scotland's blog
b) Host a focus group with teachers to discuss practical implications and limitations
c) Offer public facing lectures at the Scottish Learning Festival and Edinburgh International Science Festival


For both the tech community and education community we plan to leverage social media to network promote our findings. We also plan to take the first steps toward converting our learning task into a gamified learning app to leverage the value integrating cognitive research and technology.


2. How will we evaluate impact?

For blogs, articles, lectures, and tweets, we will judge impact by the size of the audience or readership, number of retweets. For our data science resources, we will judge impact by the number of people forking and downloading our tasks and datasets. We will use our interactions with the business and tech community at our collider and research days and the education community at our focus group to seek direct feedback on our practical impact and will interact with our KE officer at Edinburgh and our Advisory panel internationally on a rolling basis to get assessments of our impact success and will potentially adjust our impact strategy where necessary as a result."
21,1184AD3F-81EC-4910-8963-A961B812C3E5,Automating electron microscopy: machine learning for cluster identification,"Nanoparticles are of particular interest in the field of catalysis because of the high proportion of their atoms that are available at their surface (a high surface-to-volume ratio). Catalysis happens only at the surface of materials and is controlled by the electronic and spatial configuration of surface atoms. Nanoparticles of certain sizes are known to take up a small number of fixed shapes that possess well known configurations of atoms at their surface. Determining the shape of nanoparticles is a difficult question that requires very high-resolution characterisation techniques, such as electron microscopy.

In this project, a convolutional neural network will be trained to recognize the different shapes of small nanoparticles. A convolutional neural network is a type of machine learning algorithm that can be trained to recognize image features. Once trained, the neural network will be used to determine the proportion of different particle shapes found in platinum nanoparticles of different sizes. This will determine which particle shapes have the lowest potential energy for each size and therefore will guide scientists to know which particles are likely to act as better catalysts for chemical reactions and processes.

The trained neural network will be made available for anyone to use via its incorporation in to open-source software. This will allow anyone with electron microscope images of nanoparticles to use the same technique to analyse the shape of small nanoparticles.",,
22,1BF440F7-FFEC-4321-954B-C87A17B90F3D,COHERENT: COllaborative HiErarchical Robotic ExplaNaTions,"For robots to build trustable interactions with users two aspects will be crucial during the next decade. First, the ability to produce explainable decisions combining reasons from all the levels
of the robotic architecture from low to high level; and second, to be able to effectively communicate such decisions and re-plan according to new user inputs in real-time along with the execution.

COHERENT will develop a novel framework to combine explanations originated at the different robotic levels into a single explanation. This combination is not unique and may depend on several factors including the step into the action sequence, or the temporal importance of each information source. Robotic tasks are interesting because they entail performing a sequence of actions, and thus the system must be able to deliver these explanations also during the execution of the task, either because the user requested or actively because an unforeseen situation occurs. COHERENT will propose effective evaluation metrics oriented to the special case of explanations in HRI systems. The proposed measures, based on trustworthiness and acceptance, will be defined together with the definition of benchmark tasks that are repeatable and enable the comparison of results across different explainable developments.

We will demonstrate our framework for hierarchical explanation components through a manipulation task of assisting a human to fold clothes. Cloth manipulation is a very rich example that requires considering bi-manual manipulations, environmental constraints, and perception of textiles for its state estimation. Eventually, the robot can even require the user to help in doing difficult actions by providing relevant information, so interaction opportunities are multiple. We will build on previous results on cloth manipulation to develop explainable machine learning techniques from the perception, learned movements, task planning and interaction layers, based on a novel generic representation, the Cohesion Graph, that is shared across the layers. The COHERENT framework will be integrated into the standard planning system ROSplan to increase its visibility and adoption.",,
23,2ED8BC7E-599B-4E9C-B174-D839389F4395,BBC Prosperity Partnership: Future Personalised Object-Based Media Experiences Delivered at Scale Anywhere,"Personalisation of media experiences for the individual is vital for audience engagement of young and old, allowing more meaningful encounters tailored to their interest, making them part of the story, and increasing accessibility. The goal of the BBC Prosperity Partnership is to realise a transformation to future personalised content creation and delivery at scale for the public at home or on the move.

Evolution of mass-media audio-visual 'broadcast' content (news, sports, music, drama) has moved increasingly towards Internet delivery, which creates exciting potential for hyper-personalised media experiences delivered at scale to mass audiences. This radical new user-centred approach to media creation and delivery has the potential to disrupt the media landscape by directly engaging individuals at the centre of their experience, rather than predefining the content as with existing media formats (radio, TV, film). This will allow a new form of user-centred media experience which dynamically adapts to the individual, their location, the media content and producer storytelling intent, together with the platform/device and the network/compute resources available for rendering the content.The BBC Prosperity Partnership will position the BBC at the forefront of this 'Personalised Media' revolution enabling the creation and delivery of new services, and positioning the UK creative industry to lead future personalised media creation and intelligent network distribution to render personalised experiences for everyone anywhere.

Realisation of personalised experiences at scale presents three fundamental research challenges: capture of object-based representations of the content to enable dynamic adaption for personalisation at the point of rendering; production to create personalised experiences which enhance the perceived quality of experience for each user; and delivery at scale with intelligent utilisation of the available network, edge and device resources for mass audiences. The BBC Prosperity Partnership will address the major technical and creative challenges to delivering user-centred personalised audience experiences at scale. Advances in audio-visual AI for machine understanding of captured content will enable the automatic transformation of captured 2D video streams to an object-based media (OBM) representation. OBM will allow adaptation for efficient production, delivery and personalisation of the media experience whilst maintaining the perceived quality of the captured video content. To deliver personalised experiences to audiences of millions requires transformation of media processing and distribution architectures into a hybrid and distributed low-latency computation platform, allowing flexible deployment of compute-intensive tasks across the network. This will achieve efficiency in terms of cost and energy use, while providing optimal quality of experience for the audience within the technical constraints of the system.",,
24,88D43B70-A4CA-4BDA-9DED-49B5A69CEDD3,"TAPS: Assessing, Mitigating and Raising Awareness of the Security and Privacy Risks of Thermal Imaging","Thermal imaging technologies are continuously becoming more affordable and accessible to everyone. Today, a thermal camera can be bought for less than &pound;150. Thermal imaging can be used maliciously to infer the user input on keyboards and touchscreens. For example, taking a thermal image of a keyboard after a user has interacted with it reveals recent input such as passwords, or sensitive messages. This project aims to 1) assess the viability of thermal attacks in everyday computer and mobile usage scenarios, 2) develop and evaluate methods for resisting them on desktop and mobile settings, and 3) raise awareness about this threat and possible countermeasures through impact activities that engage with Logitech, a major manufacturer of input peripherals, and local partners such as CENSIS. This project will produce 1) a dataset of thermal images for research on thermal attacks, 2) empirical findings that explain which factors impact the effectiveness of thermal attacks in realistic everyday scenarios in desktop and mobile settings, 3) recommendations for users and manufacturers for resisting thermal attacks on touchscreens and keyboards, 4) a novel machine learning model to be used by researchers and practitioners to analyse the effectiveness of thermal attacks and evaluate countermeasures, 5) a novel machine learning model that predicts vulnerability to thermal attacks and tools that use it to mitigate the risk, and 6) material to raise awareness about thermal attacks and possible countermeasures.",,
0,A4F633B9-CB46-4786-8786-AACB1B4C57AF,The Lexicon of miRISC: Deconstructing the functional complexity of the miRNA induced silencing complex,"In the cell, gene expression is regulated very tightly and one mechanism relies on small RNA molecules called microRNAs, which can repress the expression of specific genes. These microRNAs, of which there are many kinds, need to be bound by proteins called Argonautes for them to function correctly. Argonaute proteins are required to recruit other proteins that can repress the translation of genes into proteins - translation being a fundamental process occurring in all cells. Argonaute in combination with these repressive proteins is called the microRNA-induced silencing complex, or miRISC for short.

Human cells express four different Argonautes but the functional differences between them are unclear. Our lab works on the LIMD1 family of proteins of which there are three members - LIMD1, Ajuba and WTIP. We think that these proteins, which can bind Argonautes, regulate the diverse functions of different miRISCs in regulating gene expression.

To investigate this, we will genetically engineer cells to attach the HaloTag onto AGOs in the cell. The HaloTag permits efficient purification of tagged proteins and advanced imaging techniques. Following Argonaute purification with the HaloTag, we will sequence associated microRNAs and the genes they are regulating. We will look for patterns in the microRNAs and types of genes they regulate depending on the presence of LIMD1 in the cell. We will also get an idea of how each Argonaute protein is functionally distinct in terms of the microRNAs it binds and the genes it regulates. 

From the large amounts of microRNA, gene expression and protein data, we will use computational methods to construct a database and predictive model for how LIMD1 family proteins influence miRISC function. We will also use advanced microscopy techniques to see how the Argonautes and microRNAs behave in the cell and how LIMD1 family proteins are influencing this behaviour. Overall, our proposed research will reveal a new layer of miRISC regulation that cells have to ensure genes are expressed appropriately.","The microRNA-induced silencing complex (miRISC) is critical for post-transcriptional regulation. miRISCs consist of Argonaute proteins, which bear miRNAs, TNRC6 proteins which are critical for translational repression and a host of other effectors which destabilise and degrade mRNAs. Multiple isoforms of AGOs and TNRC6 proteins exist. Our previous work has shown that the LIMD1 family of proteins (LIMD1, Ajuba and WTIP, collectively termed LAW) interact with AGOs and TNRC6 proteins. We propose LAW proteins modify canonical cytoplasmic AGO function in human cells and the range of AGO, TNRC6 and LAW isoforms can assemble into multiple different miRISCs (miRISCLIMD1, miRISCAjuba, miRISCWTIP or combinations thereof miRISCLAW), which differ significantly in function. Our proposal focuses on an overarching hypothesis: &quot;LAW proteins drive the formation specialised miRISCs with distinct localisation patterns and functions&quot;. To address this, we have the following aims: 

Aim 1: Determine functions of different miRISCs on the silencing landscape for a predictive model of miRISC-LAW association and regulatory output.
Aim 2: To image functional miRISCs, RNA and P-bodies with regard to LAW protein association
To address our Aims, we will utilise the HaloTag system in an endogenous context to tag AGOs. This will facilitate AGO purification for RNA-seq studies on AGO-bound miRNAs and mRNAs. Such experiments will be performed in LIMD1+/+ and -/- cells in parallel so the effect of LIMD1 (and other LAW proteins once they are ablated) on miRISC function and gene expression will be ascertained. To complement, proteomic data on these cells will also be obtained. Ultimately, these datasets will feed into machine learning programmes which will identify structural and functional patterns to LAW-dependent miRNA silencing. We will use the HaloTag and microinjection of labelled RNAs for single molecule imaging techniques to visualise the localisation and function of specialised miRISCs.",
1,AC95BC11-A984-4E51-A277-FCDB3AFBE03D,EDGE - Adaptive Deep Learning Hardware for Embedded Platforms,"Deep learning (DL) is the key technique in modern artificial intelligence (AI), which has provided state-of-the-art accuracy on many machine-learning based applications. Today, although most of the computational loads of DL systems are still spent running neural networks in data centres, the ubiquity of smartphones, and the upcoming availability of self-contained wearable devices for augmented reality (AR), virtual reality (VR) and autonomous robot systems are placing heavy demands on DL-inference hardware with high energy and computing efficiencies along with rapid development of DL techniques. Recently, we have witnessed a distinct evolution in the types of DL architecture, with more sophisticated network architectures proposed to improve edge AI inference. This includes dynamic network architectures that change with each new input in a data-dependent way, where inputs and internal states are not fixed. Such new architectural concepts in DL are likely to affect the type of hardware architectures that will be required to deliver such capabilities in the future. This project precisely addresses this challenge and proposes to design a flexible hardware architecture that enables adaptive support for a variety of DL algorithms on embedded devices. Primarily, to produce lower cost, lower power and higher processing efficiency DL-inference hardware that can be configured adaptably for dedicated application specifications and operating environments, this will require radical innovation in the optimisation of both the software and the hardware of current DL techniques.

This work aims to perform fundamental research, development and practical demonstrator to enable general support for a variety of DL techniques on embedded edge devices with limited resource and latency budgets. Primarily, this requires radical innovation on the current DL architectures in terms of computing architecture, memory hierarchy and resource utilisation, as well as system latency and throughput: it is particularly important for the modern DL systems that the inference processes are dynamic, such as, the DL inference maybe input-dependent and resource-dependent. The proposal therefore seeks the following three thrusts: First, to build upon the existing work of the PI in optimising machine-learning models for resource-constrained embedded devices, towards achieving the goal that the network model could be dynamically optimised as needed through hardware-aware approximation techniques. Second, with newly-developed adaptive compute acceleration technology in programmable memory hierarchy and adaptive processing hardware, to seek a new ambitious direction to develop a set of context-aware hardware architectures to work closely with the approximation algorithms that can fully utilise the true hardware capabilities. Unlike traditional optimisation techniques for DL hardware inference engines, the proposed work will explore both software and hardware programmability of adaptive compute acceleration technology, in order to maximise the optimisation results for the target application scenarios. Third, this project will work closely with our industry and project partners to produce a practical demonstrator to showcase the effectiveness of the proposed DL framework versus traditional approaches, particularly, evaluating the effectiveness of the framework in real-world mission-critical applications.",,
2,2A300539-3E72-4DF3-9983-1756534F938D,DART: Design Accelerators by Regulating Transformations,"The DART project aims to pioneer a ground-breaking capability to enhance the performance and energy efficiency of reconfigurable hardware accelerators for next-generation computing systems. This capability will be achieved by a novel foundation for a transformation engine based on heterogeneous graphs for design optimisation and diagnosis. While hardware designers are familiar with transformations by Boolean algebra, the proposed research promotes a design-by-transformation style by providing, for the first time, tools which facilitate experimentation with design transformations and their regulation by meta-programming. These tools will cover design space exploration based on machine learning, and end-to-end tool chains mapping designs captured in multiple source languages to heterogeneous reconfigurable devices targeting cloud computing, Internet-of-Things and supercomputing. The proposed approach will be evaluated through a variety of benchmarks involving hardware acceleration, and through codifying strategies for automating the search of neural architectures for hardware implementation with both high accuracy and high efficiency.",,
3,61B3DE7A-63C4-47FB-ABB1-19B09C93B400,Transfer learning of pharmacogenomic information across disease types and preclinical models for drug sensitivity prediction.,"The failure rate for new drugs entering clinics is in excess of 90%, with more than a quarter of drugs failing due to lack of efficacy. Earlier treatment decisions for complex diseases like lung cancer considered a small number of patient factors and prescribed a fixed treatment regimen for all patients, resulting in severe drug side effects for some and highly-varying outcomes. Recently, personalised treatments have become popular through the discovery and use of genetic markers that can explain a patient's response to a drug. If the goal of personalised medicine is to give the right drug to the right patient, we may be able to combine pharmacogenomics with machine learning to help make better treatment decisions.

Due to the potential waste of testing ineffective drugs on patient cells and animal models in the laboratory, we are motivated to leverage the power of machine learning to predict drug response from a limited number of experiments. We and many others in drug development have used computational methods to learn from drug responses measured in vitro and provide evidence for clinical trials, however, existing machine learning methods do poorly at predicting drug response in disease types where we have a limited number of samples. This situation unfortunately happens quite often for rare cancers and other diseases like motor neurone disease (also known as ALS), because there are few patients or their samples are difficult to collect. Overcoming this limitation by extending machine learning to learn from different disease contexts would mean that we can reduce the time-consuming step of gathering biological resources and then accelerate drug development.

In this project, we will develop machine learning algorithms that will take into account all of the dose-response data we have for each drug tested in only a few samples. To overcome the issue of few training cases in a disease, we will develop a transfer learning framework that will use knowledge from other diseases with more drug response data to address the problem in the disease with less data. The algorithms will be developed and tested in five stages: 1) develop a learning model that maps genomic information to drug response in both the disease with more data and the disease with limited data; 2) develop an inference model for predicting drug response in the disease with limited data; 3) apply the learning and inference models to use genomic relationships to drug sensitivity in lung cancer to predict drug response in bladder cancer; 4) learn from drug responses in cell lines and predict response in mice tumour models; 5) learn and predict biomarkers that describe a particular drug's sensitivity in both lung cancer and motor neurone disease. Genomic information will be used as inputs for the prediction algorithms because they can be reliably measured in the laboratory and in the clinic. We use prediction test cases of increasing difficulty, but successes in transferring pharmacogenomics information between diseases will highlight opportunities for scientists to leverage existing data sets to solve challenges of testing a drug in a new disease.

We are conducting this interdisciplinary study as a team of computer scientists, clinicians and cell biologists with expertise in machine learning, cancer and neuroscience. The end goal is to eventually develop a suite of software tools that can be readily used flexibly by the drug development community to apply transfer learning to many different problems.",,
4,670B2A71-936A-44D4-B81D-701013A22793,Multimodal Video Search by Examples,"How to effectively and efficiently search for content from large video archives such as BBC TV programmes is a significant challenge. Search is typically done via keyword queries using pre-defined metadata such as titles, tags and viewer's notes. However, it is difficult to use keywords to search for specific moments in a video where a particular speaker talks about a specific topic at a particular location. Most videos have little or no metadata about content in the video, and automatic metadata extraction is not yet sufficiently reliable. Furthermore, metadata may change over time and cannot cover all content. Therefore, search by keyword is not a desirable approach for a comprehensive and long-lasting video search solution. 

Video search by examples is a desirable alternative as it allows search for content by one or more examples of the interested content without having to specify interest in keyword. However, video search by examples is notoriously challenging, and its performance is still poor. To improve search performance, multiple modalities should be considered - image, sound, voice and text, as each modality provides a separate search cue so multiple cues should identify more relevant content. This is multimodal video search by examples (MVSE). This is an emerging area of research, and the current state of the art is far from desirable so there is a long way to go. There is no commercial service for MVSE.

This proposal has been co-created with BBC R&amp;D through the BBC Data Science Partnership via a number of online meetings and one face to face meeting involving all partners. The proposal has been informed by recent unpublished ethnographic research on how current BBC staff (producers, journalists, archivists) search for media content. It was found that they were very interested in knowledge retrieval from archives or other sources but they required richer metadata and cataloguing of non-verbal data. 

In this proposal we will study efficient, effective, scalable and robust MVSE where video archives are large, historical and dynamic; and the modalities are person (face or voice), context, and topic. The aim is to develop a framework for MVSE and validate it through the development of a prototype search tool. Such a search tool will be useful for organisations such as the BBC and British Library, who maintain large collections of video archives and want to provide a search tool for their own staff as well as for the public. It will also be useful for companies such as Youtube who host videos from the public and want to enable video search by examples. We will address key challenges in the development of an efficient, effective, scalable and robust MVSE solution, including video segmentation, content representation, hashing, ranking and fusion. 

This proposal is planned for three years, involving three institutions (Cambridge, Surrey, Ulster) and one partner (the BBC) who will contribute significant resources (estimated at &pound;128.4k) to the project (see Letter of Support from the BBC).",,
5,B396B60B-6475-430D-B499-8263B0C6597F,Deep Learning Models for Fetal Monitoring and Decision Support in Labour,"Oxygen reaches the baby in the mother womb via the placenta and umbilical cord. During labour, the contractions squeeze the placenta and the cord, reducing the supply of oxygen to the baby. Most healthy babies cope well, but a small percentage are at risk of suffocation and brain injury. This causes each year in the UK about 100 healthy babies to die and more than 1,000 to sustain brain injury. Globally, each year, events during childbirth are estimated to account for 920,000 neonatal deaths; 1.1 million stillbirths during labour; and more than a million babies per year develop different important sequelae, from cerebral palsy to mental retardation, learning difficulties and other disabilities. 
A baby which is at higher risk can be delivered urgently by emergency Caesarean section or instrumental vaginal extraction. To monitor the baby during labour, midwives and doctors in the developed countries use the cardiotocogram (CTG), which continuously displays the womb's contractions and the baby's heart rate. But our understanding of how to read the complex CTG graphs is limited and the patterns are difficult to interpret by eye, so some babies end up injured while at the same time many unnecessary emergency interventions are performed. Nearly 50% of the NHS litigation bill relates to maternity claims (in 2000-2010 these amounted to &pound;3.1bn) and the majority of these are related to shortcomings in labour management and CTG interpretation. 
With currently available computing power and routinely collected clinical data, methods using intelligent computer-based analysis can establish the relation of the CTG patterns and other clinical factors during labour to the baby's health at birth. In our pioneering work, we have already derived from the data a first prototype of a basic automated CTG analysis model, demonstrating proof-of-concept of using routinely collected maternal-fetal clinical data from pregnancy and childbirth (using retrospectively the data of more than 22,000 births at term). More recently, using the data from over 35,000 births at term, we conducted the first ever work on the application of deep learning methods for the analysis of CTG data, demonstrating their capability to learn from the raw CTG data and supersede all prior automated algorithms. 
The main goal of this proposal is, based on an updated and unique Oxford dataset of over 100,000 births, to develop innovative deep learning models for personalised continuous fetal health risk assessment during labour. In particular, we propose to: (1) develop automated models for continuous fetal heart rate analysis based on the CTG data and clinical risk factors, incorporating convolutional neural networks and long short term memory networks into multimodal and stacked models; (2) develop a software App for tablets, capable of real-time CTG analysis and risk assessment of fetal compromise; (3) validate and demonstrate the capability, accuracy, and efficiency of the new models running on the App, by conducting simulations with real-time data at the Oxford NHS Trust as well as with retrospective data. 
The output of this proposal will constitute a crucial building block towards the team's overarching goal to deliver at the bedside an individualised data-based tool for clinical decision support, preventing brain injury of the baby during labour. The main output will be a decision-support tool ready for prospective clinical tests, which holds clear potential benefits for the individuals, society, clinicians, and the NHS. It will address unresolved challenges in this clinical field, where improvements are painfully needed and are long overdue. This project is timely and represents excellent value for money, given the existing database and substantial prior work, the enormity of NHS litigation claims and the costs of unnecessary operative deliveries.",,"Monitoring reliably and consistently the health of babies during childbirth remains a massive unmet clinical need worldwide. In the UK alone, over 1200 otherwise healthy term babies sustain permanent brain injury during labour. Recent reports have found that in the UK, problems with monitoring the fetal health during labour were a factor in over half of the infants with potentially preventable brain injury at term (Royal College of Obstetricians and Gynaecologists. Each Baby Counts: 2018 Progress Report. London: RCOG, 2018). 
Nearly 50% of the NHS litigation bill relates to maternity claims, and between the years 2000 and 2010 these amounted to &pound;3.1bn (The NHS Litigation Authority. Ten years of maternity claims: an analysis of NHS Litigation Authority data, 2012). The majority of these claims related to shortcomings in labour management and cardiotocgraphy (CTG) interpretation. In health economic terms, &quot;more than 3,000 quality-adjusted life-years are lost annually to cerebral palsy from obstetric complications resulting in fetal hypoxia, at estimated costs of &pound;62.9m&quot; (Annual report of the Chief Medical Officer: the health of the 51%: Women, Dept. of Health, London, 2015). Reducing the rate of stillbirths, neonatal deaths and brain injuries that are caused during or soon after birth continues to be a priority for the NHS and it is one of the mandate objectives from central government (The Government's mandate to NHS England 2016-2017, Dept. of Health, 2017). 
At the same time, the UK government has pledged to &quot;ensure the benefits of NHS data and innovation are fully harnessed for patients in this country&quot;, and with this goal, has recently pledged &pound;250 million for the NHS to invest in Artificial Intelligence (https://www.gov.uk/government/news/health-secretary-announces-250-million-investment-in-artificial-intelligence). 
Our proposal fits right into the heart of the above national agendas, and is based on decades of experience of the team with machine learning and working with the CTG archives. Moreover, this proposal fits perfectly within the EPSRC Healthcare Technologies strategy, addressing one of the Grand Challenges: Optimising Treatment through effective diagnosis, patient-specific prediction and evidence-based intervention. In addition, this proposal also fits within the Cross-cutting research capabilities: it is focussed on innovative intelligent technologies that can have a transformative impact on diagnosis and monitoring in healthcare.
This collaborative EPSRC proposal is a crucial building block within our larger programme at the Oxford Centre for Fetal Monitoring Technologies (please see Case for Support - Fig.1 and Pathways to Impact part of this proposal). Our research project is designed to deliver novel advanced deep learning methods for CTG analysis and novel technical capability to interact with the clinicians through an App, supporting the wider project in preparation for a large i4i NIHR Product Development Award. Our overarching goal is to deliver to the bedside, for the first time, a disruptive data-driven individualised decision support tool for fetal health risk assessment during childbirth. 
Finally, our proposal is highly innovative without an equivalent world-wide to date, both owing to the unique size and content of the Oxford dataset and to the applicants' expertise and prior work. The timing is perfect with current developments in deep learning methods, existing data archive, strong preliminary results, team's expertise, established collaborations and partnerships, and documented success with attracting funding from the NIHR. Our research programme holds clear potential benefits for the individuals, society, clinicians, and the NHS. Given the existing database, substantial prior work, enormity of NHS litigation claims, costs of unnecessary operative deliveries, and the quality-adjusted life-years lost to intrapartum hypoxic brain injury, this project represents excellent value for money."
6,9A2BC9D4-E017-441B-A5AF-FADE05B15C60,An insect-inspired approach to robotic grasping,"To be really useful, robots need to interact with objects in the world. The current inability of robots to grasp diverse objects with efficiency and reliability severely limits their range of application. Agriculture, mining and environmental clean-up arejust three examples where - unlike a factory - the items to be handled could have a huge variety of shapes and appearances, need to be identified amongst clutter, and need to be grasped firmly for transport while avoiding damage. Secure grasp of unknown objects amongst clutter remains an unsolved problem for robotics, despite improvements in 3Dsensing and reconstruction, in manipulator sophistication and the recent use of large-scale machine learning.

This project proposes a new approach inspired by the high competence exhibited by ants when performing the closely equivalent task of collecting and manipulating diverse food items. Ants have relatvely simple, robot-like 'grippers' (their mouth-parts, called 'mandibles'), limited sensing (mostly tactile, using their antennae) and tiny brains. Yet they are able to pick up and carry a wide diversity of food items, from seeds to other insect prey, which can vary enormously in shape, size, rigidity and manouverability. They can quickly choose between multiple items and find an effective position to make their grasp, readjusting if necessary. Replicating even part of this competence on robots would be a significant advance. Grasping thus makes an ideal target for applying biorobotic methods that my group has previously used with substantial success to understand and mimic insect navigation behaviours on robots.

How does an ant pick up an object? The first part of this project will be to set up the methods required to observe and analyse in detail the behaviour of ants interacting with objects. At the same time we will start to build both simulated and real robot systems that allow us to imitate the actions of an ant as it positions its body, head and mouth to make a grasp; using an omnidirectional robot base with an arm and gripper. We will also examine and imitate the sensory systems usedby the ant to determine the position, shape and size of the object before making a grasp.

What happens in the ant's brain when it picks up an object? The second part will explore what algorithms insect brains need to compute to be able to make efficient and effective grasping decisions. Grasping is a task that contains in miniature many key issues in robot intelligence. It involves tight coupling of physical, perceptual and control systems. It involves a hierarchy of control decisions (whether to grasp, how to position the body and actuators, precise contact, dealing with uncertainty, detecting failure). It requires fusion of sensory information and transformation into the action state space, and involves prediction, planning and adaptation. We aim tounderstand how insects solve these problems as a route to efficient and effective solutions for robotics.

Can a robot perform as well as an ant? The final part will test the systems we have developed in real world tasks. The first task will be to perform an object clearing task, which will also allow benchmarking of the developed system against existing research. The second task will be based ona pressing problem in environmental clean-up: detection and removal of small plastic items from amongst shoreline rocksand gravel. This novel area of research promises significant pay-off from translating biological understanding into technical advance because it addresses an important unsolved challenge for which the ant is an ideal animal model.",,
7,FA47E149-C01D-4E5B-841A-6ACDCC30D239,Integrating hospital outpatient letters into the healthcare data space,"The importance of analysing health data collected as part of clinical care and stored in electronic health records is well-established. This has led to vital research about the occurrence and progression of disease, treatment effectiveness and safety, and health service delivery. The current Covid-19 pandemic has demonstrated the public health need to efficiently use data collected at the point of care to rapidly understand patterns, risk factors and outcomes of emerging diseases. Much of this work comes from primary care electronic health records, where general practitioners (GPs) enter and use structured, coded healthcare data. The picture in hospitals, however, is very different. 

One in four people in the UK live with one or more long-term conditions like cardiovascular diseases, chronic respiratory diseases, type 2 diabetes, arthritis and cancer, which account for 70% of the NHS budget. Specialised opinion about management of long-term conditions (LTCs) is provided through hospital outpatient care. Data and insight from outpatient clinics, however, is almost entirely absent. There is, surprisingly, no national system for recording diagnoses in hospital outpatient clinics. Information about key clinical events is instead recorded in outpatient letters, which are primarily used to communicate with patients and GPs. The ways in which letters are written and their sensitive content mean that they are not available for larger-scale &quot;secondary use&quot;, i.e. to support clinical practice, research or service improvement. For example, shielding for the current pandemic relied on hospital clinical teams going through patient letters manually to identify those who needed shielding based on free-text information about diagnoses and medications, with clear time constraints and risks to under- and over-shield patients. 

Natural language processing (NLP) and text mining develop computer algorithms to automatically extract relevant information from free-text documents. This project will establish a partnership between academia, secondary care and industry to develop a standards-based information management framework to safely unlock information stored in outpatient letters, link it with other health data and demonstrate its impact and benefits through two case studies. We will develop new methods to extract key clinical events from letters and represent their details (e.g. medication used, duration of symptoms) in a computerised form so that it can be easily accessed. In doing so, we will use the NHS-adopted standards so that the outpatient letters can be linked to other hospital databases and do not live in their own silo. The protection of sensitive data that potentially appear in outpatient data is a prime concern, so we will develop clear rules on who and how can access such data, in particular considering that third parties (e.g. industry) may need to access that data for developing their tools. These rules will be developed in a close collaboration between patient representatives, clinicians and specialists to ensure safeguards, public trust and transparency of decision making. 

We will demonstrate the potential impact of the proposed methods through two case studies with our clinical and business partners. Our first case study will demonstrate how the proposed models can assist in timely, efficient, dynamic and transparent identification of patients for shielding in a pandemic, or for vaccination prioritisation. In the second case study, we will illustrate how the same information can be used address important gaps in our knowledge about health and care, including, for example, disease prevalence and drug utilisation patterns. All outputs will be developed in a way that can be scaled beyond the single clinical site and single speciality.",,
8,53C40760-E3BE-421F-8A54-122A3779521A,Security of Digital Twins in Manufacturing,"Increasing productivity in manufacturing is a critical economic goal of the UK government and digitisation has been proposed as the cornerstone of achieving that. The MadeSmarter review makes a case for widespread digitisation across sectors (including manufacturing) and indicates the economic benefits that would accrue to the UK in doing so. It also draws significant attention to the role to be played by so-called Digital Twins.


Gartner has defined a digital twin as &quot;a software design pattern that represents a physical object with the objective of understanding the asset's state, responding to changes, improving business operations and adding value&quot; and describing a DT as &quot;a digital representation of a real-world entity or system.&quot; The implementation of a digital twin is an encapsulated software object or model that mirrors a unique physical object, process, organization, person or other abstraction.&quot; For advanced manufacturing a DT has been described by the AMRC as &quot;a live digital coupling of the state of a physical asset or process to a virtual representation with a functional output.&quot; Functional output here means information sent to a system or human observer that is actionable to deliver value. 

There are many views on the precise nature of Twins Twins. Loosely speaking, there is a physical system or sensors, actuators and other assets or entities of which a &quot;digital mirror&quot; is maintained. Essentially, this is some digital model of important aspects of the system. The AMRC definition draws attention to the real-time (&quot;live&quot;) nature of Digital Twins in manufacturing. This digital model can serve many purposes, from acting as the vehicle for remote interaction with the system by its operators (and remote operation has acquired a new importance in the light of the need to develop resilience to pandemics) to being the prmiary reference model over which intrusions are detected. Digital Twins have been identified by Gartner as one of the major technologies of our time. 

Since Digital Twins are perceived as fundamental to value generation by systems so it is no surprise that their security has arisen as a problem. They may encapsulate important IPR and provide the most up to reference for the system's state. That information itself may be confidential and its integrity is critical to the effectiveness of a system to deliver ts business goals. 

Understanding of the security of Digital Twins is limited. There has hardly been any reseach in this area. In this proposal we advance a wide-ranging initial programme of work that will engage stakeholders and lead eventually to a comprehensive understanding of security priorities concerning Digital Twins. Our programme mixes concrete research with engagement and roadmapping. It fuses the use of formal mathematic approaches to specification of systems and proofs of their properties, through to exploiting machine learning to detect intrusions. Our proposal also brings to bear expertise in manufacturing, robtics and control engineering. It is significantly interdisciplinary. 

At its conclusion we will have a community aware of the risks of Digital Twins and with a fully informed sense of priorities for research and innovation. We will initiate new areas of research but also seek to understand the potential for cross-pollination and transfer of research insights from other domains.",,
9,E832210C-94DC-4BA8-8365-42BFD952AE32,Window into the Mind: Handheld Spectroscopic Eye-safe Device (EyeD) for Neurodiagnostics,"Traumatic brain injury (TBI) is a leading cause of morbidity and mortality worldwide, with a high complication rate requiring long-term care, creates prolonged post-traumatic neurological disorders and is potentially fatal with annual socioeconomic cost in the UK of &pound;7.5 Billion per year. While critical decisions affecting treatment must be made rapidly, TBI is notoriously hard to diagnose pre-hospitalisation, sometimes resulting in incorrect patient management. Timely assessment of injury severity is a priority in the correct treatment of TBI patients. However, this is poorly supported by current technologies, which fall short of the diagnostic needs, exhibiting poor-sensitivity, special-handling requirements and complicated, costly procedures.

A non-invasive portable technique to diagnose and monitor TBI and neurodegenerative diseases is proposed, by measuring changes to the optic nerve, visible at the back of the eye. The optic nerve, bathed in cerebrospinal fluid, which is in continuity with the central nervous system, constitutes an optically clear 'window to the brain'. The aim is to develop a portable technology to detect biochemical changes in cerebrospinal fluid in response to brain injury using specialised optical collected using a technique known as Raman spectroscopy. This provides a non-invasive, highly-sensitive method for the detection of biomarkers in the eye, and yet it can be packaged as a low-cost, hand-held device. 

For delivering such a sensitive and rapid diagnostic technology, it is crucial to accurately identify these specialised Raman signals originating from different parts of the eye. To tackle these challenges, advanced computational methods, known as &quot;machine learning&quot;, will be developed to embrace the 'noise' from the data and enable a generic framework for intelligent diagnosis. Unlike traditional packages, this method will perform data analysis directly in the web browser using cloud technologies as an open-source. Building upon these, this innovative technology will allow TBI measurements to take place at the point-of-care via a non-ionizing scan of the back-of-the-eye to detect biochemical changes without the need for a painful lumbar puncture (to extract the cerebrospinal fluid) or expensive, dangerous radiological scans. 

Our prime objective is to deliver a technology offering improved health, more effective patient-care and a better quality of life for patients suffering from neurotrauma. It will be designed for use on-site by doctors and paramedic crews to provide timely and cost-effective diagnosis and triaging and will be used by ambulance trusts, sports organisations, GPs, hospitals and the Ministry of Defence. Rapid diagnosis in the early-clinical phase in a non-invasive, cost-effective way will lay the platform for a range of improvements in personalised medicine and management. Predominantly focussed on timely TBI-detection, our device would allow for better patient triaging, reducing the strain on the healthcare system. In addition to delivering the timely intervention and organised trauma-care to nearly a million individuals nationally, it will decelerate the patients' cognitive decline, reduce in-hospital mortality, save thousands of lives a year, avoid long-term hospital stays, and reduce a major burden on the NHS and the taxpayer.",,
10,F7C3E1B2-9D9D-43CD-AAEB-B4D40E5ADEE9,Window into the Mind: Handheld Spectroscopic Eye-safe Device (EyeD) for Neurodiagnostics,"Traumatic brain injury (TBI) is a leading cause of morbidity and mortality worldwide, with a high complication rate requiring long-term care, creates prolonged post-traumatic neurological disorders and is potentially fatal with annual socioeconomic cost in the UK of &pound;7.5 Billion per year. While critical decisions affecting treatment must be made rapidly, TBI is notoriously hard to diagnose pre-hospitalisation, sometimes resulting in incorrect patient management. Timely assessment of injury severity is a priority in the correct treatment of TBI patients. However, this is poorly supported by current technologies, which fall short of the diagnostic needs, exhibiting poor-sensitivity, special-handling requirements and complicated, costly procedures.

A non-invasive portable technique to diagnose and monitor TBI and neurodegenerative diseases is proposed, by measuring changes to the optic nerve, visible at the back of the eye. The optic nerve, bathed in cerebrospinal fluid, which is in continuity with the central nervous system, constitutes an optically clear 'window to the brain'. The aim is to develop a portable technology to detect biochemical changes in cerebrospinal fluid in response to brain injury using specialised optical collected using a technique known as Raman spectroscopy. This provides a non-invasive, highly-sensitive method for the detection of biomarkers in the eye, and yet it can be packaged as a low-cost, hand-held device. 

For delivering such a sensitive and rapid diagnostic technology, it is crucial to accurately identify these specialised Raman signals originating from different parts of the eye. To tackle these challenges, advanced computational methods, known as &quot;machine learning&quot;, will be developed to embrace the 'noise' from the data and enable a generic framework for intelligent diagnosis. Unlike traditional packages, this method will perform data analysis directly in the web browser using cloud technologies as an open-source. Building upon these, this innovative technology will allow TBI measurements to take place at the point-of-care via a non-ionizing scan of the back-of-the-eye to detect biochemical changes without the need for a painful lumbar puncture (to extract the cerebrospinal fluid) or expensive, dangerous radiological scans. 

Our prime objective is to deliver a technology offering improved health, more effective patient-care and a better quality of life for patients suffering from neurotrauma. It will be designed for use on-site by doctors and paramedic crews to provide timely and cost-effective diagnosis and triaging and will be used by ambulance trusts, sports organisations, GPs, hospitals and the Ministry of Defence. Rapid diagnosis in the early-clinical phase in a non-invasive, cost-effective way will lay the platform for a range of improvements in personalised medicine and management. Predominantly focussed on timely TBI-detection, our device would allow for better patient triaging, reducing the strain on the healthcare system. In addition to delivering the timely intervention and organised trauma-care to nearly a million individuals nationally, it will decelerate the patients' cognitive decline, reduce in-hospital mortality, save thousands of lives a year, avoid long-term hospital stays, and reduce a major burden on the NHS and the taxpayer.",,
11,1851A8D7-69BF-42DE-A06D-C07BFD65AEC0,LISI - Learning to Imitate Nonverbal Communication Dynamics for Human-Robot Social Interaction,"We are approaching a future where robots will progressively become widespread in many aspects of our daily lives, including education, healthcare, work and personal use. All of these practical applications require that humans and robots work together in human environments, where social interaction is unavoidable. Along with verbal communication, successful social interaction is closely coupled with the interplay between nonverbal perception and action mechanisms, such as observation of one's gaze behaviour and following their attention, coordinating the form and function of hand-arm gestures. Humans perform social interaction in an instinctive and adaptive manner, with no effort. For robots to be successful in our social landscape, they should therefore engage in social interactions in a human-like manner, with increasing levels of autonomy. 

Despite the exponential growth in the fields of human-robot interaction and social robotics, the capabilities of current social robots are still limited. First, most of the interaction contexts has been handled through tele-operation, whereby a human operator controls the robot remotely. However, this approach will be labour-intensive and impractical as the robots become more commonplace in our society. Second, designing interaction logic by manually programming each behaviour is exceptionally difficult, taking into account the complexity of the problem. Once fixed, it will be limited, not transferrable to unseen interaction contexts, and not robust to unpredicted inputs from the robot's environment (e.g., sensor noise). 

Data-driven approaches are a promising path for addressing these shortcomings as modelling human-human interaction is the most natural guide to designing human-robot interaction interfaces that can be usable and understandable by everyone. This project aims (1) to develop novel methods for learning the principles of human-human interaction autonomously from data and learning to imitate these principles via robots using the techniques of computer vision and machine learning, and (2) to synergistically integrate these methods into the perception and control of real humanoid robots. This project will set the basis for the next generation of robots that will be able to learn how to act in concert with humans by watching human-human interaction videos.",,
12,7F975C71-7E66-4DA0-947D-77F81A8AE9C0,Digital navigation of chemical space for function,"Materials both enable the technologies we rely on today and drive advances in scientific understanding. The new scientific phenomena produced by novel materials (for example, lithium transition metal oxides) enable the creation of technologies (electric vehicles), emphasising the connection between the capability to create new materials and economic prosperity. New materials offer a route to clean growth that is essential for the future of society in the face of climate change and resource scarcity.

To harness the power of functional materials for a sustainable future, we must improve our ability to identify them. This is a daunting task, because materials are assembled from the vast and largely unknown coupled chemical and structural spaces. As a result, we are forced to work mostly by analogy with known materials to identify new ones. This necessarily incremental approach restricts the diversity of outcome from both scientific and technological perspectives. We need to be able to design materials beyond this &quot;paradigm of analogues&quot; if we are to exploit their potential to tackle societal challenges.

This project will transform our ability to access functional materials with unprecedented chemical and structural diversity by fusing physical and computer science. We will develop a digital discovery platform that will advance the frontier of knowledge by creating new materials classes with novel structure and bonding and tackle key application challenges, thus focussing the developed capability on well-defined targets of scientific novelty and application performance. The discovery platform will be shaped by the need to identify new materials and by the performance needed in applications. This performance is both enabled by and creates the need for the new materials classes, emphasising the interdependent nature of the project strands.

We will strengthen cutting-edge physical science (PS) capability and thinking by exploiting the extensive synergies with computer science (CS), to boost the ability of the physical scientist to navigate the space of possible materials. Computers can assimilate large databases and handle multivariate complexity in a complementary way to human experts, so we will develop models that fuse the knowledge and needs from PS with the insights from CS on how to balance precision and efficiency in the quest for promising regions in chemical space. The development of mixed techniques that use explainable symbolic AI-based automated reasoning and model construction approaches coupled with machine learning is just one example that illustrates how this opportunity goes far beyond interpolative machine learning, itself valuable as a baseline evaluation of our current knowledge.

By working collaboratively across the CS/PS interface, we can digitally explore the unknown space, informed and guided by PS expertise, to transform our ability to harvest disruptive functional materials. Only testing against the hard constraints of PS novelty and functional value will drive the discovery platform to the level needed to deliver this aim. As we are navigating uncharted space, the tools and models that we develop will be compass-like guides, rather than satellite navigation-like directors, for the expert PS team. The magnitude of the opportunity to transform materials discovery produces intense international competition with significant investments at pace from industry (e.g., Toyota Research Institute $1bn) and government (e.g., DoE $27m; a new centre at NIMS, Japan, both in 2019). Our transformative vision exploits recent UK advances in autonomous robotic researchers and artificial intelligence-guided identification of outperforming functional materials that are not based on analogues. The scale and flexibility of this PG will ensure the UK is at the forefront of this vital area.",,
13,18B8B288-503A-4810-B8E7-344404B44E58,Autonomous NAnotech GRAph Memory (ANAGRAM),"Artificial intelligence (AI) is transforming our societies, but the more it proliferates, the higher the customer demands for functionality and efficiency (most notably energy). Thus, as time progresses the limitations of statistical learning-based AI that has underpinned most AI work so far are beginning to naturally become more exposed. Tasks such as variable binding and manipulation, inductive reasoning and 1-shot learning, at which statistical learning is not as strong, suggest solutions in the sphere of abstract symbol processing AI. The commonly referenced 'next wave of AI' that is capable of such exploits (towards &quot;strong AI&quot;) is likely to make extensive use of symbol processing capabilities and simultaneously demand a bespoke set of hardware solutions. The proposed project primarily addresses the issue of developing general-purpose (platform-level) hardware for precisely symbolic AI.

The proposed project seeks to develop a memory module that features: a) an internal structure and b) in-memory computing capabilities that render it particularly suitable for symbolic processing-based artificial intelligence (AI) systems. Ultimately the project seeks to deliver: 1) Two microchip iterations prototyping the memory system. 2) A software environment (infrastructure) for easy programming and operation of the resulting microchips (includes simulation capabilities for proof-of-concept tests). 3) A demonstration of the memory cell operating together with a symbolic processor as an aggregate system. 4) A functioning set of starter applications illustrating the capabilities of the design.

The overall effort is driven by a philosophy of co-optimising the memory across the entire trio of fundamental device components, symbolic AI mechanics and hardware design facets. Specifically: functionality in the proposed memory system will be pursued by: a) Designing a resistive RAM-based (ReRAM) memory unit where operation of the ReRAM devices and ReRAM tech specifications themselves are subservient to the specific operational goals of the memory system. b) Adapting the mathematical machinery of the system in order to map functional operations to hardware-friendly machine-code level operations: the stress is on hardware-friendliness, not mathematical elegance. This will be inextricably linked to the design of the memory's instruction set. c) Designing an architecture that runs the symbolic memory efficiently by using memory allocation techniques that maximise locality and making extensive use of power-gating. Simultaneously, implementation of a solid software stack infrastructure will enable efficient and fast prototyping and hypothesis testing.

The cornerstone of the targeted project impact is to lay the foundations for launching an industrial-scale design effort towards hardware for symbolic AI. Hence the bulk of the effort is in chip design (prototype-based de-risking of the idea) and toolchain development (impact acceleration by lowering barriers to user uptake). Simultaneously, it is expected that the project will play a significant role in enhancing interest in symbol-level AI and very crucially, inducing interest in connecting symbolic AI with statistical learning one; thereby significant impact on knowledge is achieved. Finally, the increased in the capabilities of AI, as well as the transparency of decision-making (typically readily expressible via formal expressions or even in pseudo-natural language) offered by the symbolic approach promise to make a significant impact in enhancing acceptance of AI by society, providing a solid scientific foundation for certification processes (AI trust - broadening the scope of applications that accept an AI solution). With hardware available for this task, significant impact on productivity and quality of life is to be achieved.

The project is self-contained and is designed to launch a much broader, sustainable effort, headed by the PI in this field.",,
14,5B8AD322-DDB1-41D5-8A78-8C54491B0ADB,Integrating Clinical Infrared and Raman Spectroscopy with digital pathology and AI: CLIRPath-AI,"A key feature of the diagnosis of any disease, but particularly various forms of cancer, is the critical information obtained through a biopsy. A biopsy involves the removal of a small sample of tissue, or a few cells, from the patient for examination by a pathologist looking down an optical microscope. In current practice is that the sample is stained with a combination of dyes to help gain some contrast in the image which helps the pathologist see the cells. Generally, based upon this visual inspection of the sample and other relevant medical information, a diagnosis is made. This process, however, is far from ideal since it relies on the expertise of the clinician concerned and is subject to intra in inter observer error. (In other words the process is not exact and depends upon the opinion of the clinicians which may differ). Recently a number of developments have been made in the field of Digital Pathology and Artificial Intelligence (AI). This is where a high resolution photograph of the biopsy slide is taken and examined by a computer algorithm which helps the pathologist make a diagnosis. However analysing the data from just the visible region of the spectrum severely restricts information content of the images obtained. Recently a number of proof of concept studies have shown that molecular spectroscopic techniques such as infrared and Raman are capable of distinguishing diseased from non-diseased cells and tissue based upon the inherent chemistry contained within the cells. (These regions of the spectrum have 40 times the bandwidth of the visible and therefore contain 40 times the amount of information.) 

The UK is at the forefront in developments associated with both Digital Pathology and AI, the latter augmented by five new technology centres funded by the Industrial Strategy Challenge Fund. In addition, partly due to an EPSRC funded network (CLIRSPEC) the UK is also world leading in the field biomedical infrared and Raman spectroscopy. At present however the Digital pathology/AI and biomedical infrared/Raman these two communities are separate and are not interacting. As a result therefore, the advances made in one area are not being translated to another. In both areas of research there are many hurdles that need to be overcome if this technology is to move from the proof of concept stage through the translational stage and into the clinical setting. It is the belief of the academic community that we are much more likely to overcome these hurdles if we pool our resources, bring in both industrial and clinical partners and work on these generic problems together. This application is for funding to support such a network of partners that will develop dynamic and synergistic interaction between these separate communities for the next four years, for the specific aim of benefiting patients.",,
15,07CE573B-86EA-4E30-AF16-76E8F30F2A34,Future blood testing for inclusive monitoring and personalised analytics Network+,"There is an extremely high demand for laboratory-based blood tests from community settings in the UK and analysis suggests an important role in the future for remote blood monitoring that would enable patients and health professionals to carry out their own tests remotely, greatly benefiting patients and speeding up decision making. The COVID-19 pandemic has further highlighted the need for remote and connected blood testing that is beyond the online virtual clinics in the NHS outpatient setting. In current blood testing services for community healthcare, it is challenging to obtain and process blood samples outside of the clinical setting without training and lab facilities, and patients are required to attend a GP surgery or hospital for tests with travel burden and infection risk. Many blood analyses are done in batches that take a long time to build up, meaning the speed of blood sample analysis of routine tests and time taken for diagnosis are further challenges. Despite recent innovations in point of care, current blood analysis tools in practice are mainly mechanical or labour-intensive that require extensive filtering and manual tweaking and not suitable for regular at-home monitoring and longitudinal analytics. There is no personalised real-time approach available to inform disease complexity and conditions over time, which are critical for early detection of acute diseases and the management of chronic conditions. In England, around 95% of clinical pathways rely on patients having access to efficient, timely and cost-effective pathology services and there are 500 million biochemistry and 130 million haematology tests are carried out per year. This means inefficient and infrequent blood testing leads to late diagnosis, incomplete knowledge of disease progression and potential complications in a wide range of populations. Taking those challenges into account and current digital transformation in healthcare, this is a timely opportunity to bring researchers, clinicians and industrialist together to address the challenges of blood monitoring and analytics.
The proposed Network+ will build an interdisciplinary community that will explore future blood testing solutions to achieve remote, inclusive, rapid, affordable and personalised blood monitoring, and address the above challenges in community health and care. To achieve the Network+ vision, research of technologies will be conducted from collaborations among information and communication technology (ICT), data and analytical science, clinical science, applied optics, biochemistry, engineering and social sciences in the Network+. The network will address three key technical challenges in blood testing: Remote monitoring, ICT, Personalised data and AI in a range of examplar clinical areas including cancer, autoimmune diseases, sickle cell disease, preoperative care, pathology services and general primary care.",,
16,08B976EE-2B57-4A62-BD1F-8FB7E4646947,Machine Learning for the Discovery of Metal-Organic Frameworks for Hydrogen Storage Applications,"EPSRC : Paula Nkulikiyinka : EP/R513027/1
Metal-organic frameworks (MOFs) are a class of crystalline materials with ultrahigh porosity and high surface areas. With these properties, along with variability for both the organic and inorganic components of their structures, MOFs are of interest for potential applications in clean energy, most significantly as storage media for gases such as hydrogen and methane, and as high-capacity adsorbents to meet various separation needs. The use of quantitative structure-property/activity relationships (QSPRs - a form of machine learning) is an emerging and helpful mathematical tool that allows the link between physical or chemical properties to predict the behaviour or desired characteristic of a molecule. The purpose of this study is to exploit both fields in order to obtain the optimal MOF for hydrogen storage, as well as investigating what parameters affect the hydrogen uptake capabilities. Following the screening of optimal MOFs, they will be synthesized in the lab to test performance and validate the methodology.
This research will be conducted by a PhD student (Paula Nkulikiyinka, who is funded by an EPSRC DTP grant) in collaboration with Dr A Howarth's research group at Concordia University, Canada. The research proposal has been discussed and agreed by both Dr Clough and Dr Howarth.",,
17,B1EC98BF-97EF-4348-8BD9-A5682814D8D9,Machine Learning to Unravel Anti-Ageing Compounds,"The ageing process is one of the greatest scientific mysteries of our time. Ageing of the population is also a major biomedical, social and economic challenge. In the past two decades, a number of drugs have been identified that can modulate ageing and preserve health in model organisms, from invertebrates like worms to mammals such as mice. In spite of this recent progress, understanding the best strategies to develop human anti-ageing interventions and preserve health in old age is still very incomplete. Besides, although a few anti-ageing drugs are being clinically tested as treatments for age-related diseases, including drugs initially associated with ageing in worms, new methods are necessary to identify and prioritize drugs that can be suitable for human applications. Indeed, identifying new longevity drugs is of widespread interest. 

In this project, we will develop new computational methods that take advantage of large amounts of multi-omics data available on the web, and our own compilations of the effects of hundreds of ageing-related drugs, to predict new drugs with anti-ageing properties. Specifically, methods will be developed that determine which attributes make some drugs extend animal lifespan. Furthermore, once we determine which attributes these drugs have in common, we can predict novel pro-longevity, health-promoting drugs. Worms will be employed for the experimental validation of the machine learning findings in this project because of their short lifespans and high fecundity. 

All methods developed will be made available to the scientific community to help guide experiments, including in other organisms. Moreover, we will develop a webserver for predicting life-extending compounds that will be made freely available online. Overall, this project will provide a significant impetus to employing predictive biology in ageing studies.","Ageing can be manipulated in model systems by hundreds of genes and compounds. Nonetheless, ageing is still a poorly understood process, and identifying the most important modulators of ageing remains a challenge. Given the intrinsic costs of performing animal ageing studies, developing predictive computational tools is of utmost importance, and the accuracy and specificity of current predictive computational methods is still very limited.

In this project we will employ machine learning to provide insights into life-extending drugs and predict those with the greatest potential for human translation. Specifically, we aim to: 1) Apply machine learning methods to predict new life-extending effects of compounds in animal models; 2) Test if the main compounds with new life-extending effects predicted in Aim #1 extend worms' lifespan and healthspan; 3) Predict and prioritize compounds with potential for retarding mammalian ageing using machine learning.

The methods developed could be employed in other organisms and will be made available to the research community. Moreover, we will develop a webserver for predicting life-extending compounds that will be made freely available online. Therefore, this project will establish novel computational approaches suitable to guide hypothesis-driven studies on ageing that will be applicable to other systems. 

Our proposed project will open up new directions of research, and potentially accelerate the development of medicines for ageing diseases and open opportunities for future translational research.",
18,38D8A0E8-4595-429B-8BEE-3E3417A0F30A,ConCur: Knowledge Base Construction and Curation,"Knowledge graphs are graph-structured knowledge resources which are often expressed as triples such as (&quot;UK&quot;, &quot;hasCapital&quot;, &quot;London&quot;) and (&quot;London&quot;, &quot;instanceOf&quot;, &quot;City&quot;). As well as such basic &quot;facts&quot;, knowledge graphs often include structural knowledge about the domain, typically based on a hierarchy of entity types (AKA classes or concepts); e.g., (&quot;City&quot;, &quot;subClassOf&quot;, &quot;HumanSettlement&quot;). A knowledge graph that consist largely or wholly of structural knowledge is often called an ontology.

Some knowledge graphs are general purpose, such as Wikidata and the Google knowledge graph, while others are developed for specific domains such as medicine. They are rapidly gaining in importance and are playing a key role in many applications. For example, Google uses its knowledge graph for search, question answering and Google Assistant, while Amazon and Apple also use knowledge graphs to power their personal assistants Alexa and Siri, respectively. Knowledge graphs are widely used in the domain of health and wellbeing, e.g., for organising and exchanging information and to power clinical artificial intelligence (AI). One example is FoodOn, an ontology representing food knowledge such as fine-grained food product categorization, nutrition and allergens, as well as related activities such as agriculture.

Knowledge graph construction and maintenance is, however, very challenging, and may require a considerable amount of human effort. Notwithstanding the high cost of knowledge creation, knowledge graphs are often still biased, incomplete or too coarse-grained. Take HeLis, an ontology for health and lifestyle, as an example. Its food knowledge is quite simple and often represents many different variants with a single entity (e.g., &quot;Banana&quot; for all kinds and derivatives of bananas), and its knowledge of health is highly incomplete when compared with dedicated biomedical ontologies. In addition, it is hard to avoid errors such as incorrect facts and categorisations in knowledge graphs; e.g., FoodOn categorises soy milk as a kind of milk, but not as a kind of soy product. Such errors may be inherited from the information source or be caused by the construction procedure. These issues significantly impact the usefulness of knowledge graphs and the reliability of the systems that use them; e.g., the categorisation of soy milk could be dangerous if the knowledge graph were used in a food allergen alert system.

Therefore, effective knowledge graph construction and curation is urgently required and will play a critical role in exploiting the full value of knowledge graphs. As there are now many available knowledge resources, one possible approach is to use multiple sources to address both coverage and quality issues, e.g., via integration and cross-checking. For example, integrating HeLis with FoodOn would combine fine-grained categorization of food products (including bananas) with lifestyle knowledge. Moreover, cross-checking FoodOn with HeLis will reveal the problem with soy milk, which is correctly categorized as a soy product in HeLis. Automating the integration of knowledge resources is challenging, but combining semantic and learning-based techniques seems to be a very promising approach, and we have already obtained some encouraging preliminary results in this direction.

The proposed research will therefore study a range of semantic and machine learning techniques, and how to combine them to support knowledge graph construction and curation. As well as its application to knowledge graph construction and curation, this research will also contribute to the development of new neural-symbolic theories, paradigms and methods, such as deep semantic embedding for learning representations for expressive knowledge, and knowledge-guided learning for addressing sample shortage problems. These techniques promise to revolutionize many AI and big data technologies.",,
19,1EBF1E1F-4318-441F-A7EB-BD0321D6B433,FAIR: Framework for responsible adoption of Artificial Intelligence in the financial seRvices industry,"AI technologies have the potential to unlock significant growth for the UK financial services sector through novel personalised products and services, improved cost-efficiency, increased consumer confidence, and more effective management of financial, systemic, and security risks. However, there are currently significant barriers to adoption of these technologies, which stem from a capability deficit in translating high-level principles (of which there is an abundance) concerning trustworthy design, development and deployment of AI technologies (&quot;trustworthy AI&quot;), including safety, fairness, privacy-awareness, security, transparency, accountability, robustness and resilience, to concrete engineering, governance, and commercial practice. 

In developing an actionable framework for trustworthy AI, the major research challenge that needs to be overcome lies in resolving the tensions and tradeoffs which inevitably arise between all these aspects when considering specific application settings.For example, reducing systemic risk may require data sharing that creates security risks; testing algorithms for fairness may require gathering more sensitive personal data; increasing the accuracy of predictive models may pose threats to fair treatment of customers; improved transparency may open systems up to being &quot;gamed&quot; by adversarial actors, creating vulnerabilities to system-wide risks. 

This comes with a business challenge to match. Financial service providers that are adopting AI approaches will experience a profound transformation in key areas of business as customer engagement, risk, decisioning, compliance and other functions transition to largely data-driven and algorithmically mediated processes that involve less and less human oversight. Yet, adapting current innovation, governance, partnership and stakeholder relation management practice in response to these changes can only be successfully achieved once assurances can be confidently given regarding the trustworthiness of target AI applications. 

Our research hypothesis is based on recognising the close interplay between these research and business challenges: Notions of trustworthiness in AI can only be operationalised sufficiently to provide necessary assurances in a concrete business setting that generates specific requirements to drive fundamental research into practical solutions, with solutions which balance all of these potentially conflicting requirements simultaneously.

Recognising the importance of close industry-academia collaboration to enable responsible innovation in this area, the partnership will embark on a systematic programme of industrially-driven interdisciplinary research, building on the strength of the existing Turing-HSBC partnership. 

It will achieve a step change in terms of the ability of financial service providers to enable trustworthy data-driven decision making while enhancing their resilience, accountability and operational robustness using AI by improving our understanding of sequential data-driven decision making, privacy- and security- enhancing technologies, methods to balance ethical, commercial, and regulatory requirements, the connection between micro- and macro-level risk, validation and certification methods for AI models, and synthetic data generation. 

To help drive innovation across the industry in a safe way which will help establish the appropriate regulatory and governance framework, and a common &quot;sandbox&quot; environment to enable experimentation with emerging solutions and to test their viability in a real-world business context. This will also provide the cornerstone for impact anticipation and continual stakeholder engagement in the spirit of responsible research and innovation.",,
20,444590DD-6BF3-45AD-A301-AC9DC7898A72,ProTechThem: Building Awareness for Safer and Technology-Savvy Sharenting,"Especially over the past 15 years, the creation of new &quot;online identities&quot; (the social identity that we acquire in cyberspace) and the expansion of the usability of our &quot;digital identity&quot; (the digital storage of our attributed, biographical or even biological identities) have entailed, alongside many advantages, also new and emerging risks and crime vulnerabilities, as identity information can be misused in many ways and create severe harms. Existing research in this context has so far focused on the illegal access to personal information (e.g. through hacking or social engineering techniques) but has overlooked the risky behaviours of individuals willingly sharing identifying (and potentially sensitive) information online. In this context, an area of particular interest that has been particularly overlooked is the one connected to the sharing of identifying and sensitive information of minors, who are often overexposed online in good-faith by parents and guardians in so called &quot;sharenting&quot; practices. Beyond risks due to negative psychological repercussions in ignoring children's desire to having (or not) an online identity, there are concerns regarding the potential for grooming and child abuse, and the potential for identity crimes (such as identity fraud and identity theft), especially keeping in mind that today's children, in a few years, will be those employing digital identities in many aspects of their lives, and will need a clean and curated digital identity to be fully part of many aspects of our society.

The proposed project combines traditional and innovative cross-disciplinary approaches to further this emerging line of inquiry. The project does so by offering a better understanding of sharenting practices, their motivations, and the risks associated with them. It provides a better understanding of the existing technical and regulatory loopholes and gaps enabling potentially harmful sharenting practices. It also develops a better understanding of the perception of the problem by parents and guardians (our &quot;target population&quot;). The project can therefore enable better targeted awareness-raising activities; an improvement of the tools we currently have to study, prevent and mitigate the negative impacts of sharenting practices.

The result of this research will be of significant importance for social media users (and specifically for those in our target population) by raising awareness and promoting sustained behavioural change to minimise cyber risks. The results will also be of relevance for the work of law enforcement in better addressing crimes potentially facilitated by certain sharenting practices such as grooming and identity crimes.

More in general, the proposed approach will improve our understanding of criminogenic opportunities available in social media, supporting new avenues of investigation. By integrating insights and expertise from criminology and computer sciences, the proposed project also has important implications for demonstrating interdisciplinary methodological developments and promoting best practice for ethical online research. 

The research project is structured around seven cumulative work-packages to allow the research team to build a solid body of original data (currently not available to researchers) but also to promote engagement and effective communication with a non-academic audience (primarily, law enforcement, and parents and guardians). Throughout the project, we will be supported by our Project Partners (UK Safer Internet Centre; Kidscape; Arma dei Carabinieri); together with Dame Prof. Wendy Hall and other stakeholders, the Project Partners will be also part of our Advisory Board.",,
21,D6F54E67-0BAC-4FDD-8CAE-3F7B2195C3C5,Machine Learning to Unravel Anti-Ageing Compounds,"The ageing process is one of the greatest scientific mysteries of our time. Ageing of the population is also a major biomedical, social and economic challenge. In the past two decades, a number of drugs have been identified that can modulate ageing and preserve health in model organisms, from invertebrates like worms to mammals such as mice. In spite of this recent progress, understanding the best strategies to develop human anti-ageing interventions and preserve health in old age is still very incomplete. Besides, although a few anti-ageing drugs are being clinically tested as treatments for age-related diseases, including drugs initially associated with ageing in worms, new methods are necessary to identify and prioritize drugs that can be suitable for human applications. Indeed, identifying new longevity drugs is of widespread interest. 

In this project, we will develop new computational methods that take advantage of large amounts of multi-omics data available on the web, and our own compilations of the effects of hundreds of ageing-related drugs, to predict new drugs with anti-ageing properties. Specifically, methods will be developed that determine which attributes make some drugs extend animal lifespan. Furthermore, once we determine which attributes these drugs have in common, we can predict novel pro-longevity, health-promoting drugs. Worms will be employed for the experimental validation of the machine learning findings in this project because of their short lifespans and high fecundity. 

All methods developed will be made available to the scientific community to help guide experiments, including in other organisms. Moreover, we will develop a webserver for predicting life-extending compounds that will be made freely available online. Overall, this project will provide a significant impetus to employing predictive biology in ageing studies.","Ageing can be manipulated in model systems by hundreds of genes and compounds. Nonetheless, ageing is still a poorly understood process, and identifying the most important modulators of ageing remains a challenge. Given the intrinsic costs of performing animal ageing studies, developing predictive computational tools is of utmost importance, and the accuracy and specificity of current predictive computational methods is still very limited.

In this project we will employ machine learning to provide insights into life-extending drugs and predict those with the greatest potential for human translation. Specifically, we aim to: 1) Apply machine learning methods to predict new life-extending effects of compounds in animal models; 2) Test if the main compounds with new life-extending effects predicted in Aim #1 extend worms' lifespan and healthspan; 3) Predict and prioritize compounds with potential for retarding mammalian ageing using machine learning.

The methods developed could be employed in other organisms and will be made available to the research community. Moreover, we will develop a webserver for predicting life-extending compounds that will be made freely available online. Therefore, this project will establish novel computational approaches suitable to guide hypothesis-driven studies on ageing that will be applicable to other systems. 

Our proposed project will open up new directions of research, and potentially accelerate the development of medicines for ageing diseases and open opportunities for future translational research.",
22,A0F4EF93-A110-4910-86C4-F7E8217F1A36,Swarm Social Sound Modulators,"The UK is a world-leader in creating interactive applications that are enabled by computational manipulation of acoustic wave fronts. Three examples of such applications include mid-air haptics, directional audio, and volumetric 3D particle displays. Using a phased array of ultrasonic speakers that are precisely and individually controlled, we can create high pressure focal points in mid-air. Modulating these focal points in various ways, it is possible to 1) create rich tactile sensations when touched by the bare hands, 2) steer directional sounds that propagate over long distances un-attenuated, 3) levitate small particles that when rapidly moved in space emulate volumetric 3D shapes due to a phenomenon called persistence of vision.

The exploitation of these amazing new technologies is uniquely available to Ultraleap (a UK based company) and Sussex University who have a long and productive history of collaboration. For example, Ultraleap is currently combining hand-tracking and ultrasonic mid-air haptic feedback solutions for applications ranging from VR training simulators, automotive interfaces, gaming machines, and next generation digital signage kiosks. Similarly, Sussex University is creating multimodal 3D displays based on rapidly updating ultrasonic phased arrays to create persistence of vision when moving acoustically levitated objects. A significant constraint to the wide-scale deployment of the underlying technology of phased arrays is the cost and complexity of a non-modular system because it limits applicability. For example, there is no one size fits all phased array with most integrated solutions needing to be custom developed.
In this project, we will circumvent such problems altogether by creating simple and low cost modular spatial sound modulator (SSM) units i.e. smaller arrays of acoustic sources, to be placed around the interactive space, that can collectively out-perform the single large monolithic solution we currently have. Moreover, we will take a leap forward in sound-field control by removing scalability and reusability issues thus opening up the exploitation of phased array technologies into other applications domains that can benefit from the non-contact delivery of haptic feedback, steerable directional sound, and/or volumetric 3D particle displays. Specifically, we will draw on the well-developed literature of multi-agent game theory and distributed computing and use them to build a decentralised swarm architecture that can flexibly accommodate numerous SSM units. Each SSM will emit and modulate the sound-field nearby it while sharing a common awareness of the contextual details with its swarm host, while the desired collective behaviour will emerge from the interactions between multiple SSMs and their interactions with the environment.

There are several anticipated benefits to our proposed approach. Firstly, by designing simple, independent SSMs we are able to address multiple commercial applications using the same primitive unit, while simultaneously streamlining the manufacturing pipeline. These modular units can be used individually or combined in a myriad of ways to create new applications. Secondly, by enabling a distributed control architecture, swarm SSMs can seamlessly and progressively scale up. By incrementally combining larger numbers of modular units, customers can initially use a small number of SSM units, and dynamically grow the capabilities of their interactive multimodal system by adding new devices according to the application needs. Finally, by using game theory we will enable SSMs to dynamically cooperate with each other as to meet application objectives independent of the application logic and the arrangement of our modular devices thus simplifying the development and design process and enabling creative designers to focus on the delivery of evermore immersive and multimodal experiences.",,
23,6E92A689-92EB-4039-B06F-1FFF2595F825,Three-dimensional electrical tomography for imaging large concrete members,"Concrete structures are ubiquitous and critical components of UK and international civil infrastructure, cityscapes, waste containment facilities, and much more. Now, more than ever, we demand more from our concrete structures, from safety, durability, economy, and environmental perspectives. For example, in the UK alone, there are approximately 67,400 highway bridges carrying heavy goods vehicles and over 70% are reinforced concrete, prestressed concrete, or concrete culvert bridges. Of these, nearly 70% were built between 1960 and 1990. Given this information, the cost of replacing or remediating ageing national bridges in the UK alone is huge. For example, the total maintenance backlog for council-managed (in Great Britain) road bridges has been estimated to be &pound;6.7bn. This indicates that council-managed concrete bridges in Great Britain are currently in a &pound;4.7bn maintenance backlog. The costs associated with deconstructing and reconstructing these bridges can be estimated as a figure approximately two orders of magnitude larger than the current maintenance backlog. Therefore, feasibly addressing contemporary concrete infrastructure challenges requires more than a monetary solution, it requires technical innovation. 

To begin addressing the demands of an ageing concrete infrastructure, we must first be able reliably assess the condition of concrete structures during their lifespan in order to, e.g., rehabilitate them before replacement or discontinuing service before failure. At present, however, many non-destructive testing/structural health monitoring (NDT/SHM) imaging methods offer limited information regarding the internal condition of concrete structures. 

This project responds to this issue by determining the feasibility of a new 3D approach (electrical tomography) for use in assessing the condition and health of concrete structural members, such as beams, slabs, and columns. Outcomes from the project have the potential to enable (i) experimental studies of the fundamental behaviour of reinforced concrete members by allowing researchers to quantifiably &quot;see&quot; inside members exposed to extreme environmental and/or loading conditions and (ii) quantitative 3D condition monitoring of constructed concrete structures. From an asset management perspective, this non-destructive technology would allow maintainers to &quot;see&quot; inside concrete structures. In doing this, maintainers can detect internal damage, such as cracking and reinforcement corrosion, and repair/replace individual members before they jeopardise safety or give rise to expensive systematic problems requiring replacement of the entire structure.",,
24,CB6FBF2C-3C27-48F7-A27B-95B79D4B558C,Severe Storm Wave Loads on Offshore Wind Turbine Foundations (SEA-SWALLOWS),"Offshore structures, including offshore wind turbine foundations, marine renewable energy device support structures, bridge piers, and floating vessels, are routinely exposed to harsh environmental loads. These frequently drive the design. The physics and statistics of wave-structure interaction are complex and still not fully understood for strongly non-linear loads as experienced in the most severe conditions.

The particular focus of this project is fixed offshore wind turbines. These are one of the most promising sources of clean energy; and central to the UK's ambitions to become carbon neutral. The price of offshore wind has fallen significantly over the past ten years. Part of this reduction has been due to improvements in technical understanding leading to less conservative designs. Recently, there has been a trend to move to more exposed and deeper water locations with 'better' wind resources. However, such locations are susceptible to more extreme wave heights and subsequently more severe loading. These changes have increased the importance of wave loading models able to give accurate predictions of base shear and moment time-series. It is important that such models predict not only the magnitude of the load but also the correct frequency content of the loading. For instance, a large slamming load may be of sufficiently short duration that the load is not simply transmitted to the foundation. Further, structures are typically designed so as to avoid the natural frequency of the storm waves. However, if loading was to occur at higher harmonics of the fundamental wave frequencies these may coincide with the structure's natural frequencies, thus greatly increasing their importance for design. For structural fatigue assessment very long time series are required. Therefore, experimental and high-fidelity numerical models are too resource-intensive to be directly useful for practical engineering calculations. A highly efficient yet still sufficiently accurate alternative is required.

The physics of wave loading is typically split into non-breaking and breaking loads. These have different magnitudes and timescales as they are dominated by different physical phenomena. For non-breaking waves, traditionally the Morison equation has been widely accepted as the starting point for calculating wave loading on offshore structures by most modern design standards. For slender cylinders in the inertia regime such as the monopiles used for offshore wind, extensions have been made to the Morison model, taking wave kinematics as inputs. Predicting wave kinematics is itself a difficult task, particularly for severe yet random sea-states where both standard regular wave stream function theory and 2nd order random wave theory are imperfect models.

Breaking waves are notoriously difficult to model numerically and to measure experimentally due to the violence of the hydrodynamics and scaling issues. Various models have been proposed to simulate the time history of the loading. However, when calculating extreme responses and foundation reactions for dynamically sensitive structures, it is generally sufficient to know the total applied impulse (and where it acts) for impact loads rather than the exact time-history. Estimating the impulse is far more robust, quicker and the physics can more easily be modelled. 

We aim to revolutionize load calculations on offshore structures using novel fluid mechanics to develop fast reduced-order engineering models. While the focus of this work will be examining the impact of extreme wave loading on offshore wind turbine foundations, the ideas and tools generated will be more broadly applicable. We will develop a computationally fast method and an open source tool to be used by practicing engineers in industry to model long-term cyclic loading, leading to more efficient designs of offshore structures, reducing construction cost whilst preserving function and reliability.",,
0,8FB74F99-EB9B-480E-9963-39AD01382C43,"Advancing Probabilistic Machine Learning to Deliver Safer, More Efficient, and Predictable Air Traffic Control","The ambition of this partnership between NATS and The Alan Turing Institute is to develop the fundamental science to deliver the world's first AI system to control a section of airspace in live trials.

Our research will take a hierarchical approach to air traffic control (ATC) by developing a digital twin alongside a multi-agent machine-learning control system for UK airspace. Furthermore, the partnership will develop technical approaches to deploy trustworthy AI systems, considering how safety, explainability and ethics are embedded within our methods, so that we can deliver new tools which work in harmony with human air traffic controllers in a safety-critical environment.

Little has changed in the fundamental infrastructure of UK airspace in the past 50 years, but demand for aviation has increased a hundredfold. Aviation 2050, a recent government green paper, underlines the importance of the aviation network to the prosperity of the UK to the value of &pound;22 billion annually. Yet our nation is at risk without rapid action to modernise our airspace and control methods, to ensure they can handle a future increase in UK passenger traffic of over 50% by 2050 and new challenges arising from unmanned aircraft, both against a backdrop of increasing global pressures to transform the sector's environmental impact. 

The augmentation of live air traffic control through the use of AI agents which can handle the complexity and uncertainties in the system has transformative potential for NATS's business. This will positively impact live operations, as well as a research tool and training facility for new ATCOs. Correspondingly, NATS's research vision is to exploit new approaches to AI that enable increases in safety, capacity and environmental sustainability while streamlining air traffic controller training.

The anticipated benefits of AI systems to air traffic control have come at a critical time, providing us with an opportunity to respond effectively to the unprecedented challenges which arise from a triad of crises: the coronavirus 2019 (Covid-19) pandemic, Brexit and global warming. The UK must develop independent technical advances in the sector, without compromising sustainability targets.

The Alan Turing Institute is positioned at the rapidly evolving frontiers of probabilistic machine learning, safe and trustworthy AI and reproducible software engineering. Matching this with the world-leading expertise of NATS, supported by a world-first data set of more than 20 million flight records, means that this partnership is in a unique position to build the first multi AI agents system to deliver tactical control of UK airspace.",,
1,587F0788-E435-41AA-B99B-8F90DA509EE8,HUman-machine teaming for Maritime Environments (HUME),"The offshore energy and defence sectors share a vision of the future where people are taken out of harsh, extreme environments and replaced by teams of smart robots able to do the 'dirty and dangerous jobs', collaborating seamlessly as a team with each other and with the human operators and experts on-shore. In this new world, remote data collection, fusion and interpretation become central, together with the ability to generate transparent, safe actionable decisions from this data. We propose the HUME project (HUman-machine teaming for Maritime Environments), whose vision is to develop a coherent framework that enables humans and machines to work seamlessly as a team by establishing and maintaining a single shared view of the world and each other's intents through transparent interaction, robust to a highly dynamic and unpredictable maritime environments. The HUME project's ambitious and fundamental research programme will address fundamental research questions in the field of machine-machine and human-machine collaboration, robot perception and explainable autonomy and AI. 

The Prosperity Partnership would build on a 20 year strategic relationship between SeeByte and HWU, with SeeByte originally a spin-out of Heriot-Watt University in 2001 and now a world-leader in maritime autonomy worldwide in the Oil &amp; Gas and Defence sectors. This grant would facilitate a shift to lower TRL research and development, providing seeding for early-stage research that can have a broad, longer-term and more disruptive impact. This proposed work aims at establishing a durable model, through which SeeByte and HWU can remain connected to foster long-term research relationships on projects of interest, as they emerge in this rapidly changing field.",,
2,F1AE551D-C159-4525-AC18-BB0E65C28435,Keep Learning,"Combinatorial problems are ubiquitous across many sectors in today's world: delivering optimised solutions can lead to considerable economic benefits in many fields such as logistics, packing, design and scheduling (of either people or processes). In a typical scenario, instances (for example, a set of goods to deliver) arrive frequently in a continual stream and a solution needs to be quickly produced. Although there are many well-known approaches to developing optimisation algorithms, most suffer from a problem that is now becoming apparent across the breadth of Artificial Intelligence: systems are limited to performing well on data that is similar to that encountered in their design process, and are unable to adapt when encountering situations outside of their original programming.


For real-world optimisation this is particularly problematic. If optimisers are trained in a one-off process then deployed, the system remains static, despite the fact that optimisation occurs in a dynamic world of changing instance characteristics, changing user-requirements and changes in operating environments that influence solution quality (e.g. breakdowns in a factory or traffic in a city). Such changes may be either gradual, or sudden. In the best case this leads to systems that deliver sub-optimal performance, while at worst, systems that are completely unfit for purpose. Moreover, a system that does not adapt wastes an obvious opportunity to improve its own performance over time as it solves more and more instances.

The targeted breakthrough of this proposal is to develop a dynamic optimisation system that continually adapts its operating mechanism and its algorithms over time to remain fit-for-purpose - a radical switch from the current one-off design and deployment approach to design of optimisers. The system will:

- Go beyond simply being reactive to being proactive in that it will predict the nature of upcoming instances and speculate about potential future scenarios. In response to these predictions, it will autonomously pre-generate and/or reconfigure suitable algorithms, followed by creation of appropriate mappings from instance to solver, in order to pre-prepare for these future scenarios. It will also respond to user requests to generate instances with specific characteristics and solvers to match them, based on the user's in-depth knowledge of their own business and sector.

- Autonomously improve its own behaviour over time, continually updating its algorithms and methods as it learns from its experience of solving more and more instances.

- Support optimisation with respect to multiple user objectives and requirements via its use of a diverse portfolios of algorithms, that range from those which generate acceptable solutions in a very short time to those that have long running time but deliver the highest possible quality.

To succeed we will make novel advances in building proactive, continually self-adapting systems and in optimisation/algorithm-selection, enhanced by integration with the latest tools from machine-learning. Benefits will be realised by any business that attempts to optimise their processes in dynamic environments, in which customer demands vary, business requirements change, and the operating environment is subject to unexpected changes. Relevant application domains include (but are not limited to) workforce scheduling, logistics and infrastructure design",,
3,C3AF7D13-51AF-4539-B6D9-B5E5B7E94FA6,RAPID: ReAl-time Process ModellIng and Diagnostics: Powering Digital Factories,"Modern manufacturing involves highly controlled and automated processes meticulously designed to deliver products to certain needs within strict specifications and in a cost-efficient and sustainable way. To ensure performance in variable and often harsh conditions, sensors capture continuous data streams about the state of the process, e.g. equipment, and the product. The ability to analyse this data in real time, however, offers unique advantages that are currently out of reach. Learning to calibrate its operation from sensor data, monitor its health status and make accurate forecasts on product outcomes and maintenance requirements, are process attributes of future autonomous factories. 

We have teamed up with Seagate, a major manufacturer of hard drives, to design, develop and implement such a technology in their factory, GSK, a leading pharmaceutical manufacturer, who have recognised the potential of real-time process analytics and NVIDIA, the global GPU provider. The goal is to establish a level of production robustness against major disruptions and market volatility that create uncertainties on workforce numbers and supply chain continuity. 

This vision paves the way for responsive manufacturing systems and digitally controlled factories but to materialise technology that can seamlessly analyse sensor-obtained data and translate it to actionable information. Whilst companies capture large datasets, their ability to process them and react in real-time, is hindered by the algorithms' complexity and scale of the data. Indeed, if anything, the current pandemic has reinforced the need to enhance manufacturing capability to cope with sudden increases in demand, production repurposing, and possibly even unmanned, autonomous production. 

A step change is needed in the processing capability and manufacturing systems where the data can be analysed in real time at the edge, i.e. on the factory floor, making it secure and thus ensuring more effective performance by being less reliant on external communications and high-performance processing resources. We propose that this be done in a methodological and secure way with minimal dependencies on external factors, thus prompting us to investigate ways of performing real-time analytics in a practical, cost-effective and sustainable manner. 

RAPID proposes a two-pronged approach to reduce the computational dimensionality through novel 'data sketching' algorithms and optimisation using 'transprecision computing' on GPU technology to provide further acceleration. In detailed interactions with Seagate and GSK both based in the UK, we have identified manufacturing stages where real-time analytics can play a major part in transforming processes and outcomes. 

In particular, the proposed technology will be applied to a 'diagnostic analytics' case study involving optical imaging data for a critical metrology stage in disk manufacture and two 'predictive analytics' examples for model learning to predict the health state of silicon wafers and for improved fault detection, feature extraction and monitoring of chemical products. 

Data sketching dramatically reduces the complexity of computations by randomly sampling few, the most informative, data and model entries leading to small-scale computations that can be performed very quickly with a small compromise on precision. Sketching trades off precision and speed, and if done optimally a two-order of magnitude speedup is feasible, when sampling around 10% of the data. 

To exploit this advantage further, the sketched computations are implemented using transprecision computing that challenges traditional computing to further accelerate computations when high precision is not required. In computing with noisy data and learned statistical models in factory environments, a controllable reduction in precision is prudent for performance improvement but also essential for noise robustness.",,
4,4A148473-2283-445A-BEFC-7A240328DBE7,RAPID: ReAl-time Process ModellIng and Diagnostics: Powering Digital Factories,"Modern manufacturing involves highly controlled and automated processes meticulously designed to deliver products to certain needs within strict specifications and in a cost-efficient and sustainable way. To ensure performance in variable and often harsh conditions, sensors capture continuous data streams about the state of the process, e.g. equipment, and the product. The ability to analyse this data in real time, however, offers unique advantages that are currently out of reach. Learning to calibrate its operation from sensor data, monitor its health status and make accurate forecasts on product outcomes and maintenance requirements, are process attributes of future autonomous factories. 

We have teamed up with Seagate, a major manufacturer of hard drives, to design, develop and implement such a technology in their factory, GSK, a leading pharmaceutical manufacturer, who have recognised the potential of real-time process analytics and NVIDIA, the global GPU provider. The goal is to establish a level of production robustness against major disruptions and market volatility that create uncertainties on workforce numbers and supply chain continuity. 

This vision paves the way for responsive manufacturing systems and digitally controlled factories but to materialise technology that can seamlessly analyse sensor-obtained data and translate it to actionable information. Whilst companies capture large datasets, their ability to process them and react in real-time, is hindered by the algorithms' complexity and scale of the data. Indeed, if anything, the current pandemic has reinforced the need to enhance manufacturing capability to cope with sudden increases in demand, production repurposing, and possibly even unmanned, autonomous production. 

A step change is needed in the processing capability and manufacturing systems where the data can be analysed in real time at the edge, i.e. on the factory floor, making it secure and thus ensuring more effective performance by being less reliant on external communications and high-performance processing resources. We propose that this be done in a methodological and secure way with minimal dependencies on external factors, thus prompting us to investigate ways of performing real-time analytics in a practical, cost-effective and sustainable manner. 

RAPID proposes a two-pronged approach to reduce the computational dimensionality through novel 'data sketching' algorithms and optimisation using 'transprecision computing' on GPU technology to provide further acceleration. In detailed interactions with Seagate and GSK both based in the UK, we have identified manufacturing stages where real-time analytics can play a major part in transforming processes and outcomes. 

In particular, the proposed technology will be applied to a 'diagnostic analytics' case study involving optical imaging data for a critical metrology stage in disk manufacture and two 'predictive analytics' examples for model learning to predict the health state of silicon wafers and for improved fault detection, feature extraction and monitoring of chemical products. 

Data sketching dramatically reduces the complexity of computations by randomly sampling few, the most informative, data and model entries leading to small-scale computations that can be performed very quickly with a small compromise on precision. Sketching trades off precision and speed, and if done optimally a two-order of magnitude speedup is feasible, when sampling around 10% of the data. 

To exploit this advantage further, the sketched computations are implemented using transprecision computing that challenges traditional computing to further accelerate computations when high precision is not required. In computing with noisy data and learned statistical models in factory environments, a controllable reduction in precision is prudent for performance improvement but also essential for noise robustness.",,
5,94E8A38C-3849-42F3-A5A0-2C19E820D1F7,The Automatic Computer Scientist,"Algorithms are ubiquitous: they track our sleep, find us cheap flights, and even help us see black holes. However, designing novel algorithms is extremely difficult, and we do not have efficient algorithms for many fundamental problems.

This project aims to accelerate algorithm discovery by building an automatic computer scientist (AutoCS).

The development of machines that automatically write computer programs is a long-standing grand challenge in artificial intelligence (AI). Such a development would revolutionise the field, offering the potential to build bug-free and efficient programs without required specialist knowledge.

To work towards this grand challenge, this project will build on major recent breakthroughs in inductive logic programming (ILP), a form of symbolic machine learning based on mathematical logic.

Because of the major recent breakthroughs, ILP currently has the ability of a first-year computer science student: given much guidance, it can learn simple algorithms and implement small programs.

This project aims to significantly advance ILP to the level of a computer science PhD student so that given little guidance it can discover novel and complex algorithms and implement large programs.

As a marker of success, a key objective of this project is to use an AutoCS to discover a novel algorithm and publish it in a computer science journal. Such a result would be a landmark achievement for AI and would herald a new era of automatic scientific discovery.",,
6,F1BC92FA-8735-4170-91A7-A2E1A1BB0C69,''Mechanically-intelligent'' Intra-operative Tissue Assessment for Robot-Assisted Surgery (MIRAS),"Intra-operational tissue assessment is a key enabling technology for minimally invasive surgery. Surgeons operating along a &quot;keyhole&quot; or similar means of access for minimally invasive surgery need to identify different structures or diseased areas, even when these all may look similar. This work is aimed at identifying the resection margin in cancer surgery, to allow the removal of a tumour together with a margin which is just enough to ensure complete cancer excision, but without unnecessary excess tissue removal. Currently, such a surgical margin is identified using a combination of the surgeon's experience, images of various kinds taken prior to the operation coupled with any visual observations, or tactile 'feel' in the scenario of open surgery, that the surgeon can make during the operation. 

Ultimate confirmation of the surgical margin relies on post-operative histopathology, where the removed tissue is assessed microscopically. Only then, will it be known if the removal has been successful or if further surgery and/or more aggressive post-operative treatment is required. These challenges are particularly acute in surgical removal of tumours from the rectum and some pelvic organs, where wider surgical excision is constrained by close proximity of anatomical structures with high functional importance, e.g. nerves and vasculature supplying bladder, bowel, sexual organs and lower limbs. 

The development of minimally invasive techniques (such as laparoscopy or operations along body ducts, such as in the rectum or colon) have removed surgical 'feel' for tissue characteristics, including assessment of surgical margin. This highlights an unmet clinical need for a quantitative, robust, reliable and evidence-based method of determining the optimal surgical margin and providing feedback to the surgeon in a way that it can be used to make decisions during the operation.

Robot-Assisted Surgery (RAS) is the next development in minimally invasive surgery and has seen rapid development in treatment of a wide variety of conditions. It offers improved clinical accuracy by giving surgeons better control of instruments and providing features such as 3D visualisation. Such developments are particularly useful in confined spaces such as the pelvis and rectum. So far, RAS has found limited application in oncological surgery, mostly because current RAS systems rely almost entirely on visual feedback, and do not provide support for clinical decision making. This work aims to provide a novel function in RAS to enhance intra-operative clinical decision making. This technology would accelerate development of RAS in many types of visceral and solid-organ surgery where visual feedback is limited or inadequate to determine surgical margins reliably.

This partnership brings together 4 distinct and complementary engineering groups with two clinical specialisms and is supported by two industries, an SME in the medical sensors area and a manufacturer of surgical robots. The group will focus on two principal aims: 
1. to devise a microfabricated probe deployable via a standard minimally invasive surgery instrument capable of making intra-operative mechanical measurements on the tissue surface. 
2. to establish data modelling methods in order to process the real-time measurement data to produce quantitative assessment of surgical margin as intra-operative feedback to the surgeon.

The approach will be developed in a staged series of trials, including on ex vivo human tissue and in vivo animal models, with ultimate demonstration in a surgical environment. Through the work, the partnership expects to develop a unique and future-proof 'RAS-made-smarter' technology for applications in intra-operative identification of tumours and tumour margins and, by extension, in other surgical areas.",,
7,BD50D818-E6E5-4AFC-B8DD-EA6D5C4A1B10,The Mathematics of Deep Learning,"Machine learning (ML), in particular Deep Learning (DL) is one of the fastest growing areas of modern science and technology, which has potentially enormous and transformative impact on all areas of our life. The applications of DL embrace many disciplines such as (bio-)medical sciences, computer vision, the physical sciences, the social sciences, speech recognition, gaming, music and finance. DL based algorithms are now used to play chess and GO at the highest level, diagnose illness, drive cars, recruit staff and even make legal judgements. The possible applications in the future are almost unlimited. Perhaps DL methods will be used in the future to predict the weather and climate, of even human behaviour. However, alongside this explosive growth has been a concern that there is a lack of explainability behind DL and the way that DL based algorithms make their decisions. This leads to a lack of trustworthiness in the use of the algorithms. A reason for this is that the huge successes of deep learning is not well understood, the results are mysterious, and there is a lack of a clear link between the data training DL algorithms (which is often vague and unstructured) and the decisions made by these algorithms. 

Part of the reason for this is that DL has advanced so fast, that there is a lack of understanding of its foundations. According to the leading computer scientist Ali Rahimi at NIPS 2017: 'We say things like &quot;machine learning is the new electricity&quot;. I'd like to offer another analogy. Machine learning has become alchemy!'
Indeed, despite the roots of ML lying in mathematics, statistics and computer science there currently is hardly any rigorous mathematical theory for the setup, training and application performance of deep neural networks. 

We urgently need the opportunity to change machine learning from alchemy into science. This programme grant aims to rise to this challenge, and, by doing so, to unlock the future potential of artificial intelligence. It aims to put deep learning onto a firm mathematical basis, and will combine theory, modelling, data, computation to unlock the next generation of deep learning.

The grant will comprise an interlocked set of work packages aimed to address both the theoretical development of DL (so that it becomes explainable) and the algorithmic development (so that it becomes trustworthy). These will then be linked to the development of DL in a number of key application areas including image processing, partial differential equations and environmental problems. For example we will explore the question of whether it is possible to use DL based algorithms to forecast the weather and climate faster and more accurately than the existing physics based algorithms.

The investigators on the grant will be doing both theoretical investigations and will work with end-users of DL in many application areas. Mindful that policy makers are trying to address the many issues raised by DL, the investigators will also reach out to them through a series of workshops and conferences. The results of the work will also be presented to the public at science festivals and other open events.",,
8,09D38BFD-5498-43EE-ACB2-5F41D789408A,"Datasounds, datasets and datasense: Unboxing the hidden layers between musical data, knowledge and creativity","This network aims to identify core questions that will drive forward the next phase in data-rich music research, focused in particular on creative music making. The increased availability of digital music data combined with new data science techniques are already opening new possibilities for making, studying and engaging with music. This direction is only likely to speed up upending many current practices, opening up creative avenues and offering new opportunities for research. However, the rapid technological progress with new techniques producing surprising results in rapid succession, is often disconnected from the knowledge and knowhow gained by musicians through creativity, practice and research. By bringing together researchers and practitioners from different underlying disciplines and with a wide range of expertise the network will enable a better foundation for future research. Performers, composers, and improvisers will contribute through embodied knowledge and practice-based methods; researchers in psychology will bring insights about cognitive, affective and behavioural processes underpinning musical experience; and data scientists will add analytical expertise as well as relevant theories, methods and techniques. These will lead to significant conceptual breakthroughs in data driven approaches and technologies applied to music.
The new data-based technologies usually rely on large data sets, they can also produce very large amounts of data. As part of the network activities we will map the limitations of existing music representations, identify the gaps that need to be addressed and propose pathways to improve representation formats. We do not envision developing a single, all encompassing representation that captures the full richness of musical experience. Nevertheless, through the dialogue that this network will facilitate we will be able to outline ways of improving on existing representation formats and develop methods for visualising, analysing, and interpreting large data sets. The network will also consider ethical and legal implications such as how best to address the challenges that Artificial Intelligence (AI) poses to existing musical practices and the fear that this technology induces. Some of these are common to many fields where AI is being applied to tasks which were until very recently the preserve of humans. Music offers a unique perspective on these wider problems - the opacity of 'black box' generative models is a low-risk research challenge not a potentially dangerous tool that may entrench existing injustices. By embedding the ethical dimension into the discussion of the future of data driven music research the network will serve as a model for other fields. 
The core activity of the network are two workshops where short presentations will provide a springboard for in-depth discussions; a concert by practitioners with relevant experience will help connect the theoretical discussions to the reality of music making. These will enable a multidimensional exchange of ideas and methods. Material from these workshops will be shared online to document the process and provide a platform to engage wider audiences with the approach taken and the significant results obtained. 
Data driven technologies are already having an effect on the way in which we understand, make and consume music within current cultural and economic contexts, raising complex and unprecedented ethical and legal considerations. This network will identify core questions that can propel forward data driven research into creative music making that consider social and individual needs. We will also be able to outline specific research projects that address the shared concerns and bridge the gaps between the different methods that, in many ways, bound our disciplines. The network builds on previous AHRC funded research by the PI (AH/N504531/1 and AH/R004706/1) applying data to creative music making in a particular domain.",,
9,8799DA1B-E687-44D8-95EF-B84222F584ED,MAESTRO Jr. - Multi-sensing AI Environment for Surgical Task &amp; Role Optimisation,"This project is about devising and implementing a smart operating room environment, powered by trustable, human-understanding artificial intelligence, able to continually adapt and learn the best way to optimise safety, efficacy, teamwork, economy, and clinical outcomes. 

We call this concept MAESTRO. 

A fitting analogy for MAESTRO is that of an orchestra conductor, a 'maestro', who oversees, overhears and directs a group of people on a common task, towards a common goal: a masterful musical performance. Although the music score is identical for all orchestras, there is no doubt that they all perform it in different ways and some significantly better than others. Although the quality and personality of orchestra musicians is very important, it is widely accepted that the role of the maestro is crucial, and extends beyond the duration of the musical performance to rehearsals and understanding of the context behind the music score. 
Thus, while it is possible for orchestras to perform without conductors, most cannot function without one.

Our proposed MAESTRO AI-powered operating room of the future rotates around four key elements:

(a) The holistic sensing of patient, staff, operating room environment and equipment through an array of diverse sensor devices.

(b) Artificial intelligence focused on humans (human-centric), able to continually understand situations and actions developing in the operating room, and of intervening when necessary.

(c) The use of advanced human-machine user interfaces for augmenting task performance.

(d) A secure device interconnectivity platform, allowing the full integration of all above key elements.

As in our orchestra analogy, our envisioned MAESTRO directs the OR staff and surgical devices before, during and after a surgical procedure by:

(1) Sensing surgical procedures in all their aspects, including those which are currently neglected such as the physiological responses of staff (e.g., heart rate, blood pressure, sweating, pupil dilation), focus of attention, brain activity, as well as harmful events that may escape the attention of the clinical team.

(2) Overseeing individual and team performance in real-time, throughout the operation and across different types of surgeries and different teams.

(3) Guiding and assisting the surgical team via automated checkpoints, virtual and augmented visualisations, warnings, individualised and broadcasted alerts, automation, semi-automation, robotics, and other aids and factors that can affect performance in the operating room.

(4) Augmenting and optimising individual and collective operational capabilities, skills, and task ergonomics, through novel human-machine interaction and interfacing modalities.

The project is designed to have a significant societal, economic and technological impact, and to establish the NHS as a leading healthcare paradigm worldwide. MAESTRO leverages the expertise of top researchers in the areas of robotics, sensing, artificial intelligence, human factors, health policies and patient safety. It is co-designed in collaboration with top clinicians, one of the largest NHS Trusts in England, patient groups, performing artists, and several small and medium-sized enterprises and large multinational industries operating in the areas of artificial intelligence, medical devices, digital health, large networks, cloud services, cyber security.",,
10,6F9D0D31-43E0-43BF-9AC2-02B694F91057,Digital Roads,"This partnership started 10 years ago, when Costain started a collaborative research programme with Highways England and the University of Cambridge that sponsored 27 PhD studentships and led to the establishment of three major efforts: (i) the Centre for Smart Infrastructure and Construction, which grew into the National Research Facility for Infrastructure Sensing to develop new methods for infrastructure data collection and analysis using sensors. Our joint work through this activity has collectively shaped national policy; (ii) the EPSRC Materials for Life project that led to a Programme Grant called Resilient Materials for Life, which led to the first UK demonstration of self-healing structures on the A465 road scheme; and (iii) the two EPSRC Centres for Doctoral Training in Future Infrastructure and the Built Environment, which led to Costain acquiring SSL, a data technology company that has accelerated our technology service transformation. All this steered the team to co-create an integrated and focused partnership programme through co-creation workshops, the outcome of which is the proposed Digital Roads partnership.
Digital Roads is inherently a concept for how to disrupt the roads infrastructure sector in its entirety. We envision a future where every road is made of smart materials, has its own digital twin, and can measure and monitor its own performance. This will make roads considerably cheaper, more reliable, and safer. Our ambition is therefore to make roads (i) out of smart materials aware of their state and properties, (ii) documented in Digital Twins and monitored automatically, (iii) maintained proactively, and (iv) able to serve additional functionalities, therefore bringing automation efficiency to the road network.
The Digital Roads concept rethinks roads as an integrated physical and digital product and associated lifecycle processes that continuously interact with each other to ensure efficiency and strong performance in terms of cost, time, quality, safety, sustainability, and resilience. To support this concept, the grant will therefore investigate how digital twins (for the digital product), smart materials (for the physical product), data science (for the digital lifecycle processes), and robotic monitoring (for the physical lifecycle processes) can work together to create a connected physical and digital product and associated processes with a strong focus on the flow of data between them to leverage their complementarity. For instance, starting from the smart materials: we will use graphene infused concrete coatings to enable self-sensing on both the road surface and the median barrier, that informs the road's digital twin through robotic monitoring, who in turn, along with other pre-existing data, informs the data-science enabled digital processes, and back.
This is very timely and necessary, as, after failing to fully leverage technological advances repeatedly over the last 50 years, all the stars are aligned for the road infrastructure industry to leverage advances in information modelling, machine learning, automation and smart materials that now enable the team to have confidence in deliver the Digital Roads vision.
Beyond the partnership, the Digital Roads team aims to develop outcomes by 2030 to a commercial stage and to follow the same development journey for other road assets such as bridges and tunnels - and eventually the entire strategic road network by 2040. This will allow Costain to create a leading digital service to deliver for Highways England who will be able to demonstrate greater value roads and enable other industries to do the same. This will ensure that roads become safer, benefiting us all; serviceable at lower cost reducing the burden on the tax payer; and maintained more efficiently and sustainably to benefit the stakeholders, society and the environment making the UK a global leader in Digital Roads technology.",,
11,E2C13423-027B-4700-B2A1-3C9367F90891,AMBITION: AI-driven biomedical robotic automation for research continuity,"Artificial Intelligence (AI) is transforming the world. AI is the core technology of many of the biggest companies in the world, Amazon, Google, Facebook, etc. that effect all our lives. AI is now starting to transform science and technology.

Most people in the EU now live better than Kings did in the past: they have better food, medical care, transport, etc. This miracle has been made possible through better technology based on science. To meet the great challenges the 21st century world faces: climate change, food insecurity, disease, etc., we need to make science and technology even more efficient. 

We propose the AMBITION project to harness the power of AI and laboratory robotics to provide researchers in the UK, and beyond, with continuous, uninterrupted, remote access to AI/robotic augmented biomedical research capabilities. 

This will enable more robust, efficient and reproducible biomedical research. The UK's life sciences, biotechnological and pharmaceutical industry are world-leading. However, the Covid-19 pandemic has clearly demonstrated the vital importance of biomedical research and the critical need to maintain research continuity at all times. Yet, lockdowns and social distancing pose a severe threat to research continuity, forcing laboratories to shut down, risking loss of years of research. Integrating AI with laboratory automation will also enable the automation of routine parts of scientific theory formation and experimentation. This will enable results to be obtained both more efficiently and faster compared to the state-of-the-art where human scientists must make all the decisions. AMBITION does not aim to replace humans, but empower them by reasoning and data processing capabilities to better support their decision making.

Biomedical science is facing a 'reproducibility crisis'. Despite reproducibility being fundamental to science, the reproducibility of few biomedical results is currently tested, and when reproducibility is tested, the results are dismal, with only 10 to 20% of published biomedical research found to be reproducible. Finally, automated laboratories will make scientific results more reproducible, as AI systems describe experiments in more clearly than human scientists, and robots execute experimental protocols more accurately than human scientists. 

The project will focus on the development of the AI part of the system and iterative testing in real-world laboratory settings employing state-of-the-art robotics equipment. We will initially focus on cancer drug discovery as a first demonstration case, bringing together the power of AI and laboratory robotics.

In the medium-term (3-5 years horizon). We plan to extend the approach to clinical patient care, and to provide real-time cancer treatment decision support system for patients in the UK and beyond based on automated testing of hundreds of treatment options on patient-derived tumour material, thereby leading to a reduction in animal experimentation, and giving clinicians an evidence-based, real-time input for their expert treatment decision. 

In the long-term (5-15 years horizon) we will rollout automated research capabilities and real-time treatment guidance across all of biomedicine, especially fields such as antibiotic treatment/ antimicrobial resistance, inflammatory diseases, etc. 

In 30 years, autonomous laboratories will transform the health sector. They will lower the costs of laboratory experiments, augment researchers' technical capabilities (making more elaborate and complex tests possible), reduce the risks associated with the presence of humans in the labs (working with hazardous substances, risk of infections), ensure reproducibility, increase accuracy of results, and ensure overall accountability and trust in the process. Autonomous laboratories will speed up and scale up the development of new drugs, remote testing of patients, and will be an enabler for personalised medicine.",,
12,328EBE5A-3D5C-492F-9230-7532B9460E6B,Unlocking spiking neural networks for machine learning research,"In the last decade there has been an explosion in artificial intelligence research in which artificial neural networks, emulating biological brains, are used to solve problems ranging from obstacle avoidance in self-driving cars to playing complex strategy games. This has been driven by mathematical advances and powerful new computer hardware which has allowed large 'deep networks' to be trained on huge amounts of data. For example, after training a deep network on 'ImageNet' - which consists of over 14 million manually annotated images - it can accurately identify the content of images. However, while these deep networks have been shown to learn similar patterns of connections to those found in the parts of our brains responsible for early visual processing, they differ from real brains in several important ways, especially in how the individual neurons communicate. Neurons in real brains exchange information using relatively infrequent electrical pulses known as 'spikes', whereas, in typical artificial neural network models, the spikes are abstracted away and values representing the 'rates' at which spikes would be emitted are continuously exchanged instead. However, neuroscientists believe that large amounts of information is transmitted in the precise times at which spikes are produced. Artificial 'spiking neural networks' can harness these properties, making them useful in applications which are challenging for current models such as real-world robotics and processing data with a temporal component, such as video. However, spiking neural networks can only be used effectively if suitable computer hardware and software is available. While there is existing software for simulating spiking neural networks, it has mostly been designed for studying real brains, rather than building AI systems. In this project, I am going to build a new software package which bridges this gap. It will use abstractions and processes familiar to machine learning researchers, but with techniques developed for brain simulation, allowing exciting new SNN models to be used by AI researchers. We will also explore how spiking models can be used with a special new type of sensors which directly outputs spikes rather than a stream of images. 

In the first phase of the project, I will focus on using Graphics Processing Units to accelerate spiking neuron networks. These devices were originally developed to speed up 3D games but have evolved into general purpose devices, widely used to accelerate scientific and AI applications. However, while these devices have become incredibly powerful and are well-suited to processing lots of data simultaneously, they are less suited to 'live' applications such as when video must be processed as fast as possible. In these situations, Field Programmable Gate Arrays - devices where the hardware itself can be re-programmed - can be significantly faster and are already being used behind the scenes in data centres. In this project, by incorporating support for FPGAs into our new software, we will make these devices more accessible to AI researchers and unlock new possibilities of using biologically-inspired spiking neural networks to learn in real-time.

As well as working on these new research strands, I will also dedicate time during my fellowship to advocate for research software engineering as a valuable component of academic institutions, both via knowledge exchange and research funding. In the shorter term, I will work to develop a community of researchers involved in writing software at Sussex by organising an informal monthly 'surgery' as well as delivering specialised training on programming Graphics Processing Units and more fundamental computational and programming training for new PhD students. Finally, I will develop internship and career development opportunities for undergraduate students, to gain experience in research software engineering.",,
