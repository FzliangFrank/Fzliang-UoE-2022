lasso_wf <- workflow() %>%
add_recipe(review_rec) %>%
add_model(lasso_spec)
#install.packages("textrecipes")
library(textrecipes)
#set validation set
set.seed(123)
review_split <- initial_split(reviews_parsed, strata = rating)
review_train <- training(review_split)
review_test <- testing(review_split)
review_rec <- recipe(rating ~ text, data = review_train) %>%
step_tokenize(text) %>%
step_stopwords(text) %>%
step_tokenfilter(text, max_tokens = 500) %>%
step_tfidf(text) %>%
step_normalize(all_predictors())
review_prep <- prep(review_rec)
review_prep
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
lasso_wf <- workflow() %>%
add_recipe(review_rec) %>%
add_model(lasso_spec)
Lasso_wf
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
lasso_wf <- workflow() %>%
add_recipe(review_rec) %>%
add_model(lasso_spec)
Lasso_wf
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
lasso_wf <- workflow() %>%
add_recipe(review_rec) %>%
add_model(lasso_spec)
lasso_wf
lambda_grid <- grid_regular(penalty(), levels = 40)
set.seed(42)
review_folds <- bootstraps(review_train, strata = rating)
review_folds
doParallel::registerDoParallel()
#doParallel::registerDoParallel()
set.seed(42)
lasso_grid <- tune_grid(
lasso_wf,
resamples = review_folds,
grid = lambda_grid,
metrics = metric_set(roc_auc, ppv, npv)
)
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
lasso_wf <- workflow() %>%
add_recipe(review_rec) %>%
add_model(lasso_spec)
lasso_wf
lambda_grid <- grid_regular(penalty(), levels = 40)
set.seed(42)
review_folds <- bootstraps(review_train, strata = rating)
review_folds
#doParallel::registerDoParallel()
set.seed(42)
lasso_grid <- tune_grid(
lasso_wf,
resamples = review_folds,
grid = lambda_grid,
metrics = metric_set(roc_auc, ppv, npv)
)
#doParallel::registerDoParallel()
library(glmnet)
install.packages("glmnet")
#install.packages("glmnet")
library(glmnet)
#doParallel::registerDoParallel()
#install.packages("glmnet")
library(glmnet)
set.seed(42)
lasso_grid <- tune_grid(
lasso_wf,
resamples = review_folds,
grid = lambda_grid,
metrics = metric_set(roc_auc, ppv, npv)
)
lasso_grid %>%
collect_metrics()
lasso_grid %>%
collect_metrics() %>%
ggplot(aes(penalty, mean, color = .metric)) +
geom_line(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
best_auc <- lasso_grid %>%
select_best("roc_auc")
best_auc
user_reviews <- readr::read_tsv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv")
user_reviews %>%
count(grade) %>%
ggplot(aes(grade, n)) +
geom_col(fill = "midnightblue", alpha = 0.7) +
scale_x_continuous(breaks = 0:10)
library(tidyverse)
install.packages("tidytext")
library(tidytext)
#install.packages("tidymodels")
library(tidymodels)
#install.packages("wordcloud")
library(wordcloud)
#install.packages("testrecipes")
#library(testrecipes)
library(quanteda)
devtools::install_github("kbenoit/quanteda.dictionaries")
library(quanteda.dictionaries)
library(tidyverse)
install.packages("tidytext")
library(tidytext)
#install.packages("tidymodels")
library(tidymodels)
#install.packages("wordcloud")
library(wordcloud)
#install.packages("testrecipes")
#library(testrecipes)
#devtools::install_github("kbenoit/quanteda.dictionaries")
library(quanteda.dictionaries)
install.packages("tidytext")
reviews_parsed <- user_reviews %>%
mutate(text = str_remove(text, "Expand$")) %>%
mutate(rating = case_when(
grade > 7 ~ "good",
TRUE ~ "bad"
))
reviews_parsed %>%
count(rating)
words_per_review <- reviews_parsed %>%
unnest_tokens(word, text) %>%
count(user_name, name = "total_words")
words_per_review %>%
ggplot(aes(total_words)) +
geom_histogram(fill = "midnightblue", alpha = 0.8, bins = 100) +
scale_x_continuous(breaks = seq(0,1000, by = 100))
#install.packages("textrecipes")
library(textrecipes)
#set validation set
set.seed(42)
review_split <- initial_split(reviews_parsed, strata = rating)
review_train <- training(review_split)
review_test <- testing(review_split)
review_rec <- recipe(rating ~ text, data = review_train) %>%
step_tokenize(text) %>%
step_stopwords(text) %>%
step_tokenfilter(text, max_tokens = 500) %>%
step_tfidf(text) %>%
step_normalize(all_predictors())
review_prep <- prep(review_rec)
review_prep
#doParallel::registerDoParallel()
#install.packages("glmnet")
library(glmnet)
set.seed(42)
lasso_grid <- tune_grid(
lasso_wf,
resamples = review_folds,
grid = lambda_grid,
metrics = metric_set(roc_auc, ppv, npv)
)
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
lasso_wf <- workflow() %>%
add_recipe(review_rec) %>%
add_model(lasso_spec)
lasso_wf
lambda_grid <- grid_regular(penalty(), levels = 40)
set.seed(42)
review_folds <- bootstraps(review_train, strata = rating)
review_folds
#doParallel::registerDoParallel()
#install.packages("glmnet")
library(glmnet)
set.seed(42)
lasso_grid <- tune_grid(
lasso_wf,
resamples = review_folds,
grid = lambda_grid,
metrics = metric_set(roc_auc, ppv, npv)
)
token_importance <- final_fit %>%
vi(lambda = best_auc$penalty) %>%
group_by(Sign) %>%
top_n(20, wt = abs(Importance)) %>%
ungroup() %>%
mutate(
Importance = abs(Importance),
Variable = str_remove(Variable, "tfidf_text_"),
Variable = fct_reorder(Variable, Importance)
)
library(wordcloud)
pal <- brewer.pal(8,"Dark2")
wordcloud(token_importance$Variable,
token_importance$Importance,
colors = pal)
library(ggwordcloud)
token_importance <- read_csv(file.choose())
library(ggwordcloud)
install.packages("ggwordcloud")
token_importance <- read_csv(file.choose())
library(ggwordcloud)
ggplot(token_importance, aes(label = Variable, size = Importance,
color = Importance)) +
geom_text_wordcloud() +
# scale_color_fermenter(palette = "PuBuGn") +
scale_color_viridis_c() +
scale_size_area(max_size = 8) +
facet_wrap(~ Sign) +
theme_void()
library(ggwordcloud)
ggplot(token_importance, aes(label = Variable, size = Importance,
color = Importance)) +
geom_text_wordcloud() +
# scale_color_fermenter(palette = "PuBuGn") +
scale_color_viridis_c() +
scale_size_area(max_size = 8) +
facet_wrap(~ Sign) +
theme_void()
token_importance <- read_csv(file.choose())
install.packages("ggwordcloud")
token_importance <- read_csv(file.choose())
#install.packages("ggwordcloud")
token_importance <- read_csv(file.choose())
#install.packages("ggwordcloud")
library(ggwordcloud)
ggplot(token_importance, aes(label = Variable, size = Importance,
color = Importance)) +
geom_text_wordcloud() +
# scale_color_fermenter(palette = "PuBuGn") +
scale_color_viridis_c() +
scale_size_area(max_size = 8) +
facet_wrap(~ Sign) +
theme_void()
user_reviews <- readr::read_tsv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv")
review_dtm <- user_reviews %>%
unnest_tokens(word, text) %>%
count(user_name, word) %>%
cast_dtm(user_name, word, n)
library(tm)
install.packages("tm")
#install.packages("tm")
library(tm)
#install.packages("tm")
library(tm)
user_reviews <- readr::read_tsv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv")
review_dtm <- user_reviews %>%
unnest_tokens(word, text) %>%
count(user_name, word) %>%
cast_dtm(user_name, word, n)
library(wordcloud)
pal <- brewer.pal(8,"Dark2")
wordcloud(token_importance$Variable,
token_importance$Importance,
colors = pal)
#install.packages("tm")
library(tm)
user_reviews <- readr::read_tsv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv")
review_dtm <- user_reviews %>%
unnest_tokens(word, text) %>%
count(user_name, word) %>%
cast_dtm(user_name, word, n)
#install.packages("tm")
library(tm)
user_reviews <- readr::read_tsv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv")
review_dtm <- user_reviews %>%
unnest_tokens(word, text) %>%
count(user_name, word) %>%
cast_dtm(user_name, word, n)
as.matrix(review_dtm)[1:10, 1:10]
review_lda <- LDA(review_dtm, k = 4, control = list(seed = 42))
?LDA
?LDA()
library("topicmodels")
library(topicmodels)
install.package("topicmodels")
install.packages("topicmodels")
#install.packages("topicmodels")
library(topicmodels)
#install.packages("tm")
library(tm)
#install.packages("topicmodels")
library(topicmodels)
user_reviews <- readr::read_tsv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv")
review_dtm <- user_reviews %>%
unnest_tokens(word, text) %>%
count(user_name, word) %>%
cast_dtm(user_name, word, n)
as.matrix(review_dtm)[1:10, 1:10]
review_lda <- LDA(review_dtm, k = 4, control = list(seed = 42))
terms(review_lda, 10)
#install.packages("tm")
library(tm)
#install.packages("topicmodels")
library(topicmodels)
user_reviews <- readr::read_tsv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv")
review_dtm <- user_reviews %>%
unnest_tokens(word, text) %>%
count(user_name, word) %>%
cast_dtm(user_name, word, n)
as.matrix(review_dtm)[1:10, 1:10]
review_lda <- LDA(review_dtm, k = 4, control = list(seed = 42))
terms(review_lda, 10)
library(tsne)
install.packages("tsne")
#install.packages("tsne")
library(tsne)
svd_tsne <- function(x) tsne(svd(x)$u)
lda_json <- createJSON(phi = posterior(review_lda)$terms,
theta = posterior(review_lda)$topics,
vocab = posterior(review_lda)$terms %>% colnames,
doc.length = rowSums(as.matrix(review_dtm)),
term.frequency = colSums(as.matrix(review_dtm)),
mds.method = svd_tsne)
#install.packages("tsne")
install.packages("LDAvis")
library(ggtext)
library(tsne)
svd_tsne <- function(x) tsne(svd(x)$u)
lda_json <- createJSON(phi = posterior(review_lda)$terms,
theta = posterior(review_lda)$topics,
vocab = posterior(review_lda)$terms %>% colnames,
doc.length = rowSums(as.matrix(review_dtm)),
term.frequency = colSums(as.matrix(review_dtm)),
mds.method = svd_tsne)
library(tidyverse)
df_com <- read_csv('./complete.csv')
df_scr <- read_csv('./scrubbed.csv')
# Geo Library
library(leaflet)
library(rgdal)
library(leaflet.providers)
library(htmltools)
# Time Series
library(lubridate)
setwd("~/Dropbox/Projects/UFO sighting")
library(tidyverse)
df_com <- read_csv('./complete.csv')
df_scr <- read_csv('./scrubbed.csv')
# Geo Library
library(leaflet)
library(rgdal)
library(leaflet.providers)
library(htmltools)
# Time Series
library(lubridate)
df <- df_scr[,c("comments","datetime","latitude","longitude","duration (seconds)")]
df_2 <- df_com[,c("comments","datetime","latitude","longitude","duration (seconds)")]
df$time <- mdy_hm(df$datetime)
df_2$time <- mdy_hm(df_2$datetime)
df_f <- df_2 %>%
drop_na("duration (seconds)") %>%
drop_na("time") %>%
transform(`duration (seconds)` = as.numeric(`duration (seconds)`))
with(df_f, plot(x = time, y = duration..seconds., type = "h"))
df_2 %>%
drop_na("duration (seconds)") %>%
pull("duration (seconds)") %>%
as.numeric() %>%
density() %>%
plot(xlim = c(0, 300000))
#y <- c(1960, 1980)
#t <- as.POSIXct(y)
#with (df_f, plot(x= time, y = duration..seconds. ,
#                 type = "h",
#                 xlim = t
#                 ))
#sapply(df_f, class)
df <- df_scr[,c("comments","datetime","latitude","longitude","duration (seconds)")]
df_2 <- df_com[,c("comments","datetime","latitude","longitude","duration (seconds)")]
df$time <- mdy_hm(df$datetime)
df_2$time <- mdy_hm(df_2$datetime)
df_f <- df_2 %>%
drop_na("duration (seconds)") %>%
drop_na("time") %>%
transform(`duration (seconds)` = as.numeric(`duration (seconds)`))
with(df_f, plot(x = time, y = duration..seconds., type = "h"))
df_2 %>%
drop_na("duration (seconds)") %>%
pull("duration (seconds)") %>%
as.numeric() %>%
density() %>%
plot(xlim = c(0, 300000))
#y <- c(1960, 1980)
#t <- as.POSIXct(y)
#with (df_f, plot(x= time, y = duration..seconds. ,
#                 type = "h",
#                 xlim = t
#                 ))
#sapply(df_f, class)
ufo_icon = makeIcon(iconUrl = "./asset/ufo.png",
iconWidth = 25, iconHeight = 25)
pri_icon = makeIcon(iconUrl = "./asset/pyramid.png",
iconWidth = 40, iconHeight = 40)
uk_lon <- 0
uk_lat <- 55.3781
class(df$time[1])
filter <- (year(df$time) == "2012" )
select <- df[filter, c("latitude","longitude","comments","time","duration (seconds)")]
leaflet(select) %>%
setView(lng = uk_lon, lat = uk_lat, zoom = 5) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addMarkers(~longitude, ~latitude,
icon = ufo_icon,
clusterOptions = markerClusterOptions(
iconCreateFunction = JS("
function(cluster) {
return new L.DivIcon({
html: '<div style=\"background-color:rgba(77,77,77,0.5)\"><span>' + cluster.getChildCount() + '</div><span>',
className: 'marker-cluster'
});
}")
),
label = ~time,
popup = ~comments,
)  %>%
addMiniMap()
uk_lon <- 0
uk_lat <- 55.3781
class(df$time[1])
filter <- (year(df$time) == "1990" )
select <- df[filter, c("latitude","longitude","comments","time","duration (seconds)")]
leaflet(select) %>%
setView(lng = uk_lon, lat = uk_lat, zoom = 5) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addMarkers(~longitude, ~latitude,
icon = ufo_icon,
clusterOptions = markerClusterOptions(
iconCreateFunction = JS("
function(cluster) {
return new L.DivIcon({
html: '<div style=\"background-color:rgba(77,77,77,0.5)\"><span>' + cluster.getChildCount() + '</div><span>',
className: 'marker-cluster'
});
}")
),
label = ~time,
popup = ~comments,
)  %>%
addMiniMap()
filter <- (year(df$time) == "1994" )
uk_lon <- 0
uk_lat <- 55.3781
class(df$time[1])
filter <- (year(df$time) == "1994" )
select <- df[filter, c("latitude","longitude","comments","time","duration (seconds)")]
leaflet(select) %>%
setView(lng = uk_lon, lat = uk_lat, zoom = 5) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addMarkers(~longitude, ~latitude,
icon = ufo_icon,
clusterOptions = markerClusterOptions(
iconCreateFunction = JS("
function(cluster) {
return new L.DivIcon({
html: '<div style=\"background-color:rgba(77,77,77,0.5)\"><span>' + cluster.getChildCount() + '</div><span>',
className: 'marker-cluster'
});
}")
),
label = ~time,
popup = ~comments,
)  %>%
addMiniMap()
uk_lon <- 0
uk_lat <- 55.3781
class(df$time[1])
filter <- (year(df$time) == "1969" )
select <- df[filter, c("latitude","longitude","comments","time","duration (seconds)")]
leaflet(select) %>%
setView(lng = uk_lon, lat = uk_lat, zoom = 5) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addMarkers(~longitude, ~latitude,
icon = ufo_icon,
clusterOptions = markerClusterOptions(
iconCreateFunction = JS("
function(cluster) {
return new L.DivIcon({
html: '<div style=\"background-color:rgba(77,77,77,0.5)\"><span>' + cluster.getChildCount() + '</div><span>',
className: 'marker-cluster'
});
}")
),
label = ~time,
popup = ~comments,
)  %>%
addMiniMap()
uk_lon <- 0
uk_lat <- 55.3781
class(df$time[1])
filter <- (year(df$time) == "1996" )
select <- df[filter, c("latitude","longitude","comments","time","duration (seconds)")]
leaflet(select) %>%
setView(lng = uk_lon, lat = uk_lat, zoom = 5) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addMarkers(~longitude, ~latitude,
icon = ufo_icon,
clusterOptions = markerClusterOptions(
iconCreateFunction = JS("
function(cluster) {
return new L.DivIcon({
html: '<div style=\"background-color:rgba(77,77,77,0.5)\"><span>' + cluster.getChildCount() + '</div><span>',
className: 'marker-cluster'
});
}")
),
label = ~time,
popup = ~comments,
)  %>%
addMiniMap()
